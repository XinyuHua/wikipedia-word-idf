<doc id="13681" url="https://en.wikipedia.org/wiki?curid=13681" title="Hyperinflation">
Hyperinflation

In economics, hyperinflation is very high and typically accelerating inflation. It quickly erodes the real value of the local currency, as the prices of all goods increase. This causes people to minimize their holdings in that currency as they usually switch to more stable foreign currencies, often the US Dollar. Prices typically remain stable in terms of other relatively stable currencies.

Unlike low inflation, where the process of rising prices is protracted and not generally noticeable except by studying past market prices, hyperinflation sees a rapid and continuing increase in nominal prices, the nominal cost of goods, and in the supply of money. Typically, however, the general price level rises even more rapidly than the money supply as people try ridding themselves of the devaluing currency as quickly as possible. As this happens, the real stock of money (i.e., the amount of circulating money divided by the price level) decreases considerably.

Hyperinflation is often associated with some stress to the government budget, such as wars or their aftermath, sociopolitical upheavals, a collapse in aggregate supply or one in export prices, or other crises that make it difficult for the government to collect tax revenue. A sharp decrease in real tax revenue coupled with a strong need to maintain government spending, together with an inability or unwillingness to borrow, can lead a country into hyperinflation.

In 1956, Phillip Cagan wrote "The Monetary Dynamics of Hyperinflation", the book often regarded as the first serious study of hyperinflation and its effects (though "The Economics of Inflation" by C. Bresciani-Turroni on the German hyperinflation was published in Italian in 1931). In his book, Cagan defined a hyperinflationary episode as starting in the month that the monthly inflation rate exceeds 50%, and as ending when the monthly inflation rate drops below 50% and stays that way for at least a year. Economists usually follow Cagan’s description that hyperinflation occurs when the monthly inflation rate exceeds 50%.

The International Accounting Standards Board has issued guidance on accounting rules in a hyperinflationary environment. It does not establish an absolute rule on when hyperinflation arises. Instead, it lists factors that indicate the existence of hyperinflation:

While there can be a number of causes of high inflation, most hyperinflations have been caused by government budget deficits financed by money creation. Peter Bernholz analysed 29 hyperinflations (following Cagan's definition) and concludes that at least 25 of them have been caused in this way. A necessary condition for hyperinflation is the use of paper money, instead of gold or silver coins. Most hyperinflations in history, with some exceptions, such as the French hyperinflation of 1789-1796, occurred after the use of fiat currency became widespread in the late 19th century. The French hyperinflation took place after the introduction of a non-convertible paper currency, the assignats.

Hyperinflation occurs when there is a continuing (and often accelerating) rapid increase in the amount of money that is not supported by a corresponding growth in the output of goods and services.

The increases in price that result from the rapid money creation creates a vicious circle, requiring ever growing amounts of new money creation to fund government deficits. Hence both monetary inflation and price inflation proceed at a rapid pace. Such rapidly increasing prices cause widespread unwillingness of the local population to hold the local currency as it rapidly loses its buying power. Instead they quickly spend any money they receive, which increases the velocity of money flow; this in turn causes further acceleration in prices. This means that the increase in the price level is greater than that of the money supply. The real stock of money, M/P, decreases. Here M refers to the money stock and P to the price level.

This results in an imbalance between the supply and demand for the money (including currency and bank deposits), causing rapid inflation. Very high inflation rates can result in a loss of confidence in the currency, similar to a bank run. Usually, the excessive money supply growth results from the government being either unable or unwilling to fully finance the government budget through taxation or borrowing, and instead it finances the government budget deficit through the printing of money.

Governments have sometimes resorted to excessively loose monetary policy, as it allows a government to devalue its debts and reduce (or avoid) a tax increase. Inflation is effectively a regressive tax on the users of money, but less overt than levied taxes and is therefore harder to understand by ordinary citizens. Inflation can obscure quantitative assessments of the true cost of living, as published price indices only look at data in retrospect, so may increase only months later. Monetary inflation can become hyperinflation if monetary authorities fail to fund increasing government expenses from taxes, government debt, cost cutting, or by other means, because either

Theories of hyperinflation generally look for a relationship between seigniorage and the inflation tax. In both Cagan's model and the neo-classical models, a tipping point occurs when the increase in money supply or the drop in the monetary base makes it impossible for a government to improve its financial position. Thus when fiat money is printed, government obligations that are not denominated in money increase in cost by more than the value of the money created.

From this, it might be wondered why any rational government would engage in actions that cause or continue hyperinflation. One reason for such actions is that often the alternative to hyperinflation is either depression or military defeat. The root cause is a matter of more dispute. In both classical economics and monetarism, it is always the result of the monetary authority irresponsibly borrowing money to pay all its expenses. These models focus on the unrestrained seigniorage of the monetary authority, and the gains from the inflation tax.

In neo-classical economic theory, hyperinflation is rooted in a deterioration of the monetary base, that is the confidence that there is a store of value that the currency will be able to command later. In this model, the perceived risk of holding currency rises dramatically, and sellers demand increasingly high premiums to accept the currency. This in turn leads to a greater fear that the currency will collapse, causing even higher premiums. One example of this is during periods of warfare, civil war, or intense internal conflict of other kinds: governments need to do whatever is necessary to continue fighting, since the alternative is defeat. Expenses cannot be cut significantly since the main outlay is armaments. Further, a civil war may make it difficult to raise taxes or to collect existing taxes. While in peacetime the deficit is financed by selling bonds, during a war it is typically difficult and expensive to borrow, especially if the war is going poorly for the government in question. The banking authorities, whether central or not, "monetize" the deficit, printing money to pay for the government's efforts to survive. The hyperinflation under the Chinese Nationalists from 1939 to 1945 is a classic example of a government printing money to pay civil war costs. By the end, currency was flown in over the Himalayas, and then old currency was flown out to be destroyed.

Hyperinflation is a complex phenomenon and one explanation may not be applicable to all cases. In both of these models, however, whether loss of confidence comes first, or central bank seigniorage, the other phase is ignited. In the case of rapid expansion of the money supply, prices rise rapidly in response to the increased supply of money relative to the supply of goods and services, and in the case of loss of confidence, the monetary authority responds to the risk premiums it has to pay by "running the printing presses."

Nevertheless, the immense acceleration process that occurs during hyperinflation (such as during the German hyperinflation of 1922/23) still remains unclear and unpredictable. The transformation of an inflationary development into the hyperinflation has to be identified as a very complex phenomenon, which could be a further advanced research avenue of the complexity economics in conjunction with research areas like mass hysteria, bandwagon effect, social brain, and mirror neurons.

A number of hyperinflations were caused by some sort of extreme negative supply shock, often but not always associated with wars, the breakdown of the communist system or natural disasters.

Since hyperinflation is visible as a monetary effect, models of hyperinflation center on the demand for money. Economists see both a rapid increase in the money supply and an increase in the velocity of money if the (monetary) inflating is not stopped. Either one, or both of these together are the root causes of inflation and hyperinflation. A dramatic increase in the velocity of money as the cause of hyperinflation is central to the "crisis of confidence" model of hyperinflation, where the risk premium that sellers demand for the paper currency over the nominal value grows rapidly. The second theory is that there is first a radical increase in the amount of circulating medium, which can be called the "monetary model" of hyperinflation. In either model, the second effect then follows from the first—either too little confidence forcing an increase in the money supply, or too much money destroying confidence.

In the "confidence model", some event, or series of events, such as defeats in battle, or a run on stocks of the specie that back a currency, removes the belief that the authority issuing the money will remain solvent—whether a bank or a government. Because people do not want to hold notes that may become valueless, they want to spend them. Sellers, realizing that there is a higher risk for the currency, demand a greater and greater premium over the original value. Under this model, the method of ending hyperinflation is to change the backing of the currency, often by issuing a completely new one. War is one commonly cited cause of crisis of confidence, particularly losing in a war, as occurred during Napoleonic Vienna, and capital flight, sometimes because of "contagion" is another. In this view, the increase in the circulating medium is the result of the government attempting to buy time without coming to terms with the root cause of the lack of confidence itself.

In the "monetary model", hyperinflation is a positive feedback cycle of rapid monetary expansion. It has the same cause as all other inflation: money-issuing bodies, central or otherwise, produce currency to pay spiraling costs, often from lax fiscal policy, or the mounting costs of warfare. When business people perceive that the issuer is committed to a policy of rapid currency expansion, they mark up prices to cover the expected decay in the currency's value. The issuer must then accelerate its expansion to cover these prices, which pushes the currency value down even faster than before. According to this model the issuer cannot "win" and the only solution is to abruptly stop expanding the currency. Unfortunately, the end of expansion can cause a severe financial shock to those using the currency as expectations are suddenly adjusted. This policy, combined with reductions of pensions, wages, and government outlays, formed part of the Washington consensus of the 1990s.

Whatever the cause, hyperinflation involves both the supply and velocity of money. Which comes first is a matter of debate, and there may be no universal story that applies to all cases. But once the hyperinflation is established, the pattern of increasing the money stock, by whichever agencies are allowed to do so, is universal. Because this practice increases the supply of currency without any matching increase in demand for it, the price of the currency, that is the exchange rate, naturally falls relative to other currencies. Inflation becomes hyperinflation when the increase in money supply turns specific areas of pricing power into a general frenzy of spending quickly before money becomes worthless. The purchasing power of the currency drops so rapidly that holding cash for even a day is an unacceptable loss of purchasing power. As a result, no one holds currency, which increases the velocity of money, and worsens the crisis.

Because rapidly rising prices undermine the role of money as a store of value, people try to spend it on real goods or services as quickly as possible. Thus, the monetary model predicts that the velocity of money will increase as a result of an excessive increase in the money supply. At the point when money velocity and prices rapidly accelerate in a vicious circle, hyperinflation is out of control, because ordinary policy mechanisms, such as increasing reserve requirements, raising interest rates, or cutting government spending will be ineffective and be responded to by shifting away from the rapidly devalued money and towards other means of exchange.

During a period of hyperinflation, bank runs, loans for 24-hour periods, switching to alternate currencies, the return to use of gold or silver or even barter become common. Many of the people who hoard gold today expect hyperinflation, and are hedging against it by holding specie. There may also be extensive capital flight or flight to a "hard" currency such as the US dollar. This is sometimes met with capital controls, an idea that has swung from standard, to anathema, and back into semi-respectability. All of this constitutes an economy that is operating in an "abnormal" way, which may lead to decreases in real production. If so, that intensifies the hyperinflation, since it means that the amount of goods in "too much money chasing too few goods" formulation is also reduced. This is also part of the vicious circle of hyperinflation.

Once the vicious circle of hyperinflation has been ignited, dramatic policy means are almost always required. Simply raising interest rates is insufficient. Bolivia, for example, underwent a period of hyperinflation in 1985, where prices increased 12,000% in the space of less than a year. The government raised the price of gasoline, which it had been selling at a huge loss to quiet popular discontent, and the hyperinflation came to a halt almost immediately, since it was able to bring in hard currency by selling its oil abroad. The crisis of confidence ended, and people returned deposits to banks. The German hyperinflation (1919–November 1923) was ended by producing a currency based on assets loaned against by banks, called the Rentenmark. Hyperinflation often ends when a civil conflict ends with one side winning.

Although wage and price controls are sometimes used to control or prevent inflation, no episode of hyperinflation has been ended by the use of price controls alone, because price controls that force merchants to sell at prices far below their restocking costs result in shortages that cause prices to rise still further.

Nobel prize winner Milton Friedman said "We economists don't know much, but we do know how to create a shortage. If you want to create a shortage of tomatoes, for example, just pass a law that retailers can't sell tomatoes for more than two cents per pound. Instantly you'll have a tomato shortage. It's the same with oil or gas."

Hyperinflation effectively wipes out the purchasing power of private and public savings; distorts the economy in favor of the hoarding of real assets; causes the monetary base, whether specie or hard currency, to flee the country; and makes the afflicted area anathema to investment.

One of the most important characteristics of hyperinflation is the accelerating substitution of the inflating money by stable money—gold and silver in former times, then relatively stable foreign currencies after the breakdown of the gold or silver standards (Thiers' Law). If inflation is high enough, government regulations like heavy penalties and fines, often combined with exchange controls, cannot prevent this currency substitution. As a consequence, the inflating currency is usually heavily undervalued compared to stable foreign money in terms of purchasing power parity. So foreigners can live cheaply and buy at low prices in the countries hit by high inflation. It follows that governments that do not succeed in engineering a successful currency reform in time must finally legalize the stable foreign currencies (or, formerly, gold and silver) that threaten to fully substitute the inflating money. Otherwise, their tax revenues, including the inflation tax, will approach zero. The last episode of hyperinflation in which this process could be observed was in Zimbabwe in the first decade of the 21st century. In this case, the local money was mainly driven out by the US dollar and the South African rand.

Enactment of price controls to prevent discounting the value of paper money relative to gold, silver, hard currency, or other commodities fail to force acceptance of a paper money that lacks intrinsic value. If the entity responsible for printing a currency promotes excessive money printing, with other factors contributing a reinforcing effect, hyperinflation usually continues. Hyperinflation is generally associated with paper money, which can easily be used to increase the money supply: add more zeros to the plates and print, or even stamp old notes with new numbers. Historically, there have been numerous episodes of hyperinflation in various countries followed by a return to "hard money". Older economies would revert to hard currency and barter when the circulating medium became excessively devalued, generally following a "run" on the store of value.

Much attention on hyperinflation centers on the effect on savers whose investments become worthless. Interest rate changes often cannot keep up with hyperinflation or even high inflation, certainly with contractually fixed interest rates. For example, in the 1970s in the United Kingdom inflation reached 25% per annum, yet interest rates did not rise above 15%—and then only briefly—and many fixed interest rate loans existed. Contractually, there is often no bar to a debtor clearing his long term debt with "hyperinflated cash", nor could a lender simply somehow suspend the loan. Contractual "early redemption penalties" were (and still are) often based on a penalty of "n" months of interest/payment; again no real bar to paying off what had been a large loan. In interwar Germany, for example, much private and corporate debt was effectively wiped out—certainly for those holding fixed interest rate loans.

Ludwig von Mises used the term "crack-up boom" (German: Katastrophenhausse) to describe the economic consequences of an unmitigated increasing in the base-money supply. As more and more money is provided, interest rates decline towards zero. Realizing that fiat money is losing value, investors will try to place money in assets such as real estate, stocks, even art; as these appear to represent "real" value. Asset prices are thus becoming inflated. This potentially spiraling process will ultimately lead to the collapse of the monetary system. The Cantillon effect says that those institutions that receive the new money first are the beneficiaries of the policy.

Hyperinflation is ended by drastic remedies, such as imposing the shock therapy of slashing government expenditures or altering the currency basis. One form this may take is dollarization, the use of a foreign currency (not necessarily the U.S. dollar) as a national unit of currency. An example was dollarization in Ecuador, initiated in September 2000 in response to a 75% loss of value of the Ecuadorian sucre in early 2000. But usually the "dollarization" takes place in spite of all efforts of the government to prevent it by exchange controls, heavy fines and penalties. The government has thus to try to engineer a successful currency reform stabilizing the value of the money. If it does not succeed with this reform the substitution of the inflating by stable money goes on. Thus it is not surprising that there have been at least seven historical cases in which the good (foreign) money did fully drive out the use of the inflating currency. In the end the government had to legalize the former, for otherwise its revenues would have fallen to zero.

Hyperinflation has always been a traumatic experience for the people who suffer it, and the next political regime almost always enacts policies to try to prevent its recurrence. Often this means making the central bank very aggressive about maintaining price stability, as was the case with the German Bundesbank, or moving to some hard basis of currency, such as a currency board. Many governments have enacted extremely stiff wage and price controls in the wake of hyperinflation, but this does not prevent further inflation of the money supply by the central bank, and always leads to widespread shortages of consumer goods if the controls are rigidly enforced.

In countries experiencing hyperinflation, the central bank often prints money in larger and larger denominations as the smaller denomination notes become worthless. This can result in the production of unusually large demoninations of banknotes, including those denominated in amounts of 1,000,000,000 or more.

One way to avoid the use of large numbers is by declaring a new unit of currency. (As an example, instead of 10,000,000,000 dollars, a central bank might set 1 new dollar = 1,000,000,000 old dollars, so the new note would read "10 new dollars".) A recent example of this is Turkey's revaluation of the Lira on 1 January 2005, when the old Turkish lira (TRL) was converted to the New Turkish lira (TRY) at a rate of 1,000,000 old to 1 new Turkish Lira. While this does not lessen the actual value of a currency, it is called redenomination or revaluation and also occasionally happens in countries with lower inflation rates. During hyperinflation, currency inflation happens so quickly that bills reach large numbers before revaluation.

Some banknotes were stamped to indicate changes of denomination, as it would have taken too long to print new notes. By the time new notes were printed, they would be obsolete (that is, they would be of too low a denomination to be useful).

Metallic coins were rapid casualties of hyperinflation, as the scrap value of metal enormously exceeded its face value. Massive amounts of coinage were melted down, usually illicitly, and exported for hard currency.

Governments will often try to disguise the true rate of inflation through a variety of techniques. None of these actions addresses the root causes of inflation; and if discovered, they tend to further undermine trust in the currency, causing further increases in inflation. Price controls will generally result in shortages and hoarding and extremely high demand for the controlled goods, causing disruptions of supply chains. Products available to consumers may diminish or disappear as businesses no longer find it economic to continue producing and/or distributing such goods at the legal prices, further exacerbating the shortages.

There are also issues with computerized money-handling systems. In Zimbabwe, during the hyperinflation of the Zimbabwe dollar, many automated teller machines and payment card machines struggled with arithmetic overflow errors as customers required many billions and trillions of dollars at one time.

During the Crisis of the Third Century, Rome underwent hyperinflation caused by years of coinage devaluation.

In 1922, inflation in Austria reached 1,426%, and from 1914 to January 1923, the consumer price index rose by a factor of 11,836, with the highest banknote in denominations of 500,000 Austrian krones. After World War I, essentially all State enterprises ran at a loss, and the number of state employees in the capital, Vienna, was greater than in the earlier monarchy, even though the new republic was nearly one-eighth of the size.

Observing the Austrian response to developing hyperinflation, which included the hoarding of food and the speculation in foreign currencies, Owen S. Phillpotts, the Commercial Secretary at the British Legation in Vienna wrote: "The Austrians are like men on a ship who cannot manage it, and are continually signalling for help. While waiting, however, most of them begin to cut rafts, each for himself, out of the sides and decks. The ship has not yet sunk despite the leaks so caused, and those who have acquired stores of wood in this way may use them to cook their food, while the more seamanlike look on cold and hungry. The population lack courage and energy as well as patriotism."


As the first user of fiat currency, China was also the first country to experience hyperinflation. Paper currency was introduced during the Tang Dynasty, and was generally welcomed. It maintained its value, as successive Chinese governments put in place strict controls on issuance. The convenience of paper currency for trade purposes led to strong demand for paper currency. It was only when discipline on quantity supplied broke down that hyperinflation emerged. The Yuan Dynasty (1271–1368) was the first to print large amounts of fiat paper money to fund their wars, resulting in hyperinflation. Much later, the Republic of China went through hyperinflation from 1948–49. In 1947, the highest denomination bill was 50,000 yuan. By mid-1948, the highest denomination was 180,000,000 yuan. The 1948 currency reform replaced the yuan by the gold yuan at an exchange rate of 1 gold yuan = 3,000,000 yuan. In less than a year, the highest denomination was 10,000,000 gold yuan. In the final days of the civil war, the Silver Yuan was briefly introduced at the rate of 500,000,000 Gold Yuan. Meanwhile, the highest denomination issued by a regional bank was 6,000,000,000 yuan (issued by Xinjiang Provincial Bank in 1949). After the renminbi was instituted by the new communist government, hyperinflation ceased, with a revaluation of 1:10,000 old Renminbi in 1955.


During the French Revolution and first Republic, the National Assembly issued bonds, some backed by seized church property, called assignats. Napoleon replaced them with the franc in 1803, at which time the assignats were basically worthless.
Stephen D. Dillaye pointed out that one of the reasons for the failure was massive counterfeiting of the paper currency, "the Assignats" – largely through London – where, according to Dillaye: "Seventeen manufacturing establishments were in full operation in London, with a force of four hundred men devoted to the production of false and forged Assignats." 


By November 1922, the value in gold of money in circulation had fallen from £300 million before World War I to £20 million. The Reichsbank responded by the unlimited printing of notes, thereby accelerating the devaluation of the mark. In his report to London, Lord D'Abernon wrote: "In the whole course of history, no dog has ever run after its own tail with the speed of the Reichsbank." Germany went through its worst inflation in 1923. In 1922, the highest denomination was 50,000 Marks. By 1923, the highest denomination was 100,000,000,000,000 (10) Marks. In December 1923 the exchange rate was 4,200,000,000,000 (4.2 × 10) Marks to 1 US dollar. In 1923, the rate of inflation hit 3.25 × 10 percent per month (prices double every two days). Beginning on 20 November 1923, 1,000,000,000,000 old Marks were exchanged for 1 Rentenmark, so that 4.2 Rentenmarks were worth 1 US dollar, exactly the same rate the Mark had in 1914.


With the German invasion in April 1941, there was an abrupt increase in prices. This was due to psychological factors related to the fear of shortages and to the hoarding of goods. During the German and Italian Axis occupation of Greece (1941-1944), the agricultural, mineral, industrial etc. production of Greece were used to sustain the occupation forces, but also to secure provisions for the Afrika Korps. One part of these "sales" of provisions was settled with bilateral clearing through the German DEGRIGES and the Italian Sagic companies at very low prices. As the value of Greek exports in drachmas fell, the demand for drachmas followed suit and so did its forex rate.
While shortages started due to naval blockades and hoarding, the prices of commodities soared. The other part of the "purchases" was settled with drachmas secured from the Bank of Greece and printed for this purpose by private printing presses. As prices soared, the Germans and Italians started requesting more and more drachmas from the Bank of Greece to offset price increases; each time prices increased, the note circulation followed suit soon afterwards. For the year November 1943 - November 1944, the inflation rate was 2.5 × 10%, the circulation was 6.28 × 10 drachmae and one gold sovereign cost 43,167 billion drachmas.
The hyperinflation started subsiding immediately after the departure of the German occupation forces, but inflation rates took several years before they fell below 50%.

The Treaty of Trianon and political instability between 1919 and 1924 led to a major inflation of Hungary's currency. In 1921, in an attempt to stop inflation, the national assembly of Hungary passed the Hegedűs reforms, including a 20% levy on bank deposits. This action precipitated a mistrust of banks by the public, especially the peasants, and resulted in a reduction in savings and in the amount of currency in circulation. Unable to tax adequately, the government resorted to printing money, and in 1923 inflation in Hungary reached 98% per month.

Between the end of 1945 and July 1946, Hungary went through the worst inflation ever recorded. In 1944, the highest denomination was 1,000 pengő. By the end of 1945, it was 10,000,000 pengő. The highest denomination in mid-1946 was 100,000,000,000,000,000,000 (10) pengő. A special currency, the adópengő – or tax pengő – was created for tax and postal payments. The value of the adópengő was adjusted each day, by radio announcement. On 1 January 1946 one adópengő equaled one pengő. By late July, one adópengő equaled 2,000,000,000,000,000,000,000 or 2×10 (2 sextillion) pengő. When the pengő was replaced in August 1946 by the forint, the total value of all Hungarian banknotes in circulation amounted to / of one US dollar. This is the most severe known incident of inflation recorded, peaking at 1.3 × 10 percent per month (prices double every 15 hours). The overall impact of hyperinflation: On 18 August 1946, 400,000,000,000,000,000,000,000,000,000 or 4 (four hundred quadrilliard on the long scale used in Hungary; four hundred octillion on short scale) pengő became 1 forint.


North Korea has most likely experienced hyperinflation from December 2009 to mid-January 2011. Based on the price of rice, North Korea's hyperinflation peaked in mid-January 2010, but according to black market exchange-rate data, and calculations based on purchasing power parity, North Korea experienced its peak month of inflation in early March 2010. These data are unofficial, however, and therefore must be treated with a degree of caution.

In modern history, Peru underwent a period of hyperinflation period in the 1980s to the early 1990s starting with President Fernando Belaúnde's second administration, heightened during Alan García's first administration, to the beginning of Alberto Fujimori's term. Over 3,210,000,000 old soles would be worth one USD. Garcia's term introduced the inti, which worsened inflation into hyperinflation. Peru's currency and economy were pacified under Fujimori's Nuevo Sol program, which remains Peru's currency.

Poland has gone through two episodes of hyperinflation since the country regained independence following the World War I, the first in 1923, the second in 1989-1990. Both events resulted in the introduction of new currencies. In 1924, the "złoty" replaced the original currency of post-war Poland, the mark. This currency was subsequently replaced by another of the same name in 1950, which was assigned the ISO code of PLZ. As a result of the second hyperinflation crisis, the current "new złoty" was introduced in 1990 (ISO code: PLN). See the article on Polish złoty for more information about the currency's history.

The newly independent Poland had been struggling with a large budget deficit since its inception in 1918 but it was in 1923 when inflation reached its peak. The exchange rate to the American dollar went from 9 Polish marks per dollar in 1918 to 6,375,000 marks per dollar at the end of 1923. A new personal 'inflation tax' was introduced. The resolution of the crisis is attributed to Władysław Grabski, who became prime minister of Poland in December 1923. Having nominated an all-new government and being granted extraordinary lawmaking powers by the Sejm for a period of six months, he introduced a new currency, established a new national bank and scrapped the inflation tax, which took place throughout 1924.

The economic crisis in Poland in the 1980s was accompanied by rising inflation when new money was printed to cover a budget deficit. Although inflation was not as acute as in 1920s, it is estimated that its annual rate reached around 600% in a period of over a year spanning parts of 1989 and 1990. The economy was stabilised by the adoption of the Balcerowicz Plan in 1989, named after the main author of the reforms, minister of finance Leszek Balcerowicz. The plan was largely inspired by the previous Grabski's reforms.

The Japanese government occupying the Philippines during World War II issued fiat currencies for general circulation. The Japanese-sponsored Second Philippine Republic government led by Jose P. Laurel at the same time outlawed possession of other currencies, most especially "guerrilla money." The fiat money's lack of value earned it the derisive nickname "Mickey Mouse money". Survivors of the war often tell tales of bringing suitcases or "bayong" (native bags made of woven coconut or buri leaf strips) overflowing with Japanese-issued bills. Early on, 75 Mickey Mouse pesos could buy one duck egg. In 1944, a box of matches cost more than 100 Mickey Mouse pesos.

In 1942, the highest denomination available was 10 pesos. Before the end of the war, because of inflation, the Japanese government was forced to issue 100-, 500-, and 1000-peso notes.


Malaya and Singapore were under Japanese occupation from 1942 until 1945. The Japanese issued banana money as the official currency to replace the Straits currency issued by the British. During that time, the cost of basic necessities increased drastically. As the occupation proceeded, the Japanese authorities printed more money to fund their wartime activities, which resulted in hyperinflation and a severe depreciation in value of the banana note.

From February to December 1942, $100 of Straits currency was worth $100 in Japanese scrip, after which the value of Japanese scrip began to erode, reaching $385 on December 1943 and $1,850 one year later. By 1 August 1945, this had inflated to $10,500, and 11 days later it had reached $95,000. After 13 August 1945, Japanese scrip had become valueless.

A seven-year period of uncontrollable spiralling inflation occurred in the early Soviet Union, running from the earliest days of the Bolshevik Revolution in November 1917 to the reestablishment of the gold standard with the introduction of the chervonets as part of the New Economic Policy. The inflationary crisis effectively ended in March 1924 with the introduction of the so-called "gold ruble" as the country's standard currency.

The early Soviet hyperinflationary period was marked by three successive redenominations of its currency, in which "new rubles" replaced old at the rates of 10,000-to-1 (1 January 1922), 100-to-1 (1 January 1923), and 50,000-to-1 (7 March 1924), respectively.

Between 1921 and 1922, inflation in the Soviet Union reached 213%.

Venezuela's hyperinflation began in November 2016. Inflation of Venezuela's bolivar fuerte (VEF) in 2014 reached 69% and was the highest in the world. In 2015, inflation was 181%, the highest in the world and the highest in the country's history at that time, 800% in 2016 over 4,000% in 2017, and 1,698,488% in 2018. with Venezuela spiraling into hyperinflation. While the Venezuelan government "has essentially stopped" producing official inflation estimates as of early 2018, one estimate of the rate at that time was 5,220%, according to inflation economist Steve Hanke of Johns Hopkins University.

Inflation has affected Venezuelans so much that in 2017, some people became video game gold farmers and could be seen playing games such as "RuneScape" to sell in-game currency or characters for real currency. In many cases, these gamers made more money than salaried workers in Venezuela even though they were earning just a few dollars per day. During the Christmas season of 2017, some shops would no longer use price tags since prices would inflate so quickly, so customers were required to ask staff at stores how much each item was.

The International Monetary Fund estimated in 2018 that Venezuela's inflation rate would reach 1,000,000% by the end of the year.. This forecast was criticized by Steve H. Hanke, professor of applied economics at The Johns Hopkins University and senior fellow at the Cato Institute. According to Hanke, the IMF had released a "bogus forecast" because "no one has ever been able to accurately forecast the course or the duration of an episode of hyperinflation. But that has not stopped the IMF from offering inflation forecasts for Venezuela that have proven to be wildly inaccurate". 

In July 2018, Hyperinflation in Venezuela was sitting 33,151%, "the 23rd most severe episode of hyperinflation in history" .

Some private companies predict that by the end of 2019, it is expected 31,000,000% of inflation rate.


Yugoslavia went through a period of hyperinflation and subsequent currency reforms from 1989–1994. One of several regional conflicts accompanying the dissolution of Yugoslavia was the Bosnian War (1992–1995). The Belgrade government of Slobodan Milošević backed ethnic Serbian forces in the conflict, resulting in a United Nations boycott of Yugoslavia. The UN boycott collapsed an economy already weakened by regional war, with the projected monthly inflation rate accelerating to one million percent by December 1993 (prices double every 2.3 days).

The highest denomination in 1988 was 50,000 dinars. By 1989 it was 2,000,000 dinars. In the 1990 currency reform, 1 new dinar was exchanged for 10,000 old dinars. In the 1992 currency reform, 1 new dinar was exchanged for 10 old dinars. The highest denomination in 1992 was 50,000 dinars. By 1993, it was 10,000,000,000 dinars. In the 1993 currency reform, 1 new dinar was exchanged for 1,000,000 old dinars. Before the year was over, however, the highest denomination was 500,000,000,000 dinars. In the 1994 currency reform, 1 new dinar was exchanged for 1,000,000,000 old dinars. In another currency reform a month later, 1 novi dinar was exchanged for 13 million dinars (1 novi dinar = 1 German mark at the time of exchange). The overall impact of hyperinflation was that 1 novi dinar was equal to 1 × 10~1.3 × 10 pre-1990 dinars. Yugoslavia's rate of inflation hit 5 × 10 percent cumulative inflation over the time period 1 October 1993 and 24 January 1994. 

Hyperinflation in Zimbabwe was one of the few instances that resulted in the abandonment of the local currency. At independence in 1980, the Zimbabwe dollar (ZWD) was worth about USD 1.25. Afterwards, however, rampant inflation and the collapse of the economy severely devalued the currency. Inflation was steady until British Prime Minister Tony Blair reneged on Land reform agreements arrived at between Margaret Thatcher and Robert Mugabe that continued land redistribution from the white farming community in 1998, resulting in reductions in food production and the decline of foreign investment. Several multinational companies began hoarding retail goods in warehouses in Zimbabwe and just south of the border, preventing commodities from becoming available on the market The result was that to pay its expenditures Mugabe's government and Gideon Gono's Reserve Bank printed more and more notes with higher face values.

Hyperinflation began early in the 21st century, reaching 624% in 2004. It fell back to low triple digits before surging to a new high of 1,730% in 2006. The Reserve Bank of Zimbabwe revalued on 1 August 2006 at a ratio of 1,000 ZWD to each second dollar (ZWN), but year-to-year inflation rose by June 2007 to 11,000% (versus an earlier estimate of 9,000%). Larger denominations were progressively issued in 2008:

Inflation by 16 July officially surged to 2,200,000% with some analysts estimating figures surpassing 9,000,000%. As of 22 July 2008 the value of the ZWN fell to approximately 688 billion per 1 USD, or 688 trillion pre-August 2006 Zimbabwean dollars.

On 1 August 2008, the Zimbabwe dollar was redenominated at the ratio of ZWN to each third dollar (ZWR). On 19 August 2008, official figures announced for June estimated the inflation over 11,250,000%. Zimbabwe's annual inflation was 231,000,000% in July (prices doubling every 17.3 days). By October 2008 Zimbabwe was mired in hyperinflation with wages falling far behind inflation. In this dysfunctional economy hospitals and schools had chronic staffing problems, because many nurses and teachers could not afford bus fare to work. Most of the capital of Harare was without water because the authorities had stopped paying the bills to buy and transport the treatment chemicals. Desperate for foreign currency to keep the government functioning, Zimbabwe's central bank governor, Gideon Gono, sent runners into the streets with suitcases of Zimbabwean dollars to buy up American dollars and South African rand.

For periods after July 2008, no official inflation statistics were released. Prof. Steve H. Hanke overcame the problem by estimating inflation rates after July 2008 and publishing the Hanke Hyperinflation Index for Zimbabwe. Prof. Hanke's HHIZ measure indicated that the inflation peaked at an annual rate of 89.7 sextillion percent (89,700,000,000,000,000,000,000%) in mid-November 2008. The peak monthly rate was 79.6 billion percent, which is equivalent to a 98% daily rate, or around percent yearly rate. At that rate, prices were doubling every 24.7 hours. Note that many of these figures should be considered mostly theoretical since hyperinflation did not proceed at this rate over a whole year.

At its November 2008 peak, Zimbabwe's rate of inflation approached, but failed to surpass, Hungary's July 1946 world record. On 2 February 2009, the dollar was redenominated for the third time at the ratio of ZWR to 1 ZWL, only three weeks after the $100 trillion banknote was issued on 16 January, but hyperinflation waned by then as official inflation rates in USD were announced and foreign transactions were legalised, and on 12 April the Zimbabwe dollar was abandoned in favour of using only foreign currencies. The overall impact of hyperinflation was 1 ZWL = ZWD.


Some countries experienced very high inflation, but did not reach hyperinflation, as defined as a "monthly" inflation rate of 50%.

Between 1620 and 1622 the Kreuzer fell from 1 Reichsthaler to 124 Kreuzer in end of 1619 to 1 Reichstaler to over 600 (regionally over 1000) Kreuzer in end of 1622, during the Thirty Years' War. This is a monthly inflation rate of over 20.6% (regionally over 34.4%).

Between 1987 and 1995 the Iraqi Dinar went from an official value of 0.306 Dinars/USD (or $3.26 USD per dinar, though the black market rate is thought to have been substantially lower) to 3,000 Dinars/USD due to government printing of 10s of trillions of dinars starting with a base of only 10s of billions. That equates to approximately 315% inflation per year averaged over that eight-year period.

In spite of increased oil prices in the late 1970s (Mexico is a producer and exporter), Mexico defaulted on its external debt in 1982. As a result, the country suffered a severe case of capital flight and several years of acute inflation and peso devaluation, leading to an accumulated inflation rate of almost 27,000% between December 1975 and late 1988. On 1 January 1993, Mexico created a new currency, the "nuevo peso" ("new peso", or MXN), which chopped three zeros off the old peso (One new peso was equal to 1,000 old MXP pesos).

In Roman Egypt, where the best documentation on pricing has survived, the price of a measure of wheat was 200 drachmae in 276 AD, and increased to more than 2,000,000 drachmae in 334 AD, roughly 1,000,000% inflation in a span of 58 years.

Although the price increased by a factor of 10,000 over 58 years, the annual rate of inflation was only 17.2% (1.4% monthly) compounded.

Romania experienced high inflation in the 1990s. The highest denomination in 1990 was 100 lei and in 1998 was 100,000 lei. By 2000 it was 500,000 lei. In early 2005 it was 1,000,000 lei. In July 2005 the lei was replaced by the new leu at 10,000 old lei = 1 new leu. Inflation in 2005 was 9%. In July 2005 the highest denomination became 500 lei (= 5,000,000 old lei).

The Second Transnistrian ruble consisted solely of banknotes and suffered from high inflation, necessitating the issue of notes overstamped with higher denominations. 1 and sometimes 10 ruble become 10,000 ruble, 5 ruble become 50,000 and 10 ruble become 100,000 ruble. In 2000, a new ruble was introduced at a rate of 1 new ruble = 1,000,000 old rubles.

Since the end of 2017 Turkey has high inflation rates. It is speculated that the new elections took place frustrated because of the impending crisis to forestall. In October 2017, inflation was at 11.9%, the highest rate since July 2008. The Turkish Lira fall from 1.503 TRY = 1 US-Dollar in 2010 to 5.5695 TRY = 1 US-Dollar in August 2018.

During the Revolutionary War, when the Continental Congress authorized the printing of paper called continental currency, the monthly inflation rate reached a peak of 47 percent in November 1779 (Bernholz 2003: 48). These notes depreciated rapidly, giving rise to the expression "not worth a continental."
One cause of the inflation was counterfeiting by the British, who ran a press on HMS "Phoenix", moored in New York Harbour. The counterfeits were advertised and sold almost for the price of the paper they were printed on.

A second close encounter occurred during the U.S. Civil War, between January 1861 and April 1865, the Lerner Commodity Price Index of leading cities in the eastern Confederacy states increased from 100 to over 9,000. As the Civil War dragged on, the Confederate dollar had less and less value, until it was almost worthless by the last few months of the war. Similarly, the Union government inflated its greenbacks, with the monthly rate peaking at 40 percent in March 1864 (Bernholz 2003: 107).

Vietnam went through a period of chaos and hyperinflation in the late 1980s, with inflation peaking at 774% in 1988, after the country's "price-wage-currency" reform package, led by then-Deputy Prime Minister Trần Phương, had failed. Hyperinflation also occurred in the early stages of the socialist-oriented market economic reforms commonly referred to as the Đổi Mới.

Inflation rate is usually measured in percent per year. It can also be measured in percent per month or in price doubling time.

formula_1

formula_2

formula_3

formula_4

Often, at redenominations, three zeroes are cut from the bills. It can be read from the table that if the (annual) inflation is for example 100%, it takes 3.32 years to produce one more zero on the price tags, or 3 × 3.32 = 9.96 years to produce three zeroes. Thus can one expect a redenomination to take place about 9.96 years after the currency was introduced.





</doc>
<doc id="13682" url="https://en.wikipedia.org/wiki?curid=13682" title="Herbert Hoover">
Herbert Hoover

Herbert Clark Hoover (August 10, 1874 – October 20, 1964) was an American engineer, businessman, and politician who served as the 31st president of the United States from 1929 to 1933. A member of the Republican Party, he held office during the onset of the Great Depression. Prior to serving as president, Hoover led the Commission for Relief in Belgium, served as the director of the U.S. Food Administration, and served as the 3rd U.S. Secretary of Commerce.

Born to a Quaker family in West Branch, Iowa, Hoover took a position with a London-based mining company after graduating from Stanford University in 1895. After the outbreak of World War I, he became the head of the Commission for Relief in Belgium, an international relief organization that provided food to occupied Belgium. When the U.S. entered the war, President Woodrow Wilson appointed Hoover to lead the Food Administration, and Hoover became known as the country's "food czar". After the war, Hoover led the American Relief Administration, which provided food to the inhabitants of Central Europe and Eastern Europe. Hoover's war-time service made him a favorite of many progressives, and he unsuccessfully sought the Republican nomination in the 1920 presidential election.

After the 1920 election, newly-elected Republican President Warren G. Harding appointed Hoover as Secretary of Commerce; Hoover continued to serve under President Calvin Coolidge after Harding died in 1923. Hoover was an unusually active and visible cabinet member, becoming known as "Secretary of Commerce and Under-Secretary of all other departments". He was influential in the development of radio and air travel and led the federal response to the Great Mississippi Flood of 1927. Hoover won the Republican nomination in the 1928 presidential election, and decisively defeated the Democratic candidate, Al Smith. The stock market crashed shortly after Hoover took office, and the Great Depression became the central issue of his presidency. Hoover pursued a variety of policies in an attempt to lift the economy, but opposed directly involving the federal government in relief efforts.

In the midst of an ongoing economic crisis, Hoover was decisively defeated by Democratic nominee Franklin D. Roosevelt in the 1932 presidential election. Hoover enjoyed one of the longest retirements of any former president, and he authored numerous works. After leaving office, Hoover became increasingly conservative, and he strongly criticized Roosevelt's foreign policy and New Deal domestic agenda. In the 1940s and 1950s, Hoover's public reputation was rehabilitated as he served for Presidents Harry S. Truman and Dwight D. Eisenhower in various assignments, including as chairman of the Hoover Commission. Nevertheless, Hoover is generally not ranked highly in historical rankings of presidents of the United States.

Herbert Hoover was born on August 10, 1874 in West Branch, Iowa. His father, Jesse Hoover, was a blacksmith and farm implement store owner of German, Swiss, and English ancestry. Hoover's mother, Hulda Randall Minthorn, was raised in Norwich, Ontario, Canada, before moving to Iowa in 1859. Like most other citizens of West Branch, Jesse and Hulda were Quakers. As a child, Hoover consistently attended schools, but he did little reading on his own aside from the Bible. Hoover's father, noted by the local paper for his "pleasant, sunshiny disposition", died in 1880 at the age of 34. Hoover's mother died in 1884, leaving Hoover, his older brother, Theodore, and his younger sister, May, as orphans.

In 1885, Hoover was sent to Newberg, Oregon to live with his uncle John Minthorn, a Quaker physician and businessman whose own son had died the year before. The Minthorn household was considered cultured and educational, and imparted a strong work ethic. Much like West Branch, Newberg was a frontier town settled largely by Midwestern Quakers. Minthorn ensured that Hoover received an education, but Hoover disliked the many chores assigned to him and often resented Minthorn. One observer described Hoover as "an orphan [who] seemed to be neglected in many ways." Hoover attended Friends Pacific Academy (now George Fox University), but dropped out at the age of thirteen to become an office assistant for his uncle's real estate office in Salem, Oregon. Though he did not attend high school, Hoover learned bookkeeping, typing, and mathematics at a night school.

Hoover entered Stanford University in 1891, its inaugural year, despite failing all the entrance exams except mathematics. During his freshman year, he switched his major from mechanical engineering to geology after working for John Casper Branner, the chair of Stanford's geology department. Hoover was a mediocre student, and he spent much of his time working in various part-time jobs or participating in campus activities. Though he was initially shy among fellow students, Hoover won election as student treasurer and became known for his distaste for fraternities and sororities. He served as student manager of both the baseball and football teams, and helped organize the inaugural Big Game versus the University of California. During the summers before and after his senior year, Hoover interned under economic geologist Waldemar Lindgren of the United States Geological Survey; these experience convinced Hoover to pursue a career as a mining geologist.

When Hoover graduated from Stanford in 1895, the country was in the midst of the Panic of 1893, and he initially struggled to find a job. He worked in various low-level mining jobs in the Sierra Nevada mountain range until he convinced prominent mining engineer Louis Janin to hire him. After working as a mine scout for a year, Hoover was hired by Bewick, Moreing & Co., a London-based company that operated gold mines in Western Australia. Hoover first went to Coolgardie, then the center of the Eastern Goldfields. Though Hoover received a $5,000 salary (), conditions were harsh in the goldfields. Hoover described the Coolgardie and Murchison rangelands on the edge of the Great Victoria Desert as a land of "black flies, red dust and white heat." 

Hoover traveled constantly across the Outback to evaluate and manage the company's mines. He convinced Bewick, Moreing to purchase the Sons of Gwalia gold mine, which proved to be one of the most successful mines in the region. Partly due to Hoover's efforts, the company eventually controlled approximately 50 percent of gold production in Western Australia. Hoover brought in many Italian immigrants to cut costs and counter the labour movement of the Australian miners. During his time with the mining company, Hoover became opposed to measures such as a minimum wage and workers' compensation, feeling that they were unfair to owners. Hoover's work impressed his employers, and in 1898 he was promoted to junior partner. An open feud developed between Hoover and his boss, Ernest Williams, but company leaders defused the situation by offering Hoover a compelling position in China.

Upon arriving in China, Hoover developed gold mines near Tianjin on behalf of Bewick, Moreing and the Chinese-owned Chinese Engineering and Mining Company. He became deeply interested in Chinese history, but quickly gave up on learning the language and viewed the Chinese people as racially inferior. He made recommendations to improve the lot of the Chinese worker, seeking to end the practice of imposing long-term servitude contracts and to institute reforms for workers based on merit. The xenophobic Boxer Rebellion broke out shortly after Hoover arrived in China, trapping the Hoovers and numerous other foreign nationals until a multi-national force defeated Boxer forces in the Battle of Tientsin. Fearing the imminent collapse of the Chinese government, the director of the Chinese Engineering and Mining Company agreed to establish a new Sino-British venture with Bewick, Moreing. After Hoover and Bewick, Moreing established effective control over the new Chinese mining company, Hoover became the operating partner of Bewick, Moreing in late 1901.

As operating partner, Hoover continually traveled the world on behalf of Bewick, Moreing, visiting mines operated by the company on different continents. Beginning in December 1902, the company faced mounting legal and financial issues after one of the partners admitted to having fraudulently sold stock in a mine. More issues arose in 1904, after the British government formed two separate royal commission to investigate Bewick, Moreing's labor practices and financial dealings in Western Australia. After the company lost a suit filed by the former director of the Chinese Engineering and Mining Company, Hoover began looking for a way to get out of the partnership, and he sold his shares in mid-1908.

After leaving Bewick, Moreing, Hoover worked as a London-based independent mining consultant and financier. Though he had risen to prominence as a geologist and mine operator, Hoover focused much of his attention on raising money, restructuring corporate organizations, and financing new ventures. He specialized in rejuvenating troubled mining operations, taking a share of the profits in exchange for his technical and financial expertise. Hoover thought of himself and his associates as "engineering doctors to sick concerns", and he earned a reputation as a "doctor of sick mines". He made investments on every continent and had offices in San Francisco, London, New York City, Paris, St. Petersburg (Florida), Florida, and Mandalay, Myanmar. By 1914, Hoover was a very wealthy man, with an estimated personal fortune of $4 million (equivalent to $ million in ).

He co-founded the Zinc Corporation to extract zinc near the Australian city of Broken Hill. The Zinc Corporation developed the froth flotation process to extract zinc from lead-silver ore, and operated the world's first selective or differential flotation plant. Hoover worked with the Burma Corporation, a British firm that produced silver, lead, and zinc in large quantities at the Namtu Bawdwin Mine. He also helped increase copper production in Kyshtym, Russia, through the use of pyritic smelting. He also agreed to manage a separate mine in the Altai Mountains that, according to Hoover, "developed probably the greatest and richest single body of ore known in the world."

In his spare time, Hoover wrote. His lectures at Columbia and Stanford universities were published in 1909 as "Principles of Mining", which became a standard textbook. The book reflects his move towards progressive ideals, as Hoover came to endorse eight-hour workdays and organized labor. Hoover became deeply interested in the history of science, and he was especially drawn to the "De re metallica", an influential 16th century work on mining and metallurgy. In 1912, Hoover and his wife published the first English translation of "De re metallica". Hoover also joined the board of trustees at Stanford, and led a successful campaign to appoint John Branner as the university's president.

During his senior year at Stanford, Hoover became smitten with a classmate named Lou Henry, though his financial situation precluded marriage at that time. The daughter of a banker from Monterey, California, Lou Henry decided to study geology at Stanford after attending a lecture delivered by John Branner. Immediately after earning a promotion in 1898, Hoover cabled Lou Henry, asking her to marry him. After she cabled back her acceptance of the proposal, Hoover briefly returned to the United States for their wedding. They would remain married until Lou Henry's death in 1944. Though his Quaker upbringing strongly influenced his career, Hoover rarely attended Quaker meetings during his adult life. Hoover and his wife had two children: Herbert Hoover Jr. (born in 1903) and Allan Henry Hoover (born in 1907). The Hoover family began living in London in 1902, though they frequently traveled as part of Hoover's career. After 1916, the Hoovers began living in the United States, maintaining homes in Palo Alto, California and Washington, D.C.

World War I broke out in June 1914, pitting the Allied Powers (France, Russia, Britain, and other countries) against the Central Powers (Germany, Austria-Hungary, and other countries). Hoover and other London-based American businessmen established a committee to organize the return of the roughly 100,000 Americans stranded in Europe. Hoover was appointed as the committee's chair and, with the assent of Congress and the executive branch, took charge of the distribution of relief to Americans in Europe. Hoover later stated, "I did not realize it at the moment, but on August 3, 1914, my career was over forever. I was on the slippery road of public life." By early October 1914, Hoover's organization had distributed relief to at least 40,000 Americans.

The German invasion of Belgium in August 1914 set off a food crisis in Belgium, which relied heavily on food imports. The Germans refused to take responsibility for feeding Belgian citizens in captured territory, and the British refused to lift their blockade of German-occupied Belgium unless the U.S. government supervised Belgian food imports as a neutral party in the war. With the cooperation of the Wilson administration and the CNSA, a Belgian relief organization, Hoover established the Commission for Relief in Belgium (CRB). The CRB obtained and imported millions of tons of foodstuffs for the CNSA to distribute, and helped ensure that the German army did not appropriate the food. Private donations and government grants supplied the majority of its $11-million-a-month budget, and the CRB became a veritable independent republic of relief, with its own flag, navy, factories, mills, and railroads. A British official described the CRB as a "piratical state organized for benevolence."

Hoover worked 14-hour days from London, administering the distribution of over two million tons of food to nine million war victims. In an early form of shuttle diplomacy, he crossed the North Sea forty times to meet with German authorities and persuade them to allow food shipments. He also convinced British Chancellor of the Exchequer David Lloyd George to allow individuals to send money to the people of Belgium, thereby lessening workload of the CRB. At the request of the French government, the CRB began delivering supplies to the people of Northern France in 1915. American diplomat Walter Page described Hoover as "probably the only man living who has privately (i.e., without holding office) negotiated understandings with the British, French, German, Dutch, and Belgian governments."

The United States declared war upon Germany in April 1917 after Germany engaged in unrestricted submarine warfare against American vessels in British waters. With the U.S. mobilizing for war, President Woodrow Wilson appointed Hoover to head the U.S. Food Administration, which was charged with ensuring the nation's food needs during the war. Hoover had hoped to join the administration in some capacity since at least 1916, and he obtained the position after lobbying several members of Congress and Wilson's confidant, Edward M. House. Earning the appellation of "food czar", Hoover recruited a volunteer force of hundreds of thousands of women and deployed propaganda in movie theaters, schools, and churches. He carefully selected men to assist in the agency leadership—Alonzo Taylor (technical abilities), Robert Taft (political associations), Gifford Pinchot (agricultural influence), and Julius Barnes (business acumen).

World War I had created a global food crisis that dramatically increased food prices and caused food riots and starvation in the countries at war. Hoover's chief goal as food czar was to provide supplies to the Allied Powers, but he also sought to stabilize domestic prices and to prevent domestic shortages. Under the broad powers granted by the Food and Fuel Control Act, the Food Administration supervised food production throughout the United States, and the administration made use of its authority to buy, import, store, and sell food. Determined to avoid rationing, Hoover established set days for people to avoid eating specified foods and save them for soldiers' rations: meatless Mondays, wheatless Wednesdays, and "when in doubt, eat potatoes". These policies were dubbed "Hooverizing" by government publicists, in spite of Hoover's continual orders that publicity should not mention him by name. The Food Administration shipped 23 million metric tons of food to the Allied Powers, preventing their collapse and earning Hoover great acclaim. As head of the Food Administration, Hoover gained a following in the United States, especially among progressives who saw in Hoover an expert administrator and symbol of efficiency.

World War I came to an end in November 1918, but Europe continued to face a critical food situation; Hoover estimated that as many as 400 million people faced the possibility of starvation. The United States Food Administration became the American Relief Administration (ARA), and Hoover was charged with providing food to Central and Eastern Europe. In addition to providing relief, the ARA rebuilt infrastructure in an effort to rejuvenate the economy of Europe. Throughout the Paris Peace Conference, Hoover served as a close adviser to President Wilson, and he largely shared Wilson's goals of establishing the League of Nations, settling borders on the basis of self-determination, and refraining from inflicting a harsh punishment on the defeated Central Powers. The following year, famed British economist John Maynard Keynes wrote in The Economic Consequences of the Peace that if Hoover's realism, "knowledge, magnanimity and disinterestedness" had found wider play in the councils of Paris, the world would have had "the Good Peace." After U.S. government funding for the ARA expired in mid-1919, Hoover transformed the ARA into a private organization, raising millions of dollars from private donors. He also established the European Children's Fund, which provided relief to fifteen million children across fourteen countries.

Despite the opposition of Senator Henry Cabot Lodge and other Republicans, Hoover provided aid to the defeated German nation after the war, as well as relief to famine-stricken Bolshevik-controlled areas of Russia. Hoover condemned Bolshevism, but warned President Wilson against an intervention in Russia, as he viewed the White Russian forces as little better than the Bolsheviks and feared the possibility of a protracted U.S. involvement. The Russian famine of 1921–22 claimed six million people, but the intervention of the ARA likely saved millions of lives. When asked if he was not helping Bolshevism by providing relief, Hoover stated, "twenty million people are starving. Whatever their politics, they shall be fed!" Reflecting the gratitude of many Europeans, in July 1922, Soviet author Maxim Gorky told Hoover that "your help will enter history as a unique, gigantic achievement, worthy of the greatest glory, which will long remain in the memory of millions of Russians whom you have saved from death."

In 1919, Hoover established the Hoover War Collection at Stanford University. He donated all the files of the Commission for Relief in Belgium, the U.S. Food Administration, and the American Relief Administration, and pledged $50,000 as an endowment (). Scholars were sent to Europe to collect pamphlets, society publications, government documents, newspapers, posters, proclamations, and other ephemeral materials related to the war and the revolutions that followed it. The collection was renamed the Hoover War Library in 1922 and is now known as the Hoover Institution. During the post-war period, Hoover also served as the president of the Federated American Engineering Societies.

Hoover had been little known among the American public before 1914, but his service in the Wilson administration established him as a contender in the 1920 presidential election. Hoover's wartime push for higher taxes, criticism of Attorney General A. Mitchell Palmer's actions during the First Red Scare, and his advocacy for measures such as the minimum wage, forty-eight-hour workweek, and elimination of child labor made him appealing to progressives of both parties. Despite his service in the Democratic administration of Woodrow Wilson, Hoover had never been closely affiliated with either the Democrats or the Republicans. He initially sought to avoid committing to any party in the 1920 election, hoping that either of the two major parties would draft him for president at their respective national convention. In March 1920, he changed his strategy and declared himself to be a Republican; he was motivated in large part by the belief that the Democratic candidate would have little chance of winning the 1920 presidential election. Despite his national renown, Hoover's service in the Wilson administration had alienated farmers and the conservative Old Guard of the GOP, and his presidential candidacy fizzled out after his defeat in the California primary by favorite son Hiram Johnson. At the 1920 Republican National Convention, Warren G. Harding emerged as a compromise candidate after the convention became deadlocked between supporters of Johnson, Leonard Wood, and Frank Orren Lowden. Hoover backed Harding's successful campaign in the general election, and he began laying the groundwork for a future presidential run by building up a base of strong supporters in the Republican Party.

After his election as president in 1920, Harding rewarded Hoover for his support, offering to appoint him as either Secretary of the Interior or Secretary of Commerce. Secretary of Commerce was considered a minor Cabinet post, with limited and vaguely defined responsibilities, but Hoover decided to accept the position. Hoover's progressive stances, continuing support for the League of Nations, and recent conversion to the Republican Party aroused opposition to his appointment from many Senate Republicans. To overcome this opposition, Harding paired Hoover's nomination with that of conservative favorite Andrew Mellon as Secretary of the Treasury, and the nominations of both Hoover and Mellon were confirmed by the Senate. Hoover would serve as Secretary of Commerce from 1921 to 1929, serving under Harding and, after Harding's death in 1923, President Calvin Coolidge. While some of the most prominent members of the Harding administration, including Attorney General Harry M. Daugherty and Secretary of Interior Albert B. Fall, were implicated in major scandals, Hoover emerged largely unscathed from investigations into the Harding administration.

Hoover envisioned the Commerce Department as the hub of the nation's growth and stability. His experience mobilizing the war-time economy convinced him that the federal government could promote efficiency by eliminating waste, increasing production, encouraging the adoption of data-based practices, investing in infrastructure, and conserving natural resources. Contemporaries described Hoover's approach as a "third alternative" between "unrestrained capitalism" and socialism, which was becoming increasingly popular in Europe. Hoover sought to foster a balance among labor, capital, and the government, and for this he has been variously labeled a corporatist or an associationalist.

Hoover demanded, and received, authority to coordinate economic affairs throughout the government. He created many sub-departments and committees, overseeing and regulating everything from manufacturing statistics to air travel. In some instances he "seized" control of responsibilities from other Cabinet departments when he deemed that they were not carrying out their responsibilities well; some began referring to him as the "Secretary of Commerce and Under-Secretary of all other departments." In response to the Depression of 1920–21, he convinced Harding to assemble a presidential commission on unemployment, which encouraged local governments to engage in countercyclical infrastructure spending. He endorsed much of Mellon's tax reduction program, but favored a more progressive tax system and opposed the treasury secretary's efforts to eliminate the estate tax.

Between 1923 and 1929, the number of families with radios grew from 300,000 to 10 million, and Hoover's tenure as Secretary of Commerce heavily influenced radio use in the United States. In the early and mid-1920s, Hoover's radio conferences played a key role in the organization, development, and regulation of radio broadcasting. Hoover also helped pass the Radio Act of 1927, which allowed the government to intervene and abolish radio stations that were deemed "non-useful" to the public. Hoover's attempts at regulating radio were not supported by all congressmen, and he received much opposition from the Senate and from radio station owners.

Hoover was also influential in the early development of air travel, and he sought to create a thriving private industry boosted by indirect government subsidies. He encouraged the development of emergency landing fields, required all runways to be equipped with lights and radio beams, and encouraged farmers to make use of planes for crop dusting. He also established the federal government's power to inspect planes and license pilots, setting a precedent for the later Federal Aviation Administration.

As Commerce Secretary, Hoover hosted national conferences on street traffic collectively known as the National Conference on Street and Highway Safety. Hoover's chief objective was to address the growing casualty toll of traffic accidents, but the scope of the conferences grew and soon embraced motor vehicle standards, rules of the road, and urban traffic control. He left the invited interest groups to negotiate agreements among themselves, which were then presented for adoption by states and localities. Because automotive trade associations were the best organized, many of the positions taken by the conferences reflected their interests. The conferences issued a model Uniform Vehicle Code for adoption by the states, and a Model Municipal Traffic Ordinance for adoption by cities. Both were widely influential, promoting greater uniformity between jurisdictions and tending to promote the automobile's priority in city streets.

With the goal of encouraging wise business investments, Hoover made the Commerce Department a clearinghouse of information. He recruited numerous academics from various fields and tasked them with publishing reports on different aspects of the economy, including steel production and films. To eliminate waste, he encouraged standardization of products like automobile tires and baby bottle nipples. Other efforts at eliminating waste included reducing labor losses from trade disputes and seasonal fluctuations, reducing industrial losses from accident and injury, and reducing the amount of crude oil spilled during extraction and shipping. He promoted international trade by opening overseas offices to advise businessmen. Hoover was especially eager to promote Hollywood films overseas. His "Own Your Own Home" campaign was a collaboration to promote ownership of single-family dwellings, with groups such as the Better Houses in America movement, the Architects' Small House Service Bureau, and the Home Modernizing Bureau. He worked with bankers and the savings and loan industry to promote the new long-term home mortgage, which dramatically stimulated home construction. Other accomplishments included winning the agreement of U.S. Steel to adopt an eight-hour workday, and the fostering of the Colorado River Compact, a water rights compact among Southwestern states.

The Great Mississippi Flood of 1927 broke the banks and levees of the lower Mississippi River in early 1927, resulting in the flooding of millions of acres and leaving 1.5 million people displaced from their homes. Although disaster response did not fall under the duties of the Commerce Department, the governors of six states along the Mississippi River specifically asked President Coolidge to appoint Hoover to coordinate the response to the flood. Believing that disaster response was not the domain of the federal government, Coolidge initially refused to become involved, but he eventually acceded to political pressure and appointed Hoover to chair a special committee to help the region. Hoover established over one hundred tent cities and a fleet of more than six hundred vessels, and raised $17 million (equivalent to $ million in ). In large part due to his leadership during the flood crisis, by 1928, Hoover had begun to overshadow President Coolidge himself. Though Hoover received wide acclaim for his role in the crisis, he ordered the suppression of reports of mistreatment of African Americans in refugee camps. He did so with the cooperation of African-American leader Robert Russa Moton, who was promised unprecedented influence once Hoover became president.

Hoover quietly built up support for a future presidential bid throughout the 1920s, but he carefully avoided alienating Coolidge, who was eligible to run for another term in the 1928 presidential election. Along with the rest of the nation, he was surprised when Coolidge announced in August 1927 that he would not seek another term. With the impending retirement of Coolidge, Hoover immediately emerged as the front-runner for the 1928 Republican nomination, and he quickly put together a strong campaign team led by Hubert Work, Will H. Hays, and Reed Smoot. Coolidge was unwilling to anoint Hoover as his successor; on one occasion he remarked that, "for six years that man has given me unsolicited advice—all of it bad." Despite his lukewarm feelings towards Hoover, Coolidge had no desire to split the party by publicly opposing the popular Commerce Secretary's candidacy. 

Many wary Republican leaders cast about for an alternative candidate, such as Treasury Secretary Andrew Mellon or former Secretary of State Charles Evans Hughes. However, Hughes and Mellon declined to run, and other potential contenders like Frank Orren Lowden and Vice President Charles G. Dawes failed to garner widespread support. Hoover won the presidential nomination on the first ballot of the 1928 Republican National Convention. Convention delegates considered re-nominating Vice President Charles Dawes to be Hoover's running mate, but Coolidge, who hated Dawes, remarked that this would be "a personal affront" to him. The convention instead selected Senator Charles Curtis of Kansas. Hoover accepted the nomination at Stanford Stadium, telling a huge crowd that he would continue the policies of the Harding and Coolidge administrations. The Democrats nominated New York governor Al Smith, who became the first Catholic major party nominee for president.

Hoover centered his campaign around the Republican record of peace and prosperity, as well as his own reputation as a successful engineer and public official. Averse to giving political speeches, Hoover largely stayed out of the fray and left the campaigning to Curtis and other Republicans. Smith was more charismatic and gregarious than Hoover, but his campaign was damaged by anti-Catholicism and his overt opposition to Prohibition. Hoover had never been a strong proponent of Prohibition, but he accepted the Republican Party's plank in favor of it and issued an ambivalent statement calling Prohibition "a great social and economic experiment, noble in motive and far-reaching in purpose." In the South, Hoover and the national party pursued a "lily-white" strategy, removing black Republicans from leadership positions in an attempt to curry favor with white Southerners.

Hoover maintained polling leads throughout the 1928 campaign, and he decisively defeated Smith on election day, taking 58 percent of the popular vote and 444 of the 531 electoral votes. Historians agree that Hoover's national reputation and the booming economy, combined with deep splits in the Democratic Party over religion and Prohibition, guaranteed his landslide victory. Hoover's appeal to Southern white voters succeeded in cracking the "Solid South", and he won five Southern states. Hoover's victory was positively received by newspapers; one wrote that Hoover would "drive so forcefully at the tasks now before the nation that the end of his eight years as president will find us looking back on an era of prodigious achievement."

Hoover's detractors wondered why he did not do anything to reapportion congress after the 1920 United States Census which saw an increase in urban and immigrant populations. The 1920 Census was the first and only Decennial Census where the results were not used to reapportion Congress, which ultimately influenced the 1928 Electoral College and impacted the Presidential Election.

Hoover saw the presidency as a vehicle for improving the conditions of all Americans by encouraging public-private cooperation—what he termed "volunteerism". He tended to oppose governmental coercion or intervention, as he thought they infringed on American ideals of individualism and self-reliance. The first major bill that he signed, the Agricultural Marketing Act of 1929, established the Federal Farm Board in order to stabilize farm prices. Hoover made extensive use of commissions to study issues and propose solutions, and many of those commissions were sponsored by private donors rather than by the government. One of the commissions started by Hoover, the Research Committee on Social Trends, was tasked with surveying the entirety of American society. He appointed a Cabinet consisting largely of wealthy, business-oriented conservatives, including Secretary of the Treasury Andrew Mellon. Lou Henry Hoover was an activist First Lady. She typified the new woman of the post-World War I era: intelligent, robust, and aware of multiple female possibilities.

On taking office, Hoover said that "given the chance to go forward with the policies of the last eight years, we shall soon with the help of God, be in sight of the day when poverty will be banished from this nation." Having seen the fruits of prosperity brought by technological progress, many shared Hoover's optimism, and the already bullish stock market climbed even higher on Hoover's accession. This optimism concealed several threats to sustained U.S. economic growth, including a persistent farm crisis, a saturation of consumer goods like automobiles, and growing income inequality. Most dangerous of all to the economy was excessive speculation that had raised stock prices far beyond their value. Some regulators and bankers had warned Coolidge and Hoover that a failure to curb speculation would lead to "one of the greatest financial catastrophes that this country has ever seen," but both presidents were reluctant to become involved with the workings of the Federal Reserve System, which regulated banks. 

In late October 1929, the Stock Market Crash of 1929 occurred, and the worldwide economy began to spiral downward into the Great Depression. The causes of the Great Depression remain a matter of debate, but Hoover viewed a lack of confidence in the financial system as the fundamental economic problem facing the nation. He sought to avoid direct federal intervention, believing that the best way to bolster the economy was through the strengthening of businesses such as banks and railroads. He also feared that allowing individuals on the "dole" would permanently weaken the country. Instead, Hoover strongly believed that local governments and private giving should address the needs of individuals.

Though he attempted to put a positive spin on Black Tuesday, Hoover moved quickly to address the stock market collapse. In the days following Black Tuesday, Hoover gathered business and labor leaders, asking them to avoid wage cuts and work stoppages while the country faced what he believed would be a short recession similar to the Depression of 1920–21. Hoover also convinced railroads and public utilities to increase spending on construction and maintenance, and the Federal Reserve announced that it would cut interest rates. In early 1930, Hoover acquired from Congress an additional $100 million to continue the Federal Farm Board lending and purchasing policies. These actions were collectively designed to prevent a cycle of deflation and provide a fiscal stimulus. At the same time, Hoover opposed congressional proposals to provide federal relief to the unemployed, as he believed that such programs were the responsibility of state and local governments and philanthropic organizations.

Hoover had taken office hoping to raise agricultural tariffs in order to help farmers reeling from the farm crisis of the 1920s, but his attempt to raise agricultural tariffs became connected with a bill that broadly raised tariffs. Hoover refused to become closely involved in the congressional debate over the tariff, and Congress produced a tariff bill that raised rates for many goods. Despite the widespread unpopularity of the bill, Hoover felt that he could not reject the main legislative accomplishment of the Republican-controlled 71st Congress. Over the objection of many economists, Hoover signed the Smoot–Hawley Tariff Act into law in June 1930. Canada, France, and other nations retaliated by raising tariffs, resulting in a contraction of international trade and a worsening of the economy. Progressive Republicans such as Senator William E. Borah were outraged when Hoover signed the tariff act, and Hoover's relations with that wing of the party never recovered.

By the end of 1930, the national unemployment rate had reached 11.9 percent, but it was not yet clear to most Americans that the economic downturn would be worse than the Depression of 1920–21. A series of bank failures in late 1930 heralded a larger collapse of the economy in 1931. While other countries left the gold standard, Hoover refused to abandon it; he derided any other monetary system as "collectivism." Hoover viewed the weak European economy as a major cause of economic troubles in the United States. In response to the collapse of the German economy, Hoover marshaled congressional support behind a one-year moratorium on European war debts. The Hoover Moratorium was warmly received in Europe and the United States, but Germany remained on the brink of defaulting on its loans. As the worldwide economy worsened, democratic governments fell; in Germany, Nazi Party leader Adolf Hitler assumed power.

By mid-1931, the unemployment rate had reached 15 percent, giving rise to growing fears that the country was experiencing a depression far worse than recent economic downturns. A reserved man with a fear of public speaking, Hoover allowed his opponents in the Democratic Party to define him as cold, incompetent, reactionary, and out-of-touch. Hoover's opponents developed defamatory epithets to discredit him such as: "Hooverville" (the shanty towns and homeless encampments), "Hoover leather" (cardboard used to cover holes in the soles of shoes), and "Hoover blanket" (old newspaper used to cover oneself from the cold). While Hoover continued to resist direct federal relief efforts, Governor Franklin D. Roosevelt of New York launched the Temporary Emergency Relief Administration to provide aid to the unemployed. Democrats positioned the program as a kinder alternative to Hoover's alleged apathy towards the unemployed.

The economy continued to worsen, with unemployment rates nearing 23 percent in early 1932, and Hoover finally heeded calls for more direct federal intervention. In January 1932, he convinced Congress to authorize the establishment of the Reconstruction Finance Corporation (RFC), which would provide government-secured loans to financial institutions, railroads, and local governments. The RFC saved numerous businesses from failure, but it failed to stimulate commercial lending as much as Hoover had hoped, partly because it was run by conservative bankers unwilling to make riskier loans. The same month the RFC was established, Hoover signed the Federal Home Loan Bank Act, establishing 12 district banks overseen by a Federal Home Loan Bank Board in a manner similar to the Federal Reserve System. He also helped arrange passage of the Glass–Steagall Act of 1932, emergency banking legislation designed to expand banking credit by expanding the collateral on which Federal Reserve banks were authorized to lend. As these measures failed to stem the economic crisis, Hoover signed the Emergency Relief and Construction Act, a $2 billion public works bill, in July 1932.

After a decade of budget surpluses, the federal government experienced a budget deficit 1931. Though some economists, like William Trufant Foster, favored deficit spending to address the Great Depression, most politicians and economists believed in the necessity of keeping a balanced budget. In late 1931, Hoover proposed a tax plan to increase tax revenue by 30 percent, resulting in the passage of the Revenue Act of 1932. The act increased taxes across the board, rolling back much of the tax cut reduction program Mellon had presided over during the 1920s. Top earners were taxed at 63 percent on their net income, the highest rate since the early 1920s. The act also doubled the top estate tax rate, cut personal income tax exemptions, eliminated the corporate income tax exemption, and raised corporate tax rates. Despite the passage of the Revenue Act, the federal government continued to run a budget deficit.

Hoover seldom mentioned civil rights while he was president. He believed that African Americans and other races could improve themselves with education and individual initiative. Hoover appointed more African Americans to federal positions than Harding and Coolidge had combined, but many African-American leaders condemned various aspects of the Hoover administration, including Hoover's unwillingness to push for a federal anti-lynching law. Hoover also continued to pursue the lily-white strategy, removing African Americans from positions of leadership in the Republican Party in an attempt to end the Democratic Party's dominance in the South. Though Robert Moton and some other black leaders accepted the lily-white strategy as a temporary measure, most African-American leaders were outraged. Hoover further alienated black leaders by nominating conservative Southern judge John J. Parker to the Supreme Court; Parker's nomination ultimately failed in the Senate due to opposition from the NAACP and organized labor. Many black voters switched to the Democratic Party in the 1932 election, and African Americans would later become an important part of Franklin Roosevelt's New Deal coalition.

As part of his efforts to limit unemployment, Hoover sought to cut immigration to the United States, and in 1930 he promulgated an executive order requiring individuals to have employment before migrating to the United States. With the goal of opening up more jobs for U.S. citizens, Secretary of Labor William N. Doak began a campaign to prosecute illegal immigrants in the United States. Though Doak did not seek to deport one specific group of immigrants, his campaign most strongly affected Mexican Americans, especially Mexican Americans living in Southern California. Many of the deportations were overseen by state and local authorities who acted on the encouragement of Doak the and Department of Labor. During the 1930s, approximately one million Mexican Americans were forcibly "repatriated" to Mexico; approximately sixty percent of those deported were birthright citizens. According to legal professor Kevin R. Johnson, the repatriation campaign meets the modern legal standards of ethnic cleansing, as it involved the forced removal of a racial minority by government actors.

On taking office, Hoover urged Americans to obey the Eighteenth Amendment and the Volstead Act, which had established Prohibition across the United States. To make public policy recommendations regarding Prohibition, he created the Wickersham Commission. Hoover had hoped that the commission's public report would buttress his stance in favor of Prohibition, but the report criticized the enforcement of the Volstead Act and noted the growing public opposition to Prohibition. After the Wickersham Report was published in 1931, Hoover rejected the advice of some of his closest allies and refused to endorse any revision of the Volstead Act or the Eighteenth Amendment, as he feared doing so would undermine his support among Prohibition advocates. As public opinion increasingly turned against Prohibition, more and more people flouted the law, and a grassroots movement began working in earnest for Prohibition's repeal. In January 1933, a constitutional amendment repealing the Eighteenth Amendment was approved by Congress and submitted to the states for ratification. By December 1933, it had been ratified by the requisite number of states to become the Twenty-first Amendment.

According to Leuchtenburg, Hoover was "the last American president to take office with no conspicuous need to pay attention to the rest of the world". Nevertheless, during Hoover's term, the world order established in the immediate aftermath of World War I began to crumble. As president, Hoover largely made good on his pledge made prior to assuming office not to interfere in Latin America's internal affairs. In 1930, he released the Clark Memorandum, a rejection of the Roosevelt Corollary and a move towards non-interventionism in Latin America. Hoover did not completely refrain from the use of the military in Latin American affairs; he thrice threatened intervention in the Dominican Republic, and he sent warships to El Salvador to support the government against a left-wing revolution. Notwithstanding those actions, he wound down the Banana Wars, ending the occupation of Nicaragua and nearly bringing an end to the occupation of Haiti.

Hoover placed a priority on disarmament, which he hoped would allow the United States to shift money from the military to domestic needs. Hoover and Secretary of State Henry L. Stimson focused on extending the 1922 Washington Naval Treaty, which sought to prevent a naval arms race. As a result of Hoover's efforts, the United States and other major naval powers signed the 1930 London Naval Treaty. The treaty represented the first time that the naval powers had agreed to cap their tonnage of auxiliary vessels, as previous agreements had only affected capital ships. At the 1932 World Disarmament Conference, Hoover urged further cutbacks in armaments and the outlawing of tanks and bombers, but his proposals were not adopted.

In 1931, Japan invaded Manchuria, defeating the Republic of China's military forces and establishing Manchukuo, a puppet state. The Hoover administration deplored the invasion, but also sought to avoid antagonizing the Japanese, fearing that taking too strong a stand would weaken the moderate forces in the Japanese government and alienate a potential ally against the Soviet Union, which he saw as a much greater threat. In response to the Japanese invasion, Hoover and Secretary of State Stimson outlined the Stimson Doctrine, which held that the United States would not recognize territories gained by force.

Thousands of World War I veterans and their families demonstrated and camped out in Washington, DC, during June 1932, calling for immediate payment of bonuses that had been promised by the World War Adjusted Compensation Act in 1924; the terms of the act called for payment of the bonuses in 1945. Although offered money by Congress to return home, some members of the "Bonus Army" remained. Washington police attempted to disperse the demonstrators, but they were outnumbered and unsuccessful. Shots were fired by the police in a futile attempt to attain order, and two protesters were killed while many officers were injured. Hoover sent U.S. Army forces led by General Douglas MacArthur to the protests. MacArthur, believing he was fighting a Communist revolution, chose to clear out the camp with military force. Though Hoover had not ordered MacArthur's clearing out of the protesters, he endorsed it after the fact. The incident proved embarrassing for the Hoover administration, and destroyed any remaining chance he had of winning re-election.

By mid-1931 few observers thought that Hoover had much hope of winning a second term in the midst of the ongoing economic crisis. Nonetheless, Hoover faced little opposition for re-nomination at the 1932 Republican National Convention, as Coolidge and other prominent Republicans all passed on the opportunity to challenge Hoover. Franklin D. Roosevelt won the presidential nomination on the fourth ballot of the 1932 Democratic National Convention, defeating the 1928 Democratic nominee, Al Smith. The Democrats attacked Hoover as the cause of the Great Depression, and for being indifferent to the suffering of millions. Historian Martin Fausold rejects the notion that the two nominees were similar ideologically, pointing to differences between the two on federal spending on public works, agricultural issues, Prohibition, and the tariff. As Governor of New York, Roosevelt had called on the New York legislature to provide aid for the needy, establishing Roosevelt's reputation for being more favorable toward government interventionism during the economic crisis. The Democratic Party, including Al Smith and other national leaders, coalesced behind Roosevelt, while progressive Republicans like George Norris and Robert La Follette Jr. deserted Hoover.

Hoover's detractors wondered why he did not do anything to reapportion congress after the 1920 United States Census which saw an increase in urban and immigrant populations. The 1920 Census was the first and only Decennial Census where the results were not used to reapportion Congress; which ultimately influenced the 1928 Electoral College and impacted the Presidential Election.

Hoover originally planned to make only one or two major speeches, and to leave the rest of the campaigning to proxies, as sitting presidents had traditionally done. However, encouraged by Republican pleas and outraged by Democratic claims, Hoover entered the public fray. In his nine major radio addresses Hoover primarily defended his administration and his philosophy of government, urging voters to hold to the "foundations of experience" and reject the notion that government interventionism could save the country from the Depression. In his campaign trips around the country, Hoover was faced with perhaps the most hostile crowds ever seen by a sitting president. Besides having his train and motorcades pelted with eggs and rotten fruit, he was often heckled while speaking, and on several occasions, the Secret Service halted attempts to kill Hoover by disgruntled citizens, including capturing one man nearing Hoover carrying sticks of dynamite, and another already having removed several spikes from the rails in front of the president's train.

Hoover's attempts to vindicate his administration fell on deaf ears, as much of the public blamed his administration for the depression. In the electoral vote, Hoover lost 59–472, carrying six states. Hoover won just 39.7 percent of the popular vote, a reduction of 26 percentage points from his result in the 1928 election. Roosevelt's performance in the popular vote made him first Democratic presidential nominee to win a majority of the popular vote since the Civil War.

Hoover departed from Washington in March 1933, bitter at his election loss and continuing unpopularity. As Coolidge, Harding, Wilson, and Taft had all died during the 1920s or early 1930s, Hoover was the sole living ex-president from 1933 to 1953. Hoover and his wife lived in Palo Alto until her death in 1944, at which point Hoover began to live permanently at the Waldorf Astoria hotel in New York City. During the 1930s, Hoover increasingly self-identified as a conservative. He closely followed national events after leaving public office, becoming a constant critic of Franklin Roosevelt. In response to continued attacks on his character and presidency, Hoover wrote more than two dozen books, including "The Challenge to Liberty" (1934), which harshly criticized Roosevelt's New Deal. Hoover described the New Deal's National Recovery Administration and Agricultural Adjustment Administration as "fascistic", and he called the 1933 Banking Act a "move to gigantic socialism."

Only 58 when he left office, Hoover held out hope for another term as president throughout the 1930s. At the 1936 Republican National Convention, Hoover's speech attacking the New Deal was well received, but the nomination went to Kansas Governor Alf Landon. In the general election, Hoover delivered numerous well-publicized speeches on behalf of Landon, but Landon was defeated by Roosevelt. Though Hoover was eager to oppose Roosevelt at every turn, Senator Arthur Vandenberg and other Republicans urged the still-unpopular Hoover to remain out of the fray during the debate over Roosevelt's proposed Judiciary Reorganization Bill of 1937. At the 1940 Republican National Convention, Hoover again hoped for the presidential nomination, but it went to the internationalist Wendell Willkie, who lost to Roosevelt in the general election.

During a 1938 trip to Europe, Hoover met with Adolf Hitler and stayed at Hermann Göring's hunting lodge. He expressed dismay at the persecution of Jews in Germany and believed that Hitler was mad, but did not present a threat to the U.S. Instead, Hoover believed that Roosevelt posed the biggest threat to peace, holding that Roosevelt's policies provoked Japan and discouraged France and the United Kingdom from reaching an "accommodation" with Germany. After the September 1939 invasion of Poland by Germany, Hoover opposed U.S. involvement in World War II, including the Lend-Lease policy. He rejected Roosevelt's offers to help coordinate relief in Europe, but, with the help of old friends from the CRB, helped establish the Commission for Polish Relief.

During a radio broadcast on June 29, 1941, one week after the Nazi invasion of the Soviet Union, Hoover disparaged any "tacit alliance" between the U.S. and the USSR, stating, "if we join the war and Stalin wins, we have aided him to impose more communism on Europe and the world... War alongside Stalin to impose freedom is more than a travesty. It is a tragedy." Much to his own frustration, Hoover was not called upon to serve after the United States entered World War II due to his differences with Roosevelt and his continuing unpopularity. He did not pursue the presidential nomination at the 1944 Republican National Convention, and, at the request of Republican nominee Thomas E. Dewey, refrained from campaigning during the general election.

Following World War II, Hoover befriended President Harry S. Truman despite their ideological differences. Because of Hoover's experience with Germany at the end of World War I, in 1946 President Truman selected the former president to tour Germany to ascertain the food needs of the occupied nation. After touring Germany, Hoover produced a number of reports critical of U.S. occupation policy. He stated in one report that "there is the illusion that the New Germany left after the annexations can be reduced to a 'pastoral state.' It cannot be done unless we exterminate or move 25,000,000 people out of it." On Hoover's initiative, a school meals program in the American and British occupation zones of Germany was begun on April 14, 1947; the program served 3,500,000 children.

In 1947, Truman appointed Hoover to a commission to reorganize the executive departments; the commission elected Hoover as chairman and became known as the Hoover Commission. The commission recommended changes designed to strengthen the president's ability to manage the federal government. Though Hoover had opposed Roosevelt's concentration of power in the 1930s, he believed that a stronger presidency was required with the advent of the Atomic Age. During the 1948 presidential election, Hoover supported Republican nominee Thomas Dewey's unsuccessful campaign against Truman, but he remained on good terms with Truman. Hoover favored the United Nations in principle, but he opposed granting membership to the Soviet Union and other Communist states. He viewed the Soviet Union to be as morally repugnant as Nazi Germany and supported the efforts of Richard Nixon and others to expose Communists in the United States.

Hoover backed conservative leader Robert A. Taft at the 1952 Republican National Convention, but the party's presidential nomination instead went to Dwight D. Eisenhower, who went on to win the 1952 election. Though Eisenhower appointed Hoover to another presidential commission, Hoover disliked Eisenhower, faulting the latter's failure to roll back the New Deal. Hoover's public work helped to rehabilitate his reputation, as did his use of self-deprecating humor; he occasionally remarked that "I am the only person of distinction who's ever had a depression named after him." In 1958, Congress passed the Former Presidents Act, offering a $25,000 yearly pension () to each former president. Hoover took the pension even though he did not need the money, possibly to avoid embarrassing Truman, whose precarious financial status played a role in the law's enactment. In the early 1960s, President John F. Kennedy offered Hoover various positions; Hoover declined the offers but defended Kennedy after the Bay of Pigs invasion and was personally distraught by Kennedy's assassination in 1963.

Hoover wrote several books during his retirement, including "The Ordeal of Woodrow Wilson", in which he strongly defended Wilson's actions at the Paris Peace Conference. In 1944, he began working on "Freedom Betrayed", which he often referred to as his "magnum opus." In "Freedom Betrayed", Hoover strongly critiques Roosevelt's foreign policy, especially Roosevelt's decision to recognize the Soviet Union in order to provide aid to that country during World War II. The book was published in 2012 after being edited by historian George H. Nash.

Hoover faced three major illnesses during the last two years of his life, including an August 1962 operation in which a growth on his large intestine was removed. He died on October 20, 1964 in New York City following massive internal bleeding. Two months earlier, he had become only the second U.S. president to reach age 90, joining John Adams. When asked how he felt on reaching the milestone, Hoover replied, "Too old." (Four subsequent presidents have outlived them both: Jimmy Carter, Gerald Ford, Ronald Reagan and George H. W. Bush.) At the time of his death Hoover had been out of office for over 31 years ( days altogether). This was the longest retirement in presidential history until Jimmy Carter broke that record in September 2012.

Hoover was honored with a state funeral in which he lay in state in the United States Capitol rotunda. Then, on October 25, he was buried in West Branch, Iowa, near his presidential library and birthplace on the grounds of the Herbert Hoover National Historic Site. Afterward, Hoover's wife, Lou Henry, who had been buried in Palo Alto, California, following her death in 1944, was re-interred beside him.

Hoover was extremely unpopular when he left office after the 1932 election, and his historical reputation would not begin to recover until the 1970s. According to Professor David E. Hamilton, historians have credited Hoover for his genuine belief in voluntarism and cooperation, as well as the innovation of some of his programs. However, Hamilton also notes that Hoover was politically inept and failed to recognize the severity of the Great Depression. Nicholas Lemann writes that Hoover has been remembered "as the man who was too rigidly conservative to react adeptly to the Depression, as the hapless foil to the great Franklin Roosevelt, and as the politician who managed to turn a Republican country into a Democratic one." Polls of historians and political scientists have generally ranked Hoover in the bottom third of presidents. A 2018 poll of the American Political Science Association’s Presidents and Executive Politics section ranked Hoover as the 36th best president. A 2017 C-Span poll of historians also ranked Hoover as the 36th best president.

Although Hoover is generally regarded as having had a failed presidency, he has also received praise for his actions as a humanitarian and public official. Biographer Glen Jeansonne writes that Hoover was "one of the most extraordinary Americans of modern times," adding that Hoover "led a life that was a prototypical Horatio Alger story, except that Horatio Alger stories stop at the pinnacle of success." Biographer Kenneth Whyte writes that, "the question of where Hoover belongs in the American political tradition remains a loaded one to this day. While he clearly played important roles in the development of both the progressive and conservative traditions, neither side will embrace him for fear of contamination with the other."

The Herbert Hoover Presidential Library and Museum is located in West Branch, Iowa next to the Herbert Hoover National Historic Site. The library is one of thirteen presidential libraries run by the National Archives and Records Administration. The Hoover–Minthorn House, where Hoover lived from 1885 to 1891, is located in Newberg, Oregon. His Rapidan fishing camp in Virginia, which he donated to the government in 1933, is now a National Historic Landmark within the Shenandoah National Park. The Lou Henry and Herbert Hoover House, built in 1919 in Stanford, California, is now the official residence of the president of Stanford University, and a National Historic Landmark. Also located at Stanford is the Hoover Institution, a think tank and research institution started by Hoover. 

Hoover has been memorialized in the names of several things, including the Hoover Dam on the Colorado River and numerous elementary, middle, and high schools across the United States. Two minor planets, 932 Hooveria and 1363 Herberta, are named in his honor. The Polish capital of Warsaw has a square named after Hoover, and the historic townsite of Gwalia, Western Australia contains the Hoover House Bed and Breakfast, where Hoover resided while managing and visiting the mine during the first decade of the twentieth century. A medicine ball game known as Hooverball is named for Hoover; it was invented by White House physician Admiral Joel T. Boone to help Hoover keep fit while serving as president.





</doc>
<doc id="13684" url="https://en.wikipedia.org/wiki?curid=13684" title="Hildegard of Bingen">
Hildegard of Bingen

Hildegard of Bingen (; ; 1098 – 17 September 1179), also known as Saint Hildegard and Sibyl of the Rhine, was a German Benedictine abbess, writer, composer, philosopher, Christian mystic, visionary, and polymath. She is considered to be the founder of scientific natural history in Germany.

Hildegard was elected "magistra" by her fellow nuns in 1136; she founded the monasteries of Rupertsberg in 1150 and Eibingen in 1165. One of her works as a composer, the "Ordo Virtutum", is an early example of liturgical drama and arguably the oldest surviving morality play. She wrote theological, botanical, and medicinal texts, as well as letters, liturgical songs, and poems, while supervising miniature illuminations in the Rupertsberg manuscript of her first work, "Scivias". She is also noted for the invention of a constructed language known as "Lingua Ignota".

Although the history of her formal consideration is complicated, she has been recognized as a saint by branches of the Roman Catholic Church for centuries. On 7 October 2012, Pope Benedict XVI named her a Doctor of the Church.

Hildegard was born around the year 1098, although the exact date is uncertain. Her parents were Mechtild of Merxheim-Nahet and Hildebert of Bermersheim, a family of the free lower nobility in the service of the Count Meginhard of Sponheim. Sickly from birth, Hildegard is traditionally considered their youngest and tenth child, although there are records of only seven older siblings. In her "Vita", Hildegard states that from a very young age she had experienced visions.

Perhaps because of Hildegard's visions, or as a method of political positioning (or both), Hildegard's parents offered her as an oblate to the Benedictine monastery at the Disibodenberg, which had been recently reformed in the Palatinate Forest. The date of Hildegard's enclosure at the monastery is the subject of debate. Her "Vita" says she was professed with an older woman, Jutta, the daughter of Count Stephan II of Sponheim, at the age of eight. However, Jutta's date of enclosure is known to have been in 1112, when Hildegard would have been fourteen. Their vows were received by Bishop Otto Bamberg on All Saints' Day, 1112. Some scholars speculate that Hildegard was placed in the care of Jutta at the age of eight, and the two women were then enclosed together six years later.

In any case, Hildegard and Jutta were enclosed together at the Disibodenberg, and formed the core of a growing community of women attached to the male monastery. Jutta was also a visionary and thus attracted many followers who came to visit her at the cloister. Hildegard tells us that Jutta taught her to read and write, but that she was unlearned and therefore incapable of teaching Hildegard sound biblical interpretation. The written record of the "Life of Jutta" indicates that Hildegard probably assisted her in reciting the psalms, working in the garden and other handiwork, and tending to the sick. This might have been a time when Hildegard learned how to play the ten-stringed psaltery. Volmar, a frequent visitor, may have taught Hildegard simple psalm notation. The time she studied music could have been the beginning of the compositions she would later create.

Upon Jutta's death in 1136, Hildegard was unanimously elected as "magistra" of the community by her fellow nuns. Abbot Kuno of Disibodenberg asked Hildegard to be Prioress, which would be under his authority. Hildegard, however, wanted more independence for herself and her nuns, and asked Abbot Kuno to allow them to move to Rupertsberg. This was to be a move towards poverty, from a stone complex that was well established to a temporary dwelling place. When the abbot declined Hildegard's proposition, Hildegard went over his head and received the approval of Archbishop Henry I of Mainz. Abbot Kuno did not relent until Hildegard was stricken by an illness that kept her paralyzed and unable to move from her bed, an event that she attributed to God's unhappiness at her not following his orders to move her nuns to a new location in Rupertsberg. It was only when the Abbot himself could not move Hildegard that he decided to grant the nuns their own monastery. Hildegard and about twenty nuns thus moved to the St. Rupertsberg monastery in 1150, where Volmar served as provost, as well as Hildegard's confessor and scribe. In 1165 Hildegard founded a second monastery for her nuns at Eibingen.

Before Hildegard's death, a problem arose with the clergy of Mainz. A man buried in Rupertsburg had died after excommunication from the Church. Therefore, the clergy wanted to remove his body from the sacred ground. Hildegard did not accept this idea, replying that it was a sin and that the man had been reconciled to the church at the time of his death.

Hildegard said that she first saw "The Shade of the Living Light" at the age of three, and by the age of five she began to understand that she was experiencing visions. She used the term 'visio' (the Latin for "vision") to describe this feature of her experience, and recognized that it was a gift that she could not explain to others. Hildegard explained that she saw all things in the light of God through the five senses: sight, hearing, taste, smell, and touch. Hildegard was hesitant to share her visions, confiding only to Jutta, who in turn told Volmar, Hildegard's tutor and, later, secretary. Throughout her life, she continued to have many visions, and in 1141, at the age of 42, Hildegard received a vision she believed to be an instruction from God, to "write down that which you see and hear." Still hesitant to record her visions, Hildegard became physically ill. The illustrations recorded in the book of Scivias were visions that Hildegard experienced, causing her great suffering and tribulations. In her first theological text, "Scivias" ("Know the Ways"), Hildegard describes her struggle within:

But I, though I saw and heard these things, refused to write for a long time through doubt and bad opinion and the diversity of human words, not with stubbornness but in the exercise of humility, until, laid low by the scourge of God, I fell upon a bed of sickness; then, compelled at last by many illnesses, and by the witness of a certain noble maiden of good conduct [the nun Richardis von Stade] and of that man whom I had secretly sought and found, as mentioned above, I set my hand to the writing. While I was doing it, I sensed, as I mentioned before, the deep profundity of scriptural exposition; and, raising myself from illness by the strength I received, I brought this work to a close – though just barely – in ten years. (...) And I spoke and wrote these things not by the invention of my heart or that of any other person, but as by the secret mysteries of God I heard and received them in the heavenly places. And again I heard a voice from Heaven saying to me, 'Cry out therefore, and write thus!'

It was between November 1147 and February 1148 at the synod in Trier that Pope Eugenius heard about Hildegard's writings. It was from this that she received Papal approval to document her visions as revelations from the Holy Spirit giving her instant credence.

On 17 September 1179, when Hildegard died, her sisters claimed they saw two streams of light appear in the skies and cross over the room where she was dying.

Hildegard's hagiography, "Vita Sanctae Hildegardis", was compiled by the monk Theoderic of Echternach after Hildegard's death. He included the hagiographical work "Libellus" or "Little Book" begun by Godfrey of Disibodenberg. Godfrey had died before he was able to complete his work. Guibert of Gembloux was invited to finish the work; however, he had to return to his monastery with the project unfinished. Theoderic utilized sources Guibert had left behind to complete the "Vita".

Hildegard's works include three great volumes of visionary theology; a variety of musical compositions for use in liturgy, as well as the musical morality play "Ordo Virtutum"; one of the largest bodies of letters (nearly 400) to survive from the Middle Ages, addressed to correspondents ranging from popes to emperors to abbots and abbesses, and including records of many of the sermons she preached in the 1160s and 1170s; two volumes of material on natural medicine and cures; an invented language called the "Lingua ignota" ("unknown language"); and various minor works, including a gospel commentary and two works of hagiography.

Several manuscripts of her works were produced during her lifetime, including the illustrated Rupertsberg manuscript of her first major work, "Scivias" (lost since 1945); the Dendermonde Codex, which contains one version of her musical works; and the Ghent manuscript, which was the first fair-copy made for editing of her final theological work, the "Liber Divinorum Operum". At the end of her life, and probably under her initial guidance, all of her works were edited and gathered into the single Riesenkodex manuscript.

Hildegard's most significant works were her three volumes of visionary theology: "Scivias" ("Know the Ways", composed 1142–1151), "Liber Vitae Meritorum" ("Book of Life's Merits" or "Book of the Rewards of Life", composed 1158–1163); and "Liber Divinorum Operum" ("Book of Divine Works", also known as "De operatione Dei", "On God's Activity", composed 1163/4–1172 or 1174). In these volumes, the last of which was completed when she was well into her seventies, Hildegard first describes each vision, whose details are often strange and enigmatic, and then interprets their theological contents in the words of the "voice of the Living Light."

With permission from Abbot Kuno of Disibodenberg, she began journaling visions she had (which is the basis for "Scivias"). "Scivias" is a contraction of "Sci vias Domini" ("Know the Ways of the Lord"), and it was Hildegard's first major visionary work, and one of the biggest milestones in her life. Perceiving a divine command to "write down what you see and hear", Hildegard began to record and interpret her visionary experiences - 26 in total visionary experiences were captured in this compilation.

"Scivias" is structured into three parts of unequal length. The first part (six visions) chronicles the order of God's creation: the Creation and Fall of Adam and Eve, the structure of the universe (famously described as the shape of an "egg"), the relationship between body and soul, God's relationship to his people through the Synagogue, and the choirs of angels. The second part (seven visions) describes the order of redemption: the coming of Christ the Redeemer, the Trinity, the Church as the Bride of Christ and the Mother of the Faithful in baptism and confirmation, the orders of the Church, Christ's sacrifice on the Cross and the Eucharist, and the fight against the devil. Finally, the third part (thirteen visions) recapitulates the history of salvation told in the first two parts, symbolized as a building adorned with various allegorical figures and virtues. It concludes with the Symphony of Heaven, an early version of Hildegard's musical compositions.

In early 1148, a commission was sent by the Pope to Disibodenberg to find out more about Hildegard and her writings. The commission found that the visions were authentic and returned to the Pope, with a portion of the "Scivias". Portions of the uncompleted work were read aloud to Pope Eugenius III at the Synod of Trier in 1148, after which he sent Hildegard a letter with his blessing. This blessing was later construed as papal approval for all of Hildegard's wide-ranging theological activities. Towards the end of her life, Hildegard commissioned a richly decorated manuscript of "Scivias" (the Rupertsberg Codex); although the original has been lost since its evacuation to Dresden for safekeeping in 1945, its images are preserved in a hand-painted facsimile from the 1920s.

In her second volume of visionary theology, composed between 1158 and 1163, after she had moved her community of nuns into independence at the Rupertsberg in Bingen, Hildegard tackled the moral life in the form of dramatic confrontations between the virtues and the vices. She had already explored this area in her musical morality play, "Ordo Virtutum", and the "Book of the Rewards of Life" takes up that play's characteristic themes. Each vice, although ultimately depicted as ugly and grotesque, nevertheless offers alluring, seductive speeches that attempt to entice the unwary soul into their clutches. Standing in our defense, however, are the sober voices of the Virtues, powerfully confronting every vicious deception.

Amongst the work's innovations is one of the earliest descriptions of purgatory as the place where each soul would have to work off its debts after death before entering heaven. Hildegard's descriptions of the possible punishments there are often gruesome and grotesque, which emphasize the work's moral and pastoral purpose as a practical guide to the life of true penance and proper virtue.

Hildegard's last and grandest visionary work had its genesis in one of the few times she experienced something like an ecstatic loss of consciousness. As she described it in an autobiographical passage included in her Vita, sometime in about 1163, she received "an extraordinary mystical vision" in which was revealed the "sprinkling drops of sweet rain" that John the Evangelist experienced when he wrote, "In the beginning was the Word..." (John 1:1). Hildegard perceived that this Word was the key to the "Work of God", of which humankind is the pinnacle. The "Book of Divine Works", therefore, became in many ways an extended explication of the Prologue to John's Gospel.

The ten visions of this work's three parts are cosmic in scale, to illustrate various ways of understanding the relationship between God and his creation. Often, that relationship is established by grand allegorical female figures representing Divine Love ("Caritas") or Wisdom ("Sapientia"). The first vision opens the work with a salvo of poetic and visionary images, swirling about to characterize God's dynamic activity within the scope of his work within the history of salvation. The remaining three visions of the first part introduce the famous image of a human being standing astride the spheres that make up the universe, and detail the intricate relationships between the human as microcosm and the universe as macrocosm. This culminates in the final chapter of Part One, Vision Four with Hildegard's commentary on the Prologue to John's Gospel (John 1:1–14), a direct rumination on the meaning of "In the beginning was the Word..." The single vision that constitutes the whole of Part Two stretches that rumination back to the opening of Genesis, and forms an extended commentary on the seven days of the creation of the world told in Genesis 1–2:3. This commentary interprets each day of creation in three ways: literal or cosmological; allegorical or ecclesiological (i.e. related to the Church's history); and moral or tropological (i.e. related to the soul's growth in virtue). Finally, the five visions of the third part take up again the building imagery of "Scivias" to describe the course of salvation history. The final vision (3.5) contains Hildegard's longest and most detailed prophetic program of the life of the Church from her own days of "womanish weakness" through to the coming and ultimate downfall of the Antichrist.

Attention in recent decades to women of the medieval Church has led to a great deal of popular interest in Hildegard's music. In addition to the "Ordo Virtutum," sixty-nine musical compositions, each with its own original poetic text, survive, and at least four other texts are known, though their musical notation has been lost. This is one of the largest repertoires among medieval composers.
One of her better known works, "Ordo Virtutum" ("Play of the Virtues"), is a morality play. It is uncertain when some of Hildegard's compositions were composed, though the "Ordo Virtutum" is thought to have been composed as early as 1151. It is an independent Latin morality play with music (82 songs); it does not supplement or pay homage to Mass or the Office of a certain feast. The most significant part of this entire composition is, however, that the "Ordo virtutum" is the earliest known, surviving musical drama that is not attached to a liturgy.

This entertainment was both performed and bemused by a select community of noblewomen and nuns. Even more fascinating about this piece, the devil has no music whatsoever in the plot of the play, he instead shouts and bellows all his lines. All other characters sing in monophonic plainchant. This includes Patriarchs, Prophets, A Happy Soul, A Unhappy Soul and A Penitent Soul along with 16 female Virtues (including Mercy, Innocence, Chasity, Obedience, Hope, and Faith).

The "Ordo virtutum" was probably performed as a manifestation of the theology Hildegard delineated in the "Scivias". The play serves as a group enchantment of the Christian story of sin, confession, repentance, and forgiveness. Notably, it is the female Virtues who restore the fallen to the community of the faithful, not the male Patriarchs or Prophets. This would have been a significant message to the nuns in Hildegard's convent. Scholars assert that the role of the Devil would have been played by Volmar, while Hildegard's nuns would have played the parts of Anima (the human souls) and the Virtues.

In addition to the "Ordo Virtutum", Hildegard composed many liturgical songs that were collected into a cycle called the "Symphonia armoniae celestium revelationum". The songs from the Symphonia are set to Hildegard's own text and range from antiphons, hymns, and sequences, to responsories. Her music is described as monophonic, that is, consisting of exactly one melodic line. Its style is characterized by soaring melodies that can push the boundaries of the more staid ranges of traditional Gregorian chant. Though Hildegard's music is often thought to stand outside the normal practices of monophonic monastic chant, current researchers are also exploring ways in which it may be viewed in comparison with her contemporaries, such as Hermannus Contractus. Another feature of Hildegard's music that both reflects twelfth-century evolutions of chant and pushes those evolutions further is that it is highly melismatic, often with recurrent melodic units. Scholars such as Margot Fassler, Marianne Richert Pfau, and Beverly Lomer also note the intimate relationship between music and text in Hildegard's compositions, whose rhetorical features are often more distinct than is common in twelfth-century chant. As with all medieval chant notation, Hildegard's music lacks any indication of tempo or rhythm; the surviving manuscripts employ late German style notation, which uses very ornamental neumes. The reverence for the Virgin Mary reflected in music shows how deeply influenced and inspired Hildegard of Bingen and her community were by the Virgin Mary and the saints.

The definition of viriditas or "greenness" is an earthly expression of the heavenly in an integrity that overcomes dualisms. This greenness or power of life appears frequently in Hildegard's works.

Despite Hildegard's self-professed view that her compositions have as object the praise of God, one scholar has asserted that Hildegard made a close association between music and the female body in her musical compositions. According to him, the poetry and music of Hildegard's Symphonia would therefore be concerned with the anatomy of female desire thus described as Sapphonic, or pertaining to Sappho, connecting her to a history of female rhetoricians.

Hildegard's medicinal and scientific writings, though thematically complementary to her ideas about nature expressed in her visionary works, are different in focus and scope. Neither claim to be rooted in her visionary experience and its divine authority. Rather, they spring from her experience helping in and then leading the monastery's herbal garden and infirmary, as well as the theoretical information she likely gained through her wide-ranging reading in the monastery's library. As she gained practical skills in diagnosis, prognosis, and treatment, she combined physical treatment of physical diseases with holistic methods centered on "spiritual healing." She became well known for her healing powers involving practical application of tinctures, herbs, and precious stones. She combined these elements with a theological notion ultimately derived from Genesis: all things put on earth are for the use of humans. In addition to her hands-on experience, she also gained medical knowledge, including elements of her humoral theory, from traditional Latin texts.

Hildegard catalogued both her theory and practice in two works. The first, "Physica," contains nine books that describe the scientific and medicinal properties of various plants, stones, fish, reptiles, and animals. The second, "Causae et Curae", is an exploration of the human body, its connections to the rest of the natural world, and the causes and cures of various diseases. Hildegard documented various medical practices in these books, including the use of bleeding and home remedies for many common ailments. She also explains remedies for common agricultural injuries such as burns, fractures, dislocations, and cuts. Hildegard may have used the books to teach assistants at the monastery. These books are historically significant because they show areas of medieval medicine that were not well documented because their practitioners (mainly women) rarely wrote in Latin. Her writings were commentated on by Mélanie Lipinska, a Polish scientist

In addition to its wealth of practical evidence, "Causae et Curae" is also noteworthy for its organizational scheme. Its first part sets the work within the context of the creation of the cosmos and then humanity as its summit, and the constant interplay of the human person as microcosm both physically and spiritually with the macrocosm of the universe informs all of Hildegard's approach. Her hallmark is to emphasize the vital connection between the "green" health of the natural world and the holistic health of the human person. "Viriditas", or greening power, was thought to sustain human beings and could be manipulated by adjusting the balance of elements within a person. Thus, when she approached medicine as a type of gardening, it was not just as an analogy. Rather, Hildegard understood the plants and elements of the garden as direct counterparts to the humors and elements within the human body, whose imbalance led to illness and disease.

Thus, the nearly three hundred chapters of the second book of "Causae et Curae" "explore the etiology, or causes, of disease as well as human sexuality, psychology, and physiology." In this section, she give specific instructions for bleeding based on various factors, including gender, the phase of the moon (bleeding is best done when moon is waning), the place of disease (use veins near diseased organ of body part) or prevention (big veins in arms), and how much blood to take (described in imprecise measurements, like "the amount that a thirsty person can swallow in one gulp"). She even includes bleeding instructions for animals to keep them healthy. In the third and fourth sections, Hildegard describes treatments for malignant and minor problems and diseases according to the humoral theory, again including information on animal health. The fifth section is about diagnosis and prognosis, which includes instructions to check the patient's blood, pulse, urine and stool. Finally, the sixth section documents a lunar horoscope to provide an additional means of prognosis for both disease and other medical conditions, such as conception and the outcome of pregnancy. For example, she indicates that a waxing moon is good for human conception and is also good for sowing seeds for plants (sowing seeds is the plant equivalent of conception). Elsewhere, Hildegard is even said to have stressed the value of boiling drinking water in an attempt to prevent infection.

As Hildegard elaborates the medical and scientific relationship between the human microcosm and the macrocosm of the universe, she often focuses on interrelated patterns of four: "the four elements (fire, air, water, and earth), the four seasons, the four humors, the four zones of the earth, and the four major winds." Although she inherited the basic framework of humoral theory from ancient medicine, Hildegard's conception of the hierarchical inter-balance of the four humors (blood, phlegm, black bile, and yellow bile) was unique, based on their correspondence to "superior" and "inferior" elements—blood and phlegm corresponding to the "celestial" elements of fire and air, and the two biles corresponding to the "terrestrial" elements of water and earth. Hildegard understood the disease-causing imbalance of these humors to result from the improper dominance of the subordinate humors. This disharmony reflects that introduced by Adam and Eve in the Fall, which for Hildegard marked the indelible entrance of disease and humoral imbalance into humankind. As she writes in "Causae et Curae" c. 42:

It happens that certain men suffer diverse illnesses. This comes from the phlegm which is superabundant within them. For if man had remained in paradise, he would not have had the "flegmata" within his body, from which many evils proceed, but his flesh would have been whole and without dark humor ["livor"]. However, because he consented to evil and relinquished good, he was made into a likeness of the earth, which produces good and useful herbs, as well as bad and useless ones, and which has in itself both good and evil moistures. From tasting evil, the blood of the sons of Adam was turned into the poison of semen, out of which the sons of man are begotten. And therefore their flesh is ulcerated and permeable [to disease]. These sores and openings create a certain storm and smoky moisture in men, from which the "flegmata" arise and coagulate, which then introduce diverse infirmities to the human body. All this arose from the first evil, which man began at the start, because if Adam had remained in paradise, he would have had the sweetest health, and the best dwelling-place, just as the strongest balsam emits the best odor; but on the contrary, man now has within himself poison and phlegm and diverse illnesses.

Hildegard also invented an alternative alphabet. "Litterae ignotae" ("Alternate Alphabet") was another work and was more or less a secret code, or even an intellectual code – much like a modern crossword puzzle today.

The text of her writing and compositions reveals Hildegard's use of this form of modified medieval Latin, encompassing many invented, conflated and abridged words. Because of her inventions of words for her lyrics and use of a constructed script, many conlangers look upon her as a medieval precursor.

Hildegard's "Lingua ignota" ("Unknown Language") was a composition that comprised a series of invented words that corresponded to an eclectic list of nouns. Scholars believe that Hildegard used her "Lingua Ignota" to increase solidarity among her nuns.

Maddocks claims that it is likely Hildegard learned simple Latin and the tenets of the Christian faith but was not instructed in the Seven Liberal Arts, which formed the basis of all education for the learned classes in the Middle Ages: the "Trivium" of grammar, dialectic, and rhetoric plus the "Quadrivium" of arithmetic, geometry, astronomy, and music. The correspondence she kept with the outside world, both spiritual and social, transcended the cloister as a space of spiritual confinement and served to document Hildegard's grand style and strict formatting of medieval letter writing.

Contributing to Christian European rhetorical traditions, Hildegard "authorized herself as a theologian" through alternative rhetorical arts. Hildegard was creative in her interpretation of theology. She believed that her monastery should exclude novices who were not from the nobility because she did not want her community to be divided on the basis of social status. She also stated that "woman may be made from man, but no man can be made without a woman."
Because of church limitation on public, discursive rhetoric, the medieval rhetorical arts included preaching, letter writing, poetry, and the encyclopedic tradition. Hildegard's participation in these arts speaks to her significance as a female rhetorician, transcending bans on women's social participation and interpretation of scripture. The acceptance of public preaching by a woman, even a well-connected abbess and acknowledged prophet, does not fit the stereotype of this time. Her preaching was not limited to the monasteries; she preached publicly in 1160 in Germany. (New York: Routledge, 2001, 9). She conducted four preaching tours throughout Germany, speaking to both clergy and laity in chapter houses and in public, mainly denouncing clerical corruption and calling for reform.

Many abbots and abbesses asked her for prayers and opinions on various matters. She traveled widely during her four preaching tours. She had several devoted followers, including Guibert of Gembloux, who wrote to her frequently and became her secretary after Volmar's death in 1173. Hildegard also influenced several monastic women, exchanging letters with Elisabeth of Schönau, a nearby visionary.

Hildegard corresponded with popes such as Eugene III and Anastasius IV, statesmen such as Abbot Suger, German emperors such as Frederick I Barbarossa, and other notable figures such as Saint Bernard of Clairvaux, who advanced her work, at the behest of her abbot, Kuno, at the Synod of Trier in 1147 and 1148. Hildegard of Bingen's correspondence is an important component of her literary output.

Hildegard was one of the first persons for whom the Roman canonization process was officially applied, but the process took so long that four attempts at canonization were not completed and she remained at the level of her beatification. Her name was nonetheless taken up in the Roman Martyrology at the end of the 16th century. Her feast day is 17 September. Numerous popes have referred to Hildegard as a saint, including Pope John Paul II and Pope Benedict XVI.

On 10 May 2012, Pope Benedict XVI extended the liturgical cult of St. Hildegard to the entire Catholic Church in a process known as "equivalent canonization," thus laying the groundwork for naming her a Doctor of the Church. On 7 October 2012, the feast of the Holy Rosary, the pope named her a Doctor of the Church, the fourth woman of 35 saints given that title by the Roman Catholic Church. He called her "perennially relevant" and "an authentic teacher of theology and a profound scholar of natural science and music."

Hildegard of Bingen also appears in the calendar of saints of various Anglican churches, such as that of the Church of England, in which she is commemorated on 17 September.

Hildegard's parish and pilgrimage church in Eibingen near Rüdesheim houses her relics.

In recent years, Hildegard has become of particular interest to feminist scholars. They note her reference to herself as a member of the weaker sex and her rather constant belittling of women. Hildegard frequently referred to herself as an unlearned woman, completely incapable of Biblical exegesis. Such a statement on her part, however, worked to her advantage because it made her statements that all of her writings and music came from visions of the Divine more believable, therefore giving Hildegard the authority to speak in a time and place where few women were permitted a voice. Hildegard used her voice to amplify the Church's condemnation of institutional corruption, in particular simony.

Hildegard has also become a figure of reverence within the contemporary New Age movement, mostly because of her holistic and natural view of healing, as well as her status as a mystic. Though her medical writings were long neglected, and then studied without reference to their context, she was the inspiration for Dr. Gottfried Hertzka's "Hildegard-Medicine", and is the namesake for June Boyce-Tillman's Hildegard Network, a healing center that focuses on a holistic approach to wellness and brings together people interested in exploring the links between spirituality, the arts, and healing. Her reputation as a medicinal writer and healer was also used by early feminists to argue for women's rights to attend medical schools. Hildegard's reincarnation has been debated since 1924 when Austrian mystic Rudolf Steiner lectured that a nun of her description was the past life of Russian poet philosopher Vladimir Soloviev, whose Sophianic visions are often compared to Hildegard's. Sophiologist Robert Powell writes that hermetic astrology proves the match, while mystical communities in Hildegard's lineage include that of artist Carl Schroeder as studied by Columbia sociologist Courtney Bender and supported by reincarnation researchers Walter Semkiw and Kevin Ryerson.

Recordings and performances of Hildegard's music have gained critical praise and popularity since 1979. See Discography listed below.

The following modern musical works are directly linked to Hildegard and her music or texts:

The artwork "The Dinner Party" features a place setting for Hildegard.

In space, the minor planet 898 Hildegard is named for her.

In film, Hildegard has been portrayed by Patricia Routledge in a BBC documentary called "Hildegard of Bingen" (1994), by Ángela Molina in "Barbarossa" (2009) and by Barbara Sukowa in the film "Vision", directed by Margarethe von Trotta.

Hildegard was the subject of a 2012 fictionalized biographic novel "Illuminations" by Mary Sharratt.

The plant genus "Hildegardia" is named after her because of her contributions to herbal medicine.

Hildegard makes an appearance in "The Baby-Sitters Club #101: Claudia Kishi, Middle School Drop-Out" by Ann M. Martin, when Anna Stevenson dresses as Hildegard for Halloween.

A feature documentary film, "," was released by American director Michael M. Conti in 2014.





Hildegard of Bingen.

Secondary Sources:



</doc>
<doc id="13686" url="https://en.wikipedia.org/wiki?curid=13686" title="Hilversum">
Hilversum

Hilversum () is a city and municipality in the province of North Holland, Netherlands. Located in the heart of the Gooi, it is the largest urban centre in that area. It is surrounded by heathland, woods, meadows, lakes, and smaller towns. Hilversum is part of the Randstad, one of the largest conurbations in Europe.

Hilversum lies south-east of Amsterdam and north of Utrecht. The town is known for its architecturally important Town Hall (Raadhuis Hilversum), designed by Willem Marinus Dudok and built in 1931.

Hilversum has one public library, two swimming pools (Van Hellemond Sport and De Lieberg), a number of sporting halls and several shopping centres (such as Hilvertshof, Winkelcentrum Kerkelanden, De Riebeeckgalerij and Winkelcentrum Seinhorst). Locally, the town centre is known as "het dorp", which means "the village".

Hilversum is often called "media city", since it is the principal centre for radio and television broadcasting in the Netherlands, and is home to an extensive complex of radio and television studios and to the administrative headquarters of the multiple broadcasting organizations which make up the Netherlands Public Broadcasting system. Hilversum is also home to many newer commercial TV production companies. Radio Netherlands, which has been broadcasting worldwide via shortwave radio since the 1920s, is also based here.

The following is a list of organizations that have, or are continuing to, broadcast from studios in Hilversum:


One result of the town's history as an important radio transmission centre is that many older radio sets throughout Europe featured "Hilversum" as a pre-marked dial position on their tuning scales.

Dutch national voting in the Eurovision Song Contest is normally co-ordinated from Hilversum.

Hilversum has a variety of international schools, such as the "Violenschool" and "International School Hilversum "Alberdingk Thijm"". Also, Nike's, Hunkemöller's and Converse's European headquarters are located in Hilversum.

Earthenware found in Hilversum gives its name to the Hilversum culture, which is an early- to mid-Bronze Age, or 800–1200 BCE material culture. Artifacts from this prehistoric civilization bear similarities to the Wessex Culture of southern Britain and may indicate that the first Hilversum residents emigrated from that area. The first brick settlements formed around 900, but it was not until 1305 that the first official mention of Hilversum ("Hilfersheem" from "Hilvertshem" meaning "houses between the hills") is found. At that point it was a part of Naarden, the oldest town in the Gooi area.
Farming, raising sheep and some wool manufacturing were the means of life for the Gooi in the Middle Ages. In 1424 Hilversum received its first official independent status. This made possible further growth in the village because permission from Naarden was no longer needed for new industrial development.

The town grew further in the 17th century when the Dutch economy as a whole entered its age of prosperity, and several canals were built connecting it indirectly to Amsterdam. 

In 1725 and 1766 large fires destroyed most of the town, leveling parts of the old townhouse and the church next to it. The town overcame these setbacks and the textile industry continued to develop, among other ways by devising a way to weave cows' hair.

In the 19th century a substantial textile and tapestry industry emerged, aided by a railway link to Amsterdam in 1874. From that time the town grew quickly with rich commuters from Amsterdam moving in, building themselves large villas in the wooded surroundings, and gradually starting to live in Hilversum permanently. Despite this growth, Hilversum was never granted city rights so it is still referred to by many locals as "het dorp," or "the village."

For the 1928 Summer Olympics in neighboring Amsterdam, it hosted all of the non-jumping equestrian and the running part of the modern pentathlon event. 

The "Nederlandse Seintoestellen Fabriek" (NSF) company established a professional transmitter and radio factory in Hilversum in the early 1920s, growing into the largest of its kind in the Netherlands.

Following the defeat of Allied forces in the Netherlands in 1940, and its occupation by Nazi Germany, Hilversum became the headquarters of the German Army ("Heer") in the Netherlands..

In 1948, NSF was taken over by Philips. However, Dutch radio broadcasting organizations (followed by television broadcasters during the 1950s) centralised their operations in Hilversum, providing a source of continuing economic growth. The concentration of broadcasters in Hilversum has given it its enduring status as the media city for the Netherlands.

In 1964, the population reached a record high – over 103,000 people called Hilversum home. However, the textile industry had started its decline; only one factory, Veneta, managed to continue into the 1960s, when it also had to close its doors. Another major industry, the chemical factory IFF, also closed by the end of the 1960s. 

After the 1960s, the population gradually declined, until stabilising at around 85,000. Several factors other than the slump in manufacturing have featured in this decline: one is the fact that the average family nowadays consists of fewer people, so fewer people live in each house; second, the town is virtually unable to expand because all the surrounding lands were sold by city architect W.M. Dudok to the Goois Natuurreservaat (""). The third reason for this decline of the population was because the property values were increasing rapidly in that moment of time, and many people were forced to move to less expensive areas in the Netherlands.

Some sources blame connections in the television world for attracting crime to Hilversum; the town has had to cope with mounting drug-related issues in a community with higher than average unemployment and ongoing housing shortage.

Hilversum was one of the first towns to have a local party of the populist movement called "Leefbaar" ("liveable"). Founded by former social-democrat party strongman Jan Nagel, it was initially held at bay for alderman positions. In 2001, Nagel from Leefbaar Hilversum teamed up with Leefbaar Utrecht leaders to found a national Leefbaar Nederland party. By strange coincidence, in 2002 the most vocal Leefbaar Rotterdam politician Pim Fortuyn was shot and killed by an animal rights activist at Hilversum Media Park just after finishing a radio interview. This happened, however, after a break between Fortuyn and Nagel during a Leefbaar Nederland board meeting in Hilversum on Fortuyn's anti-Islamic viewpoints.

The town of Hilversum has put a great deal of effort into improvements, including a recent renovation to its central train station, thorough renovation of the main shopping centre (Hilvertshof), and development of new dining and retail districts downtown including the "vintage" district in the Leeuwenstraat. Several notable architectural accomplishments include the Institute for Sound and Vision, and Zanderij Crailoo (""), the largest man-made wildlife crossing in the world.

The nearby Media Park was the scene of the 2002 assassination of politician Pim Fortuyn; in 2015, a gunman carrying a false pistol stormed into Nederlandse Omroep Stichting's headquarters, demanding airtime on the evening news.

The population declined from 103,000 in 1964 to 84,000 in 2006, but rose again to 90.000 in 2018. The decline is mostly due to the fact that families are smaller these days.

The large Catholic neo-gothic St. Vitus church (P.J.H. Cuypers, 1892, bell tower 96 metres).

The city played host to many landscape artists during the 19th century, including Barend Cornelis Koekkoek.

The 1958 Eurovision Song Contest took place in Hilversum.

Hilversum is well connected to the Dutch railway network, and has three stations.
Most local and regional buses are operated by Connexxion, but two of the bus routes are operated by Syntus Utrecht and two others by U-OV and Pouw Vervoer. Regional bus route 320 is operated by both Connexxion and Pouw Vervoer.
In 2018, major road works started to make room for a new BRT bus lane from Hilversum to Huizen, set to open in early 2021.

The municipal council of Hilversum consists of 37 seats, which are divided as followed since the last local election of 2018:


Government

After the 2018 elections, the municipal government was made up of aldermen from the political parties Hart voor Hilversum, D66 and VVD.

The mayor of Hilversum is Pieter Broertjes, former lead editor of the Volkskrant, a nationwide distributed newspaper.

It was the first city with a "Leefbaar" party (which was intended as just a local party). Today, Leefbaar Hilversum has been reduced to only 1 seat, but some other parties have their origins in Leefbaar Hilversum:

Notable people born in Hilversum:




</doc>
<doc id="13688" url="https://en.wikipedia.org/wiki?curid=13688" title="The Hound of Heaven">
The Hound of Heaven

"The Hound of Heaven" is a 182-line poem written by English poet Francis Thompson (1859–1907). The poem became famous and was the source of much of Thompson's posthumous reputation. The poem was first published in Thompson's first volume of poems in 1893. It was included in the "Oxford Book of English Mystical Verse" (1917). Thompson's work was praised by G. K. Chesterton, and it was also an influence on J. R. R. Tolkien, who presented a paper on Thompson in 1914.

This Christian poem has been described as follows:
"The name is strange. It startles one at first. It is so bold, so new, so fearless. It does not attract, rather the reverse. But when one reads the poem this strangeness disappears. The meaning is understood. As the hound follows the hare, never ceasing in its running, ever drawing nearer in the chase, with unhurrying and imperturbed pace, so does God follow the fleeing soul by His Divine grace. And though in sin or in human love, away from God it seeks to hide itself, Divine grace follows after, unwearyingly follows ever after, till the soul feels its pressure forcing it to turn to Him alone in that never ending pursuit." J.F.X. O'Conor, S.J.





</doc>
<doc id="13692" url="https://en.wikipedia.org/wiki?curid=13692" title="History of the Internet">
History of the Internet

The history of the Internet begins with the development of electronic computers in the 1950s. Initial concepts of wide area networking originated in several computer science laboratories in the United States, United Kingdom, and France. The U.S. Department of Defense awarded contracts as early as the 1960s, including for the development of the ARPANET project, directed by Robert Taylor and managed by Lawrence Roberts. The first message was sent over the ARPANET in 1969 from computer science Professor Leonard Kleinrock's laboratory at University of California, Los Angeles (UCLA) to the second network node at Stanford Research Institute (SRI).

Packet switching networks such as the NPL network, ARPANET, Merit Network, CYCLADES, and Telenet, were developed in the late 1960s and early 1970s using a variety of communications protocols. Donald Davies first demonstrated packet switching in 1967 at the National Physics Laboratory (NPL) in the UK, which became a testbed for UK research for almost two decades. The ARPANET project led to the development of protocols for internetworking, in which multiple separate networks could be joined into a network of networks.

The Internet protocol suite (TCP/IP) was developed by Robert E. Kahn and Vint Cerf in the 1970s and became the standard networking protocol on the ARPANET, incorporating concepts from the French CYCLADES project directed by Louis Pouzin. In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities, and provided interconnectivity in 1986 with the NSFNET project, which also created network access to the supercomputer sites in the United States from research and education organizations. Commercial Internet service providers (ISPs) began to emerge in the very late 1980s. The ARPANET was decommissioned in 1990. Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990, and the NSFNET was decommissioned in 1995, removing the last restrictions on the use of the Internet to carry commercial traffic.

In the 1980s, research at CERN in Switzerland by British computer scientist Tim Berners-Lee resulted in the World Wide Web, linking hypertext documents into an information system, accessible from any node on the network. Since the mid-1990s, the Internet has had a revolutionary impact on culture, commerce, and technology, including the rise of near-instant communication by electronic mail, instant messaging, voice over Internet Protocol (VoIP) telephone calls, two-way interactive video calls, and the World Wide Web with its discussion forums, blogs, social networking, and online shopping sites. The research and education community continues to develop and use advanced networks such as JANET in the United Kingdom and Internet2 in the United States. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1 Gbit/s, 10 Gbit/s, or more. The Internet's takeover of the global communication landscape was almost instant in historical terms: it only communicated 1% of the information flowing through two-way telecommunications networks in the year 1993, already 51% by 2000, and more than 97% of the telecommunicated information by 2007. Today the Internet continues to grow, driven by ever greater amounts of online information, commerce, entertainment, and social networking. However, the future of the global internet may be shaped by regional differences in the world.

The concept of data communication – transmitting data between two different places through an electromagnetic medium such as radio or an electric wire – pre-dates the introduction of the first computers. Such communication systems were typically limited to point to point communication between two end devices. Semaphore lines, telegraph systems and telex machines can be considered early precursors of this kind of communication. The Telegraph in the late 19th century was the first fully digital communication system.

Fundamental theoretical work in data transmission and information theory was developed by Claude Shannon, Harry Nyquist, and Ralph Hartley in the early 20th century.

Early computers had a central processing unit and remote terminals. As the technology evolved, new systems were devised to allow communication over longer distances (for terminals) or with higher speed (for interconnection of local devices) that were necessary for the mainframe computer model. These technologies made it possible to exchange data (such as files) between remote computers. However, the point-to-point communication model was limited, as it did not allow for direct communication between any two arbitrary systems; a physical link was necessary. The technology was also considered unsafe for strategic and military use because there were no alternative paths for the communication in case of an enemy attack.

With limited exceptions, the earliest computers were connected directly to terminals used by individual users, typically in the same building or site. Such networks became known as local area networks (LANs). Networking beyond this scope, known as wide area networks (WANs), emerged during the 1950s and became established during the 1960s.

J. C. R. Licklider, Vice President at Bolt Beranek and Newman, Inc., proposed a global network in his January 1960 paper "Man-Computer Symbiosis":

In August 1962, Licklider and Welden Clark published the paper "On-Line Man-Computer Communication" which was one of the first descriptions of a networked future.

In October 1962, Licklider was hired by Jack Ruina as director of the newly established Information Processing Techniques Office (IPTO) within DARPA, with a mandate to interconnect the United States Department of Defense's main computers at Cheyenne Mountain, the Pentagon, and SAC HQ. There he formed an informal group within DARPA to further computer research. He began by writing memos describing a distributed network to the IPTO staff, whom he called "Members and Affiliates of the Intergalactic Computer Network". As part of the information processing office's role, three network terminals had been installed: one for System Development Corporation in Santa Monica, one for Project Genie at University of California, Berkeley, and one for the Compatible Time-Sharing System project at Massachusetts Institute of Technology (MIT). Licklider's identified need for inter-networking would become obvious by the apparent waste of resources this caused.

For each of these three terminals, I had three different sets of user commands. So if I was talking online with someone at S.D.C. and I wanted to talk to someone I knew at Berkeley or M.I.T. about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them... <br><br>I said, oh man, it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go where you have interactive computing. That idea is the ARPAnet.

Although he left the IPTO in 1964, five years before the ARPANET went live, it was his vision of universal networking that provided the impetus for one of his successors, Robert Taylor, to initiate the ARPANET development. Licklider later returned to lead the IPTO in 1973 for two years.

The issue of connecting separate physical networks to form one logical network was the first of many problems. Early networks used message switched systems that required rigid routing structures prone to single point of failure. In the 1960s, Paul Baran of the RAND Corporation produced a study of survivable networks for the U.S. military in the event of nuclear war. Information transmitted across Baran's network would be divided into what he called "message blocks". Independently, Donald Davies (National Physical Laboratory, UK), proposed and was the first to put into practice a local area network based on what he called packet switching, the term that would ultimately be adopted. Larry Roberts applied Davies' concepts of packet switching for the ARPANET wide area network, and sought input from Paul Baran and Leonard Kleinrock. Kleinrock subsequently developed the mathematical theory behind the performance of this technology building on his earlier work on queueing theory.

Packet switching is a rapid store and forward networking design that divides messages up into arbitrary packets, with routing decisions made per-packet. It provides better bandwidth utilization and response times than the traditional circuit-switching technology used for telephony, particularly on resource-limited interconnection links.

Following discussions with J. C. R. Licklider, Donald Davies became interested in data communications for computer networks. At the National Physical Laboratory (United Kingdom) in 1965, Davies designed and proposed a national data network based on packet switching. The following year, he described the use of an "Interface computer" to act as a router. The proposal was not taken up nationally but by 1967, a pilot experiment had demonstrated the feasibility of packet switched networks.

By 1969 he had begun building the Mark I packet-switched network to meet the needs of the multidisciplinary laboratory and prove the technology under operational conditions. In 1976, 12 computers and 75 terminal devices were attached, and more were added until the network was replaced in 1986. NPL, followed by ARPANET, were the first two networks in the world to use packet switching, and were interconnected in the early 1970s.

Robert Taylor was promoted to the head of the information processing office at Defense Advanced Research Projects Agency (DARPA) in June 1966. He intended to realize Licklider's ideas of an interconnected networking system. Bringing in Larry Roberts from MIT, he initiated a project to build such a network. The first ARPANET link was established between the University of California, Los Angeles (UCLA) and the Stanford Research Institute at 22:30 hours on October 29, 1969.

By December 5, 1969, a 4-node network was connected by adding the University of Utah and the University of California, Santa Barbara. Building on ideas developed in ALOHAnet, the ARPANET grew rapidly. By 1981, the number of hosts had grown to 213, with a new host being added approximately every twenty days.

ARPANET development was centered around the Request for Comments (RFC) process, still used today for proposing and distributing Internet Protocols and Systems. RFC 1, entitled "Host Software", was written by Steve Crocker from the University of California, Los Angeles, and published on April 7, 1969. These early years were documented in the 1972 film .

ARPANET became the technical core of what would become the Internet, and a primary tool in developing the technologies used. The early ARPANET used the Network Control Program (NCP, sometimes Network Control Protocol) rather than TCP/IP. On January 1, 1983, known as flag day, NCP on the ARPANET was replaced by the more flexible and powerful family of TCP/IP protocols, marking the start of the modern Internet.

International collaborations on ARPANET were sparse. For various political reasons, European developers were concerned with developing the X.25 networks. Notable exceptions were the "Norwegian Seismic Array" (NORSAR) in 1972, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter Kirstein's research group in the UK, initially at the Institute of Computer Science, London University and later at University College London.

The Merit Network was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.

The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the early ARPANET design and to support network research generally. It was the first network to make the hosts responsible for reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms. Concepts of this network influenced later ARPANET architecture.

Based on ARPA's research, packet switching network standards were developed by the International Telecommunication Union (ITU) in the form of X.25 and related standards. While using packet switching, X.25 is built on the concept of virtual circuits emulating traditional telephone connections. In 1974, X.25 formed the basis for the SERCnet network between British academic and research sites, which later became JANET. The initial ITU Standard on X.25 was approved in March 1976.

The British Post Office, Western Union International and Tymnet collaborated to create the first international packet switched network, referred to as the International Packet Switched Service (IPSS), in 1978. This network grew from Europe and the US to cover Canada, Hong Kong, and Australia by 1981. By the 1990s it provided a worldwide networking infrastructure.

Unlike ARPANET, X.25 was commonly available for business use. Telenet offered its Telemail electronic mail service, which was also targeted to enterprise use rather than the general email system of the ARPANET.

The first public dial-in networks used asynchronous TTY terminal protocols to reach a concentrator operated in the public network. Some networks, such as CompuServe, used X.25 to multiplex the terminal sessions into their packet-switched backbones, while others, such as Tymnet, used proprietary protocols. In 1979, CompuServe became the first service to offer electronic mail capabilities and technical support to personal computer users. The company broke new ground again in 1980 as the first to offer real-time chat with its CB Simulator. Other major dial-in networks were America Online (AOL) and Prodigy that also provided communications, content, and entertainment features. Many bulletin board system (BBS) networks also provided on-line access, such as FidoNet which was popular amongst hobbyist computer users, many of them hackers and amateur radio operators.

In 1979, two students at Duke University, Tom Truscott and Jim Ellis, originated the idea of using Bourne shell scripts to transfer news and messages on a serial line UUCP connection with nearby University of North Carolina at Chapel Hill. Following public release of the software in 1980, the mesh of UUCP hosts forwarding on the Usenet news rapidly expanded. UUCPnet, as it would later be named, also created gateways and links between FidoNet and dial-up BBS hosts. UUCP networks spread quickly due to the lower costs involved, ability to use existing leased lines, X.25 links or even ARPANET connections, and the lack of strict use policies compared to later networks like CSNET and Bitnet. All connects were local. By 1981 the number of UUCP hosts had grown to 550, nearly doubling to 940 in 1984. – Sublink Network, operating since 1987 and officially founded in Italy in 1989, based its interconnectivity upon UUCP to redistribute mail and news groups messages throughout its Italian nodes (about 100 at the time) owned both by private individuals and small companies. Sublink Network represented possibly one of the first examples of the Internet technology becoming progress through popular diffusion.

With so many different network methods, something was needed to unify them. Robert E. Kahn of DARPA and ARPANET recruited Vinton Cerf of Stanford University to work with him on the problem. By 1973, they had worked out a fundamental reformulation, where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible. Cerf credits Hubert Zimmermann, Gerard LeLann and Louis Pouzin (designer of the CYCLADES network) with important work on this design.

The specification of the resulting protocol, "RFC 675 – Specification of Internet Transmission Control Program", by Vinton Cerf, Yogen Dalal and Carl Sunshine, Network Working Group, December 1974, contains the first attested use of the term "internet", as a shorthand for "internetworking"; later RFCs repeat this use, so the word started out as an adjective rather than the noun it is today.
With the role of the network reduced to the bare minimum, it became possible to join almost any networks together, no matter what their characteristics were, thereby solving Kahn's initial problem. DARPA agreed to fund development of prototype software, and after several years of work, the first demonstration of a gateway between the Packet Radio network in the SF Bay area and the ARPANET was conducted by the Stanford Research Institute. On November 22, 1977 a three network demonstration was conducted including the ARPANET, the SRI's Packet Radio Van on the Packet Radio Network and the Atlantic Packet Satellite network.

Stemming from the first specifications of TCP in 1974, TCP/IP emerged in mid-late 1978 in nearly its final form, as used for the first decades of the Internet, known as "IPv4". which is described in IETF publication RFC 791 (September 1981).
IPv4 uses 32-bit addresses which limits the address space to 2 addresses, i.e. addresses. The last available IPv4 address was assigned in January 2011. IPv4 is being replaced by its successor, called "IPv6", which uses 128 bit addresses, providing 2 addresses, i.e. . This is a vastly increased address space. The shift to IPv6 is expected to take many years, decades, or perhaps longer, to complete, since there were four billion machines with IPv4 when the shift began.

The associated standards for IPv4 were published by 1981 as RFCs 791, 792 and 793, and adopted for use. DARPA sponsored or encouraged the development of TCP/IP implementations for many operating systems and then scheduled a migration of all hosts on all of its packet networks to TCP/IP. On January 1, 1983, known as flag day, TCP/IP protocols became the only approved protocol on the ARPANET, replacing the earlier NCP protocol.

After the ARPANET had been up and running for several years, ARPA looked for another agency to hand off the network to; ARPA's primary mission was funding cutting edge research and development, not running a communications utility. Eventually, in July 1975, the network had been turned over to the Defense Communications Agency, also part of the Department of Defense. In 1983, the U.S. military portion of the ARPANET was broken off as a separate network, the MILNET. MILNET subsequently became the unclassified but military-only NIPRNET, in parallel with the SECRET-level SIPRNET and JWICS for TOP SECRET and above. NIPRNET does have controlled security gateways to the public Internet.

The networks based on the ARPANET were government funded and therefore restricted to noncommercial uses such as research; unrelated commercial use was strictly forbidden. This initially restricted connections to military sites and universities. During the 1980s, the connections expanded to more educational institutions, and even to a growing number of companies such as Digital Equipment Corporation and Hewlett-Packard, which were participating in research projects or providing services to those who were.

Several other branches of the U.S. government, the National Aeronautics and Space Administration (NASA), the National Science Foundation (NSF), and the Department of Energy (DOE) became heavily involved in Internet research and started development of a successor to ARPANET. In the mid-1980s, all three of these branches developed the first Wide Area Networks based on TCP/IP. NASA developed the NASA Science Network, NSF developed CSNET and DOE evolved the Energy Sciences Network or ESNet.
NASA developed the TCP/IP based NASA Science Network (NSN) in the mid-1980s, connecting space scientists to data and information stored anywhere in the world. In 1989, the DECnet-based Space Physics Analysis Network (SPAN) and the TCP/IP-based NASA Science Network (NSN) were brought together at NASA Ames Research Center creating the first multiprotocol wide area network called the NASA Science Internet, or NSI. NSI was established to provide a totally integrated communications infrastructure to the NASA scientific community for the advancement of earth, space and life sciences. As a high-speed, multiprotocol, international network, NSI provided connectivity to over 20,000 scientists across all seven continents.

In 1981 NSF supported the development of the Computer Science Network (CSNET). CSNET connected with ARPANET using TCP/IP, and ran TCP/IP over X.25, but it also supported departments without sophisticated network connections, using automated dial-up mail exchange.

In 1986, the NSF created NSFNET, a 56 kbit/s backbone to support the NSF-sponsored supercomputing centers. The NSFNET also provided support for the creation of regional research and education networks in the United States, and for the connection of university and college campus networks to the regional networks. The use of NSFNET and the regional networks was not limited to supercomputer users and the 56 kbit/s network quickly became overloaded. NSFNET was upgraded to 1.5 Mbit/s in 1988 under a cooperative agreement with the Merit Network in partnership with IBM, MCI, and the State of Michigan. The existence of NSFNET and the creation of Federal Internet Exchanges (FIXes) allowed the ARPANET to be decommissioned in 1990. NSFNET was expanded and upgraded to 45 Mbit/s in 1991, and was decommissioned in 1995 when it was replaced by backbones operated by several commercial Internet Service Providers.

The term "internet" was adopted in the first RFC published on the TCP protocol (RFC 675: Internet Transmission Control Program, December 1974) as an abbreviation of the term "internetworking" and the two terms were used interchangeably. In general, an internet was any network using TCP/IP. It was around the time when ARPANET was interlinked with NSFNET in the late 1980s, that the term was used as the name of the network, Internet, being the large and global TCP/IP network.

As interest in networking grew and new applications for it were developed, the Internet's technologies spread throughout the rest of the world. The network-agnostic approach in TCP/IP meant that it was easy to use any existing network infrastructure, such as the IPSS X.25 network, to carry Internet traffic. In 1982, one year earlier than ARPANET, University College London replaced its transatlantic satellite links with TCP/IP over IPSS.

Many sites unable to link directly to the Internet created simple gateways for the transfer of electronic mail, the most important application of the time. Sites with only intermittent connections used UUCP or FidoNet and relied on the gateways between these networks and the Internet. Some gateway services went beyond simple mail peering, such as allowing access to File Transfer Protocol (FTP) sites via UUCP or mail.

Finally, routing technologies were developed for the Internet to remove the remaining centralized routing aspects. The Exterior Gateway Protocol (EGP) was replaced by a new protocol, the Border Gateway Protocol (BGP). This provided a meshed topology for the Internet and reduced the centric architecture which ARPANET had emphasized. In 1994, Classless Inter-Domain Routing (CIDR) was introduced to support better conservation of address space which allowed use of route aggregation to decrease the size of routing tables.

Between 1984 and 1988 CERN began installation and operation of TCP/IP to interconnect its major internal computer systems, workstations, PCs and an accelerator control system. CERN continued to operate a limited self-developed system (CERNET) internally and several incompatible (typically proprietary) network protocols externally. There was considerable resistance in Europe towards more widespread use of TCP/IP, and the CERN TCP/IP intranets remained isolated from the Internet until 1989.

In 1988, Daniel Karrenberg, from Centrum Wiskunde & Informatica (CWI) in Amsterdam, visited Ben Segal, CERN's TCP/IP Coordinator, looking for advice about the transition of the European side of the UUCP Usenet network (much of which ran over X.25 links) over to TCP/IP. In 1987, Ben Segal had met with Len Bosack from the then still small company Cisco about purchasing some TCP/IP routers for CERN, and was able to give Karrenberg advice and forward him on to Cisco for the appropriate hardware. This expanded the European portion of the Internet across the existing UUCP networks, and in 1989 CERN opened its first external TCP/IP connections. This coincided with the creation of Réseaux IP Européens (RIPE), initially a group of IP network administrators who met regularly to carry out coordination work together. Later, in 1992, RIPE was formally registered as a cooperative in Amsterdam.

At the same time as the rise of internetworking in Europe, ad hoc networking to ARPA and in-between Australian universities formed, based on various technologies such as X.25 and UUCPNet. These were limited in their connection to the global networks, due to the cost of making individual international UUCP dial-up or X.25 connections. In 1989, Australian universities joined the push towards using IP protocols to unify their networking infrastructures. AARNet was formed in 1989 by the Australian Vice-Chancellors' Committee and provided a dedicated IP based network for Australia.

The Internet began to penetrate Asia in the 1980s. In May 1982 South Korea became the second country to successfully set up TCP/IP IPv4 network. Japan, which had built the UUCP-based network JUNET in 1984, connected to NSFNET in 1989. It hosted the annual meeting of the Internet Society, INET'92, in Kobe. Singapore developed TECHNET in 1990, and Thailand gained a global Internet connection between Chulalongkorn University and UUNET in 1992.

While developed countries with technological infrastructures were joining the Internet, developing countries began to experience a digital divide separating them from the Internet. On an essentially continental basis, they are building organizations for Internet resource administration and sharing operational experience, as more and more transmission facilities go into place.

At the beginning of the 1990s, African countries relied upon X.25 IPSS and 2400 baud modem UUCP links for international and internetwork computer communications.

In August 1995, InfoMail Uganda, Ltd., a privately held firm in Kampala now known as InfoCom, and NSN Network Services of Avon, Colorado, sold in 1997 and now known as Clear Channel Satellite, established Africa's first native TCP/IP high-speed satellite Internet services. The data connection was originally carried by a C-Band RSCC Russian satellite which connected InfoMail's Kampala offices directly to NSN's MAE-West point of presence using a private network from NSN's leased ground station in New Jersey. InfoCom's first satellite connection was just 64 kbit/s, serving a Sun host computer and twelve US Robotics dial-up modems.

In 1996, a USAID funded project, the Leland Initiative, started work on developing full Internet connectivity for the continent. Guinea, Mozambique, Madagascar and Rwanda gained satellite earth stations in 1997, followed by Ivory Coast and Benin in 1998.

Africa is building an Internet infrastructure. AfriNIC, headquartered in Mauritius, manages IP address allocation for the continent. As do the other Internet regions, there is an operational forum, the Internet Community of Operational Networking Specialists.

There are many programs to provide high-performance transmission plant, and the western and southern coasts have undersea optical cable. High-speed cables join North Africa and the Horn of Africa to intercontinental cable systems. Undersea cable development is slower for East Africa; the original joint effort between New Partnership for Africa's Development (NEPAD) and the East Africa Submarine System (Eassy) has broken off and may become two efforts.

The Asia Pacific Network Information Centre (APNIC), headquartered in Australia, manages IP address allocation for the continent. APNIC sponsors an operational forum, the Asia-Pacific Regional Internet Conference on Operational Technologies (APRICOT).

South Korea's first Internet system, the System Development Network (SDN) began operation on 15 May 1982. SDN was connected to the rest of the world in August 1983 using UUCP (Unixto-Unix-Copy); connected to CSNET in December 1984; and formally connected to the U.S. Internet in 1990.

In 1991, the People's Republic of China saw its first TCP/IP college network, Tsinghua University's TUNET. The PRC went on to make its first global Internet connection in 1994, between the Beijing Electro-Spectrometer Collaboration and Stanford University's Linear Accelerator Center. However, China went on to implement its own digital divide by implementing a country-wide content filter.

As with the other regions, the Latin American and Caribbean Internet Addresses Registry (LACNIC) manages the IP address space and other resources for its area. LACNIC, headquartered in Uruguay, operates DNS root, reverse DNS, and other key services.

Initially, as with its predecessor networks, the system that would evolve into the Internet was primarily for government and government body use.

However, interest in commercial use of the Internet quickly became a commonly debated topic. Although commercial use was forbidden, the exact definition of commercial use was unclear and subjective. UUCPNet and the X.25 IPSS had no such restrictions, which would eventually see the official barring of UUCPNet use of ARPANET and NSFNET connections. (Some UUCP links still remained connecting to these networks however, as administrators cast a blind eye to their operation.)
As a result, during the late 1980s, the first Internet service provider (ISP) companies were formed. Companies like PSINet, UUNET, Netcom, and Portal Software were formed to provide service to the regional research networks and provide alternate network access, UUCP-based email and Usenet News to the public. The first commercial dialup ISP in the United States was The World, which opened in 1989.

In 1992, the U.S. Congress passed the Scientific and Advanced-Technology Act, , which allowed NSF to support access by the research and education communities to computer networks which were not used exclusively for research and education purposes, thus permitting NSFNET to interconnect with commercial networks. This caused controversy within the research and education community, who were concerned commercial use of the network might lead to an Internet that was less responsive to their needs, and within the community of commercial network providers, who felt that government subsidies were giving an unfair advantage to some organizations.

By 1990, ARPANET's goals had been fulfilled and new networking technologies exceeded the original scope and the project came to a close. New network service providers including PSINet, Alternet, CERFNet, ANS CO+RE, and many others were offering network access to commercial customers. NSFNET was no longer the de facto backbone and exchange point of the Internet. The Commercial Internet eXchange (CIX), Metropolitan Area Exchanges (MAEs), and later Network Access Points (NAPs) were becoming the primary interconnections between many networks. The final restrictions on carrying commercial traffic ended on April 30, 1995 when the National Science Foundation ended its sponsorship of the NSFNET Backbone Service and the service ended. NSF provided initial support for the NAPs and interim support to help the regional research and education networks transition to commercial ISPs. NSF also sponsored the very high speed Backbone Network Service (vBNS) which continued to provide support for the supercomputing centers and research and education in the United States.

The World Wide Web (sometimes abbreviated "www" or "W3") is an information space where documents and other web resources are identified by URIs, interlinked by hypertext links, and can be accessed via the Internet using a web browser and (more recently) web-based applications. It has become known simply as "the Web". As of the 2010s, the World Wide Web is the primary tool billions use to interact on the Internet, and it has changed people's lives immeasurably.

Precursors to the web browser emerged in the form of hyperlinked applications during the mid and late 1980s (the bare concept of hyperlinking had by then existed for some decades). Following these, Tim Berners-Lee is credited with inventing the World Wide Web in 1989 and developing in 1990 both the first web server, and the first web browser, called WorldWideWeb (no spaces) and later renamed Nexus. Many others were soon developed, with Marc Andreessen's 1993 Mosaic (later Netscape), being particularly easy to use and install, and often credited with sparking the internet boom of the 1990s. Today, the major web browsers are Firefox, Internet Explorer, Google Chrome, Opera and Safari.

A boost in web users was triggered in September 1993 by NCSA Mosaic, a graphical browser which eventually ran on several popular office and home computers. This was the first web browser aiming to bring multimedia content to non-technical users, and therefore included images and text on the same page, unlike previous browser designs; its founder, Marc Andreessen, also established the company that in 1994, released Netscape Navigator, which resulted in one of the early browser wars, when it ended up in a competition for dominance (which it lost) with Microsoft Windows' Internet Explorer. Commercial use restrictions were lifted in 1995. The online service America Online (AOL) offered their users a connection to the Internet via their own internal browser.

During the first decade or so of the public internet, the immense changes it would eventually enable in the 2000s were still nascent. In terms of providing context for this period, mobile cellular devices ("smartphones" and other cellular devices) which today provide near-universal access, were used for business and not a routine household item owned by parents and children worldwide. Social media in the modern sense had yet to come into existence, laptops were bulky and most households did not have computers. Data rates were slow and most people lacked means to video or digitize video; media storage was transitioning slowly from analog tape to digital optical discs (DVD and to an extent still, floppy disc to CD). Enabling technologies used from the early 2000s such as PHP, modern Javascript and Java, technologies such as AJAX, HTML 4 (and its emphasis on CSS), and various software frameworks, which enabled and simplified speed of web development, largely awaited invention and their eventual widespread adoption.

The Internet was widely used for mailing lists, emails, e-commerce and early popular online shopping (Amazon and eBay for example), online forums and bulletin boards, and personal websites and blogs, and use was growing rapidly, but by more modern standards the systems used were static and lacked widespread social engagement. It awaited a number of events in the early 2000s to change from a communications technology to gradually develop into a key part of global society's infrastructure.

Typical design elements of these "Web 1.0" era websites included: Static pages instead of dynamic HTML; content served from filesystems instead of relational databases; pages built using Server Side Includes or CGI instead of a web application written in a dynamic programming language; HTML 3.2-era structures such as frames and tables to create page layouts; online guestbooks; overuse of GIF buttons and similar small graphics promoting particular items; and HTML forms sent via email. (Support for server side scripting was rare on shared servers so the usual feedback mechanism was via email, using mailto forms and their email program.

During the period 1997 to 2001, the first speculative investment bubble related to the Internet took place, in which "dot-com" companies (referring to the ".com" top level domain used by businesses) were propelled to exceedingly high valuations as investors rapidly stoked stock values, followed by a market crash; the first dot-com bubble. However this only temporarily slowed enthusiasm and growth, which quickly recovered and continued to grow.

The changes that would propel the Internet into its place as a social system took place during a relatively short period of no more than five years, starting from around 2004. They included:

and shortly after (approximately 2007–2008 onward):
With the call to Web 2.0, the period up to around 2004–2005 was retrospectively named and described by some as Web 1.0.

The term "Web 2.0" describes websites that emphasize user-generated content (including user-to-user interaction), usability, and interoperability. It first appeared in a January 1999 article called "Fragmented Future" written by Darcy DiNucci, a consultant on electronic information design, where she wrote:

The term resurfaced during 2002 – 2004, and gained prominence in late 2004 following presentations by Tim O'Reilly and Dale Dougherty at the first Web 2.0 Conference. In their opening remarks, John Battelle and Tim O'Reilly outlined their definition of the "Web as Platform", where software applications are built upon the Web as opposed to upon the desktop. The unique aspect of this migration, they argued, is that "customers are building your business for you". They argued that the activities of users generating content (in the form of ideas, text, videos, or pictures) could be "harnessed" to create value.

Web 2.0 does not refer to an update to any technical specification, but rather to cumulative changes in the way Web pages are made and used. Web 2.0 describes an approach, in which sites focus substantially upon allowing users to interact and collaborate with each other in a social media dialogue as creators of user-generated content in a virtual community, in contrast to Web sites where people are limited to the passive viewing of content. Examples of Web 2.0 include social networking sites, blogs, wikis, folksonomies, video sharing sites, hosted services, Web applications, and mashups. Terry Flew, in his 3rd Edition of "New Media" described what he believed to characterize the differences between Web 1.0 and Web 2.0:
This era saw several household names gain prominence through their community-oriented operation – YouTube, Twitter, Facebook, Reddit and Wikipedia being some examples.

The process of change generally described as "Web 2.0" was itself greatly accelerated and transformed only a short time later by the increasing growth in mobile devices. This mobile revolution meant that computers in the form of smartphones became something many people used, took with them everywhere, communicated with, used for photographs and videos they instantly shared or to shop or seek information "on the move" – and used socially, as opposed to items on a desk at home or just used for work.

Location-based services, services using location and other sensor information, and crowdsourcing (frequently but not always location based), became common, with posts tagged by location, or websites and services becoming location aware. Mobile-targeted websites (such as "m.website.com") became common, designed especially for the new devices used. Netbooks, ultrabooks, widespread 4G and Wi-Fi, and mobile chips capable or running at nearly the power of desktops from not many years before on far lower power usage, became enablers of this stage of Internet development, and the term "App" emerged (short for "Application program" or "Program") as did the "App store".

The first Internet link into low earth orbit was established on January 22, 2010 when astronaut T. J. Creamer posted the first unassisted update to his Twitter account from the International Space Station, marking the extension of the Internet into space. (Astronauts at the ISS had used email and Twitter before, but these messages had been relayed to the ground through a NASA data link before being posted by a human proxy.) This personal Web access, which NASA calls the Crew Support LAN, uses the space station's high-speed Ku band microwave link. To surf the Web, astronauts can use a station laptop computer to control a desktop computer on Earth, and they can talk to their families and friends on Earth using Voice over IP equipment.

Communication with spacecraft beyond earth orbit has traditionally been over point-to-point links through the Deep Space Network. Each such data link must be manually scheduled and configured. In the late 1990s NASA and Google began working on a new network protocol, Delay-tolerant networking (DTN) which automates this process, allows networking of spaceborne transmission nodes, and takes the fact into account that spacecraft can temporarily lose contact because they move behind the Moon or planets, or because space weather disrupts the connection. Under such conditions, DTN retransmits data packages instead of dropping them, as the standard TCP/IP Internet Protocol does. NASA conducted the first field test of what it calls the "deep space internet" in November 2008. Testing of DTN-based communications between the International Space Station and Earth (now termed Disruption-Tolerant Networking) has been ongoing since March 2009, and is scheduled to continue until March 2014.

This network technology is supposed to ultimately enable missions that involve multiple spacecraft where reliable inter-vessel communication might take precedence over vessel-to-earth downlinks. According to a February 2011 statement by Google's Vint Cerf, the so-called "Bundle protocols" have been uploaded to NASA's EPOXI mission spacecraft (which is in orbit around the Sun) and communication with Earth has been tested at a distance of approximately 80 light seconds.

As a globally distributed network of voluntarily interconnected autonomous networks, the Internet operates without a central governing body. It has no centralized governance for either technology or policies, and each constituent network chooses what technologies and protocols it will deploy from the voluntary technical standards that are developed by the Internet Engineering Task Force (IETF). However, throughout its entire history, the Internet system has had an "Internet Assigned Numbers Authority" (IANA) for the allocation and assignment of various technical identifiers needed for the operation of the Internet. The Internet Corporation for Assigned Names and Numbers (ICANN) provides oversight and coordination for two principal name spaces in the Internet, the Internet Protocol address space and the Domain Name System.

The IANA function was originally performed by USC Information Sciences Institute (ISI), and it delegated portions of this responsibility with respect to numeric network and autonomous system identifiers to the Network Information Center (NIC) at Stanford Research Institute (SRI International) in Menlo Park, California. ISI's Jonathan Postel managed the IANA, served as RFC Editor and performed other key roles until his premature death in 1998.

As the early ARPANET grew, hosts were referred to by names, and a HOSTS.TXT file would be distributed from SRI International to each host on the network. As the network grew, this became cumbersome. A technical solution came in the form of the Domain Name System, created by ISI's Paul Mockapetris in 1983. The Defense Data Network—Network Information Center (DDN-NIC) at SRI handled all registration services, including the top-level domains (TLDs) of .mil, .gov, .edu, .org, .net, .com and .us, root nameserver administration and Internet number assignments under a United States Department of Defense contract. In 1991, the Defense Information Systems Agency (DISA) awarded the administration and maintenance of DDN-NIC (managed by SRI up until this point) to Government Systems, Inc., who subcontracted it to the small private-sector Network Solutions, Inc.

The increasing cultural diversity of the Internet also posed administrative challenges for centralized management of the IP addresses. In October 1992, the Internet Engineering Task Force (IETF) published RFC 1366, which described the "growth of the Internet and its increasing globalization" and set out the basis for an evolution of the IP registry process, based on a regionally distributed registry model. This document stressed the need for a single Internet number registry to exist in each geographical region of the world (which would be of "continental dimensions"). Registries would be "unbiased and widely recognized by network providers and subscribers" within their region.
The RIPE Network Coordination Centre (RIPE NCC) was established as the first RIR in May 1992. The second RIR, the Asia Pacific Network Information Centre (APNIC), was established in Tokyo in 1993, as a pilot project of the Asia Pacific Networking Group.

Since at this point in history most of the growth on the Internet was coming from non-military sources, it was decided that the Department of Defense would no longer fund registration services outside of the .mil TLD. In 1993 the U.S. National Science Foundation, after a competitive bidding process in 1992, created the InterNIC to manage the allocations of addresses and management of the address databases, and awarded the contract to three organizations. Registration Services would be provided by Network Solutions; Directory and Database Services would be provided by AT&T; and Information Services would be provided by General Atomics.

Over time, after consultation with the IANA, the IETF, RIPE NCC, APNIC, and the Federal Networking Council (FNC), the decision was made to separate the management of domain names from the management of IP numbers. Following the examples of RIPE NCC and APNIC, it was recommended that management of IP address space then administered by the InterNIC should be under the control of those that use it, specifically the ISPs, end-user organizations, corporate entities, universities, and individuals. As a result, the American Registry for Internet Numbers (ARIN) was established as in December 1997, as an independent, not-for-profit corporation by direction of the National Science Foundation and became the third Regional Internet Registry.

In 1998, both the IANA and remaining DNS-related InterNIC functions were reorganized under the control of ICANN, a California non-profit corporation contracted by the United States Department of Commerce to manage a number of Internet-related tasks. As these tasks involved technical coordination for two principal Internet name spaces (DNS names and IP addresses) created by the IETF, ICANN also signed a memorandum of understanding with the IAB to define the technical work to be carried out by the Internet Assigned Numbers Authority. The management of Internet address space remained with the regional Internet registries, which collectively were defined as a supporting organization within the ICANN structure. ICANN provides central coordination for the DNS system, including policy coordination for the split registry / registrar system, with competition among registry service providers to serve each top-level-domain and multiple competing registrars offering DNS services to end-users.

The Internet Engineering Task Force (IETF) is the largest and most visible of several loosely related ad-hoc groups that provide technical direction for the Internet, including the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF).

The IETF is a loosely self-organized group of international volunteers who contribute to the engineering and evolution of Internet technologies. It is the principal body engaged in the development of new Internet standard specifications. Much of the work of the IETF is organized into "Working Groups". Standardization efforts of the Working Groups are often adopted by the Internet community, but the IETF does not control or patrol the Internet.

The IETF grew out of quarterly meeting of U.S. government-funded researchers, starting in January 1986. Non-government representatives were invited by the fourth IETF meeting in October 1986. The concept of Working Groups was introduced at the fifth meeting in February 1987. The seventh meeting in July 1987 was the first meeting with more than one hundred attendees. In 1992, the Internet Society, a professional membership society, was formed and IETF began to operate under it as an independent international standards body. The first IETF meeting outside of the United States was held in Amsterdam, The Netherlands, in July 1993. Today, the IETF meets three times per year and attendance has been as high as ca. 2,000 participants. Typically one in three IETF meetings are held in Europe or Asia. The number of non-US attendees is typically ca. 50%, even at meetings held in the United States.

The IETF is not a legal entity, has no governing board, no members, and no dues. The closest status resembling membership is being on an IETF or Working Group mailing list. IETF volunteers come from all over the world and from many different parts of the Internet community. The IETF works closely with and under the supervision of the Internet Engineering Steering Group (IESG) and the Internet Architecture Board (IAB). The Internet Research Task Force (IRTF) and the Internet Research Steering Group (IRSG), peer activities to the IETF and IESG under the general supervision of the IAB, focus on longer term research issues.

Request for Comments (RFCs) are the main documentation for the work of the IAB, IESG, IETF, and IRTF. RFC 1, "Host Software", was written by Steve Crocker at UCLA in April 1969, well before the IETF was created. Originally they were technical memos documenting aspects of ARPANET development and were edited by Jon Postel, the first RFC Editor.

RFCs cover a wide range of information from proposed standards, draft standards, full standards, best practices, experimental protocols, history, and other informational topics. RFCs can be written by individuals or informal groups of individuals, but many are the product of a more formal Working Group. Drafts are submitted to the IESG either by individuals or by the Working Group Chair. An RFC Editor, appointed by the IAB, separate from IANA, and working in conjunction with the IESG, receives drafts from the IESG and edits, formats, and publishes them. Once an RFC is published, it is never revised. If the standard it describes changes or its information becomes obsolete, the revised standard or updated information will be re-published as a new RFC that "obsoletes" the original.

The Internet Society (ISOC) is an international, nonprofit organization founded during 1992 "to assure the open development, evolution and use of the Internet for the benefit of all people throughout the world". With offices near Washington, DC, USA, and in Geneva, Switzerland, ISOC has a membership base comprising more than 80 organizational and more than 50,000 individual members. Members also form "chapters" based on either common geographical location or special interests. There are currently more than 90 chapters around the world.

ISOC provides financial and organizational support to and promotes the work of the standards settings bodies for which it is the organizational home: the Internet Engineering Task Force (IETF), the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF). ISOC also promotes understanding and appreciation of the Internet model of open, transparent processes and consensus-based decision-making.

Since the 1990s, the Internet's governance and organization has been of global importance to governments, commerce, civil society, and individuals. The organizations which held control of certain technical aspects of the Internet were the successors of the old ARPANET oversight and the current decision-makers in the day-to-day technical aspects of the network. While recognized as the administrators of certain aspects of the Internet, their roles and their decision-making authority are limited and subject to increasing international scrutiny and increasing objections. These objections have led to the ICANN removing themselves from relationships with first the University of Southern California in 2000, and in September 2009, gaining autonomy from the US government by the ending of its longstanding agreements, although some contractual obligations with the U.S. Department of Commerce continued. Finally, on October 1, 2016 ICANN ended its contract with the United States Department of Commerce National Telecommunications and Information Administration (NTIA), allowing oversight to pass to the global Internet community.

The IETF, with financial and organizational support from the Internet Society, continues to serve as the Internet's ad-hoc standards body and issues Request for Comments.

In November 2005, the World Summit on the Information Society, held in Tunis, called for an Internet Governance Forum (IGF) to be convened by United Nations Secretary General. The IGF opened an ongoing, non-binding conversation among stakeholders representing governments, the private sector, civil society, and the technical and academic communities about the future of Internet governance. The first IGF meeting was held in October/November 2006 with follow up meetings annually thereafter. Since WSIS, the term "Internet governance" has been broadened beyond narrow technical concerns to include a wider range of Internet-related policy issues.

Due to its prominence and immediacy as an effective means of mass communication, the Internet has also become more politicized as it has grown. This has led in turn, to discourses and activities that would once have taken place in other ways, migrating to being mediated by internet.

Examples include political activities such as public protest and canvassing of support and votes, but also –

On April 23, 2014, the Federal Communications Commission (FCC) was reported to be considering a new rule that would permit Internet service providers to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On May 15, 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U.S. Congress H. R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported that the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to "The New York Times".

On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept."

On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new "Net Neutrality" regulations.

On December 14, 2017, the F.C.C Repealed their March 12, 2015 decision by a 3-2 vote regarding net neutrality rules.

E-mail has often been called the killer application of the Internet. It predates the Internet, and was a crucial tool in creating it. Email started in 1965 as a way for multiple users of a time-sharing mainframe computer to communicate. Although the history is undocumented, among the first systems to have such a facility were the System Development Corporation (SDC) Q32 and the Compatible Time-Sharing System (CTSS) at MIT.

The ARPANET computer network made a large contribution to the evolution of electronic mail. An experimental inter-system transferred mail on the ARPANET shortly after its creation. In 1971 Ray Tomlinson created what was to become the standard Internet electronic mail addressing format, using the @ sign to separate mailbox names from host names.

A number of protocols were developed to deliver messages among groups of time-sharing computers over alternative transmission systems, such as UUCP and IBM's VNET email system. Email could be passed this way between a number of networks, including ARPANET, BITNET and NSFNET, as well as to hosts connected directly to other sites via UUCP. See the history of SMTP protocol.

In addition, UUCP allowed the publication of text files that could be read by many others. The News software developed by Steve Daniel and Tom Truscott in 1979 was used to distribute news and bulletin board-like messages. This quickly grew into discussion groups, known as newsgroups, on a wide range of topics. On ARPANET and NSFNET similar discussion groups would form via mailing lists, discussing both technical issues and more culturally focused topics (such as science fiction, discussed on the sflovers mailing list).

During the early years of the Internet, email and similar mechanisms were also fundamental to allow people to access resources that were not available due to the absence of online connectivity. UUCP was often used to distribute files using the 'alt.binary' groups. Also, FTP e-mail gateways allowed people that lived outside the US and Europe to download files using ftp commands written inside email messages. The file was encoded, broken in pieces and sent by email; the receiver had to reassemble and decode it later, and it was the only way for people living overseas to download items such as the earlier Linux versions using the slow dial-up connections available at the time. After the popularization of the Web and the HTTP protocol such tools were slowly abandoned.

As the Internet grew through the 1980s and early 1990s, many people realized the increasing need to be able to find and organize files and information. Projects such as Archie, Gopher, WAIS, and the FTP Archive list attempted to create ways to organize distributed data. In the early 1990s, Gopher, invented by Mark P. McCahill offered a viable alternative to the World Wide Web. However, in 1993 the World Wide Web saw many advances to indexing and ease of access through search engines, which often neglected Gopher and Gopherspace. As popularity increased through ease of use, investment incentives also grew until in the middle of 1994 the WWW's popularity gained the upper hand. Then it became clear that Gopher and the other projects were doomed fall short.

One of the most promising user interface paradigms during this period was hypertext. The technology had been inspired by Vannevar Bush's "Memex" and developed through Ted Nelson's research on Project Xanadu and Douglas Engelbart's research on NLS. Many small self-contained hypertext systems had been created before, such as Apple Computer's HyperCard (1987). Gopher became the first commonly used hypertext interface to the Internet. While Gopher menu items were examples of hypertext, they were not commonly perceived in that way.
In 1989, while working at CERN, Tim Berners-Lee invented a network-based implementation of the hypertext concept. By releasing his invention to public use, he ensured the technology would become widespread. For his work in developing the World Wide Web, Berners-Lee received the Millennium technology prize in 2004. One early popular web browser, modeled after HyperCard, was ViolaWWW.

A turning point for the World Wide Web began with the introduction of the Mosaic web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana–Champaign (NCSA-UIUC), led by Marc Andreessen. Funding for Mosaic came from the High-Performance Computing and Communications Initiative, a funding program initiated by the High Performance Computing and Communication Act of 1991, also known as the "Gore Bill". Mosaic's graphical interface soon became more popular than Gopher, which at the time was primarily text-based, and the WWW became the preferred interface for accessing the Internet. (Gore's reference to his role in "creating the Internet", however, was ridiculed in his presidential election campaign. See the full article Al Gore and information technology).

Mosaic was superseded in 1994 by Andreessen's Netscape Navigator, which replaced Mosaic as the world's most popular browser. While it held this title for some time, eventually competition from Internet Explorer and a variety of other browsers almost completely displaced it. Another important event held on January 11, 1994, was "The Superhighway Summit" at UCLA's Royce Hall. This was the "first public conference bringing together all of the major industry, government and academic leaders in the field [and] also began the national dialogue about the "Information Superhighway" and its implications."

"24 Hours in Cyberspace", "the largest one-day online event" (February 8, 1996) up to that date, took place on the then-active website, "cyber24.com." It was headed by photographer Rick Smolan. A photographic exhibition was unveiled at the Smithsonian Institution's National Museum of American History on January 23, 1997, featuring 70 photos from the project.

Even before the World Wide Web, there were search engines that attempted to organize the Internet. The first of these was the Archie search engine from McGill University in 1990, followed in 1991 by WAIS and Gopher. All three of those systems predated the invention of the World Wide Web but all continued to index the Web and the rest of the Internet for several years after the Web appeared. There are still Gopher servers as of 2006, although there are a great many more web servers.

As the Web grew, search engines and Web directories were created to track pages on the Web and allow people to find things. The first full-text Web search engine was WebCrawler in 1994. Before WebCrawler, only Web page titles were searched. Another early search engine, Lycos, was created in 1993 as a university project, and was the first to achieve commercial success. During the late 1990s, both Web directories and Web search engines were popular—Yahoo! (founded 1994) and Altavista (founded 1995) were the respective industry leaders. By August 2001, the directory model had begun to give way to search engines, tracking the rise of Google (founded 1998), which had developed new approaches to relevancy ranking. Directory features, while still commonly available, became after-thoughts to search engines.

Database size, which had been a significant marketing feature through the early 2000s, was similarly displaced by emphasis on relevancy ranking, the methods by which search engines attempt to sort the best results first. Relevancy ranking first became a major issue circa 1996, when it became apparent that it was impractical to review full lists of results. Consequently, algorithms for relevancy ranking have continuously improved. Google's PageRank method for ordering the results has received the most press, but all major search engines continually refine their ranking methodologies with a view toward improving the ordering of results. As of 2006, search engine rankings are more important than ever, so much so that an industry has developed ("search engine optimizers", or "SEO") to help web-developers improve their search ranking, and an entire body of case law has developed around matters that affect search engine rankings, such as use of trademarks in metatags. The sale of search rankings by some search engines has also created controversy among librarians and consumer advocates.

On June 3, 2009, Microsoft launched its new search engine, Bing. The following month Microsoft and Yahoo! announced a deal in which Bing would power Yahoo! Search.

Resource or file sharing has been an important activity on computer networks from well before the Internet was established and was supported in a variety of ways including bulletin board systems (1978), Usenet (1980), Kermit (1981), and many others. The File Transfer Protocol (FTP) for use on the Internet was standardized in 1985 and is still in use today. A variety of tools were developed to aid the use of FTP by helping users discover files they might want to transfer, including the Wide Area Information Server (WAIS) in 1991, Gopher in 1991, Archie in 1991, Veronica in 1992, Jughead in 1993, Internet Relay Chat (IRC) in 1988, and eventually the World Wide Web (WWW) in 1991 with Web directories and Web search engines.

In 1999, Napster became the first peer-to-peer file sharing system. Napster used a central server for indexing and peer discovery, but the storage and transfer of files was decentralized. A variety of peer-to-peer file sharing programs and services with different levels of decentralization and anonymity followed, including: Gnutella, eDonkey2000, and Freenet in 2000, FastTrack, Kazaa, Limewire, and BitTorrent in 2001, and Poisoned in 2003.

All of these tools are general purpose and can be used to share a wide variety of content, but sharing of music files, software, and later movies and videos are major uses. And while some of this sharing is legal, large portions are not. Lawsuits and other legal actions caused Napster in 2001, eDonkey2000 in 2005, Kazaa in 2006, and Limewire in 2010 to shut down or refocus their efforts. The Pirate Bay, founded in Sweden in 2003, continues despite a trial and appeal in 2009 and 2010 that resulted in jail terms and large fines for several of its founders. File sharing remains contentious and controversial with charges of theft of intellectual property on the one hand and charges of censorship on the other.

Suddenly the low price of reaching millions worldwide, and the possibility of selling to or hearing from those people at the same moment when they were reached, promised to overturn established business dogma in advertising, mail-order sales, customer relationship management, and many more areas. The web was a new killer app—it could bring together unrelated buyers and sellers in seamless and low-cost ways. Entrepreneurs around the world developed new business models, and ran to their nearest venture capitalist. While some of the new entrepreneurs had experience in business and economics, the majority were simply people with ideas, and did not manage the capital influx prudently. Additionally, many dot-com business plans were predicated on the assumption that by using the Internet, they would bypass the distribution channels of existing businesses and therefore not have to compete with them; when the established businesses with strong existing brands developed their own Internet presence, these hopes were shattered, and the newcomers were left attempting to break into markets dominated by larger, more established businesses. Many did not have the ability to do so.

The dot-com bubble burst in March 2000, with the technology heavy NASDAQ Composite index peaking at 5,048.62 on March 10 (5,132.52 intraday), more than double its value just a year before. By 2001, the bubble's deflation was running full speed. A majority of the dot-coms had ceased trading, after having burnt through their venture capital and IPO capital, often without ever making a profit. But despite this, the Internet continues to grow, driven by commerce, ever greater amounts of online information and knowledge and social networking.

The first mobile phone with Internet connectivity was the Nokia 9000 Communicator, launched in Finland in 1996. The viability of Internet services access on mobile phones was limited until prices came down from that model, and network providers started to develop systems and services conveniently accessible on phones. NTT DoCoMo in Japan launched the first mobile Internet service, i-mode, in 1999 and this is considered the birth of the mobile phone Internet services. In 2001, the mobile phone email system by Research in Motion (now BlackBerry Limited) for their BlackBerry product was launched in America. To make efficient use of the small screen and tiny keypad and one-handed operation typical of mobile phones, a specific document and networking model was created for mobile devices, the Wireless Application Protocol (WAP). Most mobile device Internet services operate using WAP. The growth of mobile phone services was initially a primarily Asian phenomenon with Japan, South Korea and Taiwan all soon finding the majority of their Internet users accessing resources by phone rather than by PC. Developing countries followed, with India, South Africa, Kenya, the Philippines, and Pakistan all reporting that the majority of their domestic users accessed the Internet from a mobile phone rather than a PC. The European and North American use of the Internet was influenced by a large installed base of personal computers, and the growth of mobile phone Internet access was more gradual, but had reached national penetration levels of 20–30% in most Western countries. The cross-over occurred in 2008, when more Internet access devices were mobile phones than personal computers. In many parts of the developing world, the ratio is as much as 10 mobile phone users to one PC user.

Web pages were initially conceived as structured documents based upon Hypertext Markup Language (HTML) which can allow access to images, video, and other content. Hyperlinks in the page permit users to navigate to other pages. In the earliest browsers, images opened in a separate "helper" application. Marc Andreessen's 1993 Mosaic and 1994 Netscape introduced mixed text and images for non-technical users. HTML evolved during the 1990s, leading to HTML 4 which introduced large elements of CSS styling and, later, extensions to allow browser code to make calls and ask for content from servers in a structured way (AJAX).

There are nearly insurmountable problems in supplying a historiography of the Internet's development. The process of digitization represents a twofold challenge both for historiography in general and, in particular, for historical communication research. A sense of the difficulty in documenting early developments that led to the internet can be gathered from the quote:




</doc>
<doc id="13693" url="https://en.wikipedia.org/wiki?curid=13693" title="Horace">
Horace

Quintus Horatius Flaccus (8 December 65 BC – 27 November 8 BC), known in the English-speaking world as Horace (), was the leading Roman lyric poet during the time of Augustus (also known as Octavian). The rhetorician Quintilian regarded his "Odes" as just about the only Latin lyrics worth reading: "He can be lofty sometimes, yet he is also full of charm and grace, versatile in his figures, and felicitously daring in his choice of words."

Horace also crafted elegant hexameter verses ("Satires" and "Epistles") and caustic iambic poetry ("Epodes"). The hexameters are amusing yet serious works, friendly in tone, leading the ancient satirist Persius to comment: "as his friend laughs, Horace slyly puts his finger on his every fault; once let in, he plays about the heartstrings".

His career coincided with Rome's momentous change from a republic to an empire. An officer in the republican army defeated at the Battle of Philippi in 42 BC, he was befriended by Octavian's right-hand man in civil affairs, Maecenas, and became a spokesman for the new regime. For some commentators, his association with the regime was a delicate balance in which he maintained a strong measure of independence (he was "a master of the graceful sidestep") but for others he was, in John Dryden's phrase, "a well-mannered court slave".

Horace can be regarded as the world's first autobiographer – In his writings, he tells us far more about himself, his character, his development, and his way of life than any other great poet in antiquity. Some of the biographical writings contained in his writings can be supplemented from the short but valuable "Life of Horace" by Suetonius (in his "Lives of the Poets").

He was born on 8 December 65 BC in the Samnite south of Italy. His home town, Venusia, lay on a trade route in the border region between Apulia and Lucania (Basilicata). Various Italic dialects were spoken in the area and this perhaps enriched his feeling for language. He could have been familiar with Greek words even as a young boy and later he poked fun at the jargon of mixed Greek and Oscan spoken in neighbouring Canusium. One of the works he probably studied in school was the "Odyssia" of Livius Andronicus, taught by teachers like the 'Orbilius' mentioned in one of his poems. Army veterans could have been settled there at the expense of local families uprooted by Rome as punishment for their part in the Social War (91–88 BC). Such state-sponsored migration must have added still more linguistic variety to the area. According to a local tradition reported by Horace, a colony of Romans or Latins had been installed in Venusia after the Samnites had been driven out early in the third century. In that case, young Horace could have felt himself to be a Roman though there are also indications that he regarded himself as a Samnite or Sabellus by birth. Italians in modern and ancient times have always been devoted to their home towns, even after success in the wider world, and Horace was no different. Images of his childhood setting and references to it are found throughout his poems.

Horace's father was probably a Venutian taken captive by Romans in the Social War, or possibly he was descended from a Sabine captured in the Samnite Wars. Either way, he was a slave for at least part of his life. He was evidently a man of strong abilities however and managed to gain his freedom and improve his social position. Thus Horace claimed to be the free-born son of a prosperous 'coactor'. The term 'coactor' could denote various roles, such as tax collector, but its use by Horace was explained by scholia as a reference to 'coactor argentareus' i.e. an auctioneer with some of the functions of a banker, paying the seller out of his own funds and later recovering the sum with interest from the buyer.

The father spent a small fortune on his son's education, eventually accompanying him to Rome to oversee his schooling and moral development. The poet later paid tribute to him in a poem that one modern scholar considers the best memorial by any son to his father. The poem includes this passage:
If my character is flawed by a few minor faults, but is otherwise decent and moral, if you can point out only a few scattered blemishes on an otherwise immaculate surface, if no one can accuse me of greed, or of prurience, or of profligacy, if I live a virtuous life, free of defilement (pardon, for a moment, my self-praise), and if I am to my friends a good friend, my father deserves all the credit... As it is now, he deserves from me unstinting gratitude and praise. I could never be ashamed of such a father, nor do I feel any need, as many people do, to apologize for being a freedman's son. "Satires 1.6.65–92"
He never mentioned his mother in his verses and he might not have known much about her. Perhaps she also had been a slave.

Horace left Rome, possibly after his father's death, and continued his formal education in Athens, a great centre of learning in the ancient world, where he arrived at nineteen years of age, enrolling in The Academy. Founded by Plato, The Academy was now dominated by Epicureans and Stoics, whose theories and practises made a deep impression on the young man from Venusia. Meanwhile, he mixed and lounged about with the elite of Roman youth, such as Marcus, the idle son of Cicero, and the Pompeius to whom he later addressed a poem. It was in Athens too that he probably acquired deep familiarity with the ancient tradition of Greek lyric poetry, at that time largely the preserve of grammarians and academic specialists (access to such material was easier in Athens than in Rome, where the public libraries had yet to be built by Asinius Pollio and Augustus).

Rome's troubles following the assassination of Julius Caesar were soon to catch up with him. Marcus Junius Brutus came to Athens seeking support for the republican cause. Brutus was fêted around town in grand receptions and he made a point of attending academic lectures, all the while recruiting supporters among the young men studying there, including Horace. An educated young Roman could begin military service high in the ranks and Horace was made tribunus militum (one of six senior officers of a typical legion), a post usually reserved for men of senatorial or equestrian rank and which seems to have inspired jealousy among his well-born confederates. He learned the basics of military life while on the march, particularly in the wilds of northern Greece, whose rugged scenery became a backdrop to some of his later poems. It was there in 42 BC that Octavian (later Augustus) and his associate Mark Antony crushed the republican forces at the Battle of Philippi. Horace later recorded it as a day of embarrassment for himself, when he fled without his shield, but allowance should be made for his self-deprecating humour. Moreover, the incident allowed him to identify himself with some famous poets who had long ago abandoned their shields in battle, notably his heroes Alcaeus and Archilochus. The comparison with the latter poet is uncanny: Archilochus lost his shield in a part of Thrace near Philippi, and he was deeply involved in the Greek colonization of Thasos, where Horace's die-hard comrades finally surrendered.

Octavian offered an early amnesty to his opponents and Horace quickly accepted it. On returning to Italy, he was confronted with yet another loss: his father's estate in Venusia was one of many throughout Italy to be confiscated for the settlement of veterans (Virgil lost his estate in the north about the same time). Horace later claimed that he was reduced to poverty and this led him to try his hand at poetry. In reality, there was no money to be had from versifying. At best, it offered future prospects through contacts with other poets and their patrons among the rich. Meanwhile, he obtained the sinecure of "scriba quaestorius", a civil service position at the "aerarium" or Treasury, profitable enough to be purchased even by members of the "ordo equester" and not very demanding in its work-load, since tasks could be delegated to "scribae" or permanent clerks. It was about this time that he began writing his "Satires" and "Epodes".

The "Epodes" belong to iambic poetry. Iambic poetry features insulting and obscene language; sometimes, it is referred to as "blame poetry". "Blame poetry", or "shame poetry", is poetry written to blame and shame fellow citizens into a sense of their social obligations. Horace modelled these poems on the poetry of Archilochus. Social bonds in Rome had been decaying since the destruction of Carthage a little more than a hundred years earlier, due to the vast wealth that could be gained by plunder and corruption. These social ills were magnified by rivalry between Julius Caesar, Mark Antony and confederates like Sextus Pompey, all jockeying for a bigger share of the spoils. One modern scholar has counted a dozen civil wars in the hundred years leading up to 31 BC, including the Spartacus rebellion, eight years before Horace's birth. As the heirs to Hellenistic culture, Horace and his fellow Romans were not well prepared to deal with these problems:

Horace's Hellenistic background is clear in his Satires, even though the genre was unique to Latin literature. He brought to it a style and outlook suited to the social and ethical issues confronting Rome but he changed its role from public, social engagement to private meditation. Meanwhile, he was beginning to interest Octavian's supporters, a gradual process described by him in one of his satires. The way was opened for him by his friend, the poet Virgil, who had gained admission into the privileged circle around Maecenas, Octavian's lieutenant, following the success of his "Eclogues". An introduction soon followed and, after a discreet interval, Horace too was accepted. He depicted the process as an honourable one, based on merit and mutual respect, eventually leading to true friendship, and there is reason to believe that his relationship was genuinely friendly, not just with Maecenas but afterwards with Augustus as well. On the other hand, the poet has been unsympathetically described by one scholar as "a sharp and rising young man, with an eye to the main chance." There were advantages on both sides: Horace gained encouragement and material support, the politicians gained a hold on a potential dissident. His republican sympathies, and his role at Philippi, may have caused him some pangs of remorse over his new status. However most Romans considered the civil wars to be the result of "contentio dignitatis", or rivalry between the foremost families of the city, and he too seems to have accepted the principate as Rome's last hope for much needed peace.

In 37 BC, Horace accompanied Maecenas on a journey to Brundisium, described in one of his poems as a series of amusing incidents and charming encounters with other friends along the way, such as Virgil. In fact the journey was political in its motivation, with Maecenas en route to negotiatie the Treaty of Tarentum with Antony, a fact Horace artfully keeps from the reader (political issues are largely avoided in the first book of satires). Horace was probably also with Maecenas on one of Octavian's naval expeditions against the piratical Sextus Pompeius, which ended in a disastrous storm off Palinurus in 36 BC, briefly alluded to by Horace in terms of near-drowning. There are also some indications in his verses that he was with Maecenas at the Battle of Actium in 31 BC, where Octavian defeated his great rival, Antony. By then Horace had already received from Maecenas the famous gift of his Sabine farm, probably not long after the publication of the first book of "Satires". The gift, which included income from five tenants, may have ended his career at the Treasury, or at least allowed him to give it less time and energy. It signalled his identification with the Octavian regime yet, in the second book of "Satires" that soon followed, he continued the apolitical stance of the first book. By this time, he had attained the status of "eques Romanus", perhaps as a result of his work at the Treasury.

"Odes" 1–3 were the next focus for his artistic creativity. He adapted their forms and themes from Greek lyric poetry of the seventh and sixth centuries BC. The fragmented nature of the Greek world had enabled his literary heroes to express themselves freely and his semi-retirement from the Treasury in Rome to his own estate in the Sabine hills perhaps empowered him to some extent also yet even when his lyrics touched on public affairs they reinforced the importance of private life. Nevertheless, his work in the period 30–27 BC began to show his closeness to the regime and his sensitivity to its developing ideology. In "Odes" 1.2, for example, he eulogized Octavian in hyperboles that echo Hellenistic court poetry. The name "Augustus", which Octavian assumed in January 27 BC, is first attested in "Odes" 3.3 and 3.5. In the period 27–24 BC, political allusions in the "Odes" concentrated on foreign wars in Britain (1.35), Arabia (1.29) Spain (3.8) and Parthia (2.2). He greeted Augustus on his return to Rome in 24 BC as a beloved ruler upon whose good health he depended for his own happiness (3.14).

The public reception of "Odes" 1–3 disappointed him, however. He attributed the lack of success to jealousy among imperial courtiers and to his isolation from literary cliques. Perhaps it was disappointment that led him to put aside the genre in favour of verse letters. He addressed his first book of "Epistles" to a variety of friends and acquaintances in an urbane style reflecting his new social status as a knight. In the opening poem, he professed a deeper interest in moral philosophy than poetry but, though the collection demonstrates a leaning towards stoic theory, it reveals no sustained thinking about ethics. Maecenas was still the dominant confidante but Horace had now begun to assert his own independence, suavely declining constant invitations to attend his patron. In the final poem of the first book of "Epistles", he revealed himself to be forty-four years old in the consulship of Lollius and Lepidus i.e. 21 BC, and "of small stature, fond of the sun, prematurely grey, quick-tempered but easily placated".

According to Suetonius, the second book of "Epistles" was prompted by Augustus, who desired a verse epistle to be addressed to himself. Augustus was in fact a prolific letter-writer and he once asked Horace to be his personal secretary. Horace refused the secretarial role but complied with the emperor's request for a verse letter. The letter to Augustus may have been slow in coming, being published possibly as late as 11 BC. It celebrated, among other things, the 15 BC military victories of his stepsons, Drusus and Tiberius, yet it and the following letter were largely devoted to literary theory and criticism. The literary theme was explored still further in "Ars Poetica", published separately but written in the form of an epistle and sometimes referred to as "Epistles" 2.3 (possibly the last poem he ever wrote). He was also commissioned to write odes commemorating the victories of Drusus and Tiberius and one to be sung in a temple of Apollo for the Secular Games, a long-abandoned festival that Augustus revived in accordance with his policy of recreating ancient customs ("Carmen Saeculare").

Suetonius recorded some gossip about Horace's sexual activities late in life, claiming that the walls of his bedchamber were covered with obscene pictures and mirrors, so that he saw erotica wherever he looked. The poet died at 56 years of age, not long after his friend Maecenas, near whose tomb he was laid to rest. Both men bequeathed their property to Augustus, an honour that the emperor expected of his friends.

The dating of Horace's works isn't known precisely and scholars often debate the exact order in which they were first 'published'. There are persuasive arguments for the following chronology:

Horace composed in traditional metres borrowed from Archaic Greece, employing hexameters in his "Satires" and "Epistles", and iambs in his "Epodes", all of which were relatively easy to adapt into Latin forms. His "Odes" featured more complex measures, including alcaics and sapphics, which were sometimes a difficult fit for Latin structure and syntax. Despite these traditional metres, he presented himself as a partisan in the development of a new and sophisticated style. He was influenced in particular by Hellenistic aesthetics of brevity, elegance and polish, as modelled in the work of Callimachus.
In modern literary theory, a distinction is often made between immediate personal experience ("Urerlebnis") and experience mediated by cultural vectors such as literature, philosophy and the visual arts ("Bildungserlebnis"). The distinction has little relevance for Horace however since his personal and literary experiences are implicated in each other. "Satires" 1.5, for example, recounts in detail a real trip Horace made with Virgil and some of his other literary friends, and which parallels a Satire by Lucilius, his predecessor. Unlike much Hellenistic-inspired literature, however, his poetry was not composed for a small coterie of admirers and fellow poets, nor does it rely on abstruse allusions for many of its effects. Though elitist in its literary standards, it was written for a wide audience, as a public form of art. Ambivalence also characterizes his literary persona, since his presentation of himself as part of a small community of philosophically aware people, seeking true peace of mind while shunning vices like greed, was well adapted to Augustus's plans to reform public morality, corrupted by greedhis personal plea for moderation was part of the emperor's grand message to the nation.

Horace generally followed the examples of poets established as classics in different genres, such as Archilochus in the "Epodes", Lucilius in the "Satires" and Alcaeus in the "Odes", later broadening his scope for the sake of variation and because his models weren't actually suited to the realities confronting him. Archilochus and Alcaeus were aristocratic Greeks whose poetry had a social and religious function that was immediately intelligible to their audiences but which became a mere artifice or literary motif when transposed to Rome. However, the artifice of the "Odes" is also integral to their success, since they could now accommodate a wide range of emotional effects, and the blend of Greek and Roman elements adds a sense of detachment and universality. Horace proudly claimed to introduce into Latin the spirit and iambic poetry of Archilochus but (unlike Archilochus) without persecuting anyone ("Epistles" 1.19.23–5). It was no idle boast. His "Epodes" were modelled on the verses of the Greek poet, as 'blame poetry', yet he avoided targeting real scapegoats. Whereas Archilochus presented himself as a serious and vigorous opponent of wrong-doers, Horace aimed for comic effects and adopted the persona of a weak and ineffectual critic of his times (as symbolized for example in his surrender to the witch Canidia in the final epode). He also claimed to be the first to introduce into Latin the lyrical methods of Alcaeus ("Epistles" 1.19.32–3) and he actually was the first Latin poet to make consistent use of Alcaic meters and themes: love, politics and the symposium. He imitated other Greek lyric poets as well, employing a 'motto' technique, beginning each ode with some reference to a Greek original and then diverging from it.

The satirical poet Lucilius was a senator's son who could castigate his peers with impunity. Horace was a mere freedman's son who had to tread carefully. Lucilius was a rugged patriot and a significant voice in Roman self-awareness, endearing himself to his countrymen by his blunt frankness and explicit politics. His work expressed genuine freedom or libertas. His style included 'metrical vandalism' and looseness of structure. Horace instead adopted an oblique and ironic style of satire, ridiculing stock characters and anonymous targets. His libertas was the private freedom of a philosophical outlook, not a political or social privilege. His "Satires" are relatively easy-going in their use of meter (relative to the tight lyric meters of the "Odes") but formal and highly controlled relative to the poems of Lucilius, whom Horace mocked for his sloppy standards ("Satires" 1.10.56–61)

The "Epistles" may be considered among Horace's most innovative works. There was nothing like it in Greek or Roman literature. Occasionally poems had had some resemblance to letters, including an elegiac poem from Solon to Mimnermus and some lyrical poems from Pindar to Hieron of Syracuse. Lucilius had composed a satire in the form of a letter, and some epistolary poems were composed by Catullus and Propertius. But nobody before Horace had ever composed an entire collection of verse letters, let alone letters with a focus on philosophical problems. The sophisticated and flexible style that he had developed in his "Satires" was adapted to the more serious needs of this new genre. Such refinement of style was not unusual for Horace. His craftsmanship as a wordsmith is apparent even in his earliest attempts at this or that kind of poetry, but his handling of each genre tended to improve over time as he adapted it to his own needs. Thus for example it is generally agreed that his second book of "Satires", where human folly is revealed through dialogue between characters, is superior to the first, where he propounds his ethics in monologues. Nevertheless, the first book includes some of his most popular poems.

Horace developed a number of inter-related themes throughout his poetic career, including politics, love, philosophy and ethics, his own social role, as well as poetry itself. His "Epodes" and "Satires" are forms of 'blame poetry' and both have a natural affinity with the moralising and diatribes of Cynicism. This often takes the form of allusions to the work and philosophy of Bion of Borysthenes but it is as much a literary game as a philosophical alignment. By the time he composed his "Epistles", he was a critic of Cynicism along with all impractical and "high-falutin" philosophy in general. The "Satires" also include a strong element of Epicureanism, with frequent allusions to the Epicurean poet Lucretius. So for example the Epicurean sentiment "carpe diem" is the inspiration behind Horace's repeated punning on his own name ("Horatius ~ hora") in "Satires" 2.6. The "Satires" also feature some Stoic, Peripatetic and Platonic ("Dialogues") elements. In short, the "Satires" present a medley of philosophical programs, dished up in no particular ordera style of argument typical of the genre. The "Odes" display a wide range of topics. Over time, he becomes more confident about his political voice. Although he is often thought of as an overly intellectual lover, he is ingenious in representing passion. The "Odes" weave various philosophical strands together, with allusions and statements of doctrine present in about a third of the "Odes" Books 1–3, ranging from the flippant (1.22, 3.28) to the solemn (2.10, 3.2, 3.3). Epicureanism is the dominant influence, characterizing about twice as many of these odes as Stoicism. A group of odes combines these two influences in tense relationships, such as "Odes" 1.7, praising Stoic virility and devotion to public duty while also advocating private pleasures among friends. While generally favouring the Epicurean lifestyle, the lyric poet is as eclectic as the satiric poet, and in "Odes" 2.10 even proposes Aristotle's golden mean as a remedy for Rome's political troubles. Many of Horace's poems also contain much reflection on genre, the lyric tradition, and the function of poetry. "Odes" 4, thought to be composed at the emperor's request, takes the themes of the first three books of "Odes" to a new level. This book shows greater poetic confidence after the public performance of his "Carmen saeculare" or "Century hymn" at a public festival orchestrated by Augustus. In it, Horace addresses the emperor Augustus directly with more confidence and proclaims his power to grant poetic immortality to those he praises. It is the least philosophical collection of his verses, excepting the twelfth ode, addressed to the dead Virgil as if he were living. In that ode, the epic poet and the lyric poet are aligned with Stoicism and Epicureanism respectively, in a mood of bitter-sweet pathos. The first poem of the "Epistles" sets the philosophical tone for the rest of the collection: "So now I put aside both verses and all those other games: What is true and what befits is my care, this my question, this my whole concern." His poetic renunciation of poetry in favour of philosophy is intended to be ambiguous. Ambiguity is the hallmark of the "Epistles". It is uncertain if those being addressed by the self-mocking poet-philosopher are being honoured or criticized. Though he emerges as an Epicurean, it is on the understanding that philosophical preferences, like political and social choices, are a matter of personal taste. Thus he depicts the ups and downs of the philosophical life more realistically than do most philosophers.

The reception of Horace's work has varied from one epoch to another and varied markedly even in his own lifetime. "Odes" 1–3 were not well received when first 'published' in Rome, yet Augustus later commissioned a ceremonial ode for the Centennial Games in 17 BC and also encouraged the publication of "Odes" 4, after which Horace's reputation as Rome's premier lyricist was assured. His Odes were to become the best received of all his poems in ancient times, acquiring a classic status that discouraged imitation: no other poet produced a comparable body of lyrics in the four centuries that followed (though that might also be attributed to social causes, particularly the parasitism that Italy was sinking into). In the seventeenth and eighteenth centuries, ode-writing became highly fashionable in England and a large number of aspiring poets imitated Horace both in English and in Latin.

In a verse epistle to Augustus (Epistle 2.1), in 12 BC, Horace argued for classic status to be awarded to contemporary poets, including Virgil and apparently himself. In the final poem of his third book of Odes he claimed to have created for himself a monument more durable than bronze ("Exegi monumentum aere perennius", "Carmina" 3.30.1). For one modern scholar, however, Horace's personal qualities are more notable than the monumental quality of his achievement:
Yet for men like Wilfred Owen, scarred by experiences of World War I, his poetry stood for discredited values:

The same motto, "Dulce et decorum est pro patria mori", had been adapted to the ethos of martyrdom in the lyrics of early Christian poets like Prudentius.

These preliminary comments touch on a small sample of developments in the reception of Horace's work. More developments are covered epoch by epoch in the following sections.

Horace's influence can be observed in the work of his near contemporaries, Ovid and Propertius. Ovid followed his example in creating a completely natural style of expression in hexameter verse, and Propertius cheekily mimicked him in his third book of elegies. His "Epistles" provided them both with a model for their own verse letters and it also shaped Ovid's exile poetry.

His influence had a perverse aspect. As mentioned before, the brilliance of his "Odes" may have discouraged imitation. Conversely, they may have created a vogue for the lyrics of the archaic Greek poet Pindar, due to the fact that Horace had neglected that style of lyric (see Pindar#Influence and legacy). The iambic genre seems almost to have disappeared after publication of Horace's "Epodes". Ovid's "Ibis" was a rare attempt at the form but it was inspired mainly by Callimachus, and there are some iambic elements in Martial but the main influence there was Catullus. A revival of popular interest in the satires of Lucilius may have been inspired by Horace's criticism of his unpolished style. Both Horace and Lucilius were considered good role-models by Persius, who critiqued his own satires as lacking both the acerbity of Lucillius and the gentler touch of Horace. Juvenal's caustic satire was influenced mainly by Lucilius but Horace by then was a school classic and Juvenal could refer to him respectfully and in a round-about way as ""the Venusine lamp"".

Statius paid homage to Horace by composing one poem in Sapphic and one in Alcaic meter (the verse forms most often associated with "Odes"), which he included in his collection of occasional poems, "Silvae". Ancient scholars wrote commentaries on the lyric meters of the "Odes", including the scholarly poet Caesius Bassus. By a process called "derivatio", he varied established meters through the addition or omission of syllables, a technique borrowed by Seneca the Younger when adapting Horatian meters to the stage.

Horace's poems continued to be school texts into late antiquity. Works attributed to Helenius Acro and Pomponius Porphyrio are the remnants of a much larger body of Horatian scholarship. Porphyrio arranged the poems in non-chronological order, beginning with the "Odes", because of their general popularity and their appeal to scholars (the "Odes" were to retain this privileged position in the medieval manuscript tradition and thus in modern editions also). Horace was often evoked by poets of the fourth century, such as Ausonius and Claudian. Prudentius presented himself as a Christian Horace, adapting Horatian meters to his own poetry and giving Horatian motifs a Christian tone. On the other hand, St Jerome, modelled an uncompromising response to the pagan Horace, observing: ""What harmony can there be between Christ and the Devil? What has Horace to do with the Psalter?"" By the early sixth century, Horace and Prudentius were both part of a classical heritage that was struggling to survive the disorder of the times. Boethius, the last major author of classical Latin literature, could still take inspiration from Horace, sometimes mediated by Senecan tragedy. It can be argued that Horace's influence extended beyond poetry to dignify core themes and values of the early Christian era, such as self-sufficiency, inner contentment and courage.

Classical texts almost ceased being copied in the period between the mid sixth century and the Carolingian revival. Horace's work probably survived in just two or three books imported into northern Europe from Italy. These became the ancestors of six extant manuscripts dated to the ninth century. Two of those six manuscripts are French in origin, one was produced in Alsace, and the other three show Irish influence but were probably written in continental monasteries (Lombardy for example). By the last half of the ninth century, it was not uncommon for literate people to have direct experience of Horace's poetry. His influence on the Carolingian Renaissance can be found in the poems of Heiric of Auxerre and in some manuscripts marked with neumes, mysterious notations that may have been an aid to the memorization and discussion of his lyric meters. "Ode" is neumed with the melody of a hymn to John the Baptist, "Ut queant laxis", composed in Sapphic stanzas. This hymn later became the basis of the solfege system ("Do, re, mi...")an association with western music quite appropriate for a lyric poet like Horace, though the language of the hymn is mainly Prudentian. Lyons argues that the melody in question was linked with Horace's Ode well before Guido d'Arezzo fitted Ut queant laxis to it. However, the melody is unlikely to be a survivor from classical times, although Ovid testifies to Horace's use of the lyre while performing his Odes.

The German scholar, Ludwig Traube, once dubbed the tenth and eleventh centuries "The age of Horace" ("aetas Horatiana"), and placed it between the "aetas Vergiliana" of the eighth and ninth centuries, and the "aetas Ovidiana" of the twelfth and thirteenth centuries, a distinction supposed to reflect the dominant classical Latin influences of those times. Such a distinction is over-schematized since Horace was a substantial influence in the ninth century as well. Traube had focused too much on Horace's "Satires". Almost all of Horace's work found favour in the Medieval period. In fact medieval scholars were also guilty of over-schematism, associating Horace's different genres with the different ages of man. A twelfth-century scholar encapsulated the theory: "...Horace wrote four different kinds of poems on account of the four ages, the "Odes" for boys, the "Ars Poetica" for young men, the "Satires" for mature men, the "Epistles" for old and complete men." It was even thought that Horace had composed his works in the order in which they had been placed by ancient scholars. Despite its naivety, the schematism involved an appreciation of Horace's works as a collection, the "Ars Poetica", "Satires" and "Epistles" appearing to find favour as well as the "Odes". The later Middle Ages however gave special significance to "Satires" and "Epistles", being considered Horace's mature works. Dante referred to Horace as "Orazio satiro", and he awarded him a privileged position in the first circle of Hell, with Homer, Ovid and Lucan.

Horace's popularity is revealed in the large number of quotes from all his works found in almost every genre of medieval literature, and also in the number of poets imitating him in quantitative Latin meter . The most prolific imitator of his "Odes" was the Bavarian monk, Metellus of Tegernsee, who dedicated his work to the patron saint of Tegernsee Abbey, St Quirinus, around the year 1170. He imitated all Horace's lyrical meters then followed these up with imitations of other meters used by Prudentius and Boethius, indicating that variety, as first modelled by Horace, was considered a fundamental aspect of the lyric genre. The content of his poems however was restricted to simple piety. Among the most successful imitators of "Satires" and "Epistles" was another Germanic author, calling himself Sextus Amarcius, around 1100, who composed four books, the first two exemplifying vices, the second pair mainly virtues.

Petrarch is a key figure in the imitation of Horace in accentual meters. His verse letters in Latin were modelled on the "Epistles" and he wrote a letter to Horace in the form of an ode. However he also borrowed from Horace when composing his Italian sonnets. One modern scholar has speculated that authors who imitated Horace in accentual rhythms (including stressed Latin and vernacular languages) may have considered their work a natural sequel to Horace's metrical variety. In France, Horace and Pindar were the poetic models for a group of vernacular authors called the Pléiade, including for example Pierre de Ronsard and Joachim du Bellay. Montaigne made constant and inventive use of Horatian quotes. The vernacular languages were dominant in Spain and Portugal in the sixteenth century, where Horace's influence is notable in the works of such authors as Garcilaso de la Vega, Juan Boscán Sá de Miranda, Antonio Ferreira and Fray Luis de León, the latter for example writing odes on the Horatian theme "beatus ille" ("happy the man"). The sixteenth century in western Europe was also an age of translations (except in Germany, where Horace wasn't translated until well into the seventeenth century). The first English translator was Thomas Drant, who placed translations of Jeremiah and Horace side by side in "Medicinable Morall", 1566. That was also the year that the Scot George Buchanan paraphrased the Psalms in a Horatian setting. Ben Jonson put Horace on the stage in 1601 in "Poetaster", along with other classical Latin authors, giving them all their own verses to speak in translation. Horace's part evinces the independent spirit, moral earnestness and critical insight that many readers look for in his poems.

During the seventeenth and eighteenth centuries, or the Age of Enlightenment, neo-classical culture was pervasive. English literature in the middle of that period has been dubbed Augustan. It is not always easy to distinguish Horace's influence during those centuries (the mixing of influences is shown for example in one poet's pseudonym, "Horace Juvenal"). However a measure of his influence can be found in the diversity of the people interested in his works, both among readers and authors.

New editions of his works were published almost yearly. There were three new editions in 1612 (two in Leiden, one in Frankfurt) and again in 1699 (Utrecht, Barcelona, Cambridge). Cheap editions were plentiful and fine editions were also produced, including one whose entire text was engraved by John Pine in copperplate. The poet James Thomson owned five editions of Horace's work and the physician James Douglas had five hundred books with Horace-related titles. Horace was often commended in periodicals such as The Spectator, as a hallmark of good judgement, moderation and manliness, a focus for moralising. His verses offered a fund of mottoes, such as "simplex munditiis", (elegance in simplicity) "splendide mendax" (nobly untruthful.), "sapere aude", "nunc est bibendum", "carpe diem" (the latter perhaps being the only one still in common use today), quoted even in works as prosaic as Edmund Quincy's "A treatise of hemp-husbandry" (1765). The fictional hero Tom Jones recited his verses with feeling. His works were also used to justify commonplace themes, such as patriotic obedience, as in James Parry's English lines from an Oxford University collection in 1736:

Horatian-style lyrics were increasingly typical of Oxford and Cambridge verse collections for this period, most of them in Latin but some like the previous ode in English. John Milton's Lycidas first appeared in such a collection. It has few Horatian echoes yet Milton's associations with Horace were lifelong. He composed a controversial version of "Odes" 1.5, and Paradise Lost includes references to Horace's 'Roman' "Odes" 3.1–6 (Book 7 for example begins with echoes of "Odes" 3.4). Yet Horace's lyrics could offer inspiration to libertines as well as moralists, and neo-Latin sometimes served as a kind of discrete veil for the risqué. Thus for example Benjamin Loveling authored a catalogue of Drury Lane and Covent Garden prostitutes, in Sapphic stanzas, and an encomium for a dying lady "of salacious memory". Some Latin imitations of Horace were politically subversive, such as a marriage ode by Anthony Alsop that included a rallying cry for the Jacobite cause. On the other hand, Andrew Marvell took inspiration from Horace's "Odes" 1.37 to compose his English masterpiece Horatian Ode upon Cromwell's Return from Ireland, in which subtly nuanced reflections on the execution of Charles I echo Horace's ambiguous response to the death of Cleopatra (Marvell's ode was suppressed in spite of its subtlety and only began to be widely published in 1776). Samuel Johnson took particular pleasure in reading "The Odes". Alexander Pope wrote direct "Imitations" of Horace (published with the original Latin alongside) and also echoed him in "Essays" and The Rape of the Lock. He even emerged as "a quite Horatian Homer" in his translation of the "Iliad". Horace appealed also to female poets, such as Anna Seward ("Original sonnets on various subjects, and odes paraphrased from Horace", 1799) and Elizabeth Tollet, who composed a Latin ode in Sapphic meter to celebrate her brother's return from overseas, with tea and coffee substituted for the wine of Horace's sympotic settings:
Horace's "Ars Poetica" is second only to Aristotle's "Poetics" in its influence on literary theory and criticism. Milton recommended both works in his treatise "of Education". Horace's "Satires" and "Epistles" however also had a huge impact, influencing theorists and critics such as John Dryden. There was considerable debate over the value of different lyrical forms for contemporary poets, as represented on one hand by the kind of four-line stanzas made familiar by Horace's Sapphic and Alcaic "Odes" and, on the other, the loosely structured Pindarics associated with the odes of Pindar. Translations occasionally involved scholars in the dilemmas of censorship. Thus Christopher Smart entirely omitted "Odes" and re-numbered the remaining odes. He also removed the ending of "Odes" . Thomas Creech printed "Epodes" and in the original Latin but left out their English translations. Philip Francis left out both the English and Latin for those same two epodes, a gap in the numbering the only indication that something was amiss. French editions of Horace were influential in England and these too were regularly bowdlerized.

Most European nations had their own 'Horaces': thus for example Friedrich von Hagedorn was called "The German Horace" and Maciej Kazimierz Sarbiewski "The Polish Horace" (the latter was much imitated by English poets such as Henry Vaughan and Abraham Cowley). Pope Urban VIII wrote voluminously in Horatian meters, including an ode on gout.

Horace maintained a central role in the education of English-speaking elites right up until the 1960s. A pedantic emphasis on the formal aspects of language-learning at the expense of literary appreciation may have made him unpopular in some quarters yet it also confirmed his influencea tension in his reception that underlies Byron's famous lines from "Childe Harold" (Canto iv, 77):

William Wordsworth's mature poetry, including the preface to Lyrical Ballads, reveals Horace's influence in its rejection of false ornament and he once expressed "a wish / to meet the shade of Horace...". John Keats echoed the opening of Horace's "Epodes" 14 in the opening lines of "Ode to a Nightingale".

The Roman poet was presented in the nineteenth century as an honorary English gentleman. William Thackeray produced a version of "Odes" in which Horace's questionable 'boy' became 'Lucy', and Gerard Manley Hopkins translated the boy innocently as 'child'. Horace was translated by Sir Theodore Martin (biographer of Prince Albert) but minus some ungentlemanly verses, such as the erotic "Odes" and "Epodes" 8 and 12. Lord Lytton produced a popular translation and William Gladstone also wrote translations during his last days as Prime Minister.

Edward FitzGerald's "Rubaiyat of Omar Khayyam", though formally derived from the Persian "ruba'i", nevertheless shows a strong Horatian influence, since, as one modern scholar has observed,""...the quatrains inevitably recall the stanzas of the 'Odes', as does the narrating first person of the world-weary, ageing Epicurean Omar himself, mixing sympotic exhortation and 'carpe diem' with splendid moralising and 'memento mori' nihilism."" Matthew Arnold advised a friend in verse not to worry about politics, an echo of "Odes" , yet later became a critic of Horace's inadequacies relative to Greek poets, as role models of Victorian virtues, observing: ""If human life were complete without faith, without enthusiasm, without energy, Horace...would be the perfect interpreter of human life."" Christina Rossetti composed a sonnet depicting a woman willing her own death steadily, drawing on Horace's depiction of 'Glycera' in "Odes" and Cleopatra in "Odes" . A. E. Housman considered "Odes" , in Archilochian couplets, the most beautiful poem of antiquity and yet he generally shared Horace's penchant for quatrains, being readily adapted to his own elegiac and melancholy strain. The most famous poem of Ernest Dowson took its title and its heroine's name from a line of "Odes" , "Non sum qualis eram bonae sub regno Cynarae", as well as its motif of nostalgia for a former flame. Kipling wrote a famous parody of the "Odes", satirising their stylistic idiosyncrasies and especially the extraordinary syntax, but he also used Horace's Roman patriotism as a focus for British imperialism, as in the story "Regulus" in the school collection "Stalky & Co.", which he based on "Odes" . Wilfred Owen's famous poem, quoted above, incorporated Horatian text to question patriotism while ignoring the rules of Latin scansion. However, there were few other echoes of Horace in the war period, possibly because war is not actually a major theme of Horace's work.
Both W.H.Auden and Louis MacNeice began their careers as teachers of classics and both responded as poets to Horace's influence. Auden for example evoked the fragile world of the 1930s in terms echoing "Odes" , where Horace advises a friend not to let worries about frontier wars interfere with current pleasures.

The American poet, Robert Frost, echoed Horace's "Satires" in the conversational and sententious idiom of some of his longer poems, such as "The Lesson for Today" (1941), and also in his gentle advocacy of life on the farm, as in "Hyla Brook" (1916), evoking Horace's "fons Bandusiae" in "Ode" . Now at the start of the third millennium, poets are still absorbing and re-configuring the Horatian influence, sometimes in translation (such as a 2002 English/American edition of the "Odes" by thirty-six poets) and sometimes as inspiration for their own work (such as a 2003 collection of odes by a New Zealand poet).

Horace's "Epodes" have largely been ignored in the modern era, excepting those with political associations of historical significance. The obscene qualities of some of the poems have repulsed even scholars yet more recently a better understanding of the nature of Iambic poetry has led to a re-evaluation of the "whole" collection. A re-appraisal of the "Epodes" also appears in creative adaptations by recent poets (such as a 2004 collection of poems that relocates the ancient context to a 1950s industrial town).







</doc>
<doc id="13694" url="https://en.wikipedia.org/wiki?curid=13694" title="Microsoft Windows version history">
Microsoft Windows version history

Microsoft Windows was announced by Bill Gates on November 10, 1983. Microsoft introduced Windows as a graphical user interface for MS-DOS, which had been introduced a couple of years earlier. In the 1990s, the product line evolved from an operating environment into a fully complete, modern operating system over two lines of development, each with their own separate codebase.

The first versions of Windows (1.0 through to 3.11) were graphical shells that run from MS-DOS, later on, Windows 95, though still being based on MS-DOS, was its own operating system, using a 16-bit DOS-based kernel and a 32-bit user space. Windows 95 introduced many features that have been part of the product ever since, including the Start menu, the taskbar, and Windows Explorer (renamed File Explorer in Windows 8). In 1997, Microsoft released Internet Explorer 4 which included the (at the time) controversial Windows Desktop Update. It aimed to integrate Internet Explorer and the web into the user interface and also brought many new features into Windows, such as the ability to display JPEG images as the desktop wallpaper and single window navigation in Windows Explorer. In 1998, Microsoft released Windows 98, which also included the Windows Desktop Update and Internet Explorer 4 by default. The inclusion of Internet Explorer 4 and the Desktop Update led to an anti-trust case in the United States. Windows 98 also includes plug and play, which allows devices to work when plugged in without requiring a system reboot or manual configuration, and USB support out of the box. Windows ME, the last DOS-based version of Windows, was aimed at consumers and released in 2000. It introduced System Restore, Help and Support Center, updated versions of the Disk Defragmenter and other system tools.

In 1993, Microsoft released Windows NT 3.1, the first version of the newly-developed Windows NT operating system. Unlike the Windows 9x series of operating systems, it is a fully 32-bit operating system. NT 3.1 introduced NTFS, a file system designed to replace the older File Allocation Table (FAT) which was used by DOS and the DOS-based Windows operating systems. In 1996, Windows NT 4.0 was released, which includes a fully 32-bit version of Windows Explorer written specifically for it, making the operating system work just like Windows 95. Windows NT was originally designed to be used on high-end systems and servers, however with the release of Windows 2000, many consumer-oriented features from Windows 95 and Windows 98 were included, such as the Windows Desktop Update, Internet Explorer 5, USB support and Windows Media Player. These consumer-oriented features were continued and further extended in Windows XP, which introduced a new theme called Luna, a more user-friendly interface, updated versions of Windows Media Player and Internet Explorer, and extended features from Windows Me, such as the Help and Support Center and System Restore. Windows Vista focused on securing the Windows operating system against computer viruses and other malicious software by introducing features such as User Account Control. New features include Windows Aero, updated versions of the standard games (e.g. Solitaire), Windows Movie Maker, and Windows Mail to replace Outlook Express. Despite this, Windows Vista was critically panned for its poor performance on older hardware and its at-the-time high system requirements. Windows 7 followed two and a half years later, and despite technically having higher system requirements, reviewers noted that it ran better than Windows Vista. Windows 7 also removed many extra features, such as Windows Movie Maker, Windows Photo Gallery and Windows Mail, instead requiring users download a separate Windows Live Essentials to gain those features and other online services. Windows 8 and Windows 8.1, a free upgrade for Windows 8, introduced many controversial changes, such as the replacement of the Start menu with the Start Screen, the removal of the Aero glass interface in favor of a flat, colored interface as well as the introduction of "Metro" apps (later renamed to Universal Windows Platform apps) and the Charms Bar user interface element, all of which received considerable criticism from reviewers.

The current version of Windows, Windows 10, reintroduced the Start menu and added the ability to run Universal Windows Platform apps in a window instead of always in full screen. Windows 10 was well-received, with many reviewers stating that Windows 10 is what Windows 8 should have been. Windows 10 also marks the last version of Windows to be traditionally released. Instead, "feature updates" are released twice a year with names such as "Creators Update" and "Fall Creators Update" that introduce new capabilities.

The first independent version of Microsoft Windows, version 1.0, released on November 20, 1985, achieved little popularity. The project was briefly codenamed "Interface Manager" before the windowing system was developed - contrary to popular belief that it was the original name for Windows and Rowland Hanson, the head of marketing at Microsoft, convinced the company that the name "Windows" would be more appealing to customers.

Windows 1.0 was not a complete operating system, but rather an "operating environment" that extended MS-DOS, and shared the latter's inherent flaws and errors.

The first version of Microsoft Windows included a simple graphics painting program called Windows Paint; Windows Write, a simple word processor; an appointment calendar; a card-filer; a notepad; a clock; a control panel; a computer terminal; Clipboard; and RAM driver. It also included the MS-DOS Executive and a game called Reversi.

Microsoft had worked with Apple Computer to develop applications for Apple's new Macintosh computer, which featured a graphical user interface. As part of the related business negotiations, Microsoft had licensed certain aspects of the Macintosh user interface from Apple; in later litigation, a district court summarized these aspects as "screen displays".
In the development of Windows 1.0, Microsoft intentionally limited its borrowing of certain GUI elements from the Macintosh user interface, to comply with its license. For example, windows were only displayed "tiled" on the screen; that is, they could not overlap or overlie one another.

Microsoft Windows version 2 came out on December 9, 1987, and proved slightly more popular than its predecessor.
Much of the popularity for Windows 2.0 came by way of its inclusion as a "run-time version" with Microsoft's new graphical applications, Excel and Word for Windows. They could be run from MS-DOS, executing Windows for the duration of their activity, and closing down Windows upon exit.

Microsoft Windows received a major boost around this time when Aldus PageMaker appeared in a Windows version, having previously run only on Macintosh. Some computer historians date this, the first appearance of a significant "and" non-Microsoft application for Windows, as the start of the success of Windows.

Versions 2.0x used the real-mode memory model, which confined it to a maximum of 1 megabyte of memory.
In such a configuration, it could run under another multitasker like DESQview, which used the 286 protected mode.

Later, two new versions were released: Windows/286 2.1 and Windows/386 2.1. Like prior versions of Windows, Windows/286 2.1 used the real-mode memory model, but was the first version to support the High Memory Area. Windows/386 2.1 had a protected mode kernel with LIM-standard EMS emulation. All Windows and DOS-based applications at the time were real mode, running over the protected mode kernel by using the virtual 8086 mode, which was new with the 80386 processor.

Version 2.03, and later 3.0, faced challenges from Apple over its overlapping windows and other features Apple charged mimicked the ostensibly copyrighted "look and feel" of its operating system and "embodie[d] and generated a copy of the Macintosh" in its OS. Judge William Schwarzer dropped all but 10 of Apple's 189 claims of copyright infringement, and ruled that most of the remaining 10 were over uncopyrightable ideas.

Windows 3.0, released in May 1990, improved capabilities given to native applications. It also allowed users to better multitask older MS-DOS based software compared to Windows/386, thanks to the introduction of virtual memory.

Windows 3.0's user interface finally resembled a serious competitor to the user interface of the Macintosh computer. PCs had improved graphics by this time, due to VGA video cards, and the protected/enhanced mode allowed Windows applications to use more memory in a more painless manner than their DOS counterparts could. Windows 3.0 could run in real, standard, or 386 enhanced modes, and was compatible with any Intel processor from the 8086/8088 up to the 80286 and 80386. This was the first version to run Windows programs in protected mode, although the 386 enhanced mode kernel was an enhanced version of the protected mode kernel in Windows/386.

Windows 3.0 received two updates. A few months after introduction, Windows 3.0a was released as a maintenance release, resolving bugs and improving stability. A "multimedia" version, Windows 3.0 with Multimedia Extensions 1.0, was released in October 1991. This was bundled with "multimedia upgrade kits", comprising a CD-ROM drive and a sound card, such as the Creative Labs Sound Blaster Pro. This version was the precursor to the multimedia features available in Windows 3.1 (first released in April 1992) and later, and was part of Microsoft's specification for the Multimedia PC.

The features listed above and growing market support from application software developers made Windows 3.0 wildly successful, selling around 10 million copies in the two years before the release of version 3.1. Windows 3.0 became a major source of income for Microsoft, and led the company to revise some of its earlier plans. Support was discontinued on December 31, 2001.

During the mid to late 1980s, Microsoft and IBM had cooperatively been developing OS/2 as a successor to DOS. OS/2 would take full advantage of the aforementioned protected mode of the Intel 80286 processor and up to 16 MB of memory. OS/2 1.0, released in 1987, supported swapping and multitasking and allowed running of DOS executables.

IBM licensed Windows's GUI for OS/2 as Presentation Manager, and the two companies stated that it and Windows 2.0 would be almost identical. Presentation Manager was not available with OS/2 until version 1.1, released in 1988. Its API was incompatible with Windows. Version 1.2, released in 1989, introduced a new file system, HPFS, to replace the FAT file system.

By the early 1990s, conflicts developed in the Microsoft/IBM relationship. They cooperated with each other in developing their PC operating systems, and had access to each other's code. Microsoft wanted to further develop Windows, while IBM desired for future work to be based on OS/2. In an attempt to resolve this tension, IBM and Microsoft agreed that IBM would develop OS/2 2.0, to replace OS/2 1.3 and Windows 3.0, while Microsoft would develop a new operating system, OS/2 3.0, to later succeed OS/2 2.0.

This agreement soon fell apart however, and the Microsoft/IBM relationship was terminated. IBM continued to develop OS/2, while Microsoft changed the name of its (as yet unreleased) OS/2 3.0 to Windows NT. Both retained the rights to use OS/2 and Windows technology developed up to the termination of the agreement; Windows NT, however, was to be written anew, mostly independently (see below).

After an interim 1.3 version to fix up many remaining problems with the 1.x series, IBM released OS/2 version 2.0 in 1992. This was a major improvement: it featured a new, object-oriented GUI, the Workplace Shell (WPS), that included a desktop and was considered by many to be OS/2's best feature. Microsoft would later imitate much of it in Windows 95. Version 2.0 also provided a full 32-bit API, offered smooth multitasking and could take advantage of the 4 gigabytes of address space provided by the Intel 80386. Still, much of the system had 16-bit code internally which required, among other things, device drivers to be 16-bit code also. This was one of the reasons for the chronic shortage of OS/2 drivers for the latest devices. Version 2.0 could also run DOS and Windows 3.0 programs, since IBM had retained the right to use the DOS and Windows code as a result of the breakup.

In response to the impending release of OS/2 2.0, Microsoft developed Windows 3.1 (first released in April 1992), which included several improvements to Windows 3.0, such as display of TrueType scalable fonts (developed jointly with Apple), improved disk performance in 386 Enhanced Mode, multimedia support, and bugfixes. It also removed Real Mode, and only ran on an 80286 or better processor. Later Microsoft also released Windows 3.11, a touch-up to Windows 3.1 which included all of the patches and updates that followed the release of Windows 3.1 in 1992.

In 1992 and 1993, Microsoft released Windows for Workgroups (WfW), which was available both as an add-on for existing Windows 3.1 installations and in a version that included the base Windows environment and the networking extensions all in one package. Windows for Workgroups included improved network drivers and protocol stacks, and support for peer-to-peer networking. There were two versions of Windows for Workgroups, WfW 3.1 and WfW 3.11. Unlike prior versions, Windows for Workgroups 3.11 ran in 386 Enhanced Mode only, and needed at least an 80386SX processor. One optional download for WfW was the "Wolverine" TCP/IP protocol stack, which allowed for easy access to the Internet through corporate networks.

All these versions continued version 3.0's impressive sales pace. Even though the 3.1x series still lacked most of the important features of OS/2, such as long file names, a desktop, or protection of the system against misbehaving applications, Microsoft quickly took over the OS and GUI markets for the IBM PC. The Windows API became the de facto standard for consumer software.

Meanwhile, Microsoft continued to develop Windows NT. The main architect of the system was Dave Cutler, one of the chief architects of VMS at Digital Equipment Corporation (later acquired by Compaq, now part of Hewlett-Packard). Microsoft hired him in October 1988 to create a successor to OS/2, but Cutler created a completely new system instead. Cutler had been developing a follow-on to VMS at DEC called Mica, and when DEC dropped the project he brought the expertise and around 20 engineers with him to Microsoft. DEC also believed he brought Mica's code to Microsoft and sued. Microsoft eventually paid US$150 million and agreed to support DEC's Alpha CPU chip in NT.

Windows NT Workstation (Microsoft marketing wanted Windows NT to appear to be a continuation of Windows 3.1) arrived in Beta form to developers at the July 1992 Professional Developers Conference in San Francisco. Microsoft announced at the conference its intentions to develop a successor to both Windows NT and Windows 3.1's replacement (Windows 95, codenamed Chicago), which would unify the two into one operating system. This successor was codenamed Cairo. In hindsight, Cairo was a much more difficult project than Microsoft had anticipated and, as a result, NT and Chicago would not be unified until Windows XP—albeit Windows 2000, oriented to business, had already unified most of the system’s bolts and gears, it was XP that was sold to home consumers like Windows 95 and came to be viewed as the final unified OS. Parts of Cairo have still not made it into Windows as of 2017 - most notably, the WinFS file system, which was the much touted Object File System of Cairo. Microsoft announced that they have discontinued the separate release of WinFS for Windows XP and Windows Vista and will gradually incorporate the technologies developed for WinFS in other products and technologies, notably Microsoft SQL Server.

Driver support was lacking due to the increased programming difficulty in dealing with NT's superior hardware abstraction model. This problem plagued the NT line all the way through Windows 2000. Programmers complained that it was too hard to write drivers for NT, and hardware developers were not going to go through the trouble of developing drivers for a small segment of the market. Additionally, although allowing for good performance and fuller exploitation of system resources, it was also resource-intensive on limited hardware, and thus was only suitable for larger, more expensive machines.

However, these same features made Windows NT perfect for the LAN server market (which in 1993 was experiencing a rapid boom, as office networking was becoming common). NT also had advanced network connectivity options and NTFS, an efficient file system. Windows NT version 3.51 was Microsoft's entry into this field, and took away market share from Novell (the dominant player) in the following years.

One of Microsoft's biggest advances initially developed for Windows NT was a new 32-bit API, to replace the legacy 16-bit Windows API. This API was called Win32, and from then on Microsoft referred to the older 16-bit API as Win16. The Win32 API had three levels of implementation: the complete one for Windows NT, a subset for Chicago (originally called Win32c) missing features primarily of interest to enterprise customers (at the time) such as security and Unicode support, and a more limited subset called Win32s which could be used on Windows 3.1 systems. Thus Microsoft sought to ensure some degree of compatibility between the Chicago design and Windows NT, even though the two systems had radically different internal architectures. Windows NT was the first Windows operating system based on a hybrid kernel.

As released, Windows NT 3.x went through three versions (3.1, 3.5, and 3.51), changes were primarily internal and reflected back end changes. The 3.5 release added support for new types of hardware and improved performance and data reliability; the 3.51 release was primarily to update the Win32 APIs to be compatible with software being written for the Win32c APIs in what became Windows 95.

After Windows 3.11, Microsoft began to develop a new consumer oriented version of the operating system codenamed Chicago. Chicago was designed to have support for 32-bit preemptive multitasking like OS/2 and Windows NT, although a 16-bit kernel would remain for the sake of backward compatibility. The Win32 API first introduced with Windows NT was adopted as the standard 32-bit programming interface, with Win16 compatibility being preserved through a technique known as "thunking". A new object oriented GUI was not originally planned as part of the release, although elements of the Cairo user interface were borrowed and added as other aspects of the release (notably Plug and Play) slipped.

Microsoft did not change all of the Windows code to 32-bit, parts of it remained 16-bit (albeit not directly using real mode) for reasons of compatibility, performance, and development time. Additionally it was necessary to carry over design decisions from earlier versions of Windows for reasons of backwards compatibility, even if these design decisions no longer matched a more modern computing environment. These factors eventually began to impact the operating system's efficiency and stability.

Microsoft marketing adopted Windows 95 as the product name for Chicago when it was released on August 24, 1995. Microsoft had a double gain from its release: first, it made it impossible for consumers to run Windows 95 on a cheaper, non-Microsoft DOS, secondly, although traces of DOS were never completely removed from the system and MS DOS 7 would be loaded briefly as a part of the booting process, Windows 95 applications ran solely in 386 enhanced mode, with a flat 32-bit address space and virtual memory. These features make it possible for Win32 applications to address up to 2 gigabytes of virtual RAM (with another 2 GB reserved for the operating system), and in theory prevented them from inadvertently corrupting the memory space of other Win32 applications. In this respect the functionality of Windows 95 moved closer to Windows NT, although Windows 95/98/ME did not support more than 512 megabytes of physical RAM without obscure system tweaks.

IBM continued to market OS/2, producing later versions in OS/2 3.0 and 4.0 (also called Warp). Responding to complaints about OS/2 2.0's high demands on computer hardware, version 3.0 was significantly optimized both for speed and size. Before Windows 95 was released, OS/2 Warp 3.0 was even shipped preinstalled with several large German hardware vendor chains. However, with the release of Windows 95, OS/2 began to lose market share.

It is probably impossible to choose one specific reason why OS/2 failed to gain much market share. While OS/2 continued to run Windows 3.1 applications, it lacked support for anything but the Win32s subset of Win32 API (see above). Unlike with Windows 3.1, IBM did not have access to the source code for Windows 95 and was unwilling to commit the time and resources to emulate the moving target of the Win32 API. IBM later introduced OS/2 into the United States v. Microsoft case, blaming unfair marketing tactics on Microsoft's part.

Microsoft went on to release five different versions of Windows 95:

OSR2, OSR2.1, and OSR2.5 were not released to the general public, rather, they were available only to OEMs that would preload the OS onto computers. Some companies sold new hard drives with OSR2 preinstalled (officially justifying this as needed due to the hard drive's capacity).

The first Microsoft Plus! add-on pack was sold for Windows 95.

Windows NT 4.0 was the successor of 3.5 (1994) and 3.51 (1995). Microsoft released Windows NT 4.0 to manufacturing in July 1996, one year after the release of Windows 95. Major new features included the new Explorer shell from Windows 95, scalability and feature improvements to the core architecture, kernel, USER32, COM and MSRPC.

Windows NT 4.0 came in four versions:

On June 25, 1998, Microsoft released Windows 98 (codenamed Memphis). It included new hardware drivers and the FAT32 file system which supports disk partitions that are larger than 2 GB (first introduced in Windows 95 OSR2). USB support in Windows 98 is marketed as a vast improvement over Windows 95. The release continued the controversial inclusion of the Internet Explorer browser with the operating system that started with Windows 95 OEM Service Release 1. The action eventually led to the filing of the United States v. Microsoft case, dealing with the question of whether Microsoft was introducing unfair practices into the market in an effort to eliminate competition from other companies such as Netscape.

In 1999, Microsoft released Windows 98 Second Edition, an interim release. One of the more notable new features was the addition of Internet Connection Sharing, a form of network address translation, allowing several machines on a LAN (Local Area Network) to share a single Internet connection. Hardware support through device drivers was increased and this version shipped with Internet Explorer 5. Many minor problems that existed in the first edition were fixed making it, according to many, the most stable release of the Windows 9x family.
Microsoft released Windows 2000 on February 17, 2000. It has the version number Windows NT 5.0. Windows 2000 has had four official service packs. It was successfully deployed both on the server and the workstation markets. Amongst Windows 2000's most significant new features was Active Directory, a near-complete replacement of the NT 4.0 Windows Server domain model, which built on industry-standard technologies like DNS, LDAP, and Kerberos to connect machines to one another. Terminal Services, previously only available as a separate edition of NT 4, was expanded to all server versions. A number of features from Windows 98 were incorporated also, such as an improved Device Manager, Windows Media Player, and a revised DirectX that made it possible for the first time for many modern games to work on the NT kernel. Windows 2000 is also the last NT-kernel Windows operating system to lack product activation.

While Windows 2000 upgrades were available for Windows 95 and Windows 98, it was not intended for home users.

Windows 2000 was available in four editions:

In September 2000, Microsoft released a successor to Windows 98 called Windows ME, short for "Millennium Edition". It was the last DOS-based operating system from Microsoft. Windows ME introduced a new multimedia-editing application called Windows Movie Maker, came standard with Internet Explorer 5.5 and Windows Media Player 7, and debuted the first version of System Restore – a recovery utility that enables the operating system to revert system files back to a prior date and time. System Restore was a notable feature that would continue to thrive in all later versions of Windows.

Windows ME was conceived as a quick one-year project that served as a stopgap release between Windows 98 and Windows XP. Many of the new features were available from the Windows Update site as updates for older Windows versions ("System Restore" and "Windows Movie Maker" were exceptions). Windows ME was criticized for stability issues, as well as for lacking real mode DOS support, to the point of being referred to as the "Mistake Edition" or "Many Errors." Windows ME was the last operating system to be based on the Windows 9x (monolithic) kernel and MS-DOS.

On October 25, 2001, Microsoft released Windows XP (codenamed "Whistler"). The merging of the Windows NT/2000 and Windows 95/98/Me lines was finally achieved with Windows XP. Windows XP uses the Windows NT 5.1 kernel, marking the entrance of the Windows NT core to the consumer market, to replace the aging 16/32-bit branch. The initial release met with considerable criticism, particularly in the area of security, leading to the release of three major Service Packs. Windows XP SP1 was released in September 2002, SP2 came out in August 2004 and SP3 came out in April 2008. Service Pack 2 provided significant improvements and encouraged widespread adoption of XP among both home and business users. Windows XP lasted longer as Microsoft's flagship operating system than any other version of Windows, from October 25, 2001 to January 30, 2007 when it was succeeded by Windows Vista.

Windows XP is available in a number of versions:

On April 25, 2003, Microsoft launched Windows Server 2003, a notable update to Windows 2000 Server encompassing many new security features, a new "Manage Your Server" wizard that simplifies configuring a machine for specific roles, and improved performance. It has the version number NT 5.2. A few services not essential for server environments are disabled by default for stability reasons, most noticeable are the "Windows Audio" and "Themes" services; users have to enable them manually to get sound or the "Luna" look as per Windows XP. The hardware acceleration for display is also turned off by default, users have to turn the acceleration level up themselves if they trust the display card driver.

December 2005, Microsoft released Windows Server 2003 R2, which is actually Windows Server 2003 with SP1 (Service Pack 1) plus an add-on package.
Among the new features are a number of management features for branch offices, file serving, printing and company-wide identity integration.

Windows Server 2003 is available in six editions:

Windows Server 2003 R2, an update of Windows Server 2003, was released to manufacturing on December 6, 2005. It is distributed on two CDs, with one CD being the Windows Server 2003 SP1 CD. The other CD adds many optionally installable features for Windows Server 2003. The R2 update was released for all x86 and x64 versions, except Windows Server 2003 R2 Enterprise Edition, which was not released for Itanium.

On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003, x64 Editions in Standard, Enterprise and Datacenter SKUs. Windows XP Professional x64 Edition is an edition of Windows XP for x86-64 personal computers. It is designed to use the expanded 64-bit memory address space provided by the x86-64 architecture.

Windows XP Professional x64 Edition is based on the Windows Server 2003 codebase, with the server features removed and client features added. Both "Windows Server 2003 x64" and Windows XP Professional x64 Edition use identical kernels.

Windows XP "Professional" "x64 Edition" is not to be confused with Windows XP "64-bit Edition", as the latter was designed for Intel Itanium processors. During the initial development phases, Windows XP Professional x64 Edition was named "Windows XP 64-Bit Edition for 64-Bit Extended Systems".

In July 2005, Microsoft released a thin-client version of Windows XP Service Pack 2, called Windows Fundamentals for Legacy PCs (WinFLP). It is only available to Software Assurance customers. The aim of WinFLP is to give companies a viable upgrade option for older PCs that are running Windows 95, 98, and ME that will be supported with patches and updates for the next several years. Most user applications will typically be run on a remote machine using Terminal Services or Citrix.
Windows Home Server (codenamed Q, Quattro) is a server product based on Windows Server 2003, designed for consumer use. The system was announced on January 7, 2007 by Russel Adolfo. Windows Home Server can be configured and monitored using a console program that can be installed on a client PC. Such features as Media Sharing, local and remote drive backup and file duplication are all listed as features. The release of Windows Home Server Power Pack 3 added support for Windows 7 to Windows Home Server.

Windows Vista was released on November 30, 2006 to business customers - consumer versions followed on January 30, 2007. Windows Vista intended to have enhanced security by introducing a new restricted user mode called User Account Control, replacing the "administrator-by-default" philosophy of Windows XP. Vista was the target of much criticism and negative press, and in general was not well regarded, this was seen as leading to the relatively swift release of Windows 7.

One major difference between Vista and earlier versions of Windows, Windows 95 and later, is that the original start button was replaced with the Windows icon in a circle (called the Start Orb). Vista also features new graphics features, the Windows Aero GUI, new applications (such as Windows Calendar, Windows DVD Maker and some new games including Chess, Mahjong, and Purble Place), Internet Explorer 7, Windows Media Player 11, and a large number of underlying architectural changes. Windows Vista has the version number NT 6.0. Since its release, Windows Vista has had two service packs.

Windows Vista ships in six editions:

All editions (except Starter edition) are currently available in both 32-bit and 64-bit versions. The biggest advantage of the 64-bit version is breaking the 4 gigabyte memory barrier, which 32-bit computers cannot fully access.

Windows Server 2008, released on February 27, 2008, was originally known as Windows Server Codename "Longhorn". Windows Server 2008 builds on the technological and security advances first introduced with Windows Vista, and is significantly more modular than its predecessor, Windows Server 2003.

Windows Server 2008 ships in ten editions:

Windows 7 was released to manufacturing on July 22, 2009, and reached general retail availability on October 22, 2009. It was previously known by the codenames Blackcomb and Vienna. Windows 7 has the version number NT 6.1. Since its release, Windows 7 has had one service pack.

Some features of Windows 7 are faster booting, Device Stage, Windows PowerShell, less obtrusive User Account Control, multi-touch, and improved window management. Features included with Windows Vista and not in Windows 7 include the sidebar (although gadgets remain) and several programs that were removed in favor of downloading their Windows Live counterparts.

Windows 7 ships in six editions:
In some countries (Austria, Belgium, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, United Kingdom, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, and Switzerland), there are other editions that lack some features such as Windows Media Player, Windows Media Center and Internet Explorer - these editions were called names such as "Windows 7 N."
Microsoft focuses on selling Windows 7 Home Premium and Professional. All editions, except the Starter edition, are available in both 32-bit and 64-bit versions.
Unlike the corresponding Vista editions, the Professional and Enterprise editions are supersets of the Home Premium edition.

At the Professional Developers Conference (PDC) 2008, Microsoft also announced Windows Server 2008 R2, as the server variant of Windows 7. Windows Server 2008 R2 ships in 64-bit versions (x64 and Itanium) only.

In 2010, Microsoft released Windows Thin PC or WinTPC, which is a feature- and size-reduced locked-down version of Windows 7 expressly designed to turn older PCs into thin clients. WinTPC is available for software assurance customers and relies on cloud computing in a business network. Wireless operation is supported since WinTPC has full wireless stack integration, but wireless operation may not be as good as the operation on a wired connection.
Windows Home Server 2011 code named 'Vail' was released on April 6, 2011. Windows Home Server 2011 is built on the Windows Server 2008 R2 code base and removed the Drive Extender drive pooling technology in the original Windows Home Server release. Windows Home Server 2011 is considered a "major release". Its predecessor was built on Windows Server 2003. WHS 2011 only supports x86-64 hardware.

Microsoft decided to discontinue Windows Home Server 2011 on July 5, 2012 while including its features into Windows Server 2012 Essentials. Windows Home Server 2011 was supported until April 12, 2016.

On October 26, 2012, Microsoft released Windows 8 to the public. One edition, Windows RT, runs on some system-on-a-chip devices with mobile 32-bit ARM (ARMv7) processors. Windows 8 features a redesigned user interface, designed to make it easier for touchscreen users to use Windows. The interface introduced an updated Start menu known as the Start screen, and a new full-screen application platform. The desktop interface is also present for running windowed applications, although Windows RT will not run any desktop applications not included in the system. On the Building Windows 8 blog, it was announced that a computer running Windows 8 can boot up much faster than Windows 7. New features also include USB 3.0 support, the Windows Store, the ability to run from USB drives with Windows To Go, and others. Windows 8 was given the kernel number NT 6.2, with its successor 8.1 receiving the kernel number 6.3. So far, neither has had any service packs yet, although many consider Windows 8.1 to be a service pack for Windows 8.

Windows 8 is available in the following editions:

The first public preview of Windows Server 2012 and was also shown by Microsoft at the 2011 Microsoft Worldwide Partner Conference.

Windows 8 Release Preview and Windows Server 2012 Release Candidate were both released on May 31, 2012. Product development on Windows 8 was completed on August 1, 2012, and it was released to manufacturing the same day. Windows Server 2012 went on sale to the public on September 4, 2012. Windows 8 went on sale October 26, 2012.

Windows 8.1 and Windows Server 2012 R2 were released on October 17, 2013. Windows 8.1 is available as an update in the Windows store for Windows 8 users only and also available to download for clean installation. The update adds new options for resizing the live tiles on the Start screen.

Windows 10, codenamed Threshold (Later Redstone), is the current release of the Microsoft Windows operating system. Unveiled on September 30, 2014, it was released on July 29, 2015. It was distributed without charge to Windows 7 and 8.1 users for one year after release. A number of new features like Cortana, the Microsoft Edge web browser, the ability to view Windows Store apps as a window instead of fullscreen, virtual desktops, revamped core apps, Continuum, and a unified Settings app were all features debuted in Windows 10. Microsoft has announced that Windows 10 will be the last major version of its series of operating systems to be released. Instead, Microsoft will release major updates to the operating system via download or in Windows Update, similar to the way updates are delivered in macOS.

So far, seven major versions of Windows 10 have been released, with the version Redstone 5 being the latter, having been released in October 2018:

Windows Server 2016 is a release of the Microsoft Windows Server operating system that was unveiled on September 30, 2014. Windows Server 2016 was officially released at Microsoft's Ignite Conference, September 26–30, 2016.

Windows Server 2019 is a release of the Microsoft Windows Server operating system.



</doc>
<doc id="13696" url="https://en.wikipedia.org/wiki?curid=13696" title="Helsinki">
Helsinki

Helsinki (, ; ) is the capital and most populous city of Finland. Located on the shore of the Gulf of Finland, it is the seat of the region of Uusimaa in southern Finland, and has a population of . The city's urban area has a population of , making it by far the most populous urban area in Finland as well as the country's most important center for politics, education, finance, culture, and research. Helsinki is located north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. It has close historical ties with these three cities.

Together with the cities of Espoo, Vantaa, and Kauniainen, and surrounding commuter towns, Helsinki forms the Greater Helsinki metropolitan area, which has a population of nearly 1.5 million. Often considered to be Finland's only metropolis, it is the world's northernmost metro area with over one million people as well as the northernmost capital of an EU member state. After Stockholm and Oslo, Helsinki is the third largest municipality in the Nordic countries. The city is served by the international Helsinki Airport, located in the neighboring city of Vantaa, with frequent service to many destinations in Europe and Asia.

Helsinki was the World Design Capital for 2012, the venue for the 1952 Summer Olympics, and the host of the 52nd Eurovision Song Contest.

Helsinki has one of the highest urban standards of living in the world. In 2011, the British magazine "Monocle" ranked Helsinki the world's most liveable city in its liveable cities index. In the Economist Intelligence Unit's 2016 liveability survey, Helsinki was ranked ninth among 140 cities.

According to a theory presented in the 1630s, settlers from Hälsingland in central Sweden had arrived to what is now known as the Vantaa River and called it "Helsingå" ("Helsinge River"), which gave rise to the names of Helsinge village and church in the 1300s. This theory is questionable, because dialect research suggests that the settlers arrived from Uppland and nearby areas. Others have proposed the name as having been derived from the Swedish word "helsing", an archaic form of the word "hals" (neck), referring to the narrowest part of a river, the rapids. Other Scandinavian cities at similar geographic locations were given similar names at the time, e.g. Helsingør in Denmark and Helsingborg in Sweden.

When a town was founded in Forsby village in 1548, it was named "Helsinge fors," "Helsinge rapids". The name refers to the Vanhankaupunginkoski rapids at the mouth of the river. The town was commonly known as "Helsinge" or "Helsing", from which the contemporary Finnish name arose.

Official Finnish Government documents and Finnish language newspapers have used the name "Helsinki" since 1819, when the Senate of Finland moved itself into the city from Turku. The decrees issued in Helsinki were dated with Helsinki as the place of issue. This is how the form Helsinki came to be used in written Finnish. As part of the Grand Duchy of Finland in the Russian Empire, Helsinki was known as "Gelsingfors" in Russian.

In Helsinki slang, the city is called "Stadi" (from the Swedish word "stad", meaning "city"). "Hesa" (short for Helsinki), is not used by natives of the city. "" is the Northern Sami name of Helsinki.

In the Iron Age the area occupied by present day Helsinki was inhabited by Tavastians. They used the area for fishing and hunting, but due to a lack of archeological finds it is difficult to say how extensive their settlements were. Pollen analysis has shown that there were cultivating settlements in the area in the 10th century and surviving historical records from the 14th century describe Tavastian settlements in the area.

Swedes colonized the coastline of the Helsinki region in the late 13th century after the successful Second Crusade to Finland, which led to the defeat of the Tavastians.

Helsinki was established as a trading town by King Gustav I of Sweden in 1550 as the town of Helsingfors, which he intended to be a rival to the Hanseatic city of Reval (today known as Tallinn). In order to populate his newly founded town, the King issued an order to resettle the bourgeoisie of Porvoo, Ekenäs, Rauma and Ulvila into the town. Little came of the plans as Helsinki remained a tiny town plagued by poverty, wars, and diseases. The plague of 1710 killed the greater part of the inhabitants of Helsinki. The construction of the naval fortress Sveaborg (in Finnish "Viapori", today also "Suomenlinna") in the 18th century helped improve Helsinki's status, but it was not until Russia defeated Sweden in the Finnish War and annexed Finland as the autonomous Grand Duchy of Finland in 1809 that the town began to develop into a substantial city. Russians besieged the Sveaborg fortress during the war, and about one quarter of the town was destroyed in an 1808 fire.

Russian Emperor Alexander I of Russia moved the Finnish capital from Turku to Helsinki in 1812 to reduce Swedish influence in Finland, and to bring the capital closer to Saint Petersburg. Following the Great Fire of Turku in 1827, the Royal Academy of Turku, which at the time was the country's only university, was also relocated to Helsinki and eventually became the modern University of Helsinki. The move consolidated the city's new role and helped set it on a path of continuous growth. This transformation is highly apparent in the downtown core, which was rebuilt in the neoclassical style to resemble Saint Petersburg, mostly to a plan by the German-born architect C. L. Engel. As elsewhere, technological advancements such as railroads and industrialization were key factors behind the city's growth.

Despite the tumultuous nature of Finnish history during the first half of the 20th century (including the Finnish Civil War and the Winter War which both left marks on the city), Helsinki continued its steady development. A landmark event was the 1952 Olympic Games, held in Helsinki. Finland's rapid urbanization in the 1970s, occurring late relative to the rest of Europe, tripled the population in the metropolitan area, and the Helsinki Metro subway system was built. The relatively sparse population density of Helsinki and its peculiar structure have often been attributed to the lateness of its growth.

Called the "Daughter of the Baltic", Helsinki is on the tip of a peninsula and on 315 islands. The inner city is located on a southern peninsula, "Helsinginniemi" (”Helsinki’s peninsula”), which is rarely referred to by its actual name, Vironniemi (”Estonia’s peninsula”). Population density in certain parts of Helsinki's inner city area is comparatively higher, reaching in the district of Kallio, but as a whole Helsinki's population density of ranks the city as rather sparsely populated in comparison to other European capital cities. Outside of the inner city, much of Helsinki consists of postwar suburbs separated by patches of forest. A narrow, long Helsinki Central Park, stretching from the inner city to Helsinki's northern border, is an important recreational area for residents. The City of Helsinki has about 11,000 boat berths and possesses over 14,000 hectares (34,595 acres; 54.1 sq mi) of marine fishing waters adjacent to the Capital Region. Some 60 fish species are found in this area and recreational fishing is popular.

Major islands in Helsinki include Seurasaari, Vallisaari, Lauttasaari, and Korkeasaari – the lattermost being the site of Finland’s largest zoo. Other noteworthy islands are the fortress island of Suomenlinna (Sveaborg), the military island of Santahamina, and Isosaari. Pihlajasaari island is a favorite summer spot for gay men and naturists, comparable to Fire Island in New York City.

The Helsinki metropolitan area, also known as the Capital Region (Finnish: "Pääkaupunkiseutu", Swedish: "Huvudstadsregionen") comprises four municipalities: Helsinki, Espoo, Vantaa, and Kauniainen. The Helsinki urban area is considered to be the only metropolis in Finland. It has a population of over 1,1 million, and is the most densely populated area of Finland. The Capital Region spreads over a land area of and has a population density of . With over 20 percent of the country's population in just 0.2 percent of its surface area, the area's housing density is high by Finnish standards.

The Helsinki Metropolitan Area (Greater Helsinki) consists of the cities of Helsinki Capital Region and ten surrounding municipalities. The Metropolitan Area covers and has a population of over 1.4 million, or about a fourth of the total population of Finland. The metropolitan area has a high concentration of employment: approximately 750,000 jobs. Despite the intensity of land use, the region also has large recreational areas and green spaces. The Greater Helsinki area is the world's northernmost urban area with a population of over one million people, and the northernmost EU capital city.

The Helsinki urban area is an officially recognized urban area in Finland, defined by its population density. The area stretches throughout 11 municipalities, and is the largest such area in Finland, with a land area of and approximately 1,2 million inhabitants.

Helsinki has a humid continental climate (Köppen: "Dfb") similar to that of Hokkaido or Nova Scotia coastal. Owing to the mitigating influence of the Baltic Sea and North Atlantic Current (see also Extratropical cyclone), temperatures during the winter are higher than the northern location might suggest, with the average in January and February around .

Winters in Helsinki are notably warmer than in the north of Finland, and the snow season is much shorter in the capital, due to it being in extreme Southern Finland and the urban heat island effect. Temperatures below occur a few times a year at most. However, because of the latitude, days last 5 hours and 48 minutes around the winter solstice with very low sun (at noon, the sun is a little bit over 6 degrees in the sky), and the cloudy weather at this time of year exacerbates darkness. Conversely, Helsinki enjoys long daylight during the summer; during the summer solstice, days last 18 hours and 57 minutes.

The average maximum temperature from June to August is around . Due to the marine effect, especially during hot summer days, daily temperatures are a little cooler and night temperatures higher than further inland. The highest temperature ever recorded in the city centre was , on 18 July 1945, and the lowest was , on 10 January 1987. Unofficial low of -35 was recorded in December 1876. Helsinki Airport (in Vantaa, north of the Helsinki city centre) recorded a temperature of , on 29 July 2010, and a low of , on 9 January 1987. Precipitation is received from frontal passages and thunderstorms. Thunderstorms are most common in the summer.

Carl Ludvig Engel, appointed to plan a new city centre on his own, designed several neoclassical buildings in Helsinki. The focal point of Engel's city plan was the Senate Square. It is surrounded by the Government Palace (to the east), the main building of Helsinki University (to the west), and (to the north) the large Helsinki Cathedral, which was finished in 1852, twelve years after Engel's death. Helsinki's epithet, "The White City of the North", derives from this construction era.

Helsinki is also home to numerous Art Nouveau-influenced (Jugend in Finnish) buildings belonging to the romantic nationalism trend, designed in the early 20th century and strongly influenced by "Kalevala", which was a common theme of the era. Helsinki's Art Nouveau style is also featured in central residential districts, such as Katajanokka and Ullanlinna. An important architect of the Finnish Art Nouveau style was Eliel Saarinen, whose architectural masterpiece was the Helsinki Central Station.

Helsinki also features several buildings by Finnish architect Alvar Aalto, recognized as one of the pioneers of architectural functionalism. However, some of his works, such as the headquarters of the paper company Stora Enso and the concert venue Finlandia Hall, have been subject to divided opinions from the citizens.

Functionalist buildings in Helsinki by other architects include the Olympic Stadium, the Tennis Palace, the Rowing Stadium, the Swimming Stadium, the Velodrome, the Glass Palace, the Töölö Sports Hall, and Helsinki-Malmi Airport. The sports venues were built to serve the 1940 Helsinki Olympic Games; the games were initially cancelled due to the Second World War, but the venues fulfilled their purpose in the 1952 Olympic Games. Many of them are listed by DoCoMoMo as significant examples of modern architecture. The Olympic Stadium and Helsinki-Malmi Airport are also catalogued by the Finnish National Board of Antiquities as cultural-historical environments of national significance.

Helsinki's neoclassical buildings were often used as a backdrop for scenes set to take place in the Soviet Union in many Cold War era Hollywood movies, when filming in the USSR was not possible. Some of them include "The Kremlin Letter" (1970), "Reds" (1981), and "Gorky Park" (1983). Because some streetscapes were reminiscent of Leningrad's and Moscow's old buildings, they too were used in movie productions. At the same time the government secretly instructed Finnish officials not to extend assistance to such film projects.

The start of the 21st century marked the beginning of highrise construction in Helsinki.

In the 21st century Helsinki has decided to allow the construction of skyscrapers. As of April 2017 there are no skyscrapers taller than 100 meters in the Helsinki area, but there are several projects under construction or planning, mainly in Pasila and Kalasatama. An international architecture competition for at least 10 high-rises to be built in Pasila is being held. Construction of the towers will start before 2020. In Kalasatama, the first 35-story (130 m, 427 ft) and 32-story (122 m, 400 ft) residential towers are already under construction. Later they will be joined by a 37-story (140 metres, 459 ft), two 32-story (122 metres, 400 feet), 31-story (120 metres, 394 ft), and 27-story (100 metres, 328 ft) residential buildings. In the Kalasatama area, there will be about 15 high-rises within 10 years.
As is the case with all Finnish municipalities, Helsinki's city council is the main decision-making organ in local politics, dealing with issues such as urban planning, schools, health care, and public transport. The council is chosen in the nationally-held municipal elections, which are held every four years.

Helsinki's city council consists of eighty-five members. Following the most recent municipal elections in 2017, the three largest parties are the National Coalition Party (25), the Green League (21), and the Social Democratic Party (12).

The Mayor of Helsinki is Jan Vapaavuori.

At 53 percent of the population, women form a greater proportion of Helsinki residents than the national average of 51 percent. Helsinki's population density of 2,739.36 people per square kilometre makes Helsinki the most densely-populated city in Finland. The life expectancy for men and women is slightly below the national averages: 75.1 years for men as compared to 75.7 years, 81.7 years for women as compared to 82.5 years.

Helsinki has experienced strong growth since the 1810s, when it replaced Turku as the capital of the Grand Duchy of Finland, which later became the sovereign Republic of Finland. The city continued its growth from that time on, with an exception during the Finnish Civil War. From the end of World War II up until the 1970s there was a massive exodus of people from the countryside to the cities of Finland, in particular Helsinki. Between 1944 and 1969 the population of the city nearly doubled from 275,000 to 525,600.

In the 1960s, the population growth of Helsinki began to decrease, mainly due to a lack of housing. Some residents began to move to the neighbouring cities of Espoo and Vantaa, resulting in increased population growth in both municipalities. Espoo's population increased ninefold in sixty years, from 22,874 people in 1950 to 244,353 in 2009. Vantaa saw an even more dramatic change in the same time span: from 14,976 in 1950 to 197,663 in 2009, a thirteenfold increase. These population changes prompted the municipalities of Greater Helsinki into more intense cooperation in areas such as public transportation – resulting in the foundation of HSL – and waste management. The increasing scarcity of housing and the higher costs of living in the capital region have pushed many daily commuters to find housing in formerly rural areas, and even further, to cities such as Lohja, Hämeenlinna, Lahti, and Porvoo.

Finnish and Swedish are the official languages of Helsinki. 79.1% of the citizens speak Finnish as their native language. 5.7% speak Swedish. The remaining 15.3% of the population speaks a native language other than Finnish or Swedish.

Helsinki slang is a regional dialect of the city. It combines influences mainly from Finnish and English, and has traditionally had strong Russian and Swedish influences. Finnish today is the common language of communication between Finnish speakers, Swedish speakers, and speakers of other languages (New Finns) in day-to-day affairs in the public sphere between unknown persons. Swedish is commonly spoken in city or national agencies specifically aimed at Finland-Swedish speakers, such as the Social Services Department on Hämeentie or the Luckan Cultural centre in Kamppi. Knowledge of Finnish is also essential in business and is usually a basic requirement in the employment market.

Finnish speakers surpassed Swedish speakers in 1890 to become the majority of the city's population. At the time, the population of Helsinki was 61,530.

As the crossroads of many international ports and Finland's largest airport, Helsinki is the global gateway to and from Finland. The city has Finland's largest immigrant population in both absolute and relative terms. There are over 140 nationalities represented in Helsinki. 

Foreign citizens make up 9.5% of the population, while the total immigrant population makes up 15.5%. In 2017, 98,269 residents spoke a native language other than Finnish, Swedish, or one of the three Sami languages spoken in Finland. The largest groups of residents not of Finnish background come from Russia (14,532), Estonia (9,065), and Somalia (6,845). One third of Finland's immigrant population lives in the city of Helsinki.

The number of people with a foreign mother tongue is expected to be 170,000 in 2030, or 23% of the population.

Greater Helsinki generates approximately one third of Finland's GDP. GDP per capita is roughly 1.3 times the national average. Helsinki profits on serviced-related IT and public sectors. Having moved from heavy industrial works, shipping companies also employ a substantial number of people.

The metropolitan area's gross value added per capita is 200% of the mean of 27 European metropolitan areas, equalling those of Stockholm and Paris. The gross value added annual growth has been around 4%.

83 of the 100 largest Finnish companies have their headquarters in Greater Helsinki. Two-thirds of the 200 highest-paid Finnish executives live in Greater Helsinki and 42% in Helsinki. The average income of the top 50 earners was 1.65 million euro.

The tap water is of excellent quality and it is supplied by long Päijänne Water Tunnel, one of the world's longest continuous rock tunnels.

The Temppeliaukio Church is a Lutheran church in the Töölö neighborhood of the city. The church was designed by architects and brothers Timo and Tuomo Suomalainen and opened in 1969. Built directly into solid rock, it is also known as the Church of the Rock and Rock Church.

Helsinki has 190 comprehensive schools, 41 upper secondary schools, and 15 vocational institutes. Half of the 41 upper secondary schools are private or state-owned, the other half municipal. Higher level education is given in eight universities (see the section "Universities" below) and four polytechnics.


Helsinki is one of the co-location centres of the Knowledge and Innovation Community (Future information and communication society) of The European Institute of Innovation and Technology (EIT).

The biggest historical museum in Helsinki is the National Museum of Finland, which displays a vast historical collection from prehistoric times to the 21st century. The museum building itself, a national romantic style neomedieval castle, is a tourist attraction. Another major historical museum is the Helsinki City Museum, which introduces visitors to Helsinki's 500-year history. The University of Helsinki also has many significant museums, including the Helsinki University Museum "Arppeanum" and the Finnish Museum of Natural History.

The Finnish National Gallery consists of three museums: Ateneum Art Museum for classical Finnish art, Sinebrychoff Art Museum for classical European art, and Kiasma Art Museum for modern art, in a building by architect Steven Holl. The old Ateneum, a neo-Renaissance palace from the 19th century, is one of the city's major historical buildings. All three museum buildings are state-owned through Senate Properties.

The city of Helsinki hosts its own art collection in the Helsinki Art Museum (HAM), primarily located in its Tennispalatsi gallery. Pieces outside of Tennispalatsi include about 200 public art pieces and all art held in property owned by the city.

The Design Museum is devoted to the exhibition of both Finnish and foreign design, including industrial design, fashion, and graphic design. Other museums in Helsinki include the Military Museum of Finland, Didrichsen Art Museum, Amos Rex Art Museum, and the Tram Museum.

Helsinki has three major theatres: The Finnish National Theatre, the Helsinki City Theatre, and the Swedish Theatre ("Svenska Teatern"). Other notable theatres in the city include the Alexander Theatre, "Q-teatteri", Savoy Theatre, KOM-theatre, and "Teatteri Jurkka".

Helsinki is home to two full-size symphony orchestras, the Helsinki Philharmonic Orchestra and the Finnish Radio Symphony Orchestra, both of which perform at the Helsinki Music Centre concert hall. Acclaimed contemporary composers Kaija Saariaho, Magnus Lindberg, Esa-Pekka Salonen, and Einojuhani Rautavaara, among others, were born and raised in Helsinki, and studied at the Sibelius Academy. The Finnish National Opera, the only full-time, professional opera company in Finland, is located in Helsinki. The opera singer Martti Wallén, one of the company's long-time soloists, was born and raised in Helsinki, as was mezzo-soprano Monica Groop.

Many widely renowned and acclaimed bands have originated in Helsinki, including Hanoi Rocks, HIM, Stratovarius, The 69 Eyes, Finntroll, Ensiferum, Wintersun, The Rasmus, Poets of the Fall, and Apocalyptica.

The city's main musical venues are the Finnish National Opera, the Finlandia concert hall, and the Helsinki Music Centre. The Music Centre also houses a part of the Sibelius Academy. Bigger concerts and events are usually held at one of the city's two big ice hockey arenas: the Hartwall Arena or the Helsinki Ice Hall. Helsinki has Finland's largest fairgrounds, the Messukeskus Helsinki.

Helsinki Arena hosted the Eurovision Song Contest 2007, the first Eurovision Song Contest arranged in Finland, following Lordi's win in 2006.

The Helsinki Festival is an annual arts and culture festival, which takes place every August (including the Night of the Arts).

Vappu is an annual carnival for students and workers.

At the Senate Square in fall 2010, Finland's largest open-air art exhibition to date took place: About 1.4 million people saw the international exhibition of "United Buddy Bears".

Helsinki was the 2012 World Design Capital, in recognition of the use of design as an effective tool for social, cultural, and economic development in the city. In choosing Helsinki, the World Design Capital selection jury highlighted Helsinki's use of 'Embedded Design', which has tied design in the city to innovation, "creating global brands, such as Nokia, Kone, and Marimekko, popular events, like the annual Helsinki Design Week, outstanding education and research institutions, such as the Aalto University School of Arts, Design and Architecture, and exemplary architects and designers such as Eliel Saarinen and Alvar Aalto".

Helsinki hosts many film festivals. Most of them are small venues, while some have generated interest internationally. The most prolific of these is the Love & Anarchy film festival, also known as Helsinki International Film Festival, which features films on a wide spectrum. Night Visions, on the other hand, focuses on genre cinema, screening horror, fantasy, and science fiction films in very popular movie marathons that last the entire night. Another popular film festival is DocPoint, a festival that focuses solely on documentary cinema.

Today, there are around 200 newspapers, 320 popular magazines, 2,100 professional magazines, 67 commercial radio stations, three digital radio channels, and one nationwide and five national public service radio channels.

Sanoma publishes Finland's journal of record, "Helsingin Sanomat", the tabloid "Ilta-Sanomat", the commerce-oriented "Taloussanomat", and the television channel Nelonen. Another Helsinki-based media house, Alma Media, publishes over thirty magazines, including the newspaper "Aamulehti", the tabloid "Iltalehti", and the commerce-oriented "Kauppalehti".

Finland's national public-broadcasting institution Yle operates five television channels and thirteen radio channels in both national languages. Yle is headquartered in the neighbourhood of Pasila. All TV channels are broadcast digitally, both terrestrially and on cable.

The commercial television channel MTV3 and commercial radio channel Radio Nova are owned by Nordic Broadcasting (Bonnier and Proventus Industrier).

Helsinki has a long tradition of sports: the city gained much of its initial international recognition during the 1952 Summer Olympics, and the city has arranged sporting events such as the first World Championships in Athletics 1983 and 2005, and the European Championships in Athletics 1971, 1994, and 2012. Helsinki hosts successful local teams in both of the most popular team sports in Finland: football and ice hockey. Helsinki houses HJK Helsinki, Finland's largest and most successful football club, and IFK Helsingfors, their local rivals with 7 championship titles. The fixtures between the two are commonly known as Stadin derby. Helsinki's track and field club Helsingin Kisa-Veikot is also dominant within Finland. Ice hockey is popular among many Helsinki residents, who usually support either of the local clubs IFK Helsingfors (HIFK) or Jokerit. HIFK, with 14 Finnish championships titles, also plays in the highest bandy division, along with Botnia-69. The Olympic stadium hosted the first ever Bandy World Championship in 1957.

Helsinki was elected host-city of the 1940 Summer Olympics, but due to World War II they were canceled. Instead Helsinki was the host of the 1952 Summer Olympics. The Olympics were a landmark event symbolically and economically for Helsinki and Finland as a whole that was recovering from the winter war and the continuation war fought with the Soviet Union. Helsinki was also in 1983 the first ever city to host the World Championships in Athletics. Helsinki also hosted the event in 2005, thus also becoming the first city to ever host the Championships for a second time. The Helsinki City Marathon has been held in the city every year since 1980, usually in August. A Formula 3000 race through the city streets was held on 25 May 1997. In 2009 Helsinki was host of the European Figure Skating Championships, and in 2017 it hosted World Figure Skating Championships.

The backbone of Helsinki's motorway network consists of three semicircular beltways, Ring I, Ring II, and Ring III, which connect expressways heading to other parts of Finland, and the western and eastern arteries of "Länsiväylä" and "Itäväylä" respectively. While variants of a "Keskustatunneli" tunnel under the city centre have been repeatedly proposed, the plan remains on the drawing board.

Helsinki has some 390 cars per 1000 inhabitants. This is less than in cities of similar population and construction density, such as Brussels' 483 per 1000, Stockholm's 401, and Oslo's 413.

The Helsinki Central Railway Station is the main terminus of the rail network in Finland. Two rail corridors lead out of Helsinki, the Main Line to the north (to Tampere, Oulu, Rovaniemi), and the Coastal Line to the west (to Turku). The railway connection to the east branches from the Main Line outside of Helsinki at Kerava, and leads via Lahti to eastern parts of Finland and to Russia.

A majority of intercity passenger services in Finland originate or terminate at the Helsinki Central Railway Station. All major cities in Finland are connected to Helsinki by rail service, with departures several times a day. The most frequent service is to Tampere, with more than 25 intercity departures per day as of 2017. There are international services from Helsinki to Saint Petersburg and to Moscow in Russia. The Saint Petersburg to Helsinki route is operated with the Allegro high-speed trains.

A Helsinki to Tallinn Tunnel has been proposed and agreed upon by representatives of the cities. The rail tunnel would connect Helsinki to the Estonian capital Tallinn, further linking Helsinki to the rest of continental Europe by Rail Baltica.

Air traffic is handled primarily from Helsinki Airport, located approximately north of Helsinki's downtown area, in the neighbouring city of Vantaa. Helsinki's own airport, Helsinki-Malmi Airport, is mainly used for general and private aviation. Charter flights are available from Hernesaari Heliport.

Like many other cities, Helsinki was deliberately founded at a location on the sea in order to take advantage of shipping. The freezing of the sea imposed limitations on sea traffic up to the end of the 19th century. But for the last hundred years, the routes leading to Helsinki have been kept open even in winter with the aid of icebreakers, many of them built in the Helsinki Hietalahti shipyard. The arrival and departure of ships has also been a part of everyday life in Helsinki. Regular route traffic from Helsinki to Stockholm, Tallinn, and Saint Petersburg began as far back as 1837. Over 300 cruise ships and 360,000 cruise passengers visit Helsinki annually. There are international cruise ship docks in South Harbour, Katajanokka, West Harbour, and Hernesaari. Helsinki is the second busiest passenger port in Europe with approximately 11 million passengers in 2013. Ferry connections to Tallinn, Mariehamn, and Stockholm are serviced by various companies. Finnlines passenger-freight ferries to Gdynia, Poland; Travemünde, Germany; and Rostock, Germany are also available. St. Peter Line offers passenger ferry service to Saint Petersburg several times a week.

In the Helsinki metropolitan area, public transportation is managed by the Helsinki Regional Transport Authority, the metropolitan area transportation authority. The diverse public transport system consists of trams, commuter rail, the metro, bus lines, two ferry lines and a public bike system.

Helsinki's tram system has been in operation with electric drive continuously since 1900. 13 routes that cover the inner part of the city are operated. As of 2017, the city is expanding the tram network, with several major tram line construction projects under way. These include the 550 trunk line (Raide-Jokeri), roughly along Ring I around the city center, and a new tramway to the island of Laajasalo.
The Helsinki Metro, opened in 1982, is the only metro system in Finland, albeit the Helsinki commuter rail trains operate at metro-like frequencies. In 2006, the construction of the long debated extension of the metro into Western Helsinki and Espoo was approved. The extension finally opened after delays in November 2017. An eastern extension into the planned new district of Östersundom and neighboring Sipoo has also been seriously debated. Helsinki's metro system currently consists of 25 stations, with 14 of them underground.

The commuter rail system includes purpose-built double track for local services in two rail corridors along intercity railways, and the Ring Rail Line, an urban double-track railway with a station at the Helsinki Airport in Vantaa. Electric operation of commuter trains was first begun in 1969, and the system has been gradually expanded since. 15 different services are operated as of 2017, some extending outside of the Helsinki region. The frequent services run at a 10-minute headway in peak traffic.

Helsinki has no official sister cities, but it has a special partnership relation with:






</doc>
<doc id="13699" url="https://en.wikipedia.org/wiki?curid=13699" title="Hobart">
Hobart

Hobart () is the capital and most populous city of the Australian island state of Tasmania. With a population of approximately 225,000 (over 40% of Tasmania's population), it is the least populated Australian state capital city, and second smallest if territories are taken into account (after Darwin, Northern Territory). Founded in 1804 as a British penal colony, Hobart, formerly known as Hobart Town or Hobarton, is Australia's second oldest capital city after Sydney, New South Wales. Prior to British settlement, the Hobart area had been occupied for possibly as long as 35,000 years, by the semi-nomadic Mouheneener tribe, a sub-group of the Nuennone, or South-East tribe. The descendants of these Aboriginal Tasmanians often refer to themselves as 'Palawa'.

Since its foundation as a colonial outpost, the city has expanded from the mouth of Sullivans Cove in a generally north-south direction along both banks of the Derwent River, from 22 km inland from the estuary at Storm Bay to the point where the river reverts to fresh water at Bridgewater. Penal transportation ended in the 1850s, after which the city experienced periods of growth and decline. The early 20th century saw an economic boom on the back of mining, agriculture and other primary industries, and the loss of men who served in the world wars was counteracted by an influx of immigration. Despite the rise in migration from Asia and other non-English speaking parts of the world, Hobart's population remains predominantly ethnically Anglo-Celtic, and has the highest percentage of Australian-born residents among the Australian capital cities.

In June 2016, the estimated greater area population was 224,462. The city is located in the state's south-east on the estuary of the Derwent River, making it the most southern of Australia's capital cities. Its harbour forms the second-deepest natural port in the world. Its skyline is dominated by the kunanyi/Mount Wellington, and much of the city's waterfront consists of reclaimed land. It is the financial and administrative heart of Tasmania, serving as the home port for both Australian and French Antarctic operations and acting as a major tourist hub, with over 1.192 million visitors in 2011/2012. The metropolitan area is often referred to as "Greater Hobart", to differentiate it from the City of Hobart, one of the five local government areas that cover the city.

The first European settlement began in 1803 as a military camp at Risdon Cove on the eastern shores of the Derwent River, amid British concerns over the presence of French explorers. In 1804, along with the military, settlers and convicts from the abandoned Port Phillip settlement, the camp at Risdon Cove was moved by Captain David Collins to a better location at the present site of Hobart at Sullivans Cove. The city, initially known as "Hobart Town" or "Hobarton", was named after Lord Hobart, the British secretary of state for war and the colonies.

The area's indigenous inhabitants were members of the semi-nomadic "Mouheneener" tribe. Violent conflict with the European settlers, and the effects of diseases brought by them, dramatically reduced the aboriginal population, which was rapidly replaced by free settlers and the convict population. Charles Darwin visited Hobart Town in February 1836 as part of the Beagle expedition. He writes of Hobart and the Derwent estuary in his "Voyage of the Beagle":
...The lower parts of the hills which skirt the bay are cleared; and the bright yellow fields of corn, and dark green ones of potatoes, appear very luxuriant... I was chiefly struck with the comparative fewness of the large houses, either built or building. Hobart Town, from the census of 1835, contained 13,826 inhabitants, and the whole of Tasmania 36,505.

The Derwent River was one of Australia's finest deepwater ports and was the centre of the Southern Ocean whaling and sealing trades. The settlement rapidly grew into a major port, with allied industries such as shipbuilding.

Hobart Town became a city on 21 August 1842, and was renamed Hobart from the beginning of 1881.

Hobart is located on the estuary of the Derwent River in the state's south-east. Geologically Hobart is built predominantly on Jurassic dolerite around the foothills interspersed with smaller areas of Triassic siltstone and Permian mudstone. Hobart extends along both sides of the Derwent River; on the western shore from the Derwent valley in the north through the flatter areas of Glenorchy which rests on older Triassic sediment and into the hilly areas of New Town, Lenah Valley. Both of these areas rest on the younger Jurassic dolerite deposits, before stretching into the lower areas such as the beaches of Sandy Bay in the south, in the Derwent estuary. South of the Derwent estuary lies Storm Bay and the Tasman Peninsula.

The Eastern Shore also extends from the Derwent valley area in a southerly direction hugging the Meehan Range in the east before sprawling into flatter land in suburbs such as Bellerive. These flatter areas of the eastern shore rest on far younger deposits from the Quaternary. From there the city extends in an easterly direction through the Meehan Range into the hilly areas of Rokeby and Oakdowns, before reaching into the tidal flatland area of Lauderdale.

Hobart has access to a number of beach areas including those in the Derwent estuary itself; Sandy Bay, Cornelian Bay, Nutgrove, Kingston, Bellerive, and Howrah Beaches as well as many more in Frederick Henry Bay such as; Seven Mile, Roaches, Cremorne, Clifton, and Goats Beaches.

Hobart has a mild temperate oceanic climate (). The highest temperature recorded was on 4 January 2013 and the lowest was on 25 June 1972 and 11 July 1981. Annually, Hobart receives 40.8 clear days. Compared to other major Australian cities, Hobart has the fewest daily average hours of sunshine, with 5.9 hours per day. However, during the summer it has the most hours of daylight of any Australian city, with 15.3 hours on the summer solstice.

Although Hobart itself rarely receives snow during the winter (the city's geographic position keeps temperatures from plummeting far below zero Celsius), the adjacent kunanyi/Mount Wellington is often seen with a snowcap. Mountain snow covering has also been known to occur during the other seasons. During the 20th century, the city itself has received snowfalls at sea level on average only once every 15 years; however, outer suburbs lying higher on the slopes of Mount Wellington receive snow more often, owing to cold air masses arriving from Antarctica coupled with them resting at higher altitude. These snow-bearing winds often carry on through Tasmania and Victoria to the Snowy Mountains in northern Victoria and southern New South Wales.

The average temperature of the sea ranges from in September to in February.

At the 2016 census there were 222,356 people in the Greater Hobart area making it the second least populated capital city in Australia. The City of Hobart local government area had a population of 50,439. According to the 2016 census, approximately 20.2% of Greater Hobart's residents were born overseas, most commonly the United Kingdom, New Zealand and China.

The most common occupation categories were professionals (22.6%), clerical and administrative workers (14.7%), technicians and trades workers (13.3%), community and personal service workers (12.8%), and managers (11.3%). The median weekly household income was $1,234, compared with $1,438 nationally.

In the 2016 census, 52.1% of Greater Hobart residents who responded to the question specified a Christian religion. Major religious affiliations were Anglican (19.8%), Catholic (17.0%) and Uniting Church (2.5%). In addition, 39.9% specified "No Religion" and 9.3% did not answer.

Hobart has a small Mormon community of around 642 (2011), with meetinghouses in Glenorchy, Rosny, and Glen Huon. There is also a synagogue where the Jewish community, of around 111 (2001), or 0.05% of the Hobart population, worships. Hobart has a Bahá'í community, with a Bahá'í Centre of Learning, located within the city.

In 2013, Hillsong Church established a Hillsong Connect campus in Hobart.

Shipping is significant to the city's economy. Hobart is the home port for the Antarctic activities of Australia and France. The port loads around 2,000 tonnes of Antarctic cargo a year for the Australian research vessel "Aurora Australis." The city is also a popular cruise ship destination during the summer months, with 47 such ships docking during the course of the 2016–17 summer season.

The city also supports many other industries. Major local employers include catamaran builder Incat, zinc refinery Nyrstar, Cascade Brewery and Cadbury's Chocolate Factory, Norske Skog and Wrest Point Casino. The city also supports a host of light industry manufacturers, as well as a range of redevelopment projects, including the $689 million Royal Hobart Hospital Redevelopment – standing as the states largest ever Health Infrastructure project. Tourism is a significant part of the economy, with visitors coming to the city to explore its historic inner suburbs and nationally acclaimed restaurants and cafes, as well as its vibrant music and nightlife culture. The two major draw-cards are the weekly market in Salamanca Place, and the Museum of Old and New Art. The city is also used as a base from which to explore the rest of Tasmania.

The last 15–20 years has seen Hobart's wine industry thrive as many vineyards have developed in countryside areas outside of the city in the Coal River Wine Region and D'Entrecasteaux Channel, including Moorilla Estate at Berriedale one of the most awarded vineyards in Australia.

Hobart is an Antarctic gateway city, with geographical proximity to East Antarctica and the Southern Ocean. Infrastructure is provided by the port of Hobart for scientific research and cruise ships, and Hobart International Airport supports an Antarctic Airlink to Wilkins Runway at Casey Station. Hobart is a logistics point for the French icebreaker "L'Astrolabe".

Hobart is the home port for the Australian and French Antarctic programs, and provides port services for other visiting Antarctic nations and Antarctic cruise ships. Antarctic and Southern Ocean expeditions are supported by a specialist cluster offering cold climate products, services and scientific expertise. The majority of these businesses and organisations are members of the Tasmanian polar network, supported in part by the Tasmanian State Government.

Tasmania has a high concentration of Antarctic and Southern Ocean scientists. Hobart is home to the following Antarctic and Southern Ocean scientific institutions:

Hobart serves as a focal point and mecca for tourism in the state of Tasmania. In 2016, Hobart received 1.8 million visitors, surpassing both Perth and Canberra, tying equally with Brisbane.

The Royal Tasmanian Botanical Gardens is a popular recreation area a short distance from the city centre. It is the second-oldest Botanic Gardens in Australia and holds extensive significant plant collections.

Hadley's Orient Hotel, on Hobart's Murray Street, is the oldest continuously operating hotel in Australia.

kunanyi/Mount Wellington, accessible by passing through Fern Tree, is the dominant feature of Hobart's skyline. Indeed, many descriptions of Hobart have used the phrase "nestled amidst the foothills", so undulating is the landscape. At 1,271 metres, the mountain has its own ecosystems, is rich in biodiversity and plays a large part in determining the local weather.

The Tasman Bridge is also a uniquely important feature of the city, connecting the two shores of Hobart and visible from many locations. The Hobart Synagogue is the oldest synagogue in Australia and a rare surviving example of an Egyptian Revival synagogue.

Hobart is known for its well-preserved historic architecture, much of it dating back to the Georgian and Victorian eras, giving the city a distinctly "Old World" feel. For locals, this became a source of discomfiture about the city's convict past, but is now a draw card for tourists. Regions within the city centre, such as Salamanca Place, contain many of the city's heritage-listed buildings. Historic homes and mansions also exist in the suburbs.

Kelly's Steps were built in 1839 by shipwright and adventurer James Kelly to provide a short-cut from Kelly Street and Arthur Circus in Battery Point to the warehouse and dockyards district of Salamanca Place. In 1835, John Lee Archer designed and oversaw the construction of the sandstone Customs House, facing Sullivans Cove. Completed in 1840, it was used as Tasmania's parliament house, and is now commemorated by a pub bearing the same name (built in 1844) which is frequented by yachtsmen after they have completed the Sydney to Hobart yacht race.

Hobart is also home to many historic churches. The Scots Church (formerly known as St Andrew's) was built in Bathurst Street from 1834 to 1836, and a small sandstone building within the churchyard was used as the city's first Presbyterian Church. The Salamanca Place warehouses and the Theatre Royal were also constructed in this period. The Greek revival St George's Anglican Church in Battery Point was completed in 1838, and a classical tower, designed by James Blackburn, was added in 1847. St Joseph's was built in 1840. St David's Cathedral, Hobart's first cathedral, was consecrated in 1874.

Hobart has very few high rise buildings in comparison to other Australian cities. This is partly a result of height limits imposed due to Hobart's proximity to Derwent River and Mount Wellington.

Hobart is home to the Tasmanian Symphony Orchestra, which is resident at the Federation Concert Hall on the city's waterfront. It offers a year-round program of concerts and is thought to be one of the finest small orchestras in the world. Hobart also plays host to the University of Tasmania's acclaimed Australian International Symphony Orchestra Institute (AISOI) which brings pre-professional advanced young musicians to town from all over Australia and internationally. The AISOI plays host to a public concert season during the first two weeks of December every year focusing on large symphonic music. Like the Tasmanian Symphony Orchestra, the AISOI uses the Federation Concert Hall as its performing base.

Hobart is home to Australia's oldest theatre, the Theatre Royal, as well as the Playhouse theatre, the Backspace theatre and many smaller stage theatres. It also has three Village Cinema complexes, one each in Hobart CBD, Glenorchy and Rosny, with the possibility of a fourth being developed in Kingston. The State Cinema in North Hobart specialises in arthouse and foreign films.

Australia's first published novel, " Quintus Servinton", was written and published in Hobart. It was written by a convict, Henry Savery, in a Hobart prison cell in 1830, while serving a sentence for forgery. A generally autobiographical work, it's the story of what happens to a well educated man from a relatively well to do family, who makes poor choices in life.

The city has also long been home to a thriving classical, jazz, folk, punk, hip-hop, electro, metal and rock music scene. Internationally recognised musicians such as metal acts Striborg and Psycroptic, indie-electro bands The Paradise Motel and The Scientists of Modern Music, singer-songwriters Sacha Lucashenko (of The Morning After Girls), Michael Noga (of The Drones), and Monique Brumby, two-thirds of indie rock band Love of Diagrams, post punk band Sea Scouts, theremin player Miles Brown, blues guitarist Phil Manning (of blues-rock band Chain), power-pop group The Innocents are all successful expatriates. In addition, founding member of Violent Femmes, Brian Ritchie, now calls Hobart home, and has formed a local band, The Green Mist. Ritchie also curates the annual international arts festival MONA FOMA, held at Salamanca Place's waterfront venue, Princes Wharf, Shed No. 1. Hobart hosts many significant festivals including winter's landmark cultural event, the "Festival of Voices", Australia's premier festival celebration of voice, and Tasmania's biennial international arts festival Ten Days On The Island. Other festivals, including the "Hobart Fringe Festival", Hobart Summer Festival, Southern Roots Festival, the Falls Festival in Marion Bay and the Soundscape Festival also capitalise on Hobart's artistic communities.

Hobart is home to the Tasmanian Museum and Art Gallery. The Meadowbank Estate winery and restaurant features a floor mural by Tom Samek, part funded by the Federal Government. The Museum of Old and New Art (MONA) opened in 2011 to coincide with the third annual MONA FOMA festival. The multi-storey MONA gallery was built directly underneath the historic Sir Roy Grounds courtyard house, overlooking the Derwent River. This building serves as the entrance to the MONA Gallery.

Designed by the prolific architect Sir Roy Grounds, the 17-storey Wrest Point Hotel Casino in Sandy Bay, opened as Australia's first legal casino in 1973.

The city's nightlife primarily revolves around Salamanca Place, the waterfront area, Elizabeth St in North Hobart and Sandy Bay, but popular pubs, bars and nightclubs exist around the city as well. Major national and international music events are usually held at the Derwent Entertainment Centre, or the Casino. Popular restaurant strips include Elizabeth Street in North Hobart, and Salamanca Place near the waterfront. These include numerous ethnic restaurants including Chinese, Thai, Greek, Pakistani, Italian, Indian and Mexican. The major shopping street in the CBD is Elizabeth Street, with the pedestrianised Elizabeth Mall and the General Post Office.

Close Shave, one of Australia's longest serving male a cappella quartets, is based in Hobart.
Hobart is internationally famous among the yachting community as the finish of the Sydney to Hobart Yacht Race which starts in Sydney on Boxing Day (the day after Christmas Day). The arrival of the yachts is celebrated as part of the Hobart Summer Festival, a food and wine festival beginning just after Christmas and ending in mid-January. The Taste of Tasmania is a major part of the festival, where locals and visitors can taste fine local and international food and wine.

The city is the finishing point of the Targa Tasmania rally car event, which has been held annually in April since 1991.

The annual Tulip Festival at the Royal Tasmanian Botanical Gardens is a popular Spring celebration in the city.

The Australian Wooden Boat Festival is a biennial event held in Hobart celebrating wooden boats. It is held concurrently with the Royal Hobart Regatta, which began in 1830 and is therefore Tasmania's oldest surviving sporting event.

Most professional Hobart-based sports teams represent Tasmania as a whole rather than exclusively the city.

Cricket is a popular game of the city. The Tasmanian Tigers cricket team plays its home games at the Bellerive Oval on the Eastern Shore. A new team, Hobart Hurricanes represent the city in the Big Bash League. Bellerive Oval has been the breeding ground of some world class cricket players including the former Australia captain Ricky Ponting.

Despite Australian rules football's huge popularity in the state of Tasmania, the state does not have a team in the Australian Football League. However, a bid for an Tasmanian AFL team is a popular topic among football fans. The State government is one of the potential sponsors of such a team. Local domestic club football is still played. Tasmanian State League football features five clubs from Hobart, and other leagues such as Southern Football League and the Old Scholars Football Association are also played each Winter.

The city has two local rugby league football teams (Hobart Tigers and South Hobart Storm) that compete in the Tasmanian Rugby League.

Tasmania is not represented by teams in the NRL, Super Rugby, ANZ Championship, A-League, or NBL. However, the Hobart Chargers do represent Hobart in the second-tier South East Australian Basketball League. Besides the bid for an AFL club which was passed over in favour of a second Queensland team, despite several major local businesses and the Premier pioneering for a club, there is also a Hobart bid for entry into the A-League.

Hockey Tasmania has a men's team (the Tasmanian Tigers) and a women's team (the Van Demons) competing in the Australian Hockey League.

The city co-hosted the basketball FIBA Oceania Championship 1975.

Five free-to-air television stations service Hobart:
Each station broadcasts a primary channel and several multichannels.

Hobart is served by twenty-eight digital free-to-air television channels:

The majority of pay television services are provided by Foxtel via satellite, although other smaller pay television providers do service Hobart.

Commercial radio stations licensed to cover the Hobart market include Triple M Hobart, HIT 100.9 and 7HO FM. Local community radio stations include Christian radio station Ultra106five, Edge Radio and 92FM which targets the wider community with specialist programmes. The five ABC radio networks available on analogue radio broadcast to Hobart via 936 ABC Hobart, Radio National, Triple J, NewsRadio and ABC Classic FM. Hobart is also home to the video creation company Biteable.

Hobart's major newspaper is "The Mercury", which was founded by John Davies in 1854 and has been continually published ever since. The paper is currently owned and operated by Rupert Murdoch's News Limited.

Greater Hobart metropolitan area consists of five local government areas of which three, City of Hobart, City of Glenorchy and City of Clarence are designated as cities. Hobart also includes the urbanised local governments of the Municipality of Kingborough and Municipality of Brighton. Each local government services all the suburbs that are within its geographical boundaries and are responsible for their own urban area, up to a certain scale, and residential planning as well as waste management and mains water storage.

Most citywide events such as the Taste of Tasmania and Hobart Summer Festival are funded by the Tasmanian State Government as a joint venture with the Hobart City Council. Urban planning of the Hobart CBD in particular the Heritage listed areas such as Sullivans Cove are also intensely scrutinised by State Government, which is operated out of Parliament House on the waterfront.

Hobart is home to the main campus of the University of Tasmania, located in Sandy Bay. On-site accommodation colleges include Christ College, Jane Franklin Hall and St John Fisher College. Other campuses are in Launceston and Burnie.

The Greater Hobart area contains 122 primary, secondary and pretertiary (College) schools distributed throughout Clarence, Glenorchy and Hobart City Councils and Kingborough and Brighton Municipalities. These schools are made up of a mix of public, catholic, private and independent run, with the heaviest distribution lying in the more densely populated West around the Hobart city core. TasTAFE operates a total of seven polytechnic campuses within the Greater Hobart area that provide vocational education and training.

The only public transportation within the city of Hobart is via a network of Metro Tasmania buses funded by
the Tasmanian Government and a small number of private bus services. Like many large Australian cities, Hobart once operated passenger tram services, a trolleybus network consisting of six routes which operated until 1968. However, the tramway closed in the early 1960s. The tracks are still visible in the older streets of Hobart.

Suburban passenger trains, run by the Tasmanian Government Railways, were closed in 1974 and the intrastate passenger service, the Tasman Limited, ceased running in 1978. Recently though there has been a push from the city, and increasingly from government, to establish a light rail network, intended to be fast, efficient, and eco-friendly, along existing tracks in a North South corridor; to help relieve the frequent jamming of traffic in Hobart CBD.

The main arterial routes within the urban area are the Brooker Highway to Glenorchy and the northern suburbs, the Tasman Bridge and Bowen Bridge across the river to Rosny and the Eastern Shore. The East Derwent Highway to Lindisfarne, Geilston Bay, and Northwards to Brighton, the South Arm Highway leading to Howrah, Rokeby, Lauderdale and Opossum Bay and the Southern Outlet south to Kingston and the D'Entrecasteaux Channel. Leaving the city, motorists can travel the Lyell Highway to the west coast, Midland Highway to Launceston and the north, Tasman Highway to the east coast, or the Huon Highway to the far south.

Ferry services from Hobart's Eastern Shore into the city were once a common form of public transportation, but with lack of government funding, as well as a lack of interest from the private sector, there has been the demise of a regular commuter ferry service – leaving Hobart's commuters relying solely on travel by automobiles and buses. There is however a water taxi service operating from the Eastern Shore into Hobart which provides an alternative to the Tasman Bridge.

Hobart is served by Hobart International Airport with flights to/from Melbourne (Qantas, Virgin Australia, Jetstar Airways and Tiger Airways Australia); Sydney (Qantas, Jetstar and Virgin); Brisbane (Virgin); Perth (Virgin); Gold Coast (Tiger Airways); and Adelaide (Jetstar).The smaller Cambridge Aerodrome mainly serves small charter airlines offering local tourist flights. In the past decade, Hobart International Airport received a huge upgrade, with the airport now being a first class airport facility.

In 2009, it was announced that Hobart Airport would receive more upgrades, including a first floor, aerobridges (currently, passengers must walk on the tarmac) and shopping facilities. Possible new international flights to Asia and New Zealand, and possible new domestic flights to Darwin and Cairns have been proposed. A second runway, possibly to be constructed in the next 15 years, would assist with growing passenger numbers to Hobart. Hobart Control Tower may be renovated and fitted with new radar equipment, and the airport's carpark may be extended further. Also, new facilities will be built just outside the airport. A new service station, hotel and day care centre have already been built and the road leading to the airport has been maintained and re-sealed. In 2016, work began on a 500-metre extension of the existing runway in addition to a $100 million upgrade of the airport. The runway extension is expected to allow international flights to land and increase air-traffic with Antarctica. This upgrade was, in part, funded under a promise made during the 2013 federal election by the Abbott government.









</doc>
<doc id="13700" url="https://en.wikipedia.org/wiki?curid=13700" title="Hesiod">
Hesiod

Hesiod (; "Hēsíodos") was a Greek poet generally thought by scholars to have been active between 750 and 650 BC, around the same time as Homer. He is generally regarded as the first written poet in the Western tradition to regard himself as an individual persona with an active role to play in his subject. Ancient authors credited Hesiod and Homer with establishing Greek religious customs. Modern scholars refer to him as a major source on Greek mythology, farming techniques, early economic thought (he is sometimes considered history's first economist), archaic Greek astronomy and ancient time-keeping.

The dating of Hesiod's life is a contested issue in scholarly circles ("see § Dating below"). Epic narrative allowed poets like Homer no opportunity for personal revelations. However, Hesiod's extant work comprises several didactic poems in which he went out of his way to let his audience in on a few details of his life. There are three explicit references in "Works and Days", as well as some passages in his "Theogony" that support inferences made by scholars. The former poem says that his father came from Cyme in Aeolis (on the coast of Asia Minor, a little south of the island Lesbos) and crossed the sea to settle at a hamlet, near Thespiae in Boeotia, named Ascra, "a cursed place, cruel in winter, hard in summer, never pleasant" ("Works" 640). Hesiod's patrimony there, a small piece of ground at the foot of Mount Helicon, occasioned lawsuits with his brother Perses, who seems, at first, to have cheated him of his rightful share thanks to corrupt authorities or "kings" but later became impoverished and ended up scrounging from the thrifty poet ("Works" 35, 396).

Unlike his father, Hesiod was averse to sea travel, but he once crossed the narrow strait between the Greek mainland and Euboea to participate in funeral celebrations for one Athamas of Chalcis, and there won a tripod in a singing competition. He also describes a meeting between himself and the Muses on Mount Helicon, where he had been pasturing sheep when the goddesses presented him with a laurel staff, a symbol of poetic authority ("Theogony" 22–35). Fanciful though the story might seem, the account has led ancient and modern scholars to infer that he was not a professionally trained rhapsode, or he would have been presented with a lyre instead.

Some scholars have seen Perses as a literary creation, a foil for the moralizing that Hesiod develops in "Works and Days", but there are also arguments against that theory. For example, it is quite common for works of moral instruction to have an imaginative setting, as a means of getting the audience's attention, but it could be difficult to see how Hesiod could have travelled around the countryside entertaining people with a narrative about himself if the account was known to be fictitious. Gregory Nagy, on the other hand, sees both "Pérsēs" ("the destroyer" from , "pérthō") and "Hēsíodos" ("he who emits the voice" from , "híēmi" and , "audḗ") as fictitious names for poetical personae. 

It might seem unusual that Hesiod's father migrated from Asia Minor westwards to mainland Greece, the opposite direction to most colonial movements at the time, and Hesiod himself gives no explanation for it. However around 750 BC or a little later, there was a migration of seagoing merchants from his original home in Cyme in Asia Minor to Cumae in Campania (a colony they shared with the Euboeans), and possibly his move west had something to do with that, since Euboea is not far from Boeotia, where he eventually established himself and his family. The family association with Aeolian Cyme might explain his familiarity with eastern myths, evident in his poems, though the Greek world might have already developed its own versions of them.

In spite of Hesiod's complaints about poverty, life on his father's farm could not have been too uncomfortable if "Works and Days" is anything to judge by, since he describes the routines of prosperous yeomanry rather than peasants. His farmer employs a friend ("Works and Days" 370) as well as servants (502, 573, 597, 608, 766), an energetic and responsible ploughman of mature years (469 ff.), a slave boy to cover the seed (441–6), a female servant to keep house (405, 602) and working teams of oxen and mules (405, 607f.). One modern scholar surmises that Hesiod may have learned about world geography, especially the catalogue of rivers in "Theogony" (337–45), listening to his father's accounts of his own sea voyages as a merchant. The father probably spoke in the Aeolian dialect of Cyme but Hesiod probably grew up speaking the local Boeotian, belonging to the same dialect group. However, while his poetry features some Aeolisms there are no words that are certainly Boeotian. His basic language was the main literary dialect of the time, Homer's Ionian.

It is probable that Hesiod wrote his poems down, or dictated them, rather than passed them on orally, as rhapsodes did—otherwise the pronounced personality that now emerges from the poems would surely have been diluted through oral transmission from one rhapsode to another. Pausanias asserted that Boeotians showed him an old tablet made of lead on which the "Works" were engraved. If he did write or dictate, it was perhaps as an aid to memory or because he lacked confidence in his ability to produce poems extempore, as trained rhapsodes could do. It certainly wasn't in a quest for immortal fame since poets in his era had probably no such notions for themselves. However, some scholars suspect the presence of large-scale changes in the text and attribute this to oral transmission. Possibly he composed his verses during idle times on the farm, in the spring before the May harvest or the dead of winter.

The personality behind the poems is unsuited to the kind of "aristocratic withdrawal" typical of a rhapsode but is instead "argumentative, suspicious, ironically humorous, frugal, fond of proverbs, wary of women." He was in fact a misogynist of the same calibre as the later poet Semonides. He resembles Solon in his preoccupation with issues of good versus evil and "how a just and all-powerful god can allow the unjust to flourish in this life". He recalls Aristophanes in his rejection of the idealised hero of epic literature in favour of an idealised view of the farmer. Yet the fact that he could eulogise kings in "Theogony" (80 ff., 430, 434) and denounce them as corrupt in "Works and Days" suggests that he could resemble whichever audience he composed for.

Various legends accumulated about Hesiod and they are recorded in several sources: 

Two different—yet early—traditions record the site of Hesiod's grave. One, as early as Thucydides, reported in Plutarch, the "Suda" and John Tzetzes, states that the Delphic oracle warned Hesiod that he would die in Nemea, and so he fled to Locris, where he was killed at the local temple to Nemean Zeus, and buried there. This tradition follows a familiar ironic convention: the oracle that predicts accurately after all. The other tradition, first mentioned in an epigram by Chersias of Orchomenus written in the 7th century BC (within a century or so of Hesiod's death) claims that Hesiod lies buried at Orchomenus, a town in Boeotia. According to Aristotle's "Constitution of Orchomenus," when the Thespians ravaged Ascra, the villagers sought refuge at Orchomenus, where, following the advice of an oracle, they collected the ashes of Hesiod and set them in a place of honour in their "agora", next to the tomb of Minyas, their eponymous founder. Eventually they came to regard Hesiod too as their "hearth-founder" (, "oikistēs"). Later writers attempted to harmonize these two accounts.

Greeks in the late 5th and early 4th centuries BC considered their oldest poets to be Orpheus, Musaeus, Hesiod and Homer—in that order. Thereafter, Greek writers began to consider Homer earlier than Hesiod. Devotees of Orpheus and Musaeus were probably responsible for precedence being given to their two cult heroes and maybe the Homeridae were responsible in later antiquity for promoting Homer at Hesiod's expense.

The first known writers to locate Homer earlier than Hesiod were Xenophanes and Heraclides Ponticus, though Aristarchus of Samothrace was the first actually to argue the case. Ephorus made Homer a younger cousin of Hesiod, the 5th century BC historian Herodotus ("Histories" II, 53) evidently considered them near-contemporaries, and the 4th century BC sophist Alcidamas in his work "Mouseion" even brought them together for an imagined poetic "ágōn" (), which survives today as the "Contest of Homer and Hesiod". Most scholars today agree with Homer's priority but there are good arguments on either side.

Hesiod certainly predates the lyric and elegiac poets whose work has come down to the modern era. Imitations of his work have been observed in Alcaeus, Epimenides, Mimnermus, Semonides, Tyrtaeus and Archilochus, from which it has been inferred that the latest possible date for him is about 650 BC.

An upper limit of 750 BC is indicated by a number of considerations, such as the probability that his work was written down, the fact that he mentions a sanctuary at Delphi that was of little national significance before c. 750 BC ("Theogony" 499), and that he lists rivers that flow into the Euxine, a region explored and developed by Greek colonists beginning in the 8th century BC. ("Theogony" 337–45).

Hesiod mentions a poetry contest at Chalcis in Euboea where the sons of one Amphidamas awarded him a tripod ("Works and Days" 654–662). Plutarch identified this Amphidamas with the hero of the Lelantine War between Chalcis and Eretria and he concluded that the passage must be an interpolation into Hesiod's original work, assuming that the Lelantine War was too late for Hesiod. Modern scholars have accepted his identification of Amphidamas but disagreed with his conclusion. The date of the war is not known precisely but estimates placing it around 730–705 BC, fit the estimated chronology for Hesiod. In that case, the tripod that Hesiod won might have been awarded for his rendition of "Theogony", a poem that seems to presuppose the kind of aristocratic audience he would have met at Chalcis.

Three works have survived which are attributed to Hesiod by ancient commentators: "Works and Days", "Theogony", and "Shield of Heracles". Other works attributed to him are only found now in fragments. The surviving works and fragments were all written in the conventional metre and language of epic. However, the "Shield of Heracles" is now known to be spurious and probably was written in the sixth century BC. Many ancient critics also rejected "Theogony" (e.g., Pausanias 9.31.3), even though Hesiod mentions himself by name in that poem. "Theogony" and "Works and Days" might be very different in subject matter, but they share a distinctive language, metre, and prosody that subtly distinguish them from Homer's work and from the "Shield of Heracles" (see Hesiod's Greek below). Moreover, they both refer to the same version of the Prometheus myth. Yet even these authentic poems may include interpolations. For example, the first ten verses of the "Works and Days" may have been borrowed from an Orphic hymn to Zeus (they were recognised as not the work of Hesiod by critics as ancient as Pausanias).
Some scholars have detected a proto-historical perspective in Hesiod, a view rejected by Paul Cartledge, for example, on the grounds that Hesiod advocates a not-forgetting without any attempt at verification. Hesiod has also been considered the father of gnomic verse. He had "a passion for systematizing and explaining things". Ancient Greek poetry in general had strong philosophical tendencies and Hesiod, like Homer, demonstrates a deep interest in a wide range of 'philosophical' issues, from the nature of divine justice to the beginnings of human society. Aristotle ("Metaphysics" 983b–987a) believed that the question of first causes may even have started with Hesiod ("Theogony" 116–53) and Homer ("Iliad" 14.201, 246).

He viewed the world from outside the charmed circle of aristocratic rulers, protesting against their injustices in a tone of voice that has been described as having a "grumpy quality redeemed by a gaunt dignity" but, as stated in the biography section, he could also change to suit the audience. This ambivalence appears to underlie his presentation of human history in "Works and Days", where he depicts a golden period when life was easy and good, followed by a steady decline in behaviour and happiness through the silver, bronze, and Iron Ages – except that he inserts a heroic age between the last two, representing its warlike men as better than their bronze predecessors. He seems in this case to be catering to two different world-views, one epic and aristocratic, the other unsympathetic to the heroic traditions of the aristocracy.

The "Theogony" is commonly considered Hesiod's earliest work. Despite the different subject matter between this poem and the "Works and Days", most scholars, with some notable exceptions, believe that the two works were written by the same man. As M.L. West writes, "Both bear the marks of a distinct personality: a surly, conservative countryman, given to reflection, no lover of women or life, who felt the gods' presence heavy about him."

The "Theogony" concerns the origins of the world (cosmogony) and of the gods (theogony), beginning with Chaos, Gaia, Tartarus and Eros, and shows a special interest in genealogy. Embedded in Greek myth, there remain fragments of quite variant tales, hinting at the rich variety of myth that once existed, city by city; but Hesiod's retelling of the old stories became, according to Herodotus, the accepted version that linked all Hellenes.

The creation myth in Hesiod has long been held to have Eastern influences, such as the Hittite Song of Kumarbi and the Babylonian Enuma Elis. This cultural crossover would have occurred in the eighth and ninth century Greek trading colonies such as Al Mina in North Syria. (For more discussion, read Robin Lane Fox's "Travelling Heroes" and Walcot's "Hesiod and the Near East.")

The "Works and Days" is a poem of over 800 lines which revolves around two general truths: labour is the universal lot of Man, but he who is willing to work will get by. Scholars have interpreted this work against a background of agrarian crisis in mainland Greece, which inspired a wave of documented colonisations in search of new land. This poem is one of the earliest known musings on economic thought.

This work lays out the five Ages of Man, as well as containing advice and wisdom, prescribing a life of honest labour and attacking idleness and unjust judges (like those who decided in favour of Perses) as well as the practice of usury. It describes immortals who roam the earth watching over justice and injustice. The poem regards labor as the source of all good, in that both gods and men hate the idle, who resemble drones in a hive. In the horror of the triumph of violence over hard work and honor, verses describing the "Golden Age" present the social character and practice of nonviolent diet through agriculture and fruit-culture as a higher path of living sufficiently.

In addition to the "Theogony" and "Works and Days", numerous other poems were ascribed to Hesiod during antiquity. Modern scholarship has doubted their authenticity, and these works are generally referred to as forming part of the "Hesiodic Corpus" whether or not their authorship is accepted. The situation is summed up in this formulation by Glenn Most:

Of these works forming the extended Hesiodic corpus, only the "Shield of Heracles" (, "Aspis Hērakleous") is transmitted intact via a medieval manuscript tradition.

Classical authors also attributed to Hesiod a lengthy genealogical poem known as "Catalogue of Women" or "Ehoiai" (because sections began with the Greek words "ē hoiē," "Or like the one who ..."). It was a mythological catalogue of the mortal women who had mated with gods, and of the offspring and descendants of these unions.

Several additional hexameter poems were ascribed to Hesiod:

In addition to these works, the "Suda" lists an otherwise unknown "dirge for Batrachus, [Hesiod's] beloved".



The Roman bronze bust, the so-called "Pseudo-Seneca," of the late first century BC found at Herculaneum is now thought not to be of Seneca the Younger. It has been identified by Gisela Richter as an imagined portrait of Hesiod. In fact, it has been recognized since 1813 that the bust was not of Seneca, when an inscribed herma portrait of Seneca with quite different features was discovered. Most scholars now follow Richter's identification.

Hesiod employed the conventional dialect of epic verse, which was Ionian. Comparisons with Homer, a native Ionian, can be unflattering. Hesiod's handling of the dactylic hexameter was not as masterful or fluent as Homer's and one modern scholar refers to his "hobnailed hexameters". His use of language and meter in "Works and Days" and "Theogony" distinguishes him also from the author of the "Shield of Heracles". All three poets, for example, employed digamma inconsistently, sometimes allowing it to affect syllable length and meter, sometimes not. The ratio of observance/neglect of digamma varies between them. The extent of variation depends on how the evidence is collected and interpreted but there is a clear trend, revealed for example in the following set of statistics.
Hesiod does not observe digamma as often as the others do. That result is a bit counter-intuitive since digamma was still a feature of the Boeotian dialect that Hesiod probably spoke, whereas it had already vanished from the Ionic vernacular of Homer. This anomaly can be explained by the fact that Hesiod made a conscious effort to compose like an Ionian epic poet at a time when digamma was not heard in Ionian speech, while Homer tried to compose like an older generation of Ionian bards, when it was heard in Ionian speech. There is also a significant difference in the results for "Theogony" and "Works and Days", but that is merely due to the fact that the former includes a catalog of divinities and therefore it makes frequent use of the definite article associated with digamma, oἱ.

Though typical of epic, his vocabulary features some significant differences from Homer's. One scholar has counted 278 un-Homeric words in "Works and Days", 151 in "Theogony" and 95 in "Shield of Heracles". The disproportionate number of un-Homeric words in "W & D" is due to its un-Homeric subject matter. Hesiod's vocabulary also includes quite a lot of formulaic phrases that are not found in Homer, which indicates that he may have been writing within a different tradition.






</doc>
<doc id="13702" url="https://en.wikipedia.org/wiki?curid=13702" title="Hebrew numerals">
Hebrew numerals

The system of Hebrew numerals is a quasi-decimal alphabetic numeral system using the letters of the Hebrew alphabet.
The system was adapted from that of the Greek numerals in the late 2nd century BCE.

The current numeral system is also known as the "Hebrew alphabetic numerals" to contrast with earlier systems of writing numerals used in classical antiquity. These systems were inherited from usage in the Aramaic and Phoenician scripts, attested from c. 800 BC in the so-called Samaria ostraca and sometimes known as "Hebrew-Aramaic numerals", ultimately derived from the Egyptian Hieratic numerals.

The Greek system was adopted in Hellenistic Judaism and had been in use in Greece since about the 5th century BC.
In this system, there is no notation for zero, and the numeric values for individual letters are added together. Each unit (1, 2, ..., 9) is assigned a separate letter, each tens (10, 20, ..., 90) a separate letter, and the first four hundreds (100, 200, 300, 400) a separate letter. The later hundreds (500, 600, 700, 800 and 900) are represented by the sum of two or three letters representing the first four hundreds. To represent numbers from 1,000 to 999,999, the same letters are reused to serve as thousands, tens of thousands, and hundreds of thousands. Gematria (Jewish numerology) uses these transformations extensively.

In Israel today, the decimal system of Arabic numerals (ex. 0, 1, 2, 3, etc.) is used in almost all cases (money, age, date on the civil calendar). The Hebrew numerals are used only in special cases, such as when using the Hebrew calendar, or numbering a list (similar to a, b, c, d, etc.), much as Roman numerals are used in the West.

The Hebrew language has names for common numbers that range from zero to one million. Letters of the Hebrew alphabet are used to represent numbers in a few traditional contexts, for example in calendars. In other situations Arabic numerals are used. Cardinal and ordinal numbers must agree in gender with the noun they are describing. If there is no such noun (e.g. telephone numbers), the feminine form is used. For ordinal numbers greater than ten the cardinal is used and numbers above the value 20 have no gender.

Note: For ordinal numbers greater than 10, cardinal numbers are used instead.

Note: For numbers greater than 20, gender does not apply. Officially, numbers greater than million were represented by the long scale; However, since January 21st, 2013, the modified short scale (under which the long scale milliard is substituted for the strict short scale billion), which was already the colloquial standard, became official.

Cardinal and ordinal numbers must agree in gender (masculine or feminine; mixed groups are treated as masculine) with the noun they are describing. If there is no such noun (e.g. a telephone number or a house number in a street address), the feminine form is used. Ordinal numbers must also agree in number and definite status like other adjectives. The cardinal number precedes the noun (e.g., "shlosha yeladim"), except for the number one which succeeds it (e.g., "yeled echad"). The number two is special: "shnayim" (m.) and "shtayim" (f.) become "shney" (m.) and "shtey" (f.) when followed by the noun they count. For ordinal numbers (numbers indicating position) greater than ten the cardinal is used.

The Hebrew numeric system operates on the additive principle in which the numeric values of the letters are added together to form the total. For example, 177 is represented as קעז which (from right to left) corresponds to 100 + 70 + 7 = 177.

Mathematically, this type of system requires 27 letters (1-9, 10-90, 100-900). In practice the last letter, "tav" (which has the value 400) is used in combination with itself and/or other letters from "kof" (100) onwards, to generate numbers from 500 and above. Alternatively, the 22-letter Hebrew numeral set is sometimes extended to 27 by using 5 "sofit" (final) forms of the Hebrew letters.

By convention, the numbers 15 and 16 are represented as ט״ו (9 + 6) and ט״ז (9 + 7), respectively, in order to refrain from using the two-letter combinations י-ה (10 + 5) and י-ו (10 + 6), which are alternate written forms for the Name of God in everyday writing. In the calendar, this manifests every full moon, since all Hebrew months start on a new moon (see for example: Tu BiShvat).

Combinations which would spell out words with negative connotations are sometimes avoided by switching the order of the letters. For instance, 744 which should be written as תשמ״ד (meaning "you/it will be destroyed") might instead be written as תשד״מ or תמש״ד (meaning "end to demon").

The Hebrew numeral system has sometimes been extended to include the five final letter forms—ך (500‎), ם (600‎), ן (700‎), ף (800‎) and ץ (900‎)—which are then used to indicate the numbers from 500 to 900.

The ordinary forms for 500 to 900 are: ת״ק (500‎), ת״ר (600‎), ת״ש (700‎), ת״ת (800‎) and תת״ק (900‎).

Gershayim (U+05F4 in Unicode, and resembling a double quote mark) (sometimes erroneously referred to as "merkha'ot", which is Hebrew for double quote) are inserted before (to the right of) the last (leftmost) letter to indicate that the sequence of letters represents a number rather than a word. This is used in the case where a number is represented by two or more Hebrew numerals ("e.g.," 28 → כ״ח).

Similarly, a single Geresh (U+05F3 in Unicode, and resembling a single quote mark) is appended after (to the left of) a single letter to indicate that the letter represents a number rather than a (one-letter) word. This is used in the case where a number is represented by a single Hebrew numeral ("e.g.," 100 → ק׳).

Note that Geresh and Gershayim merely indicate ""not a (normal) word."" Context usually determines whether they indicate a number or something else (such as ""abbreviation"").

An alternative method found in old manuscripts and still found on modern-day tombstones is to put a dot above each letter of the number.

In print, Arabic numerals are employed in Modern Hebrew for most purposes. Hebrew numerals are used nowadays primarily for writing the days and years of the Hebrew calendar; for references to traditional Jewish texts (particularly for Biblical chapter and verse and for Talmudic folios); for bulleted or numbered lists (similar to "A", "B", "C", "etc.", in English); and in numerology (gematria).

Thousands are counted separately, and the thousands count precedes the rest of the number (to the "right", since Hebrew is read from right to left). There are no special marks to signify that the “count” is starting over with thousands, which can theoretically lead to ambiguity, although a single quote mark is sometimes used after the letter. When specifying years of the Hebrew calendar in the present millennium, writers usually omit the thousands (which is presently 5 []), but if they do not this is accepted to mean 5 * 1000, with no ambiguity. The current Israeli coinage includes the thousands.

“Monday, 15 Adar 5764” (where 5764 = 5(×1000) + 400 + 300 + 60 + 4, and 15 = 9 + 6):

“Thursday, 3 Nisan 5767” (where 5767 = 5(×1000) + 400 + 300 + 60 + 7):

To see how "today's" date in the Hebrew calendar is written, see, for example, Hebcal date converter.

5780 (2019–20) = ה׳תש״פ

5779 (2018–19) = ה׳תשע״ט

5772 (2011–12) = ה׳תשע״ב

5771 (2010–11) = ה׳תשע״א

5770 (2009–10) = ה׳תש״ע

5769 (2008–09) = ה׳תשס״ט

5761 (2000–01) = ה׳תשס״א

5760 (1999–00) = ה׳תש״ס

The Abjad numerals are equivalent to the Hebrew numerals up to 400. The Greek numerals differ from the Hebrew ones from 90 upwards because in the Greek alphabet there is no equivalent for "Tsadi" (צ).




</doc>
<doc id="13704" url="https://en.wikipedia.org/wiki?curid=13704" title="Hydroxy">
Hydroxy

Hydroxy can refer to:


</doc>
<doc id="13706" url="https://en.wikipedia.org/wiki?curid=13706" title="Hero">
Hero

A hero (masculine) or heroine (feminine) (also known as "good guy" or "white hat") is a real person or a main fictional character of a literary work who, in the face of danger, combats adversity through feats of ingenuity, bravery or strength; the original hero type of classical epics did such things for the sake of glory and honor. On the other hand are post-classical and modern heroes, who perform great deeds or selfless acts for the common good instead of the classical goal of wealth, pride and fame. The antonym of a hero is a villain.

The concept of the hero can be found in classical literature. It is the main or revered character in heroic epic poetry celebrated through ancient legends of a people, often striving for military conquest and living by a continually flawed personal honor code. The definition of a hero has changed throughout time. Merriam Webster dictionary defines a hero as "a person who is admired for great or brave acts or fine qualities." Examples of heroes range from mythological figures, such as Gilgamesh, Achilles and Iphigenia, to historical figures, such as Joan of Arc, Giuseppe Garibaldi or Sophie Scholl, modern heroes like Alvin York, Audie Murphy and Chuck Yeager, and fictional superheroes, including Superman, Batman, and Wonder Woman.

The word "hero" comes from the Greek ἥρως ("hērōs"), "hero" (literally "protector" or "defender"), particularly one such as Heracles with divine ancestry or later given divine honors. Before the decipherment of Linear B the original form of the word was assumed to be *, "hērōw-", but the Mycenaean compound "ti-ri-se-ro-e" demonstrates the absence of -w-.

According to the "American Heritage Dictionary of the English Language", the Proto-Indo-European root is "*ser" meaning "to protect". According to Eric Partridge in "Origins," the Greek word "Hērōs" "is akin to" the Latin "seruāre," meaning "to safeguard". Partridge concludes, "The basic sense of both Hera and hero would therefore be 'protector'." R. S. P. Beekes rejects an Indo-European derivation and asserts that the word has a Pre-Greek origin.

A classical hero is considered to be a "warrior who lives and dies in the pursuit of honor" and asserts their greatness by "the brilliancy and efficiency with which they kill". Each classical hero's life focuses on fighting, which occurs in war or during an epic quest. Classical heroes are commonly semi-divine and extraordinarily gifted, like Achilles, evolving into heroic characters through their perilous circumstances. While these heroes are incredibly resourceful and skilled, they are often foolhardy, court disaster, risk their followers' lives for trivial matters, and behave arrogantly in a childlike manner. During classical times, people regarded heroes with the highest esteem and utmost importance, explaining their prominence within epic literature. The appearance of these mortal figures marks a revolution of audiences and writers turning away from immortal gods to mortal mankind, whose heroic moments of glory survive in the memory of their descendants, extending their legacy.

Hector was a Trojan prince and the greatest fighter for Troy in the Trojan War, which is known primarily through Homer's "The Iliad". Hector acted as leader of the Trojans and their allies in the defense of Troy, "killing 31,000 Greek fighters," offers Hyginus. Hector was known not only for his courage but also for his noble and courtly nature. Indeed, Homer places Hector as peace-loving, thoughtful as well as bold, a good son, husband and father, and without darker motives. However, his familial values conflict greatly with his heroic aspirations in "The Iliad," as he cannot be both the protector of Troy and a father to his child. Hector is ultimately betrayed by the gods when Athena appears disguised as his ally Deiphobus and convinces him to take on Achilles, leading to his death at the hands of a superior warrior.Achilles was a Greek Hero who was considered the most formidable military fighter in the entire Trojan War and the central character of "The Iliad." He was the child of Thetis and Peleus, making him a demi-god. He wielded superhuman strength on the battlefield and was blessed with a close relationship to the Gods. Achilles famously refuses to fight after his dishonoring at the hands of Agamemnon, and only returns to the war due to unadulterated rage after Hector kills his close friend Patroclus. Achilles was known for uncontrollable rage that defined many of his bloodthirsty actions, such as defiling Hector's corpse by dragging it around the city of Troy. Achilles plays a tragic role in "The Iliad" brought about by constant de-humanization throughout the epic, having his "menis" (wrath) overpower his "philos" (love).

Heroes in myth often had close but conflicted relationships with the gods. Thus Heracles's name means "the glory of Hera", even though he was tormented all his life by Hera, the Queen of the Gods. Perhaps the most striking example is the Athenian king Erechtheus, whom Poseidon killed for choosing Athena over him as the city's patron god. When the Athenians worshiped Erechtheus on the Acropolis, they invoked him as "Poseidon Erechtheus".

Fate, or destiny, plays a massive role in the stories of classical heroes. The classical hero's heroic significance stems from battlefield conquests, an inherently dangerous action. The gods in Greek Mythology, when interacting with the heroes, often foreshadow the hero's eventual death on the battlefield. Countless heroes and gods go to great lengths to alter their pre-destined fate, but with no success, as no immortal can change their prescribed outcomes by the three Fates. The most prominent example of this is found in "Oedipus Rex." After learning that his son, Oedipus, will end up killing him, the King of Thebes, Laius, takes huge steps to assure his son's death by removing him from the kingdom. But, Oedipus slays his father without an afterthought when he unknowingly encounters him in a dispute on the road many years later. The lack of recognition enabled Oedipus to slay his father, ironically further binding his father to his fate.

Stories of heroism may serve as moral examples. However, classical heroes often didn't embody the Christian notion of an upstanding, perfectly moral hero. For example, Achilles's character-issues of hateful rage lead to merciless slaughter and his overwhelming pride lead to him only joining the Trojan War because he didn't want his soldiers to win all of the glory. Classical heroes, regardless of their morality, were placed in religion. In classical antiquity, cults that venerated deified heroes such as Heracles, Perseus, and Achilles played an important role in Ancient Greek religion. These ancient Greek hero cults worshipped heroes from oral epic tradition, with these heroes often bestowing blessings, especially healing ones, on individuals.

The concept of the "Mythic Hero Archetype" was first developed by Lord Raglan in his 1936 book, "The Hero, A Study in Tradition, Myth and Drama". It is a set of 22 common traits that he said were shared by many heroes in various cultures, myths and religions throughout history and around the world. Raglan argued that the higher the score, the more likely the figure is mythical.

The concept of a story archetype of the standard monomythical "hero's quest" that was reputed to be pervasive across all cultures is somewhat controversial. Expounded mainly by Joseph Campbell in his 1949 work "The Hero with a Thousand Faces", it illustrates several uniting themes of hero stories that hold similar ideas of what a hero represents, despite vastly different cultures and beliefs. The monomyth or Hero's Journey consists of three separate stages including the Departure, Initiation, and Return. Within these stages there are several archetypes that the hero or heroine may follow including the call to adventure (which they may initially refuse), supernatural aid, proceeding down a road of trials, achieving a realization about themselves (or an apotheosis), and attaining the freedom to live through their quest or journey. Campbell offered examples of stories with similar themes such as Krishna, Buddha, Apollonius of Tyana, and Jesus. One of the themes he explores is the androgynous hero, who combines male and female traits, like Bodhisattva: "The first wonder to be noted here is the androgynous character of the Bodhisattva: masculine Avalokiteshvara, feminine Kwan Yin." In his 1968 book, "The Masks of God: Occidental Mythology", Campbell writes "It is clear that, whether accurate or not as to biographical detail, the moving legend of the Crucified and Risen Christ was fit to bring a new warmth, immediacy, and humanity, to the old motifs of the beloved Tammuz, Adonis, and Osiris cycles."

Vladimir Propp, in his analysis of the Russian fairy tale, concluded that a fairy tale had only eight "dramatis personæ", of which one was the hero, and his analysis has been widely applied to non-Russian folklore. The actions that fall into such a hero's sphere include:
Propp distinguished between "seekers" and "victim-heroes". A villain could initiate the issue by kidnapping the hero or driving him out; these were victim-heroes. On the other hand, an antagonist could rob the hero, or kidnap someone close to him, or, without the villain's intervention, the hero could realize that he lacked something and set out to find it; these heroes are seekers. Victims may appear in tales with seeker heroes, but the tale does not follow them both.

No history can be written without consideration of the lengthy list of recipients of national medals for bravery, populated by firefighters, policemen and policewomen, ambulance medics and ordinary have-a-go heroes. These persons risked their lives to try to save or protect the lives of others: for example, the Canadian Cross of Valour (C.V.) "recognizes acts of the most conspicuous courage in circumstances of extreme peril"; examples of recipients are Mary Dohey and David Gordon Cheverie.

The philosopher Hegel gave a central role to the "hero", personalized by Napoleon, as the incarnation of a particular culture's "Volksgeist", and thus of the general "Zeitgeist". Thomas Carlyle's 1841 "On Heroes, Hero Worship and the Heroic in History" also accorded a key function to heroes and great men in history. Carlyle centered history on the biography of a few central individuals such as Oliver Cromwell or Frederick the Great. His heroes were political and military figures, the founders or topplers of states. His history of great men included geniuses good and, perhaps for the first time in historical study, evil.

Explicit defenses of Carlyle's position were rare in the second part of the 20th century. Most in the philosophy of history school contend that the motive forces in history can best be described only with a wider lens than the one that Carlyle used for his portraits. For example, Karl Marx argued that history was determined by the massive social forces at play in "class struggles", not by the individuals by whom these forces are played out. After Marx, Herbert Spencer wrote at the end of the 19th century: "You must admit that the genesis of the great man depends on the long series of complex influences which has produced the race in which he appears, and the social state into which that race has slowly grown...Before he can remake his society, his society must make him." Michel Foucault argued in his analysis of societal communication and debate that history was mainly the "science of the sovereign", until its inversion by the "historical and political popular discourse".

Modern examples of the typical hero are Minnie Vautrin, Norman Bethune, Alan Turing, Raoul Wallenberg, Chiune Sugihara, Martin Luther King Jr., Mother Teresa, Nelson Mandela, Oswaldo Paya, Oscar Biscet, and Aung San Suu Kyi.

The Annales School, led by Lucien Febvre, Marc Bloch and Fernand Braudel, would contest the exaggeration of the role of individual subjects in history. Indeed, Braudel distinguished various time scales, one accorded to the life of an individual, another accorded to the life of a few human generations, and the last one to civilizations, in which geography, economics and demography play a role considerably more decisive than that of individual subjects.

Among noticeable events in the studies of the role of the hero and Great man in history one should mention Sydney Hook's book (1943) "The Hero in History". In the second half of the twentieth century such male-focused theory has been contested, among others by feminists writers such as Judith Fetterley in "The Resisting Reader" (1977) and literary theorist Nancy K. Miller, "The Heroine's Text: Readings in the French and English Novel, 1722–1782".

In the epoch of globalization an individual can still change the development of the country and of the whole world so this gives reasons to some scholars to suggest returning to the problem of the role of the hero in history from the viewpoint of modern historical knowledge and using up-to-date methods of historical analysis.

Within the frameworks of developing counterfactual history, attempts are made to examine some hypothetical scenarios of historical development. The hero attracts much attention because most of those scenarios are based on the suppositions: what would have happened if this or that historical individual had or had not been alive.

If the term "heroine" exists, "hero" is often the predominantly used term even though its neutrality can be put into question. The definitions of the heroine often refer back to the one of the hero, but sometimes insinuate that their deeds are of less value, or were obtained only thanks to their love of God or a country or of a man. Therefore, implying that an external explanation for the extraordinary nature of her deeds is needed to justify them. The warrior women is considered unholy, unnatural. These figures tend to be erased because they don't fit in the feminine values they are supposed to represent.
Acts of heroism coming from women are acceptable, during specific time, like when men are at war, during times of crisis, but they are otherwise often seen as suspicious. Moreover, women are often not individualized, but praised as a group for heroic deeds. Women in the military were often subordinated to tasks less likely to be praised than armed combat, and are rather praised for their courage as a general force, nurses during wartime are a good example of this phenomenon.
If their story gets told, they are made to fit in the acceptable script. Their story is told in a way as to match the expectations of femininity ex: maternal love, compassion, fidelity, resistance, defense. Etc. So the set of strengths in which a heroine could historically express her value are overall not the same and perceived as less valuable than their masculine counterpart.

If they get mentioned in history, the way their story is told also differs from their male counterpart, they are generally portrayed as young and beautiful, their actions are limited to a short time
lapse in opposition to the possibility of a long heroic career for male heroes, underlying feelings that led to their heroic acts are underlined, overall less details about their life are kept and
emphasis is put over their tragic death. Not to forget that heroes and heroines are part of a social construct, their history is told and changes throughout history to serve different purposes of memory, propaganda according to diverse social, political or religious evolutions.

The word "hero" or "heroine", in modern times, is sometimes used to describe the protagonist or the love interest of a story, a usage which can conflict with the superhuman expectations of heroism. A classic example is Anna Karenina, the lead character in the novel of the same title by Leo Tolstoy. In modern literature the hero is more and more a problematic concept. In 1848, for example, William Makepeace Thackeray gave "Vanity Fair" the subtitle "A Novel without a Hero", and imagined a world in which no sympathetic character was to be found. "Vanity Fair" is a satirical representation of the absence of truly moral heroes in the modern world. The story focuses on the characters Emmy Sedley and Becky Sharpe (the latter as the clearly defined anti-hero), with the plot focused on the eventual marriage of these two characters to rich men, revealing character flaws as the story progresses. Even the most sympathetic characters, like Captain Dobbin, are susceptible to weakness, as he is often narcissistic and melancholy.

The larger-than-life hero is a more common feature of fantasy (particularly in comic-books and epic fantasy) than more realist works. However, these larger-than life figures remain prevalent in society. The superhero genre is a multibillion-dollar industry that includes comic books, movies, toys and video games. Superheroes usually possess extraordinary talents and powers that no living human could ever emulate. The superhero stories often pit a super villain against the hero, with the hero fighting the crime caused by the super villain. Examples of long-running superheroes include Superman, Batman, Spider-Man and Wonder Woman.

Social psychology has begun paying attention to heroes and heroism. Zeno Franco and Philip Zimbardo point out differences between heroism and altruism, and they offer evidence that observers' perceptions of unjustified risk plays a role above and beyond risk type in determining the ascription of heroic status.

An evolutionary psychology explanation for heroic risk-taking is that it is a costly signal demonstrating the ability of the hero. It can be seen as one form of altruism for which there are also several other evolutionary explanations.

Roma Chatterji has suggested that the hero or more generally protagonist is first and foremost a symbolic representation of the person who is experiencing the story while reading, listening or watching; thus the relevance of the hero to the individual relies a great deal on how much similarity there is between the two. One reason for the hero-as-self interpretation of stories and myths is the human inability to view the world from any perspective but a personal one.

In the Pulitzer Prize-winning book "The Denial of Death", Ernest Becker argues that human civilization is ultimately an elaborate, symbolic defense mechanism against the knowledge of our mortality, which in turn acts as the emotional and intellectual response to our basic survival mechanism. Becker explains that a basic duality in human life exists between the physical world of objects and a symbolic world of human meaning. Thus, since humanity has a dualistic nature consisting of a physical self and a symbolic self, we are able to transcend the dilemma of mortality through heroism, by focusing our attention mainly on our symbolic selves. This symbolic self-focus takes the form of an individual's "immortality project" (or ""causa sui" project"), which is essentially a symbolic belief-system that ensures oneself is believed superior to physical reality. By successfully living under the terms of the immortality project, people feel they can become heroic and, henceforth, part of something eternal; something that will never die as compared to their physical body. This, in turn, gives people the feeling that their lives have meaning, a purpose, and are significant in the grand scheme of things. Another theme running throughout the book is that humanity's traditional "hero-systems", such as religion, are no longer convincing in the age of reason. Science attempts to serve as an immortality project, something that Becker believes it can never do, because it is unable to provide agreeable, absolute meanings to human life. The book states that we need new convincing "illusions" that enable us to feel heroic in ways that are agreeable. Becker, however, does not provide any definitive answer, mainly because he believes that there is no perfect solution. Instead, he hopes that gradual realization of humanity's innate motivations, namely death, can help to bring about a better world. Terror Management Theory has generated evidence supporting this perspective.



</doc>
<doc id="13711" url="https://en.wikipedia.org/wiki?curid=13711" title="Hydroxide">
Hydroxide

Hydroxide is a diatomic anion with chemical formula OH. It consists of an oxygen and hydrogen atom held together by a covalent bond, and carries a negative electric charge. It is an important but usually minor constituent of water. It functions as a base, a ligand, a nucleophile, and a catalyst. The hydroxide ion forms salts, some of which dissociate in aqueous solution, liberating solvated hydroxide ions. Sodium hydroxide is a multi-million-ton per annum commodity chemical. A hydroxide attached to a strongly electropositive center may itself ionize, liberating a hydrogen cation (H), making the parent compound an acid.

The corresponding electrically neutral compound HO is the hydroxyl radical. The corresponding covalently-bound group –OH of atoms is the hydroxy group.
Hydroxide ion and hydroxy group are nucleophiles and can act as a catalysts in organic chemistry.

Many inorganic substances which bear the word "hydroxide" in their names are not ionic compounds of the hydroxide ion, but covalent compounds which contain hydroxy groups.

The hydroxide ion is a natural part of water, because of the self-ionization reaction in which its complement, hydronium, is passed hydrogen:
The equilibrium constant for this reaction, defined as
has a value close to 10 at 25 °C, so the concentration of hydroxide ions in pure water is close to 10 mol∙dm, in order to satisfy the equal charge constraint. The pH of a solution is equal to the decimal cologarithm of the hydrogen cation concentration; the pH of pure water is close to 7 at ambient temperatures. The concentration of hydroxide ions can be expressed in terms of pOH, which is close to (14 − pH), so the pOH of pure water is also close to 7. Addition of a base to water will reduce the hydrogen cation concentration and therefore increase the hydroxide ion concentration (increase pH, decrease pOH) even if the base does not itself contain hydroxide. For example, ammonia solutions have a pH greater than 7 due to the reaction NH + H , which decreases the hydrogen cation concentration, which increases the hydroxide ion concentration. pOH can be kept at a nearly constant value with various buffer solutions.

In aqueous solution the hydroxide ion is a base in the Brønsted–Lowry sense as it can accept a proton from a Brønsted–Lowry acid to form a water molecule. It can also act as a Lewis base by donating a pair of electrons to a Lewis acid. In aqueous solution both hydrogen and hydroxide ions are strongly solvated, with hydrogen bonds between oxygen and hydrogen atoms. Indeed, the bihydroxide ion has been characterized in the solid state. This compound is centrosymmetric and has a very short hydrogen bond (114.5 pm) that is similar to the length in the bifluoride ion (114 pm). In aqueous solution the hydroxide ion forms strong hydrogen bonds with water molecules. A consequence of this is that concentrated solutions of sodium hydroxide have high viscosity due to the formation of an extended network of hydrogen bonds as in hydrogen fluoride solutions.

In solution, exposed to air, the hydroxide ion reacts rapidly with atmospheric carbon dioxide, acting as an acid, to form, initially, the bicarbonate ion.
The equilibrium constant for this reaction can be specified either as a reaction with dissolved carbon dioxide or as a reaction with carbon dioxide gas (see Carbonic acid for values and details). At neutral or acid pH, the reaction is slow, but is catalyzed by the enzyme carbonic anhydrase, which effectively creates hydroxide ions at the active site.

Solutions containing the hydroxide ion attack glass. In this case, the silicates in glass are acting as acids. Basic hydroxides, whether solids or in solution, are stored in airtight plastic containers.

The hydroxide ion can function as a typical electron-pair donor ligand, forming such complexes as tetrahydroxoaluminate/tetrahydroxidoaluminate [Al(OH)]. It is also often found in mixed-ligand complexes of the type [ML(OH)], where L is a ligand. The hydroxide ion often serves as a bridging ligand, donating one pair of electrons to each of the atoms being bridged. As illustrated by [Pb(OH)], metal hydroxides are often written in a simplified format. It can even act as a 3-electron-pair donor, as in the tetramer [PtMe(OH)].

When bound to a strongly electron-withdrawing metal centre, hydroxide ligands tend to ionise into oxide ligands. For example, the bichromate ion [HCrO] dissociates according to
with a p"K" of about 5.9.

The infrared spectra of compound containing the –OH functional group have strong absorption bands in the region centered around 3500 cm. The high frequency of molecular vibration is a consequence of the small mass of the hydrogen atom as compared to the mass of the oxygen atom and this makes detection of hydroxyl groups by infrared spectroscopy relatively easy. A band due to an OH group tends to be sharp. However, the band width increases when the OH group is involved in hydrogen bonding. A water molecule has an HOH bending mode at about 1600 cm, so the absence of this band can be used to distinguish an OH group from a water molecule.

When the OH group is bound to a metal ion in a coordination complex, an M−OH bending mode can be observed. For example, in [Sn(OH)] it occurs at 1065 cm. The bending mode for a bridging hydroxide tends to be at a lower frequency as in [(bipyridine)Cu(OH)Cu(bipyridine)] (955 cm). M−OH stretching vibrations occur below about 600 cm. For example, the tetrahedral ion [Zn(OH)] has bands at 470 cm (Raman-active, polarized) and 420 cm (infrared). The same ion has a (HO)–Zn–(OH) bending vibration at 300 cm.

Sodium hydroxide solutions, also known as lye and caustic soda, are used in the manufacture of pulp and paper, textiles, drinking water, soaps, and detergents, and as a drain cleaner. Worldwide production in 2004 was approximately 60 million tonnes. The principal method of manufacture is the chloralkali process.

Solutions containing the hydroxide ion are generated when a salt of a weak acid is dissolved in water. Sodium carbonate is used as an alkali, for example, by virtue of the hydrolysis reaction
Although the base strength of sodium carbonate solutions is lower than a concentrated sodium hydroxide solution, it has the advantage of being a solid. It is also manufactured on a vast scale (42 million tonnes in 2005) by the Solvay process. An example of the use of sodium carbonate as an alkali is when washing soda (another name for sodium carbonate) acts on insoluble esters, such as triglycerides, commonly known as fats, to hydrolyze them and make them soluble.

Bauxite, a basic hydroxide of aluminium, is the principal ore from which the metal is manufactured. Similarly, goethite (α-FeO(OH)) and lepidocrocite (γ-FeO(OH)), basic hydroxides of iron, are among the principal ores used for the manufacture of metallic iron. Numerous other uses can be found in the articles on individual hydroxides.

Aside from NaOH and KOH, which enjoy very large scale applications, the hydroxides of the other alkali metals also are useful. Lithium hydroxide is a strong base, with a p"K" of −0.36. Lithium hydroxide is used in breathing gas purification systems for spacecraft, submarines, and rebreathers to remove carbon dioxide from exhaled gas.
The hydroxide of lithium is preferred to that of sodium because of its lower mass. Sodium hydroxide, potassium hydroxide, and the hydroxides of the other alkali metals are also strong bases.

Beryllium hydroxide Be(OH) is amphoteric. The hydroxide itself is insoluble in water, with a solubility product log "K"* of −11.7. Addition of acid gives soluble hydrolysis products, including the trimeric ion [Be(OH)(HO)], which has OH groups bridging between pairs of beryllium ions making a 6-membered ring. At very low pH the aqua ion [Be(HO)] is formed. Addition of hydroxide to Be(OH) gives the soluble tetrahydroxoberyllate/tetrahydroxidoberyllate anion, [Be(OH)].

The solubility in water of the other hydroxides in this group increases with increasing atomic number. Magnesium hydroxide Mg(OH) is a strong base (up to the limit of its solubility, which is very low in pure water), as are the hydroxides of the heavier alkaline earths: calcium hydroxide, strontium hydroxide, and barium hydroxide. A solution or suspension of calcium hydroxide is known as limewater and can be used to test for the weak acid carbon dioxide. The reaction Ca(OH) + CO Ca + + OH illustrates the basicity of calcium hydroxide. Soda lime, which is a mixture of the strong bases NaOH and KOH with Ca(OH), is used as a CO absorbent.

The simplest hydroxide of boron B(OH), known as boric acid, is an acid. Unlike the hydroxides of the alkali and alkaline earth hydroxides, it does not dissociate in aqueous solution. Instead, it reacts with water molecules acting as a Lewis acid, releasing protons.
A variety of oxyanions of boron are known, which, in the protonated form, contain hydroxide groups.

Aluminium hydroxide Al(OH) is amphoteric and dissolves in alkaline solution.
In the Bayer process for the production of pure aluminium oxide from bauxite minerals this equilibrium is manipulated by careful control of temperature and alkali concentration. In the first phase, aluminium dissolves in hot alkaline solution as but other hydroxides usually present in the mineral, such as iron hydroxides, do not dissolve because they are not amphoteric. After removal of the insolubles, the so-called red mud, pure aluminium hydroxide is made to precipitate by reducing the temperature and adding water to the extract, which, by diluting the alkali, lowers the pH of the solution. Basic aluminium hydroxide AlO(OH), which may be present in bauxite, is also amphoteric.

In mildly acidic solutions, the hydroxo/hydroxido complexes formed by aluminium are somewhat different from those of boron, reflecting the greater size of Al(III) vs. B(III). The concentration of the species [Al(OH)] is very dependent on the total aluminium concentration. Various other hydroxo complexes are found in crystalline compounds. Perhaps the most important is the basic hydroxide AlO(OH), a polymeric material known by the names of the mineral forms boehmite or diaspore, depending on crystal structure. Gallium hydroxide, indium hydroxide, and thallium(III) hydroxide are also amphoteric. Thallium(I) hydroxide is a strong base.

Carbon forms no simple hydroxides. The hypothetical compound C(OH) (orthocarbonic acid or methanetetrol) is unstable in aqueous solution:
Carbon dioxide is also known as carbonic anhydride, meaning that it forms by dehydration of carbonic acid HCO (OC(OH)).

Silicic acid is the name given to a variety of compounds with a generic formula [SiO(OH)]. "Orthosilicic acid" has been identified in very dilute aqueous solution. It is a weak acid with p"K" = 9.84, p"K" = 13.2 at 25 °C. It is usually written as HSiO but the formula Si(OH) is generally accepted. Other silicic acids such as "metasilicic acid" (HSiO), "disilicic acid" (HSiO), and "pyrosilicic acid" (HSiO) have been characterized. These acids also have hydroxide groups attached to the silicon; the formulas suggest that these acids are protonated forms of polyoxyanions.

Few hydroxo complexes of germanium have been characterized. Tin(II) hydroxide Sn(OH) was prepared in anhydrous media. When tin(II) oxide is treated with alkali the pyramidal hydroxo complex is formed. When solutions containing this ion are acidified, the ion [Sn(OH)] is formed together with some basic hydroxo complexes. The structure of [Sn(OH)] has a triangle of tin atoms connected by bridging hydroxide groups. Tin(IV) hydroxide is unknown but can be regarded as the hypothetical acid from which stannates, with a formula [Sn(OH)], are derived by reaction with the (Lewis) basic hydroxide ion.

Hydrolysis of Pb in aqueous solution is accompanied by the formation of various hydroxo-containing complexes, some of which are insoluble. The basic hydroxo complex [PbO(OH)] is a cluster of six lead centres with metal–metal bonds surrounding a central oxide ion. The six hydroxide groups lie on the faces of the two external Pb tetrahedra. In strongly alkaline solutions soluble plumbate ions are formed, including [Pb(OH)].

In the higher oxidation states of the pnictogens, chalcogens, halogens, and noble gases there are oxoacids in which the central atom is attached to oxide ions and hydroxide ions. Examples include phosphoric acid HPO, and sulfuric acid HSO. In these compounds one or more hydroxide groups can dissociate with the liberation of hydrogen cations as in a standard Brønsted–Lowry acid. Many oxoacids of sulfur are known and all feature OH groups that can dissociate.

Telluric acid is often written with the formula HTeO·2HO but is better described structurally as Te(OH).

"Ortho"-periodic acid can lose all its protons, eventually forming the periodate ion [IO]. It can also be protonated in strongly acidic conditions to give the octahedral ion [I(OH)], completing the isoelectronic series, [E(OH)], E = Sn, Sb, Te, I; "z" = −2, −1, 0, +1. Other acids of iodine(VII) that contain hydroxide groups are known, in particular in salts such as the "meso"periodate ion that occurs in K[IO(OH)]·8HO.

As is common outside of the alkali metals, hydroxides of the elements in lower oxidation states are complicated. For example, phosphorous acid HPO predominantly has the structure OP(H)(OH), in equilibrium with a small amount of P(OH).

The oxoacids of chlorine, bromine, and iodine have the formula OA(OH) where "n" is the oxidation number: +1, +3, +5, or +7, and A = Cl, Br, or I. The only oxoacid of fluorine is F(OH), hypofluorous acid. When these acids are neutralized the hydrogen atom is removed from the hydroxide group.

The hydroxides of the transition metals and post-transition metals usually have the metal in the +2 (M = Mn, Fe, Co, Ni, Cu, Zn) or +3 (M = Fe, Ru, Rh, Ir) oxidation state. None are soluble in water, and many are poorly defined. One complicating feature of the hydroxides is their tendency to undergo further condensation to the oxides, a process called olation. Hydroxides of metals in the +1 oxidation state are also poorly defined or unstable. For example, silver hydroxide Ag(OH) decomposes spontaneously to the oxide (AgO). Copper(I) and gold(I) hydroxides are also unstable, although stable adducts of CuOH and AuOH are known. The polymeric compounds M(OH) and M(OH) are in general prepared by increasing the pH of an aqueous solutions of the corresponding metal cations until the hydroxide precipitates out of solution. On the converse, the hydroxides dissolve in acidic solution. Zinc hydroxide Zn(OH) is amphoteric, forming the tetrahydroxidozincate ion in strongly alkaline solution.

Numerous mixed ligand complexes of these metals with the hydroxide ion exist. In fact these are in general better defined than the simpler derivatives. Many can be made by deprotonation of the corresponding metal aquo complex.

Vanadic acid HVO shows similarities with phosphoric acid HPO though it has a much more complex vanadate oxoanion chemistry. Chromic acid HCrO, has similarities with sulfuric acid HSO; for example, both form acid salts A[HMO]. Some metals, e.g. V, Cr, Nb, Ta, Mo, W, tend to exist in high oxidation states. Rather than forming hydroxides in aqueous solution, they convert to oxo clusters by the process of olation, forming polyoxometalates.

In some cases the products of partial hydrolysis of metal ion, described above, can be found in crystalline compounds. A striking example is found with zirconium(IV). Because of the high oxidation state, salts of Zr are extensively hydrolyzed in water even at low pH. The compound originally formulated as ZrOCl·8HO was found to be the chloride salt of a tetrameric cation [Zr(OH)(HO)] in which there is a square of Zr ions with two hydroxide groups bridging between Zr atoms on each side of the square and with four water molecules attached to each Zr atom.

The mineral malachite is a typical example of a basic carbonate. The formula, CuCO(OH) shows that it is halfway between copper carbonate and copper hydroxide. Indeed, in the past the formula was written as CuCO·Cu(OH). The crystal structure is made up of copper, carbonate and hydroxide ions. The mineral atacamite is an example of a basic chloride. It has the formula, CuCl(OH). In this case the composition is nearer to that of the hydroxide than that of the chloride CuCl·3Cu(OH). Copper forms hydroxyphosphate (libethenite), arsenate (olivenite), sulfate (brochantite), and nitrate compounds. White lead is a basic lead carbonate, (PbCO)·Pb(OH), which has been used as a white pigment because of its opaque quality, though its use is now restricted because it can be a source for lead poisoning.

The hydroxide ion appears to rotate freely in crystals of the heavier alkali metal hydroxides at higher temperatures so as to present itself as a spherical ion, with an effective ionic radius of about 153 pm. Thus, the high-temperature forms of KOH and NaOH have the sodium chloride structure, which gradually freezes in a monocinically distorted sodium chloride structure at temperatures below about 300 °C. The OH groups still rotate even at room temperature around their symmetry axes and, therefore, cannot be detected by X-ray diffraction. The room-temperature form of NaOH has the thallium iodide structure. LiOH, however, has a layered structure, made up of tetrahedral Li(OH) and (OH)Li units. This is consistent with the weakly basic character of LiOH in solution, indicating that the Li–OH bond has much covalent character.

The hydroxide ion displays cylindrical symmetry in hydroxides of divalent metals Ca, Cd, Mn, Fe, and Co. For example, magnesium hydroxide Mg(OH) (brucite) crystallizes with the cadmium iodide layer structure, with a kind of close-packing of magnesium and hydroxide ions.

The amphoteric hydroxide Al(OH) has four major crystalline forms: gibbsite (most stable), bayerite, nordstrandite, and doyleite.
All these polymorphs are built up of double layers of hydroxide ions – the aluminium atoms on two-thirds of the octahedral holes between the two layers – and differ only in the stacking sequence of the layers. The structures are similar to the brucite structure. However, whereas the brucite structure can be described as a close-packed structure in gibbsite the OH groups on the underside of one layer rest on the groups of the layer below. This arrangement led to the suggestion that there are directional bonds between OH groups in adjacent layers. This is an unusual form of hydrogen bonding since the two hydroxide ion involved would be expected to point away from each other. The hydrogen atoms have been located by neutron diffraction experiments on α-AlO(OH) (diaspore). The O–H–O distance is very short, at 265 pm; the hydrogen is not equidistant between the oxygen atoms and the short OH bond makes an angle of 12° with the O–O line. A similar type of hydrogen bond has been proposed for other amphoteric hydroxides, including Be(OH), Zn(OH), and Fe(OH).

A number of mixed hydroxides are known with stoichiometry AM(OH), AM(OH), and AM(OH). As the formula suggests these substances contain M(OH) octahedral structural units. Layered double hydroxides may be represented by the formula . Most commonly, "z" = 2, and M = Ca, Mg, Mn, Fe, Co, Ni, Cu, or Zn; hence "q" = "x".

Potassium hydroxide and sodium hydroxide are two well-known reagents in organic chemistry.

The hydroxide ion may act as a base catalyst. The base abstracts a proton from a weak acid to give an intermediate that goes on to react with another reagent. Common substrates for proton abstraction are alcohols, phenols, amines, and carbon acids. The p"K" value for dissociation of a C–H bond is extremely high, but the pK alpha hydrogens of a carbonyl compound are about 3 log units lower. Typical p"K" values are 16.7 for acetaldehyde and 19 for acetone. Dissociation can occur in the presence of a suitable base.
The base should have a p"K" value not less than about 4 log units smaller or the equilibrium will lie almost completely to the left.

The hydroxide ion by itself is not a strong enough base, but it can be converted in one by adding sodium hydroxide to ethanol
to produce the ethoxide ion. The pK for self-dissociation of ethanol is about 16 so the alkoxide ion is a strong enough base The addition of an alcohol to an aldehyde to form a hemiacetal is an example of a reaction that can be catalyzed by the presence of hydroxide. Hydroxide can also act as a Lewis-base catalyst.

The hydroxide ion is intermediate in nucleophilicity between the fluoride ion F, and the amide ion . The hydrolysis of an ester
also known as saponification is an example of a nucleophilic acyl substitution with the hydroxide ion acting as a nucleophile. In this case the leaving group is an alkoxide ion, which immediately removes a proton from a water molecule to form an alcohol. In the manufacture of soap, sodium chloride is added to salt out the sodium salt of the carboxylic acid; this is an example of the application of the common ion effect.

Other cases where hydroxide can act as a nucleophilic reagent are amide hydrolysis, the Cannizzaro reaction, nucleophilic aliphatic substitution, nucleophilic aromatic substitution, and in elimination reactions. The reaction medium for KOH and NaOH is usually water but with a phase-transfer catalyst the hydroxide anion can be shuttled into an organic solvent as well, for example in the generation of the reactive intermediate dichlorocarbene.



</doc>
<doc id="13713" url="https://en.wikipedia.org/wiki?curid=13713" title="H. R. Giger">
H. R. Giger

Hans Ruedi Giger ( ; ; 5 February 1940 – 12 May 2014) was a Swiss painter, best known for airbrush images of humans and machines linked together in a cold biomechanical relationship. Later he abandoned airbrush work for pastels, markers, and ink. He was part of the special effects team that won an Academy Award for design work on the film "Alien". In Switzerland there are two themed bars that reflect his interior designs, and his work is on permanent display at the H.R. Giger Museum at Gruyères. His style has been adapted to many forms of media, including record album covers, furniture, and tattoos.

Giger was born in 1940 in Chur, the capital city of Graubünden, the largest and easternmost Swiss canton. His father, a pharmacist, viewed art as a "breadless profession" and strongly encouraged him to enter pharmacy. He moved to Zürich in 1962, where he studied architecture and industrial design at the School of Applied Arts until 1970.

Giger's first success was when H. H. Kunz, co-owner of Switzerland's first poster publishing company, printed and distributed Giger's first posters, beginning in 1969.

Giger's style and thematic execution were influential. He was part of the special effects team that won an Academy Award for Best Achievement in Visual Effects for their design work on the film "Alien". His design for the Alien was inspired by his painting "Necronom IV" and earned him an Oscar in 1980. His books of paintings, particularly "Necronomicon" and "Necronomicon II" (1985) and the frequent appearance of his art in "Omni" magazine continued his rise to international prominence. Giger was admitted to the Science Fiction and Fantasy Hall of Fame in 2013. He is also well known for artwork on several music recording albums including "" by Danzig, "Brain Salad Surgery" by Emerson, Lake & Palmer and Deborah Harry's "KooKoo".

In 1998, Giger acquired the Château St. Germain in Gruyères, Switzerland, and it now houses the H.R. Giger Museum, a permanent repository of his work.

Giger had a relationship with Swiss actress Li Tobler until she committed suicide in 1975. Li's image appears in many of his paintings. He married Mia Bonzanigo in 1979; they divorced a year and a half later.

The artist lived and worked in Zürich with his second wife, Carmen Maria Scheifele Giger, who is the Director of the H.R. Giger Museum.

On 12 May 2014, Giger died in a hospital in Zürich after having suffered injuries in a fall.

In addition to his awards, Giger was recognized by a variety of festivals and institutions. On the one year anniversary of his death, the Museum of Arts and Design in New York City staged the series "The Unseen Cinema of HR Giger" in May 2015.

"", a biographical documentary by Belinda Sallin, debuted 27 September 2014 in Zurich, Switzerland.

On 11 July 2018, the asteroid 109712 Giger was named in his memory.

Giger started with small ink drawings before progressing to oil paintings. For most of his career, Giger had worked predominantly in airbrush, creating monochromatic canvasses depicting surreal, nightmarish dreamscapes. He also worked with pastels, markers and ink.

Giger's most distinctive stylistic innovation was that of a representation of human bodies and machines in a cold, interconnected relationship, he described as "biomechanical". His main influences were painters Dado, Ernst Fuchs and Salvador Dalí. He met Salvador Dalí, to whom he was introduced by painter Robert Venosa. Giger was also influenced by the work of the sculptor Stanislas Szukalski, and by the painters Austin Osman Spare and Mati Klarwein. He was also a personal friend of Timothy Leary. Giger studied interior and industrial design at the School of Commercial Art in Zurich (from 1962 to 1965) and made his first paintings as a means of art therapy.

Giger directed a number of films, including "Swiss Made" (1968), "Tagtraum" (1973), "Giger's Necronomicon" (1975) and "Giger's Alien" (1979).

Giger created furniture designs, particularly the Harkonnen Capo Chair for a film of the novel "Dune" that was to be directed by Alejandro Jodorowsky. Many years later, David Lynch directed the film, using only rough concepts by Giger. Giger had wished to work with Lynch, as he stated in one of his books that Lynch's film "Eraserhead" was closer than even Giger's own films to realizing his vision.

Giger applied his biomechanical style to interior design. One "Giger Bar" appeared in Tokyo, but the realization of his designs was a great disappointment to him, since the Japanese organization behind the venture did not wait for his final designs, and instead used Giger's rough preliminary sketches. For that reason Giger disowned the Tokyo bar. The two Giger Bars in his native Switzerland, in Gruyères and Chur, were built under Giger's close supervision and they accurately reflect his original concepts. At The Limelight in Manhattan, Giger's artwork was licensed to decorate the VIP room, the uppermost chapel of the landmarked church, but it was never intended to be a permanent installation and bore no similarity to the bars in Switzerland. The arrangement was terminated after two years when the Limelight closed. 

Giger's art has greatly influenced tattooists and fetishists worldwide. Under a licensing deal Ibanez guitars released an H. R. Giger signature series: the Ibanez ICHRG2, an Ibanez Iceman, features "NY City VI", the Ibanez RGTHRG1 has "NY City XI" printed on it, the S Series SHRG1Z has a metal-coated engraving of "Biomechanical Matrix" on it, and a 4-string SRX bass, SRXHRG1, has "N.Y. City X" on it.

Giger is often referred to in popular culture, especially in science fiction and cyberpunk. William Gibson (who wrote an early script for "Alien 3") seems particularly fascinated: A minor character in "Virtual Light", Lowell, is described as having "New York XXIV" tattooed across his back, and in "Idoru" a secondary character, Yamazaki, describes the buildings of nanotech Japan as Giger-esque.







</doc>
<doc id="13714" url="https://en.wikipedia.org/wiki?curid=13714" title="Hispaniola">
Hispaniola

Hispaniola (; Latin and ; ; ) is an island in the Caribbean island group known as the Greater Antilles. It is the second largest island in the Caribbean after Cuba, and the most populous island in the Caribbean; it is also the eleventh most populous island in the world.

The island is divided between two separate, sovereign nations: the Spanish-speaking Dominican Republic (48,445 km, 18,705 sq mi) to the east, and French / French Creole-speaking Haiti (27,750 km, 10,710 sq mi) to the west. The only other shared island in the Caribbean is Saint Martin, which is shared between France (Saint-Martin) and the Netherlands (Sint Maarten).

Hispaniola is the site of the first permanent European settlement in the Americas, founded by Christopher Columbus on his voyages in 1492 and 1493.

The island was called by various names by its native people, the Taíno Amerindians. No known Taíno texts exist, hence, historical evidence for those names comes to us through three European historians: the Italian Pietro Martyr d‘Anghiera, and the Spaniards Bartolomé de las Casas and Gonzalo Fernández de Oviedo. Fernández de Oviedo and de las Casas both recorded that the island was called Quizqueia (supposedly "Mother of all Lands") by the Taíno. D'Anghiera added another name, Haiti ("Mountainous Land"), but later research shows that the word does not seem to derive from the original Arawak Taíno language. (Quisqueya is today mostly used in the Dominican Republic.) Although the Taínos' use of Quizqueia is verified, and the name was used by all three historians, evidence suggests that it probably was the Taíno name of the whole island, and for a region (now known as Los Haitises) in the northeastern section of the present-day Dominican Republic.

When Columbus took possession of the island in 1492, he named it Insula Hispana in Latin and La Isla Española in Spanish, with both meaning "the Spanish island". De las Casas shortened the name to "Española", and when d‘Anghiera detailed his account of the island in Latin, he rendered its name as Hispaniola. In the oldest documented map of the island, created by Andrés de Morales, Los Haitises is labeled Montes de Haití ("Haiti Mountains"), and de las Casas apparently named the whole island Haiti on the basis of that particular region, as d'Anghiera states that the name of one part was given to the whole island.

Due to Taíno, Spanish and French influences on the island, historically the whole island was often referred to as Haiti, Hayti, Santo Domingo, St. Domingue, or San Domingo. The colonial terms Saint-Domingue and Santo Domingo are sometimes still applied to the whole island, though these names refer, respectively, to the colonies that became Haiti and the Dominican Republic. Since Anghiera's literary work was translated into English and French soon after being written, the name Hispaniola became the most frequently used term in English-speaking countries for the island in scientific and cartographic works. In 1918, the United States occupation government, led by Harry Shepard Knapp, obliged the use of the name Hispaniola on the island, and recommended the use of that name to the National Geographic Society.

The name Haïti was adopted by Haitian revolutionary Jean-Jacques Dessalines in 1804, as the official name of independent Saint-Domingue, as a tribute to the Amerindian predecessors. It was also adopted as the official name of independent Santo Domingo, as the Republic of Spanish Haiti, a state that existed from November 1821 until its annexation by Haiti in February 1822.

The primary indigenous group on the island of Hispaniola was the Arawak/Taíno people. The Arawak tribe originated in the Orinoco Delta, spreading from Venezuela. They travelled to Hispaniola around 1200 CE. Each society on the island was a small independent kingdom with a lead known as a cacique. In 1492, which is considered the peak of the Taíno, there were five different kingdoms on the island, the Xaragua, Higuey (Caizcimu), Magua (Huhabo), Ciguayos (Cayabo or Maguana), and Marien (Bainoa). Many distinct Taíno languages also existed in this time period. There is still heated debate over the population of Taíno people on the island of Hispaniola in 1492, but estimates range upwards of 750,000.

An Arawak/Taíno home consisted of a circular building with woven straw and palm leaves as covering. Most individuals slept in fashioned hammocks, but grass beds were also used. The cacique lived in a different structure with larger rectangular walls and a porch. The Taíno village also had a flat court used for ball games and festivals. Religiously, the Arawak/Taíno people were polytheists, and their gods were called zemí. Religious worship and dancing were common, and medicine men or priests also consulted the zemí for advise in public ceremonies. For food, the Arawak/Taíno relied on meat and fish as a primary source for protein; some small mammals on the island were hunted such as rats, but ducks, turtles, snakes, and bats as a common food source. The Taíno also relied on agriculture as a primary food source. The indigenous people of Hispaniola raised crops in a conuco, which is a large mound packed with leaves and fixed crops to prevent erosion. Some common agricultural goods were cassava, maize, squash, beans, peppers, peanuts, cotton, and tobacco, which was used as an aspect of social life and religious ceremonies. 

The Arawak/Taíno people travelled often and used hollowed canoes with paddles when on the water for fishing or for migration purposes, and upwards of 100 people could fit into a single canoe. The Taíno came in contact with the Caribs, another indigenous tribe, often. The caribs lived mostly in modern day Puerto Rico and northeast Hispaniola and were known to be hostile towards other tribes. The Arawak/Taíno people had to defend themselves using bow and arrows with poisoned tips and some war clubs. When Columbus landed on Hispaniola, many Taíno leaders wanted protection from the Caribs.

Christopher Columbus inadvertently landed on the island during his first voyage across the Atlantic in 1492, where his flagship, the "Santa Maria", sank after running aground on December 25. A contingent of men were left at an outpost christened La Navidad, on the north coast of present-day Puerto Plata. On his return the following year, Columbus quickly established a second compound farther east in present-day Dominican Republic, La Isabela after the destruction of La Navidad.

The island was inhabited by the Taíno, one of the indigenous Arawak peoples. The Taíno helped Columbus construct La Navidad on what is now Môle-Saint-Nicolas, Haiti, in December 1492. European colonization of the island began in earnest the following year, when 1,300 men arrived from Spain under the watch of Bartolomeo Columbus. In 1496, the town of Nueva Isabela was founded. After being destroyed by a hurricane, it was rebuilt on the opposite side of the Ozama River and called Santo Domingo. It is the oldest permanent European settlement in the Americas.
Harsh enslavement by Spanish colonists, as well as redirection of food supplies and labor, had a devastating impact on both mortality and fertility of the Taíno population over the first quarter century. Colonial administrators and Dominican and Hyeronimite priests observed that the search for gold and agrarian enslavement through the encomienda system were depressing population. Demographic data from two provinces in 1514 shows a low birth rate consistent with a 3.5% annual population decline. In 1503 the colony began to import African slaves after a charter was passed in 1501 allowing the import of slaves by Ferdinand and Isabel. The Spanish believed Africans would be more capable of performing physical labor. From 1519 to 1533, the indigenous uprising known as Enriquillo's Revolt, after the Taíno cacique who lead them, ensued, resulting from escaped African slaves on the island possibly working with the Taíno people.

Precious metals played a large role in the history of the island after Columbus's arrival. One of the first inhabitants Columbus came across on this island was "a girl wearing only a gold nose plug". Soon the Taínos were trading pieces of gold for hawk's bells with their cacique declaring the gold came from Cibao. Traveling further east from Navidad, Columbus came across the Yaque del Norte River, which he named Rio de Oro because its "sands abound in gold dust".

On Columbus's return during his second voyage, he learned it was the cacique Caonabo who had massacred his settlement at Navidad. While Columbus established a new settlement at La Isabela on Jan. 1494, he sent Alonso de Ojeda and 15 men to search for the mines of Cibao. After a six-day journey, Ojeda came across an area containing gold, in which the gold was extracted from streams by the Taíno people. Columbus himself visited the mines of Cibao on 12 March 1494. He constructed the Fort of Santo Tomas, present day Janico, with Captain Pedro Margarit in command of 56 men. On 24 March 1495, Columbus with his ally Guacanagarix, embarked on a war of revenge against Caonabo, capturing him and his family while killing and capturing many natives. Afterwards, every person over the age of fourteen had to produce a hawksbill of gold.

Miguel Diaz and Francisco de Garay discovered large gold nuggets on the lower Haina River in 1496. These San Cristobal mines were later known as the Minas Viejas mines. Then, in 1499, the first major discovery of gold was made in the cordillera central, which led to a mining boom. By 1501, Columbus's cousin Giovanni Colombo, had discovered gold near Buenaventura. The deposits were later known as Minas Nuevas. Two major mining areas resulted, one along San Cristobal-Buenaventura, and another in Cibao within the La Vega-Cotuy-Bonao triangle, while Santiago de los Caballeros, Concepcion, and Bonao became mining towns. The gold rush of 1500–1508 ensued, and Ovando expropriated the gold mines of Miguel Diaz and Francisco de Garay in 1504, as pit mines became royal mines for Ferdinand, who reserved the best mines for himself, though placers were open to private prospectors. Furthermore, Ferdinand kept 967 natives in the San Cristobal mining area supervised by salaried miners.

Under Nicolás de Ovando y Cáceres' governorship, the Indians were made to work in the gold mines. By 1503, the Spanish Crown legalized the distribution of Indians to work the mines as part of the encomienda system. Once the Indians entered the mines, they were often wiped out by hunger and difficult conditions. By 1508, the Taíno population of about 400,000 was reduced to 60,000, and by 1514, only 26,334 remained. About half resided in the mining towns of Concepcion, Santiago, Santo Domingo, and Buenaventura. The repartimiento of 1514 accelerated emigration of the Spanish colonists, coupled with the exhaustion of the mines. The first documented outbreak of smallpox, previously an Eastern hemisphere disease, occurred on Hispaniola in December 1518 among enslaved African miners. Some scholars speculate that European diseases arrived before this date, but there is no compelling evidence for an outbreak. The natives had no immunity to European diseases, including smallpox. By May 1519, as many as one-third of the remaining Taínos had died.

Christopher Columbus brought sugar cane on his second voyage to the island. Molasses was the chief product. Diego Colon's plantation had 40 African slaves in 1522. By 1526, 19 mills were in operation from Azua to Santo Domingo. In 1574, a census taken of the Greater Antilles reported 1,000 Spaniards and 12,000 African slaves on Hispaniola.

As Spain conquered new regions on the mainland of the Americas (Spanish Main), its interest in Hispaniola waned, and the colony’s population grew slowly. By the early 17th century, the island and its smaller neighbors (notably Tortuga) became regular stopping points for Caribbean pirates. In 1606, the government of Philip III ordered all inhabitants of Hispaniola to move close to Santo Domingo, to avoid interaction with pirates. Rather than secure the island, his action meant that French, English and Dutch pirates established their own bases on the abandoned north and west coasts of the island.
In 1665, French colonization of the island was officially recognized by King Louis XIV. The French colony was given the name Saint-Domingue. In the 1697 Treaty of Ryswick, Spain formally ceded the western third of the island to France. Saint-Domingue quickly came to overshadow the east in both wealth and population. Nicknamed the "Pearl of the Antilles," it became the richest and most prosperous colony in the West Indies, with a system of human enslavement used to grow and harvest sugar cane during a time when sugar demand was high in Europe. Slavery kept prices low and profit was maximized. It was an important port in the Americas for goods and products flowing to and from France and Europe. 

European colonists often died young due to tropical fevers, as well as from violent slave resistance in the late eighteenth century. In 1791, during the French Revolution, a major slave revolt broke out on Saint-Domingue. When the French Republic abolished slavery in the colonies on February 4, 1794, it was a European first. The ex-slave army joined forces with France in its war against its European neighbors. In the second 1795 Treaty of Basel (July 22), Spain ceded the eastern two-thirds of the island of Hispaniola, later to become the Dominican Republic. French settlers had begun to colonize some areas in the Spanish side of the territory.

Under Napoleon, France reimposed slavery in most of its Caribbean islands in 1802 and sent an army to bring Saint-Domingue under tighter control. However, thousands of the French troops succumbed to yellow fever during the summer months, and more than half of the French army died because of disease. After the French removed the surviving 7,000 troops in late 1803, the leaders of the revolution declared western Hispaniola the new nation of independent Haiti in early 1804. France continued to rule Spanish Santo Domingo. In 1805, Haitian troops of General Henri Christophe tried to conquer all of Hispaniola. They invaded Santo Domingo and sacked the towns of Santiago de los Caballeros and Moca, killing most of their residents, but news of a French fleet sailing towards Haiti forced General Christophe to withdraw from the east, leaving it in French hands. In 1808, following Napoleon's invasion of Spain, the criollos of Santo Domingo revolted against French rule and, with the aid of the United Kingdom, returned Santo Domingo to Spanish control. Fearing the influence of a society that had successfully fought and won against their enslavers, the United States and European powers refused to recognize Haiti, the second republic in the Western Hemisphere. France demanded a high payment for compensation to slaveholders who lost their property, and Haiti was saddled with unmanageable debt for decades. It became one of the poorest countries in the Americas, while the Dominican Republic gradually has developed into the one of the largest economies of Central America and the Caribbean.

Hispaniola is the second-largest island in the Caribbean (after Cuba), with an area of , of which is under the sovereignty of the Dominican Republic occupying the eastern portion and under the sovereignty of Haiti occupying the western portion.

The island of Cuba lies to the northwest across the Windward Passage; 190 km to the southwest lies Jamaica, separated by the Jamaica Channel. Puerto Rico lies 130 km east of Hispaniola across the Mona Passage. The Bahamas and Turks and Caicos Islands lie to the north. Its westernmost point is known as Cap Carcasse. Cuba, Hispaniola, Jamaica, and Puerto Rico are collectively known as the Greater Antilles.

The island has five major mountain ranges: The Central Range, known in the Dominican Republic as the Cordillera Central, spans the central part of the island, extending from the south coast of the Dominican Republic into northwestern Haiti, where it is known as the Massif du Nord. This mountain range boasts the highest peak in the Antilles, Pico Duarte at above sea level. The Cordillera Septentrional runs parallel to the Central Range across the northern end of the Dominican Republic, extending into the Atlantic Ocean as the Samaná Peninsula. The Cordillera Central and Cordillera Septentrional are separated by the lowlands of the Cibao Valley and the Atlantic coastal plains, which extend westward into Haiti as the Plaine du Nord (Northern Plain). The lowest of the ranges is the Cordillera Oriental, in the eastern part of the country.

The Sierra de Neiba rises in the southwest of the Dominican Republic, and continues northwest into Haiti, parallel to the Cordillera Central, as the Montagnes Noires, Chaîne des Matheux and the Montagnes du Trou d'Eau. The Plateau Central lies between the Massif du Nord and the Montagnes Noires, and the Plaine de l‘Artibonite lies between the Montagnes Noires and the Chaîne des Matheux, opening westward toward the Gulf of Gonâve, the largest gulf of the Antilles.

The southern range begins in the southwestern most Dominican Republic as the Sierra de Bahoruco, and extends west into Haiti as the Massif de la Selle and the Massif de la Hotte, which form the mountainous spine of Haiti’s southern peninsula. Pic de la Selle is the highest peak in the southern range, the third highest peak in the Antilles and consequently the highest point in Haiti, at above sea level. A depression runs parallel to the southern range, between the southern range and the Chaîne des Matheux-Sierra de Neiba. It is known as the Plaine du Cul-de-Sac in Haiti, and Haiti’s capital Port-au-Prince lies at its western end. The depression is home to a chain of salt lakes, including Lake Azuei in Haiti and Lake Enriquillo in the Dominican Republic.

The island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic.

There are many bird species in Hispaniola, and the island's amphibian species are also diverse. Numerous land species on the island are endangered and could become extinct. There are many species endemic to the island including insects and other invertebrates, reptiles, and mammals. The most famous endemic mammal on the island is the Hispaniola Hutia (Plagiodontia aedium). There are also many avian species on the island. The six endemic genera are Calyptophilus, Dulus, Nesoctites, Phaenicophilus, Xenoligea and Microligea. More than half of the original ecoregion has been lost to habitat destruction impacting the local fauna. 

The island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic
In Haiti, deforestation has long been cited by scientists as a source of ecological crisis; the timber industry dates back to French colonial rule. Haiti has seen a dramatic reduction of forests due to the excessive and increasing use of charcoal as fuel for cooking. Various media outlets have suggested that the country has just 2% forest cover, but this has not been substantiated by research.

Recent in-depth studies of satellite imagery and environmental analysis regarding forest classification conclude that Haiti actually has approximately 30% tree cover; this is, nevertheless, a stark decrease from the country's 60% forest cover in 1925. The country has been significantly deforested over the last 50 years, resulting in the desertification of portions of the Haitian territory.

In the Dominican Republic, the forest cover has increased. In 2003, the Dominican forest cover had been reduced to 32% of the territory, but by 2011, forest cover had increased to nearly 40%. The success of the Dominican forest growth is due to several Dominican government policies and private organizations for the purpose, and a strong educational campaign that has resulted in increased awareness on the Dominican people of the importance of forests for their welfare and in other forms of life on the island.

Owing to its mountainous topography, Hispaniola’s climate shows considerable variation over short distances, and is the most varied of all the Antilles.

Except in the Northern Hemisphere summer season, the predominant winds over Hispaniola are the northeast trade winds. As in Jamaica and Cuba, these winds deposit their moisture on the northern mountains, and create a distinct rain shadow on the southern coast, where some areas receive as little as of rainfall, and have semi-arid climates. Annual rainfall under also occurs on the southern coast of Haiti’s northwest peninsula and in the central Azúa region of the Plaine du Cul-de-Sac. In these regions, moreover, there is generally little rainfall outside hurricane season from August to October, and droughts are by no means uncommon when hurricanes do not come.
On the northern coast, in contrast, rainfall may peak between December and February, though some rain falls in all months of the year. Annual amounts typically range from on the northern coastal lowlands; there is probably much more in the Cordillera Septentrional, though no data exist.

The interior of Hispaniola, along with the southeastern coast centered around Santo Domingo, typically receives around per year, with a distinct wet season from May to October. Usually, this wet season has two peaks: one around May, the other around the hurricane season. In the interior highlands, rainfall is much greater, around per year, but with a similar pattern to that observed in the central lowlands.

As is usual for tropical islands, variations of temperature are much less marked than rainfall variations, and depend only on altitude. Lowland Hispaniola is generally oppressively hot and humid, with temperatures averaging . with high humidity during the daytime, and around at night. At higher altitudes, temperatures fall steadily, so that frosts occur during the dry season on the highest peaks, where maxima are no higher than .

"See also: Demographics of the Dominican Republic and Demographics of Haiti" 

The Dominican Republic is a Hispanophone nation of approximately 10 million people. Spanish is spoken by all Dominicans as a primary language. Roman Catholicism is the official and dominant religion.

Haiti is a Francophone nation of roughly 10 million people. Although French is spoken as a primary language by the educated and wealthy minority, virtually the entire population speaks Haitian Creole, one of several French-derived creole languages. Roman Catholicism is the dominant religion, practiced by more than half the population, although in some cases in combination with Haitian Vodou faith. Another 25% of the populace belong to Protestant churches. Haiti emerged as the first Black republic in the world. 

"See also: People of the Dominican Republic" 

The ethnic composition of the Dominican population is 73% mulatto, 16% white and 11% black. Descendants of early Spanish settlers and of black slaves from West Africa constitute the two main racial strains.

The ethnic composition of Haiti is estimated to be 95% black and 5% white and mulatto.

In recent times, Dominican and Puerto Rican researchers identified in the current Dominican population the presence of genes belonging to the aborigines of the Canary Islands (commonly called Guanches). These types of genes also have been detected in Puerto Rico.

The island has the largest economy in the Greater Antilles, however most of the economic development is found in the Dominican Republic, the Dominican economy being nearly 800% larger than the Haitian economy.

The estimated annual per capita income is US$1,300 in Haiti and US$8,200 in Dominican Republic.

The divergence between the level of economic development between Haiti and Dominican Republic makes its border the higher contrast of all western land borders and is evident that the Dominican Republic has one of the highest migration issues in the Americas.

The island also has an economic history and current day interest and involvement in precious metals. In 1860, it was observed that the island contained a large supply of gold, of which the early Spaniards had hardly developed. By 1919, Condit and Ross noted that much of the island was covered by government granted concessions for mining different types of minerals. Besides gold, these minerals included silver, manganese, copper, magnetite, iron and nickel.

Mining operations in 2016 have taken advantage of the volcanogenic massive sulfide ore deposits (VMS) around Maimón. To the northeast, the Pueblo Viejo Gold Mine was operated by state-owned Rosario Dominicana from 1975 until 1991. In 2009, Pueblo Viejo Dominicana Corporation, formed by Barrick Gold and Goldcorp, started open-pit mining operations of the Monte Negro and Moore oxide deposits. The mined ore is processed with gold cyanidation. Pyrite and sphalerite are the main sulfide minerals found in the 120 m thick volcanic conglomerates and agglomerates, which constitute the world's second largest sulphidation gold deposit.

Between Bonao and Maimon, Falconbridge Dominicana has been mining nickel laterites since 1971. The Cerro de Maimon copper/gold open-pit mine southeast of Maimon has been operated by Perilya since 2006. Copper is extracted from the sulfide ores, while gold and silver are extracted from both the sulfide and the oxide ores. Processing is via froth flotation and cyanidation. The ore is located in the VMS Early Cretaceous Maimon Formation. Goethite enriched with gold and silver is found in the 30 m thick oxide cap. Below that cap is a supergene zone containing pyrite, chalcopyrite, and sphalerite. Below the supergene zone is found the unaltered massive sulphide mineralization.




</doc>
<doc id="13717" url="https://en.wikipedia.org/wiki?curid=13717" title="Halle Berry">
Halle Berry

Halle Maria Berry (born Maria Halle Berry; August 14, 1966) is an American actress. Berry won the 2002 Academy Award for Best Actress for her performance in the romantic drama film "Monster's Ball" (2001). , she is the only woman of African-American descent to have won the award.

Berry was one of the highest-paid actresses in Hollywood during the 2000s, and has been involved in the production of several of the films in which she performed. Berry is also a Revlon spokesmodel. Before becoming an actress, she started modeling and entered several beauty contests, finishing as the 1st runner-up in the Miss USA Pageant and coming in 6th place in the Miss World Pageant in 1986. Her breakthrough film role was in the romantic comedy "Boomerang" (1992), alongside Eddie Murphy, which led to roles in films, such as the family comedy "The Flintstones" (1994), the political comedy-drama "Bulworth" (1998) and the television film "Introducing Dorothy Dandridge" (1999), for which she won a Primetime Emmy Award and a Golden Globe Award, among other awards.

In addition to her Academy Award win, Berry garnered high-profile roles in the 2000s, such as Storm in "X-Men" (2000), the action crime thriller "Swordfish" (2001), and the spy film "Die Another Day" (2002), where she played Bond girl Jinx. She then appeared in the "X-Men" sequels, "X2" (2003) and "" (2006). In the 2010s, she appeared in a number of films, including the science-fiction film "Cloud Atlas" (2012), the crime thriller "The Call" (2013) and "" (2014). Berry was formerly married to baseball player David Justice and singer-songwriter Eric Benét.

Berry was born Maria Halle Berry; her name was legally changed to Halle Maria Berry at age five. Her parents selected her middle name from Halle's Department Store, which was then a local landmark in her birthplace of Cleveland, Ohio. Her mother, Judith Ann (née Hawkins), who is of English and German ancestry, was a psychiatric nurse. Her father, Jerome Jesse Berry, was an African-American hospital attendant in the psychiatric ward where her mother worked; he later became a bus driver. Berry's parents divorced when she was four years old; she and her older sister, Heidi Berry-Henderson, were raised exclusively by their mother.

Berry has said in published reports that she has been estranged from her father since her childhood, noting in 1992, "I haven't heard from him since [he left]. Maybe he's not alive." Her father was very abusive to her mother. Berry has recalled witnessing her mother being beaten daily, kicked down stairs and hit in the head with a wine bottle.

Berry grew up in Oakwood, Ohio and graduated from Bedford High School where she was a cheerleader, honor student, editor of the school newspaper and prom queen. She worked in the children's department at Higbee's Department store. She then studied at Cuyahoga Community College. In the 1980s, she entered several beauty contests, winning Miss Teen All American in 1985 and Miss Ohio USA in 1986. She was the 1986 Miss USA first runner-up to Christy Fichtner of Texas. In the Miss USA 1986 pageant interview competition, she said she hoped to become an entertainer or to have something to do with the media. Her interview was awarded the highest score by the judges. She was the first African-American Miss World entrant in 1986, where she finished sixth and Trinidad and Tobago's Giselle Laronde was crowned Miss World. According to the "Current Biography Yearbook", Berry "...pursued a modeling career in New York... Berry's first weeks in New York were less than auspicious: She slept in a homeless shelter and then in a YMCA".

In 1989, Berry moved to New York City to pursue her acting ambitions. During her early time there, she ran out of money and had to live briefly in a homeless shelter. Her situation improved by the end of that year, and she was cast in the role of model Emily Franklin in the short-lived ABC television series "Living Dolls", which was shot in New York and was a spin-off of the hit series "Who's the Boss?". During the taping of "Living Dolls", she lapsed into a coma and was diagnosed with Type 1 diabetes. After the cancellation of "Living Dolls", she moved to Los Angeles. She went on to have a recurring role on the long-running primetime serial "Knots Landing".

Berry's film debut was in a small role for Spike Lee's "Jungle Fever" (1991), in which she played Vivian, a drug addict. That same year, Berry had her first co-starring role in "Strictly Business". In 1992, Berry portrayed a career woman who falls for the lead character played by Eddie Murphy in the romantic comedy "Boomerang". The following year, she caught the public's attention as a headstrong biracial slave in the TV adaptation of "", based on the book by Alex Haley. Berry was in the live-action "Flintstones" movie playing the part of "Sharon Stone", a sultry secretary who seduced Fred Flintstone.

Berry tackled a more serious role, playing a former drug addict struggling to regain custody of her son in "Losing Isaiah" (1995), starring opposite Jessica Lange. She portrayed Sandra Beecher in "Race the Sun" (1996), which was based on a true story, shot in Australia, and co-starred alongside Kurt Russell in "Executive Decision". Beginning in 1996, she was a Revlon spokeswoman for seven years and renewed her contract in 2004.

She starred alongside Natalie Deselle Reid in the 1997 comedy film "B*A*P*S". In 1998, Berry received praise for her role in "Bulworth" as an intelligent woman raised by activists who gives a politician (Warren Beatty) a new lease on life. The same year, she played the singer Zola Taylor, one of the three wives of pop singer Frankie Lymon, in the biopic "Why Do Fools Fall in Love". In the 1999 HBO biopic "Introducing Dorothy Dandridge", she portrayed the first black woman to be nominated for the Academy Award for Best Actress, and it was to Berry a heart-felt project that she introduced, co-produced and fought intensely for it to come through. Berry's performance was recognized with several awards, including a Primetime Emmy Award and Golden Globe Award.

Berry portrayed the mutant superhero Storm in the film adaptation of the comic book series "X-Men" (2000) and its sequels, "X2" (2003), "" (2006) and "" (2014). In 2001, Berry appeared in the film "Swordfish", which featured her first topless scene. At first, she refused to be filmed topless in a sunbathing scene, but she changed her mind when Warner Brothers raised her fee substantially. The brief flash of her breasts added $500,000 to her fee. Berry considered these stories to be rumors and was quick to deny them. After turning down numerous roles that required nudity, she said she decided to make "Swordfish" because her then-husband, Eric Benét, supported her and encouraged her to take risks.

Berry appeared as Leticia Musgrove, the troubled wife of an executed murderer (Sean Combs), in the 2001 feature film "Monster's Ball". Her performance was awarded the National Board of Review and the Screen Actors Guild Award for Best Actress; in an interesting coincidence she became the first woman of color to win the Academy Award for Best Actress (earlier in her career, she portrayed Dorothy Dandridge, the first African American to be nominated for Best Actress, and who was born at the same hospital as Berry, in Cleveland, Ohio). The NAACP issued the statement: "Congratulations to Halle Berry and Denzel Washington for giving us hope and making us proud. If this is a sign that Hollywood is finally ready to give opportunity and judge performance based on skill and not on skin color then it is a good thing." This role generated controversy. Her graphic nude love scene with a racist character played by co-star Billy Bob Thornton was the subject of much media chatter and discussion among African Americans. Many in the African-American community were critical of Berry for taking the part. Berry responded: "I don't really see a reason to ever go that far again. That was a unique movie. That scene was special and pivotal and needed to be there, and it would be a really special script that would require something like that again."

Berry asked for a higher fee for Revlon advertisements after winning the Oscar. Ron Perelman, the cosmetics firm's chief, congratulated her, saying how happy he was that she modeled for his company. She replied, "Of course, you'll have to pay me more." Perelman stalked off in a rage. In accepting her award, she gave an acceptance speech honoring previous black actresses who had never had the opportunity. She said, "This moment is so much bigger than me. This is for every nameless, faceless woman of colour who now has a chance tonight because this door has been opened."

As Bond girl Giacinta 'Jinx' Johnson in the 2002 blockbuster "Die Another Day", Berry recreated a scene from "Dr. No", emerging from the surf to be greeted by James Bond as Ursula Andress had 40 years earlier. Lindy Hemming, costume designer on "Die Another Day", had insisted that Berry wear a bikini and knife as a homage. Berry has said of the scene: "It's splashy", "exciting", "sexy", "provocative" and "it will keep me still out there after winning an Oscar". The bikini scene was shot in Cadiz; the location was reportedly cold and windy, and footage has been released of Berry wrapped in thick towels in between takes to try to stay warm. According to an ITV news poll, Jinx was voted the fourth toughest girl on screen of all time. Berry was hurt during filming when debris from a smoke grenade flew into her eye. It was removed in a 30-minute operation. After Berry won the Academy Award, rewrites were commissioned to give her more screentime for "X2".

She starred in the psychological thriller "Gothika" opposite Robert Downey, Jr. in November 2003, during which she broke her arm in a scene with Downey, who twisted her arm too hard. Production was halted for eight weeks. It was a moderate hit at the United States box office, taking in $60 million; it earned another $80 million abroad. Berry appeared in the nu metal band Limp Bizkit's music video for "Behind Blue Eyes" for the motion picture soundtrack for the film. The same year, she was named #1 in "FHM"s 100 Sexiest Women in the World poll.

Berry starred as the title role in the film "Catwoman", for which she received US$12.5 million. An over-US$100 million movie; it grossed only US$17 million on its first weekend, and is widely regarded as one of the worst films ever made by critics. She was awarded the Worst Actress Razzie Award for her performance; she appeared at the ceremony to accept the award in person (making her the third person, and second actor, ever to do so) with a sense of humor, considering it an experience of the "rock bottom" in order to be "at the top". Holding the Academy Award in one hand and the Razzie in the other she said, "I never in my life thought that I would be here, winning a Razzie. It's not like I ever aspired to be here, but thank you. When I was a kid, my mother told me that if you could not be a good loser, then there's no way you could be a good winner."
Her next film appearance was in the Oprah Winfrey-produced ABC television movie "Their Eyes Were Watching God" (2005), an adaptation of Zora Neale Hurston's novel, with Berry portraying a free-spirited woman whose unconventional sexual mores upset her 1920s contemporaries in a small community. She received her second Primetime Emmy Award for her role. Also in 2005, she served as an executive producer in "Lackawanna Blues", and landed her voice for the character of Cappy, one of the many mechanical beings in the animated feature "Robots".

In the thriller "Perfect Stranger" (2007), Berry starred with Bruce Willis, playing a reporter who goes undercover to uncover the killer of her childhood friend. The film grossed a modest US$73 million worldwide, and received lukewarm reviews from critics, who felt that despite the presence of Berry and Willis, it is "too convoluted to work, and features a twist ending that's irritating and superfluous". Her next 2007 film release was the drama "Things We Lost in the Fire", co-starring Benicio del Toro, where she took on the role of a recent widow befriending with the troubled friend of her late husband. The film was the first time in which she worked with a female director, Danish Susanne Bier, giving her a new feeling of "thinking the same way", which she appreciated. While the film made US$8.6 million in its global theatrical run, it garnered positive reviews from writers; "The Austin Chronicle" found the film to be "an impeccably constructed and perfectly paced drama of domestic and internal volatility" and felt that "Berry is brilliant here, as good as she's ever been".

In April 2007, Berry was awarded a star on the Hollywood Walk of Fame in front of the Kodak Theatre at 6801 Hollywood Boulevard for her contributions to the film industry, and by the end of the decade, she established herself as one of the highest-paid actresses in Hollywood, earning an estimated $10 million per film.

In the independent drama "Frankie and Alice" (2010), Berry played the leading role of a young multiracial American woman with dissociative identity disorder struggling against her alter personality to retain her true self. The film received a limited theatrical release, to a mixed critical response. "The Hollywood Reporter" nevertheless described the film as "a well-wrought psychological drama that delves into the dark side of one woman's psyche" and found Berry to be "spellbinding" in it. She earned the African-American Film Critics Association Award for Best Actress and a Golden Globe Award nomination for Best Actress – Motion Picture Drama. She next made part of a large ensemble cast in Garry Marshall's romantic comedy "New Year's Eve" (2011), with Michelle Pfeiffer, Jessica Biel, Robert De Niro, Josh Duhamel, Zac Efron, Sarah Jessica Parker, and Sofía Vergara, among many others. In the film, she took on the supporting role of a nurse befriending a man in the final stages (De Niro). While the film was panned by critics, it made US$142 million worldwide.

In 2012, Berry starred as an expert diver tutor alongside then-husband Oliver Martinez in the little-seen thriller "Dark Tide", and led an ensemble cast opposite Tom Hanks and Jim Broadbent in The Wachowskis's epic science fiction film "Cloud Atlas" (2012), with each of the actors playing six different characters across a period of five centuries. Budgeted at US$128.8 million, "Cloud Atlas" made US$130.4 million worldwide, and garnered polarized reactions from both critics and audiences.

Berry appeared in a segment of the independent anthology comedy "Movie 43" (2013), which the "Chicago Sun-Times" called "the "Citizen Kane" of awful". Berry found greater success with her next performance, as a 9-1-1 operator receiving a call from a girl kidnapped by a serial killer, in the crime thriller "The Call" (2013). Berry was drawn to "the idea of being a part of a movie that was so empowering for women. We don't often get to play roles like this, where ordinary people become heroic and do something extraordinary." Manohla Dargis of "The New York Times" found the film to be "an effectively creepy thriller", while reviewer Dwight Brown felt that "the script gives Berry a blue-collar character she can make accessible, vulnerable and gutsy[...]". "The Call" was a sleeper hit, grossing US$68.6 million around the globe.

In 2014, Berry signed on to star and serve as a co-executive producer in CBS drama series "Extant", where she took on the role of Molly Woods, an astronaut who struggles to reconnect with her husband and android son after spending 13 months in space. The show ran for two seasons until 2015, receiving largely positive reviews from critics. "USA Today" remarked: "She [Halle Berry] brings a dignity and gravity to Molly, a projected intelligence that allows you to buy her as an astronaut and to see what has happened to her as frightening rather than ridiculous. Berry's all in, and you float along". Also in 2014, Berry launched a new production company, 606 Films, with producing partner Elaine Goldsmith-Thomas. It is named after the Anti-Paparazzi Bill, SB 606, that the actress pushed for and which was signed into law by California Governor Jerry Brown in the fall of 2013. The new company emerged as part of a deal for Berry to work in "Extant".

In the stand-up comedy concert film "" (2016), Berry appeared as herself, opposide Kevin Hart, attending a poker game event that goes horribly wrong. "Kidnap", an abduction thriller Berry filmed in 2014, was released in 2017. In the film, she starred as a dinner waitress tailing a vehicle when her son is kidnapped by its occupants. "Kidnap" grossed US$34 million and garnered mixed reviews from writers, who felt that it "strays into poorly scripted exploitation too often to take advantage of its pulpy premise — or the still-impressive talents of [Berry]." She next played an agent employed by a secret American spy organisation in the action comedy sequel "" (2017), as part of an ensemble cast, consisting of Colin Firth, Taron Egerton, Mark Strong, Julianne Moore, and Elton John. While critical response towards the film was mixed, it made US$414 million worldwide.

Alongside Daniel Craig, Berry starred as a working-class mother during the 1992 Los Angeles riots in Deniz Gamze Ergüven's drama "Kings" (2017). The film found a limited theatrical release following its initial screening at the Toronto International Film Festival, and as part of an overall lukewarm reception, "Variety" noted: "It should be said that Berry has given some of the best and worst performances of the past quarter-century, but this is perhaps the only one that swings to both extremes in the same movie". She has taken on the role of an assassin in the film "", which is scheduled to be released on May 17, 2019 by Lionsgate.

Berry competed against James Corden in the first rap battle on the first episode of TBS's "Drop the Mic", originally aired on October 24, 2017.

She currently serves as executive producer of the BET television series "Boomerang", based on the film in which she starred. The series premieres February 12, 2019.

Berry dated Chicago dentist John Ronan from March 1989 to October 1991. In November 1993, Ronan sued Berry for $80,000 in what he claimed were unpaid loans to help launch her career. Berry contended that the money was a gift, and a judge dismissed the case because Ronan did not list Berry as a debtor when he filed for bankruptcy in 1992. According to Berry, a beating from a former abusive boyfriend during the filming of "The Last Boy Scout" in 1991 punctured her eardrum and caused her to lose eighty percent of her hearing in her left ear. Berry has never named the abuser but has said that he is someone well known in Hollywood.

Berry first saw baseball player David Justice on TV playing in an MTV celebrity baseball game in February 1992. When a reporter from Justice's hometown of Cincinnati told her that Justice was a fan, Berry gave her phone number to the reporter to give to Justice. Berry married Justice shortly after midnight on January 1, 1993. Following their separation in February 1996, Berry stated publicly that she was so depressed that she considered taking her own life. Berry and Justice were officially divorced on June 24, 1997.

Berry married her second husband, singer-songwriter Eric Benét, on January 24, 2001, following a two-year courtship, but by early October 2003 they had separated, with the divorce finalized on January 3, 2005. Benét underwent treatment for sex addiction in 2002. In November 2005, Berry began dating French Canadian model Gabriel Aubry, whom she met at a Versace photoshoot. Berry gave birth to their daughter in March 2008. On April 30, 2010, Berry and Aubry announced their separation.

After their 2010 separation, Berry and Aubry became involved in a highly publicized custody battle, centered primarily on Berry's desire to move with their daughter from Los Angeles, where Berry and Aubry resided, to France, the home of French actor Olivier Martinez, whom Berry had started dating in 2010 after they met while filming "Dark Tide" in South Africa. Aubry objected to the move on the grounds that it would interfere with their joint custody arrangement. In November 2012, a judge denied Berry's request to move the couple's daughter to France in light of Aubry's objections. Less than two weeks later, on November 22, 2012, Aubry and Martinez were both treated at a hospital for injuries after engaging in a physical altercation at Berry's residence. Martinez performed a citizen's arrest on Aubry, and because it was considered a domestic violence incident, was granted a temporary emergency protective order preventing Aubry from coming within 100 yards of Berry, Martinez, and the child with whom he shares custody with Berry, until November 29, 2012. In turn, Aubry obtained a temporary restraining order against Martinez on November 26, 2012, asserting that the fight began when Martinez threatened to kill Aubry if he did not allow the couple to move to France. Leaked court documents included photos showing significant injuries to Aubry's face, which were widely displayed in the media.

On November 29, 2012, Berry's lawyer announced that Berry and Aubry had reached an amicable custody agreement in court. In June 2014, a Superior Court ruling called for Berry to pay Aubry $16,000 a month in child support (around 200k/year) as well as a retroactive payment of $115,000 and a sum of $300,000 for Aubry's attorney fees. Berry and Martinez confirmed their engagement in March 2012, and married in France on July 13, 2013. In October 2013, Berry gave birth to their son. After two years of marriage, in 2015 the couple announced they were divorcing. The divorce became final in December 2016.

In February 2000, Berry was involved in a traffic collision and left the scene. She pleaded no contest to misdemeanor leaving the scene of an accident.

Along with Pierce Brosnan, Cindy Crawford, Jane Seymour, Dick Van Dyke, Téa Leoni, and Daryl Hannah, Berry successfully fought in 2006 against the Cabrillo Port Liquefied Natural Gas facility that was proposed off the coast of Malibu. Berry said, "I care about the air we breathe, I care about the marine life and the ecosystem of the ocean." In May 2007, Governor Arnold Schwarzenegger vetoed the facility. Hasty Pudding Theatricals gave her its 2006 "Woman of The Year" award. Berry took part in a nearly 2,000-house cell-phone bank campaign for Barack Obama in February 2008. In April 2013, she appeared in a video clip for Gucci's "Chime for Change" campaign that aims to raise funds and awareness of women's issues in terms of education, health, and justice. In August 2013, Berry testified alongside Jennifer Garner before the California State Assembly's Judiciary Committee in support of a bill that would protect celebrities' children from harassment by photographers. The bill passed in September.

Berry was ranked No. 1 on "People" "50 Most Beautiful People in the World" list in 2003 after making the top ten seven times and appeared No. 1 on "FHM" "100 Sexiest Women in the World" the same year. She was named "Esquire" magazine's "Sexiest Woman Alive" in October 2008, about which she stated: "I don't know exactly what it means, but being 42 and having just had a baby, I think I'll take it." "Men's Health" ranked her at No. 35 on their "100 Hottest Women of All-Time" list. In 2009, she was voted #23 on "Empire"'s 100 Sexiest Film Stars. The same year, rapper Hurricane Chris released a song entitled "Halle Berry (She's Fine)", extolling Berry's beauty and sex appeal. At the age of 42 (in 2008), she was named the "Sexiest Black Woman" by Access Hollywood's TV One Access survey. Born to an African-American father and a white mother, Berry has stated that her biracial background was "painful and confusing" when she was a young woman, and she made the decision early on to identify as a black woman because she knew that was how she would be perceived.





</doc>
<doc id="13722" url="https://en.wikipedia.org/wiki?curid=13722" title="Robert Koch">
Robert Koch

Heinrich Hermann Robert Koch (; ; 11 December 1843 – 27 May 1910) was a German physician and microbiologist. As the founder of modern bacteriology, he identified the specific causative agents of tuberculosis, cholera, and anthrax and gave experimental support for the concept of infectious disease, which included experiments on humans and other animals. Koch created and improved laboratory technologies and techniques in the field of microbiology, and made key discoveries in public health. His research led to the creation of Koch's postulates, a series of four generalized principles linking specific microorganisms to specific diseases that remain today the "gold standard" in medical microbiology. For his research on tuberculosis, Koch received the Nobel Prize in Physiology or Medicine in 1905. The Robert Koch Institute is named in his honor.

Koch was born in Clausthal, Germany, on 11 December 1842, to Hermann Koch (1814–1877) and Mathilde Julie Henriette (née Biewend; 1818–1871). Koch excelled in academics from an early age. Before entering school in 1848, he had taught himself how to read and write. He graduated from high school in 1862, having excelled in science and math. At the age of 19, Koch entered the University of Göttingen, studying natural science. However, after three semesters, Koch decided to change his area of study to medicine, as he aspired to be a physician. During his fifth semester of medical school, Jacob Henle, an anatomist who had published a theory of contagion in 1840, asked him to participate in his research project on uterine nerve structure. In his sixth semester, Koch began to conduct research at the Physiological Institute, where he studied the secretion of succinic acid, which is a signaling molecule that is also involved in the metabolism of the mitochondria. This would eventually form the basis of his dissertation. In January 1866, Koch graduated from medical school, earning honors of the highest distinction.

Several years after his graduation in 1866, he worked as a surgeon in the Franco-Prussian War, and following his service, worked as a physician in in Prussian Posen (now Wolsztyn, Poland). From 1880 to 1885, Koch held a position as government advisor with the Imperial Department of Health. Koch began conducting research on microorganisms in a laboratory connected to his patient examination room. Koch's early research in this laboratory yielded one of his major contributions to the field of microbiology, as he developed the technique of growing bacteria. Furthermore, he managed to isolate and grow selected pathogens in pure laboratory culture.

From 1885 to 1890, he served as an administrator and professor at Berlin University.

In 1891, Koch relinquished his Professorship and became a director of the which consisted of a clinical division and beds for the division of clinical research. For this he accepted harsh conditions. The Prussian Ministry of Health insisted after the 1890 scandal with tuberculin, which Koch had discovered and intended as a remedy for tuberculosis, that any of Koch's inventions would unconditionally belong to the government and he would not be compensated. Koch lost the right to apply for patent protection.

In an attempt to grow bacteria, Koch began to use solid nutrients such as potato slices. Through these initial experiments, Koch observed individual colonies of identical, pure cells. He found that potato slices were not suitable media for all organisms, and later began to use nutrient solutions with gelatin. However, he soon realized that gelatin, like potato slices, was not the optimal medium for bacterial growth, as it did not remain solid at 37 °C, the ideal temperature for growth of most human pathogens. As suggested to him by Walther and Fanny Hesse, Koch began to utilize agar to grow and isolate pure cultures, because this polysaccharide remains solid at 37 °C, is not degraded by most bacteria, and results in a transparent medium.

During his time as government advisor, Koch published a report, in which he stated the importance of pure cultures in isolating disease-causing organisms and explained the necessary steps to obtain these cultures, methods which are summarized in Koch's four postulates. Koch's discovery of the causative agent of anthrax led to the formation of a generic set of postulates which can be used in the determination of the cause of most infectious diseases. These postulates, which not only outlined a method for linking cause and effect of an infectious disease but also established the significance of laboratory culture of infectious agents, are listed here:

Robert Koch is widely known for his work with anthrax, discovering the causative agent of the fatal disease to be "Bacillus anthracis". He discovered the formation of spores in anthrax bacteria, which could remain dormant under specific conditions. However, under optimal conditions, the spores were activated and caused disease. To determine this causative agent, he dry-fixed bacterial cultures onto glass slides, used dyes to stain the cultures, and observed them through a microscope. His work with anthrax is notable in that he was the first to link a specific microorganism with a specific disease, rejecting the idea of spontaneous generation and supporting the germ theory of disease.

During his time as the government advisor with the Imperial Department of Health in Berlin in the 1880s, Robert Koch became interested in tuberculosis research. At the time, it was widely believed that tuberculosis was an inherited disease. However, Koch was convinced that the disease was caused by a bacterium and was infectious, and tested his four postulates using guinea pigs. Through these experiments, he found that his experiments with tuberculosis satisfied all four of his postulates. In 1882, he published his findings on tuberculosis, in which he reported the causative agent of the disease to be the slow-growing "Mycobacterium tuberculosis". Later, Koch's attempt at developing a drug to treat tuberculosis, tuberculin, led to a scandalous failure: he did not divulge the exact composition, and the claimed treatment success did not materialize; the substance is today used for tuberculosis diagnosis. 

Koch and his relationship to Paul Ehrlich, who developed a mechanism to diagnose TB, were portrayed in the 1940 movie "Dr. Ehrlich's Magic Bullet".

Koch next turned his attention to cholera, and began to conduct research in Egypt in the hopes of isolating the causative agent of the disease. However, he was not able to complete the task before the epidemic in Egypt ended, and subsequently traveled to India to continue with the study. In 1884 in Bombay state of India, Koch resided and researched at Grant Medical College, (or by some accounts in Kolkata, formerly Calcutta in undivided British India) where he was able to determine the causative agent of cholera, isolating "Vibrio cholerae". The bacterium had originally been isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.

Koch observed the phenomenon of acquired immunity. On December 26, 1900, he arrived as part of an expedition to German New Guinea, which was then a protectorate of the German Reich. Koch serially examined the Papuan people, the indigenous inhabitants, and their blood samples and noticed they contained Plasmodium parasites, the cause of malaria, but their bouts of malaria were mild or could not even be noticed, i.e. were subclinical. On the contrary, German settlers and Chinese workers, who had been brought to New Guinea, fell sick immediately. The longer they had stayed in the country, however, the more they too seemed to develop a resistance against it.

In 1897, Koch was elected a Foreign Member of the Royal Society (ForMemRS). 
In 1905, Koch won the Nobel Prize in Physiology and Medicine for his work with tuberculosis.
In 1906, research on tuberculosis and tropical diseases won him the Prussian Order Pour le Merite and in 1908, the Robert Koch Medal, established to honour the greatest living physicians.

Koch's name is one of twenty-three, from the fields of hygiene and tropical medicine, featured on the frieze of the London School of Hygiene & Tropical Medicine building in Keppel Street, Bloomsbury.

A large marble statue of Koch stands in a small park known as Robert Koch Platz, just north of the Charity Hospital, in the Mitte section of Berlin. His life was the subject of a 1939 German produced motion picture that featured Oscar winning actor Emil Jannings in the title role. On 10 December 2017, Google showed a Doodle in celebration of Koch's birthday.

In July 1867, Koch married Emma (Emmy) Adolfine Josephine Fraatz, and the two had a daughter, Gertrude, in 1868. Their marriage ended after 26 years in 1893, and later that same year, he married actress Hedwig Freiberg (1872–1945).

On 9 April 1910, Koch suffered a heart attack and never made a complete recovery. On 27 May, three days after giving a lecture on his tuberculosis research at the Prussian Academy of Sciences, Koch died in Baden-Baden at the age of 66. Following his death, the Institute named its establishment after him in his honour. He was irreligious.




</doc>
<doc id="13726" url="https://en.wikipedia.org/wiki?curid=13726" title="Hogshead">
Hogshead

A hogshead (abbreviated "Hhd", plural "Hhds") is a large cask of liquid (or, less often, of a food commodity). More specifically, it refers to a specified volume, measured in either imperial or US customary measures, primarily applied to alcoholic beverages, such as wine, ale, or cider.

A tobacco hogshead was used in British and American colonial times to transport and store tobacco. It was a very large wooden barrel. A standardized hogshead measured long and in diameter at the head (at least , depending on the width in the middle). Fully packed with tobacco, it weighed about .

A dogshead contains about .

The "Oxford English Dictionary" (OED) notes that the hogshead was first standardized by an act of Parliament in 1423, though the standards continued to vary by locality and content. For example, the OED cites an 1897 edition of "Whitaker's Almanack", which specified the number of gallons of wine in a hogshead varying by type of wine: claret (presumably) , port , sherry ; and Madeira . The "American Heritage Dictionary" claims that a hogshead can consist of anything from (presumably) .

Eventually, a hogshead of wine came to be , while a hogshead of beer or ale is 64 gallons (250 L if old beer/ale gallons, 245 L if imperial).

A hogshead was also used as unit of measurement for sugar in Louisiana for most of the 19th century. Plantations were listed in sugar schedules as having produced "x" number of hogsheads of sugar or molasses. A hogshead was also used for the measurement of herring fished for sardines in Blacks Harbour, New Brunswick.

The etymology of hogshead is uncertain. According to English philologist Walter William Skeat (1835–1912), the origin is to be found in the name for a cask or liquid measure appearing in various forms in several Germanic languages, in Dutch "oxhooft" (modern "okshoofd"), Danish "oxehoved", Old Swedish "oxhufvod", etc. The word should therefore be "oxhead", "hogshead" being a mere corruption. It has been suggested that the name arose from the branding of such a measure with the head of an ox.

A hogshead of Madeira wine was approximately equal to 45–48 gallons (0.205–0.218) m.

A hogshead of brandy was approximately equal to 56–61 gallons (0.255–0.277) m.



</doc>
<doc id="13727" url="https://en.wikipedia.org/wiki?curid=13727" title="Huallaga">
Huallaga

Huallaga may refer to:



</doc>
<doc id="13729" url="https://en.wikipedia.org/wiki?curid=13729" title="Honda">
Honda

Honda has been the world's largest motorcycle manufacturer since 1959, as well as the world's largest manufacturer of internal combustion engines measured by volume, producing more than 14 million internal combustion engines each year. Honda became the second-largest Japanese automobile manufacturer in 2001. Honda was the eighth largest automobile manufacturer in the world in 2015.

Honda was the first Japanese automobile manufacturer to release a dedicated luxury brand, Acura, in 1986. Aside from their core automobile and motorcycle businesses, Honda also manufactures garden equipment, marine engines, personal watercraft and power generators, and other products. Since 1986, Honda has been involved with artificial intelligence/robotics research and released their ASIMO robot in 2000. They have also ventured into aerospace with the establishment of GE Honda Aero Engines in 2004 and the Honda HA-420 HondaJet, which began production in 2012. Honda has three joint-ventures in China (Honda China, Dongfeng Honda, and Guangqi Honda).

In 2013, Honda invested about 5.7% (US$6.8 billion) of its revenues in research and development. Also in 2013, Honda became the first Japanese automaker to be a net exporter from the United States, exporting 108,705 Honda and Acura models, while importing only 88,357.

Throughout his life, Honda's founder, Soichiro Honda, had an interest in automobiles. He worked as a mechanic at the Art Shokai garage, where he tuned cars and entered them in races. In 1937, with financing from his acquaintance Kato Shichirō, Honda founded Tōkai Seiki (Eastern Sea Precision Machine Company) to make piston rings working out of the Art Shokai garage. After initial failures, Tōkai Seiki won a contract to supply piston rings to Toyota, but lost the contract due to the poor quality of their products. After attending engineering school without graduating, and visiting factories around Japan to better understand Toyota's quality control processes, by 1941 Honda was able to mass-produce piston rings acceptable to Toyota, using an automated process that could employ even unskilled wartime laborers.

Tōkai Seiki was placed under control of the Ministry of Commerce and Industry (called the Ministry of Munitions after 1943) at the start of World War II, and Soichiro Honda was demoted from president to senior managing director after Toyota took a 40% stake in the company. Honda also aided the war effort by assisting other companies in automating the production of military aircraft propellers. The relationships Honda cultivated with personnel at Toyota, Nakajima Aircraft Company and the Imperial Japanese Navy would be instrumental in the postwar period. A US B-29 bomber attack destroyed Tōkai Seiki's Yamashita plant in 1944, and the Itawa plant collapsed in 13 January 1945 Mikawa earthquake. Soichiro Honda sold the salvageable remains of the company to Toyota after the war for ¥450,000, and used the proceeds to found the Honda Technical Research Institute in October 1946.

With a staff of 12 men working in a shack, they built and sold improvised motorized bicycles, using a supply of 500 two-stroke "50 cc" Tohatsu war surplus radio generator engines. When the engines ran out, Honda began building their own copy of the Tohatsu engine, and supplying these to customers to attach to their bicycles. This was the Honda A-Type, nicknamed the Bata Bata for the sound the engine made. In 1949, the Honda Technical Research Institute was liquidated for 1,000,000, or about 5,000 today; these funds were used to incorporate Honda Motor Co., Ltd. At about the same time Honda hired engineer Kihachiro Kawashima, and Takeo Fujisawa who provided indispensable business and marketing expertise to complement Soichiro Honda's technical bent. The close partnership between Soichiro Honda and Fujisawa lasted until they stepped down together in October 1973.

The first complete motorcycle, with both the frame and engine made by Honda, was the 1949 D-Type, the first Honda to go by the name Dream. Honda Motor Company grew in a short time to become the world's largest manufacturer of motorcycles by 1964.

The first production automobile from Honda was the T360 mini pick-up truck, which went on sale in August 1963. Powered by a small 356-cc straight-4 gasoline engine, it was classified under the cheaper Kei car tax bracket. The first production car from Honda was the S500 sports car, which followed the T360 into production in October 1963. Its chain-driven rear wheels pointed to Honda's motorcycle origins.

Over the next few decades, Honda worked to expand its product line and expanded operations and exports to numerous countries around the world. In 1986, Honda introduced the successful Acura brand to the American market in an attempt to gain ground in the luxury vehicle market. The year 1991 saw the introduction of the Honda NSX supercar, the first all-aluminum monocoque vehicle that incorporated a mid-engine V6 with variable-valve timing.

CEO Tadashi Kume was succeeded by Nobuhiko Kawamoto in 1990. Kawamoto was selected over Shoichiro Irimajiri, who oversaw the successful establishment of Honda of America Manufacturing, Inc. in Marysville, Ohio. Irimajiri and Kawamoto shared a friendly rivalry within Honda; owing to health issues, Irimajiri would resign in 1992.

Following the death of Soichiro Honda and the departure of Irimajiri, Honda found itself quickly being outpaced in product development by other Japanese automakers and was caught off-guard by the truck and sport utility vehicle boom of the 1990s, all which took a toll on the profitability of the company. Japanese media reported in 1992 and 1993 that Honda was at serious risk of an unwanted and hostile takeover by Mitsubishi Motors, which at the time was a larger automaker by volume and was flush with profits from its successful Pajero and Diamante models.

Kawamoto acted quickly to change Honda's corporate culture, rushing through market-driven product development that resulted in recreational vehicles such as the first-generation Odyssey and the CR-V, and a refocusing away from some of the numerous sedans and coupes that were popular with the company's engineers but not with the buying public. The most shocking change to Honda came when Kawamoto ended the company's successful participation in Formula One after the 1992 season, citing costs in light of the takeover threat from Mitsubishi as well as the desire to create a more environmentally friendly company image.

The Honda Aircraft Company was established in 1995, as a wholly owned subsidiary; its goal was to produce jet aircraft under Honda's name.

On 23 February 2015, Honda announced that CEO and President Takanobu Ito would step down and be replaced by Takahiro Hachigo by June; additional retirements by senior managers and directors were expected.

Honda is headquartered in Minato, Tokyo, Japan. Their shares trade on the Tokyo Stock Exchange and the New York Stock Exchange, as well as exchanges in Osaka, Nagoya, Sapporo, Kyoto, Fukuoka, London, Paris and Switzerland.

The company has assembly plants around the globe. These plants are located in China, the United States, Pakistan, Canada, England, Japan, Belgium, Brazil, México, New Zealand, Malaysia, Indonesia, India, Philippines, Thailand, Vietnam, Turkey, Taiwan, Perú and Argentina. As of July 2010, 89 percent of Honda and Acura vehicles sold in the United States were built in North American plants, up from 82.2 percent a year earlier. This shields profits from the yen's advance to a 15-year high against the dollar.

Honda's Net Sales and Other Operating Revenue by Geographical Regions in 2007

American Honda Motor Company is based in Torrance, California. Honda Racing Corporation (HRC) is Honda's motorcycle racing division. Honda Canada Inc. is headquartered in Markham, Ontario, it was originally planned to be located in Richmond Hill, Ontario, but delays led them to look elsewhere. Their manufacturing division, Honda of Canada Manufacturing, is based in Alliston, Ontario. Honda has also created joint ventures around the world, such as Honda Siel Cars and Hero Honda Motorcycles in India, Guangzhou Honda and Dongfeng Honda in China, Boon Siew Honda in Malaysia and Honda Atlas in Pakistan.

Following the Japanese earthquake and tsunami in March 2011 Honda announced plans to halve production at its UK plants. The decision was made to put staff at the Swindon plant on a 2-day week until the end of May as the manufacturer struggled to source supplies from Japan. It's thought around 22,500 cars were produced during this period.

For the fiscal year 2018, Honda reported earnings of US$9.534 billion, with an annual revenue of US$138.250 billion, an increase of 6.2% over the previous fiscal cycle. Honda's shares traded at over $32 per share, and its market capitalization was valued at US$50.4 billion in October 2018.

Honda's global lineup consists of the Fit, Civic, Accord, Insight, CR-V, CR-Z, Legend and two versions of the Odyssey, one for North America, and a smaller vehicle sold internationally. An early proponent of developing vehicles to cater to different needs and markets worldwide, Honda's lineup varies by country and may have vehicles exclusive to that region. A few examples are the latest Honda Odyssey minivan and the Ridgeline, Honda's first light-duty uni-body pickup truck. Both were designed and engineered primarily in North America and are produced there. Other example of exclusive models includes the Honda Civic five-door hatchback sold in Europe.

Honda's automotive manufacturing ambitions can be traced back to 1963, with the Honda T360, a kei car truck built for the Japanese market. This was followed by the two-door roadster, the Honda S500 also introduced in 1963. In 1965, Honda built a two-door commercial delivery van, called the Honda L700. Honda's first four-door sedan was not the Accord, but the air-cooled, four-cylinder, gasoline-powered Honda 1300 in 1969. The Civic was a hatchback that gained wide popularity internationally, but it wasn't the first two-door hatchback built. That was the Honda N360, another "Kei car" that was adapted for international sale as the N600. The Civic, which appeared in 1972 and replaced the N600 also had a smaller sibling that replaced the air-cooled N360, called the Honda Life that was water-cooled.

The Honda Life represented Honda's efforts in competing in the "kei" car segment, offering sedan, delivery van and small pick-up platforms on a shared chassis. The Life StepVan had a novel approach that, while not initially a commercial success, appears to be an influence in vehicles with the front passengers sitting behind the engine, a large cargo area with a flat roof and a liftgate installed in back, and utilizing a transversely installed engine with a front-wheel-drive powertrain.

As Honda entered into automobile manufacturing in the late 1960s, where Japanese manufacturers such as Toyota and Nissan had been making cars since before WWII, it appears that Honda instilled a sense of doing things a little differently than its Japanese competitors. Its mainstay products, like the Accord and Civic (with the exception of its USA-market 1993–97 Passport which was part of a vehicle exchange program with Isuzu (part of the Subaru-Isuzu joint venture)), have always employed front-wheel-drive powertrain implementation, which is currently a long-held Honda tradition. Honda also installed new technologies into their products, first as optional equipment, then later standard, like anti lock brakes, speed sensitive power steering, and multi-port fuel injection in the early 1980s. This desire to be the first to try new approaches is evident with the creation of the first Japanese luxury chain Acura, and was also evident with the all aluminum, mid-engined sports car, the Honda NSX, which also introduced variable valve timing technology, Honda calls VTEC.

The Civic is a line of compact cars developed and manufactured by Honda. In North America, the Civic is the second-longest continuously running nameplate from a Japanese manufacturer; only its perennial rival, the Toyota Corolla, introduced in 1968, has been in production longer. The Civic, along with the Accord and Prelude, comprised Honda's vehicles sold in North America until the 1990s, when the model lineup was expanded. Having gone through several generational changes, the Civic has become larger and more upmarket, and it currently slots between the Fit and Accord.

Honda produces Civic hybrid, a hybrid electric vehicle that competes with the Toyota Prius, and also produces the Insight and CR-Z.

In 2008, Honda increased global production to meet demand for small cars and hybrids in the U.S. and emerging markets. The company shuffled U.S. production to keep factories busy and boost car output, while building fewer minivans and sport utility vehicles as light truck sales fell.

Its first entrance into the pickup segment, the light duty Ridgeline, won Truck of the Year from "Motor Trend" magazine in 2006. Also in 2006, the redesigned Civic won Car of the Year from the magazine, giving Honda a rare double win of Motor Trend honors.

It is reported that Honda plans to increase hybrid sales in Japan to more than 20% of its total sales in fiscal year 2011, from 14.8% in previous year.

Five of United States Environmental Protection Agency's top ten most fuel-efficient cars from 1984 to 2010 comes from Honda, more than any other automakers. The five models are: 2000–2006 Honda Insight ( combined), 1986–1987 Honda Civic Coupe HF ( combined), 1994–1995 Honda Civic hatchback VX ( combined), 2006– Honda Civic Hybrid ( combined), and 2010– Honda Insight ( combined). The ACEEE has also rated the Civic GX as the greenest car in America for seven consecutive years.

Honda is the largest motorcycle manufacturer in Japan and has been since it started production in 1955.
At its peak in 1982, Honda manufactured almost three million motorcycles annually. By 2006 this figure had reduced to around 550,000 but was still higher than its three domestic competitors.

In 2017, India became the largest motorcycle market of Honda. In India, Honda is leading in the scooters segment, with 59 percent market share.

During the 1960s, when it was a small manufacturer, Honda broke out of the Japanese motorcycle market and began exporting to the U.S. Working with the advertising agency Grey Advertising, Honda created an innovative marketing campaign, using the slogan "You meet the nicest people on a Honda." In contrast to the prevailing negative stereotypes of motorcyclists in America as tough, antisocial rebels, this campaign suggested that Honda motorcycles were made for the everyman. The campaign was hugely successful; the ads ran for three years, and by the end of 1963 alone, Honda had sold 90,000 motorcycles.

Taking Honda's story as an archetype of the smaller manufacturer entering a new market already occupied by highly dominant competitors, the story of their market entry, and their subsequent huge success in the U.S. and around the world, has been the subject of some academic controversy. Competing explanations have been advanced to explain Honda's strategy and the reasons for their success.

The first of these explanations was put forward when, in 1975, Boston Consulting Group (BCG) was commissioned by the UK government to write a report explaining why and how the British motorcycle industry had been out-competed by its Japanese competitors. The report concluded that the Japanese firms, including Honda, had sought a very high scale of production (they had made a large number of motorbikes) in order to benefit from economies of scale and learning curve effects. It blamed the decline of the British motorcycle industry on the failure of British managers to invest enough in their businesses to profit from economies of scale and scope.
The second explanation was offered in 1984 by Richard Pascale, who had interviewed the Honda executives responsible for the firm's entry into the U.S. market. As opposed to the tightly focused strategy of low cost and high scale that BCG accredited to Honda, Pascale found that their entry into the U.S. market was a story of "miscalculation, serendipity, and organizational learning" – in other words, Honda's success was due to the adaptability and hard work of its staff, rather than any long term strategy. For example, Honda's initial plan on entering the US was to compete in large motorcycles, around 300 cc. Honda's motorcycles in this class suffered performance and reliability problems when ridden the relatively long distances of the US highways. When the team found that the scooters they were using to get themselves around their U.S. base of San Francisco attracted positive interest from consumers that they fell back on selling the Super Cub instead.

The most recent school of thought on Honda's strategy was put forward by Gary Hamel and C. K. Prahalad in 1989. Creating the concept of core competencies with Honda as an example, they argued that Honda's success was due to its focus on leadership in the technology of internal combustion engines. For example, the high power-to-weight ratio engines Honda produced for its racing bikes provided technology and expertise which was transferable into mopeds. Honda's entry into the U.S. motorcycle market during the 1960s is used as a case study for teaching introductory strategy at business schools worldwide.

Production started in 1953 with H-type engine (prior to motorcycle).

Honda power equipment reached record sales in 2007 with 6.4 million units. By 2010 this figure had decreased to 4,7 million units. Cumulative production of power products has exceeded 85 million units (as of September 2008).

Honda power equipment includes:
Honda engines powered the entire 33-car starting field of the 2010 Indianapolis 500 and for the fifth consecutive race, there were no engine-related retirements during the running of the Memorial Day Classic.

In the 1980s Honda developed the GY6 engine for use in motor scooters. Although no longer manufactured by Honda it is still commonly used in many Chinese, Korean and Taiwanese light vehicles.

Honda, despite being known as an engine company, has never built a V8 for passenger vehicles. In the late 1990s, the company resisted considerable pressure from its American dealers for a V8 engine (which would have seen use in top-of-the-line Honda SUVs and Acuras), with American Honda reportedly sending one dealer a shipment of V8 beverages to silence them. Honda considered starting V8 production in the mid-2000s for larger Acura sedans, a new version of the high end NSX sports car (which previously used DOHC V6 engines with VTEC to achieve its high power output) and possible future ventures into the American full-size truck and SUV segment for both the Acura and Honda brands, but this was cancelled in late 2008, with Honda citing environmental and worldwide economic conditions as reasons for the termination of this project.

ASIMO is the part of Honda's Research & Development robotics program. It is the eleventh in a line of successive builds starting in 1986 with Honda E0 moving through the ensuing Honda E series and the Honda P series. Weighing 54 kilograms and standing 130 centimeters tall, ASIMO resembles a small astronaut wearing a backpack, and can walk on two feet in a manner resembling human locomotion, at up to . ASIMO is the world's only humanoid robot able to ascend and descend stairs independently. However, human motions such as climbing stairs are difficult to mimic with a machine, which ASIMO has demonstrated by taking two plunges off a staircase.

Honda's robot ASIMO (see below) as an R&D project brings together expertise to create a robot that walks, dances and navigates steps.
2010 marks the year Honda has developed a machine capable of reading a user's brainwaves to move ASIMO. The system uses a helmet covered with electroencephalography and near-infrared spectroscopy sensors that monitor electrical brainwaves and cerebral blood flow—signals that alter slightly during the human thought process. The user thinks of one of a limited number of gestures it wants from the robot, which has been fitted with a Brain Machine Interface.

Honda has also pioneered new technology in its HA-420 HondaJet, manufactured by its subsidiary Honda Aircraft Company, which allows new levels of reduced drag, increased aerodynamics and fuel efficiency thus reducing operating costs.

Honda has also built a downhill racing bicycle known as the Honda RN-01. It is not available for sale to the public. The bike has a gearbox, which replaces the standard derailleur found on most bikes.

Honda has hired several people to pilot the bike, among them Greg Minnaar. The team is known as Team G Cross Honda.

Honda also builds all-terrain vehicles (ATV).
420
450r
400ex
300ex
250r

Honda's solar cell subsidiary company Honda Soltec (Headquarters: Kikuchi-gun, Kumamoto; President and CEO: Akio Kazusa) started sales throughout Japan of thin-film solar cells for public and industrial use on 24 October 2008, after selling solar cells for residential use since October 2007. Honda announced in the end of October 2013 that Honda Soltec would cease the business operation except for support for existing customers in Spring 2014 and the subsidiary would be dissolved.

Honda has been active in motorsports, like Motorcycle Grand Prix, Superbike racing and others.

Honda entered Formula One as a constructor for the first time in the 1964 season at the German Grand Prix with Ronnie Bucknum at the wheel. 1965 saw the addition of Richie Ginther to the team, who scored Honda's first point at the Belgian Grand Prix, and Honda's first win at the Mexican Grand Prix. 1967 saw their next win at the Italian Grand Prix with John Surtees as their driver. In 1968, Jo Schlesser was killed in a Honda RA302 at the French Grand Prix. This racing tragedy, coupled with their commercial difficulties selling automobiles in the United States, prompted Honda to withdraw from all international motorsport that year.

After a learning year in 1965, Honda-powered Brabhams dominated the 1966 French Formula Two championship in the hands of Jack Brabham and Denny Hulme. As there was no European Championship that season, this was the top F2 championship that year. In the early 1980s Honda returned to F2, supplying engines to Ron Tauranac's Ralt team. Tauranac had designed the Brabham cars for their earlier involvement. They were again extremely successful. In a related exercise, John Judd's Engine Developments company produced a turbo "Brabham-Honda" engine for use in IndyCar racing. It won only one race, in 1988 for Bobby Rahal at Pocono.

Honda returned to Formula One in 1983, initially with another Formula Two partner, the Spirit team, before switching abruptly to Williams in 1984. In the late 1980s and early 1990s, Honda powered cars won six consecutive Formula One Constructors Championships. WilliamsF1 won the crown in 1986 and 1987. Honda switched allegiance again in 1988. New partners McLaren won the title in 1988, 1989, 1990 and 1991. Honda withdrew from Formula One at the end of 1992, although the related Mugen-Honda company maintained a presence up to the end of 1999, winning four races with Ligier and Jordan Grand Prix.

Honda debuted in the CART IndyCar World Series as a works supplier in 1994. The engines were far from competitive at first, but after development, the company powered six consecutive drivers championships. In 2003, Honda transferred its effort to the rival IRL IndyCar Series with Ilmor as joint development until 2006. In 2004, Honda-powered cars overwhelmingly dominated the IndyCar Series, winning 14 of 16 IndyCar races, including the Indianapolis 500, and claimed the IndyCar Series Manufacturers' Championship, Drivers' Championship and Rookie of the Year titles. From 2006 to 2011, Honda was the lone engine supplier for the IndyCar Series, including the Indianapolis 500. In the 2006 Indianapolis 500, for the first time in Indianapolis 500 history, the race was run without a single engine problem. Since 2012, HPD has constructed turbocharged V-6 engines for its IndyCar effort.

During 1998, Honda considered returning to Formula One with their own team. The project was aborted after the death of its technical director, Harvey Postlethwaite. Honda instead came back as an official engine supplier to British American Racing (BAR) and Jordan Grand Prix. Honda bought a stake in the BAR team in 2004 before buying the team outright at the end of 2005, becoming a constructor for the first time since the 1960s. Honda won the 2006 Hungarian Grand Prix with driver Jenson Button.

It was announced on 5 December 2008, that Honda would be exiting Formula One with immediate effect due to the 2008 global economic crisis. The team was sold to former team principal Ross Brawn, renamed Brawn GP and subsequently Mercedes.

Honda became an official works team in the British Touring Car Championship in 2010.

Honda made an official announcement on 16 May 2013 that it planned to re-enter into Formula One in 2015 as an engine supplier to McLaren. On 15 September 2017, after a winless campaign spanning three seasons without yielding a single podium and achieving a best finish of fifth place, McLaren announced their divorce from Honda, with the latter going on to sign a multi-year deal to supply Toro Rosso, the junior team of Red Bull Racing.

Honda Racing Corporation (HRC) was formed in 1982. The company combines participation in motorcycle races throughout the world with the development of high potential racing machines. Its racing activities are an important source for the creation of leading edge technologies used in the development of Honda motorcycles. HRC also contributes to the advancement of motorcycle sports through a range of activities that include sales of production racing motorcycles, support for satellite teams, and rider education programs.

Soichiro Honda, being a race driver himself, could not stay out of international motorsport. In 1959, Honda entered five motorcycles into the Isle of Man TT race, the most prestigious motorcycle race in the world. While always having powerful engines, it took until 1961 for Honda to tune their chassis well enough to allow Mike Hailwood to claim their first Grand Prix victories in the 125 and 250 cc classes. Hailwood would later pick up their first Senior TT wins in 1966 and 1967. Honda's race bikes were known for their "sleek & stylish design" and exotic engine configurations, such as the 5-cylinder, 22,000 rpm, 125 cc bike and their 6-cylinder 250 cc and 297 cc bikes.

In 1979, Honda returned to Grand Prix motorcycle racing with the monocoque-framed, four-stroke NR500. The FIM rules limited engines to four cylinders, so the NR500 had non-circular, 'race-track', cylinders, each with 8 valves and two connecting rods, in order to provide sufficient valve area to compete with the dominant two-stroke racers. Unfortunately, it seemed Honda tried to accomplish too much at one time and the experiment failed. For the 1982 season, Honda debuted their first two-stroke race bike, the NS500 and in , Honda won their first 500 cc Grand Prix World Championship with Freddie Spencer. Since then, Honda has become a dominant marque in motorcycle Grand Prix racing, winning a plethora of top level titles with riders such as Mick Doohan and Valentino Rossi. Honda also head the number of wins at the Isle of Man TT having notched up 227 victories in the solo classes and Sidecar TT, including Ian Hutchinson's clean sweep at the 2010 races.

The outright lap record on the Snaefell Mountain Course was held by Honda, set at the 2015 TT by John McGuinness at an average speed of on a Honda CBR1000RR, bettered the next year by Michael Dunlop on a BMW S1000RR at .

In the Motocross World Championship, Honda has claimed six world championships. In the World Enduro Championship, Honda has captured eight titles, most recently with Stefan Merriman in 2003 and with Mika Ahola from 2007 to 2010. In motorcycle trials, Honda has claimed three world championships with Belgian rider Eddy Lejeune.

The Honda Civic GX was for a long time the only purpose-built natural gas vehicle (NGV) commercially available in some parts of the U.S. The Honda Civic GX first appeared in 1998 as a factory-modified Civic LX that had been designed to run exclusively on compressed natural gas. The car looks and drives just like a contemporary Honda Civic LX, but does not run on gasoline. In 2001, the Civic GX was rated the cleanest-burning internal combustion engine in the world by the U.S. Environmental Protection Agency (EPA).

First leased to the City of Los Angeles, in 2005, Honda started offering the GX directly to the public through factory trained dealers certified to service the GX. Before that, only fleets were eligible to purchase a new Civic GX. In 2006, the Civic GX was released in New York, making it the second state where the consumer is able to buy the car.

In June 2015, Honda announced its decision to phase out the commercialization of natural-gas powered vehicles to focus on the development of a new generation of electrified vehicles such as hybrids, plug-in electric cars and hydrogen-powered fuel cell vehicles. Since 2008, Honda has sold about 16,000 natural-gas vehicles, mainly to taxi and commercial fleets.

Honda's Brazilian subsidiary launched flexible-fuel versions for the Honda Civic and Honda Fit in late 2006. As other Brazilian flex-fuel vehicles, these models run on any blend of hydrous ethanol (E100) and E20-E25 gasoline. Initially, and in order to test the market preferences, the carmaker decided to produce a limited share of the vehicles with flex-fuel engines, 33 percent of the Civic production and 28 percent of the Fit models. Also, the sale price for the flex-fuel version was higher than the respective gasoline versions, around US$1,000 premium for the Civic, and US$650 for the Fit, despite the fact that all other flex-fuel vehicles sold in Brazil had the same tag price as their gasoline versions. In July 2009, Honda launched in the Brazilian market its third flexible-fuel car, the Honda City.

During the last two months of 2006, both flex-fuel models sold 2,427 cars against 8,546 gasoline-powered automobiles, jumping to 41,990 flex-fuel cars in 2007, and reaching 93,361 in 2008. Due to the success of the flex versions, by early 2009 a hundred percent of Honda's automobile production for the Brazilian market is now flexible-fuel, and only a small percentage of gasoline version is produced in Brazil for exports.

In March 2009, Honda launched in the Brazilian market the first flex-fuel motorcycle in the world. Produced by its Brazilian subsidiary Moto Honda da Amazônia, the CG 150 Titan Mix is sold for around US$2,700.

In late 1999, Honda launched the first commercial hybrid electric car sold in the U.S. market, the Honda Insight, just one month before the introduction of the Toyota Prius, and initially sold for US$20,000. The first-generation Insight was produced from 2000 to 2006 and had a fuel economy of for the EPA's highway rating, the most fuel-efficient mass-produced car at the time. Total global sales for the Insight amounted to only around 18,000 vehicles. Cumulative global sales reached 100,000 hybrids in 2005 and 200,000 in 2007.

Honda introduced the second-generation Insight in Japan in February 2009, and released it in other markets through 2009 and in the U.S. market in April 2009. At $19,800 as a five-door hatchback it will be the least expensive hybrid available in the U.S.
Since 2002, Honda has also been selling the Honda Civic Hybrid (2003 model) in the U.S. market. It was followed by the Honda Accord Hybrid, offered in model years 2005 through 2007. Sales of the Honda CR-Z began in Japan in February 2010, becoming Honda's third hybrid electric car in the market. , Honda was producing around 200,000 hybrids a year in Japan.

Sales of the Fit Hybrid began in Japan in October 2010, at the time, the lowest price for a gasoline-hybrid electric vehicle sold in the country. The European version, called Honda Jazz Hybrid, was released in early 2011. During 2011 Honda launched three hybrid models available only in Japan, the Fit Shuttle Hybrid, Freed Hybrid and Freed Spike Hybrid.

Honda's cumulative global hybrid sales passed the 1 million unit milestone at the end of September 2012, 12 years and 11 months after sales of the first generation Insight began in Japan November 1999. A total of 187,851 hybrids were sold worldwide in 2013, and 158,696 hybrids during the first six months of 2014. , Honda has sold more than 1.35 million hybrids worldwide.

In Takanezawa, Japan, on 16 June 2008, Honda Motors produced the first assembly-line FCX Clarity, a hybrid hydrogen fuel cell vehicle. More efficient than a gas-electric hybrid vehicle, the FCX Clarity combines hydrogen and oxygen from ordinary air to generate electricity for an electric motor. In July 2014 Honda announced the end of production of the Honda FCX Clarity for the 2015 model.

The vehicle itself does not emit any pollutants and its only by products are heat and water. The FCX Clarity also has an advantage over gas-electric hybrids in that it does not use an internal combustion engine to propel itself. Like a gas-electric hybrid, it uses a lithium ion battery to assist the fuel cell during acceleration and capture energy through regenerative braking, thus improving fuel efficiency. The lack of hydrogen filling stations throughout developed countries will keep production volumes low. Honda will release the vehicle in groups of 150. California is the only U.S. market with infrastructure for fueling such a vehicle, though the number of stations is still limited. Building more stations is expensive, as the California Air Resources Board (CARB) granted $6.8 million for four H2 fueling stations, costing $1.7 million USD each.

Honda views hydrogen fuel cell vehicles as the long term replacement of piston cars, not battery cars.

The all-electric Honda EV Plus was introduced in 1997 as a result of CARB's zero-emissions vehicle mandate and was available only for leasing in California. The EV plus was the first battery electric vehicle from a major automaker with non-lead–acid batteries The EV Plus had an all-electric range of . Around 276 units were sold in the U.S. and production ended in 1999.

The all-electric Honda Fit EV was introduced in 2012 and has a range of . The all-electric car was launched in the U.S. to retail customers in July 2012 with initial availability limited to California and Oregon. Production is limited to only 1,100 units over the first three years. A total of 1,007 units have been leased in the U.S. through September 2014. The Fit EV was released in Japan through leasing to local government and corporate customers in August 2012. Availability in the Japanese market is limited to 200 units during its first two years. In July 2014 Honda announced the end of production of the Fit EV for the 2015 model.

The Honda Accord Plug-in Hybrid was introduced in 2013 and has an all-electric range of Sales began in the U.S. in January 2013 and the plug-in hybrid is available only in California and New York. A total of 835 units have been sold in the U.S. through September 2014. The Accord PHEV was introduced in Japan in June 2013 and is available only for leasing, primarily to corporations and government agencies.

Starting in 1978, Honda in Japan decided to diversify its sales distribution channels and created Honda Verno, which sold established products with a higher content of standard equipment and a more sporting nature. The establishment of "Honda Verno" coincided with its new sports compact, the Honda Prelude. Later, the Honda Vigor, Honda Ballade, and Honda Quint were added to "Honda Verno" stores. This approach was implemented due to efforts in place by rival Japanese automakers Toyota and Nissan.

As sales progressed, Honda created two more sales channels, called Honda Clio in 1984, and Honda Primo in 1985. The "Honda Clio" chain sold products that were traditionally associated with Honda dealerships before 1978, like the Honda Accord, and "Honda Primo" sold the Honda Civic, kei cars such as the Honda Today, superminis like the Honda Capa, along with other Honda products, such as farm equipment, lawn mowers, portable generators, and marine equipment, plus motorcycles and scooters like the Honda Super Cub. A styling tradition was established when "Honda Primo" and "Clio" began operations in that all "Verno" products had the rear license plate installed in the rear bumper, while "Primo" and "Clio" products had the rear license plate installed on the trunk lid or rear door for minivans.

As time progressed and sales began to diminish partly due to the collapse of the Japanese "bubble economy", "supermini" and "kei" vehicles that were specific to "Honda Primo" were "badge engineered" and sold at the other two sales channels, thereby providing smaller vehicles that sold better at both "Honda Verno" and "Honda Clio" locations. As of March 2006, the three sales chains were discontinued, with the establishment of "Honda Cars" dealerships. While the network was disbanded, some Japanese Honda dealerships still use the network names, offering all Japanese market Honda cars at all locations.

Honda sells genuine accessories through a separate retail chain called "" for both their motorcycle, scooter and automobile products. In cooperation with corporate group partner Pioneer, Honda sells an aftermarket line of audio and in-car navigation equipment that can be installed in any vehicle under the brand name , which is available at Honda Access locations as well as Japanese auto parts retailers, such as Autobacs. Buyers of used vehicles are directed to a specific Honda retail chain that sells only used vehicles called "."

In the spring of 2012, Honda in Japan introduced "Honda Cars Small Store" (Japanese) which is devoted to compact cars like the Honda Fit, and "kei" vehicles like the Honda N-One and Honda S660 roadster.
In 2003, Honda released its "Cog" advertisement in the UK and on the Internet. To make the ad, the engineers at Honda constructed a Rube Goldberg Machine made entirely out of car parts from a Honda Accord Touring. To the chagrin of the engineers at Honda, all the parts were taken from two of only six hand-assembled pre-production models of the Accord. The advertisement depicted a single cog which sets off a chain of events that ends with the Honda Accord moving and Garrison Keillor speaking the tagline, "Isn't it nice when things just... work?" It took 606 takes to get it perfect.

In 2004, they produced the "Grrr" advert, usually immediately followed by a shortened version of the 2005 "Impossible Dream" advert. In December 2005, Honda released "The Impossible Dream" a two-minute panoramic advertisement filmed in New Zealand, Japan and Argentina which illustrates the founder's dream to build performance vehicles. While singing the song "Impossible Dream", a man reaches for his racing helmet, leaves his trailer on a minibike, then rides a succession of vintage Honda vehicles: a motorcycle, then a car, then a powerboat, then goes over a waterfall only to reappear piloting a hot air balloon, with Garrison Keillor saying "I couldn't have put it better myself" as the song ends. The song is from the 1960s musical "Man Of La Mancha", sung by Andy Williams.

In 2006, Honda released its "Choir" advertisement, for the UK and the internet. This had a 60-person choir who sang the car noises as film of the Honda Civic are shown.

In the mid to late 2000s in the United States, during model close-out sales for the current year before the start of the new model year, Honda's advertising has had an animated character known simply as Mr. Opportunity, voiced by Rob Paulsen. The casual looking man talked about various deals offered by Honda and ended with the phrase "I'm Mr. Opportunity, and I'm knockin'", followed by him "knocking" on the television screen or "thumping" the speaker at the end of radio ads. In addition, commercials for Honda's international hatchback, the Jazz, are parodies of well-known pop culture images such as Tetris and Thomas The Tank Engine.

In late 2006, Honda released an ad with ASIMO exploring a museum, looking at the exhibits with almost childlike wonderment (spreading out its arms in the aerospace exhibit, waving hello to an astronaut suit that resembles him, etc.), while Garrison Keillor ruminates on progress. It concludes with the tagline: "More forwards please". Honda also sponsored ITV's coverage of Formula One in the UK for 2007. However they had announced that they would not continue in 2008 due to the sponsorship price requested by ITV being too high.

In May 2007, focuses on their strengths in racing and the use of the Red H badge – a symbol of what is termed as "Hondamentalism". The campaign highlights the lengths that Honda engineers go to in order to get the most out of an engine, whether it is for bikes, cars, powerboats – even lawnmowers. Honda released its Hondamentalism campaign. In the TV spot, Garrison Keillor says, "An engineer once said to build something great is like swimming in honey", while Honda engineers in white suits walk and run towards a great light, battling strong winds and flying debris, holding on to anything that will keep them from being blown away. Finally one of the engineers walks towards a red light, his hand outstretched. A web address is shown for the Hondamentalism website. The digital campaign aims to show how visitors to the site share many of the Hondamentalist characteristics.

At the beginning of 2008, Honda released – the "Problem Playground". The advert outlines Honda's environmental responsibility, demonstrating a hybrid engine, more efficient solar panels and the FCX Clarity, a hydrogen-powered car. The 90-second advert has large-scale puzzles, involving Rubik's Cubes, large shapes and a 3-dimensional puzzle. On 29 May 2008, Honda, in partnership with Channel 4, broadcast a live advertisement. It showed skydivers jumping from an aeroplane over Spain and forming the letters H, O, N, D and A in mid-air. This live advertisement is generally agreed to be the first of its kind on British television. The advert lasted three minutes.

In 2009, American Honda released the "Dream the Impossible" documentary series, a collection of 5- to 8-minute web vignettes that focus on the core philosophies of Honda. Current short films include "Failure: The Secret to Success", "Kick Out the Ladder" and "Mobility 2088". They have Honda employees as well as Danica Patrick, Christopher Guest, Ben Bova, Chee Pearlman, Joe Johnston and Orson Scott Card. The film series plays at dreams.honda.com. In the UK, national television ads feature voice-overs from American radio host Garrison Keillor, while in the US the voice of Honda commercials is actor and director Fred Savage.

The late F1 driver Ayrton Senna stated that Honda probably played the most significant role in his three world championships. He had immense respect for founder, Soichiro Honda, and had a good relationship with Nobuhiko Kawamoto, the chairman of Honda at that time. Senna once called Honda "the greatest company in the world".

As part of its marketing campaign, Honda is an official partner and sponsor of the National Hockey League, the Anaheim Ducks of the NHL, and the arena named after it: Honda Center. Honda also sponsors The Honda Classic golf tournament and is a sponsor of Major League Soccer. The "Honda Player of the Year" award is presented in United States soccer. The "Honda Sports Award" is given to the best female athlete in each of twelve college sports in the United States. One of the twelve Honda Sports Award winners is chosen to receive the Honda-Broderick Cup, as "Collegiate Woman Athlete of the Year."

Honda sponsored La Liga club Valencia CF starting from 2014–15 season.

Honda has been a presenting sponsor of the Los Angeles Marathon since 2010 in a three-year sponsorship deal, with winners of the LA Marathon receiving a free Honda Accord. Since 1989, the Honda Campus All-Star Challenge has been a quizbowl tournament for Historically black colleges and universities.

2010 Chinese labor strike happened in Guangqi Honda, Dongfeng Honda.





</doc>
<doc id="13730" url="https://en.wikipedia.org/wiki?curid=13730" title="Handball">
Handball

Handball (also known as team handball, European handball or Olympic handball) is a team sport in which two teams of seven players each (six outcourt players and a goalkeeper) pass a ball using their hands with the aim of throwing it into the goal of the other team. A standard match consists of two periods of 30 minutes, and the team that scores more goals wins.

Modern handball is played on a court of , with a goal in the middle of each end. The goals are surrounded by a zone where only the defending goalkeeper is allowed; goals must be scored by throwing the ball from outside the zone or while "diving" into it. The sport is usually played indoors, but outdoor variants exist in the forms of field handball and Czech handball (which were more common in the past) and beach handball. The game is fast and high-scoring: professional teams now typically score between 20 and 35 goals each, though lower scores were not uncommon until a few decades ago. Body contact is permitted, the defenders trying to stop the attackers from approaching the goal. No protective equipment is mandated, but players may wear soft protective bands, pads and mouth guards.

The game was codified at the end of the 19th century in Denmark. The modern set of rules was published in 1917 in Germany, and had several revisions since. The first international games were played under these rules for men in 1925 and for women in 1930. Men's handball was first played at the 1936 Summer Olympics in Berlin as outdoors, and the next time at the 1972 Summer Olympics in Munich as indoors, and has been an Olympic sport since. Women's team handball was added at the 1976 Summer Olympics.

The International Handball Federation was formed in 1946 and, , has 197 member federations. The sport is most popular in the countries of continental Europe, which have won all medals but one in the men's world championships since 1938. In the women's world championships, only two non-European countries have won the title: South Korea and Brazil. The game also enjoys popularity in East Asia, North Africa and parts of South America.

There is evidence of ancient Roman women playing a version of handball called "expulsim ludere". There are records of handball-like games in medieval France, and among the Inuit in Greenland, in the Middle Ages. By the 19th century, there existed similar games of "håndbold" from Denmark, "házená" in the Czech Republic, "handbol" in Ukraine, and "torball" in Germany.

The team handball game of today was codified at the end of the 19th century in northern Europe: primarily in Denmark, Germany, Norway and Sweden. The first written set of team handball rules was published in 1906 by the Danish gym teacher, lieutenant and Olympic medalist Holger Nielsen from Ordrup grammar school, north of Copenhagen. The modern set of rules was published on 29 October 1917 by Max Heiser, Karl Schelenz, and Erich Konigh from Germany. After 1919 these rules were improved by Karl Schelenz. The first international games were played under these rules, between Germany and Belgium by men in 1925 and between Germany and Austria by women in 1930.

In 1926, the Congress of the International Amateur Athletics Federation nominated a committee to draw up international rules for field handball. The International Amateur Handball Federation was formed in 1928 and later the International Handball Federation was formed in 1946.

Men's field handball was played at the 1936 Summer Olympics in Berlin. During the next several decades, indoor handball flourished and evolved in the Scandinavian countries. The sport re-emerged onto the world stage as team handball for the 1972 Summer Olympics in Munich. Women's team handball was added at the 1976 Summer Olympics in Montreal. Due to its popularity in the region, the Eastern European countries that refined the event became the dominant force in the sport when it was reintroduced.

The International Handball Federation organised the men's world championship in 1938 and every four (sometimes three) years from World War II to 1995. Since the 1995 world championship in Iceland, the competition has been held every two years. The women's world championship has been held since 1957. The IHF also organizes women's and men's junior world championships. By July 2009, the IHF listed 166 member federations - approximately 795,000 teams and 19 million players.

The rules are laid out in the IHF's set of rules.

Two teams of seven players (six field players plus one goalkeeper) take the field and attempt to score points by putting the game ball into the opposing team's goal. In handling the ball, players are subject to the following restrictions:


Notable scoring opportunities can occur when attacking players jump into the goal area. For example, an attacking player may catch a pass while launching inside the goal area, and then shoot or pass before touching the floor. "Doubling" occurs when a diving attacking player passes to another diving teammate.

Handball is played on a court , with a goal in the centre of each end. The goals are surrounded by a near-semicircular area, called the zone or the crease, defined by a line six meters from the goal. A dashed near-semicircular line nine metres from the goal marks the free-throw line. Each line on the court is part of the area it encompasses. This implies that the middle line belongs to both halves at the same time.

The goals are two meters high and three meters wide. They must be securely bolted either to the floor or the wall behind.

The goal posts and the crossbar must be made out of the same material (e.g., wood or aluminium) and feature a quadratic cross section with sides of . The three sides of the beams visible from the playing field must be painted alternatingly in two contrasting colors which both have to contrast against the background. The colors on both goals must be the same.

Each goal must feature a net. This must be fastened in such a way that a ball thrown into the goal does not leave or pass the goal under normal circumstances. If necessary, a second net may be clasped to the back of the net on the inside.

The goals are surrounded by the crease. This area is delineated by two quarter circles with a radius of six metres around the far corners of each goal post and a connecting line parallel to the goal line. Only the defending goalkeeper is allowed inside this zone. However, the court players may catch and touch the ball in the air within it as long as the player starts his jump outside the zone and releases the ball before he lands (landing inside the perimeter is allowed in this case as long as the ball has been released).

If a player without the ball contacts the ground inside the goal perimeter, or the line surrounding the perimeter, he must take the most direct path out of it. However, should a player cross the zone in an attempt to gain an advantage (e.g., better position) their team cedes the ball. Similarly, violation of the zone by a defending player is penalized only if they do so in order to gain an advantage in defending.

Outside of one long edge of the playing field to both sides of the middle line are the substitution areas for each team. The areas usually contain the benches as seating opportunities. Team officials, substitutes, and suspended players must wait within this area. The area always lies to the same side as the team's own goal. During half-time, substitution areas are swapped. Any player entering or leaving the play must cross the substitution line which is part of the side line and extends from the middle line to the team's side.

 A standard match has two 30-minute halves with a 10- to 15-minute halftime break. At half-time, teams switch sides of the court as well as benches. For youths, the length of the halves is reduced—25 minutes at ages 12 to 15, and 20 minutes at ages 8 to 11; though national federations of some countries may differ in their implementation from the official guidelines.

If a decision must be reached in a particular match (e.g., in a tournament) and it ends in a draw after regular time, there are at maximum two overtimes, each consisting of two straight 5-minute periods with a one-minute break in between. Should these not decide the game either, the winning team is determined in a penalty shootout (best-of-five rounds; if still tied, extra rounds are added until one team wins).

The referees may call "timeout" according to their sole discretion; typical reasons are injuries, suspensions, or court cleaning. Penalty throws should trigger a timeout only for lengthy delays, such as a change of the goalkeeper.

Since 2012, teams can call 3 "team timeouts" per game (up to two per half), which last one minute each. This right may only be invoked by the team in possession of the ball. Team representatives must show a green card marked with a black "T" on the timekeeper's desk. The timekeeper then immediately interrupts the game by sounding an acoustic signal and stops the time. Before 2012, teams were alllwed only one timeout per half. For the purpose of calling timeouts, overtime and shootouts are extensions of the second half.

A handball match is adjudicated by two equal referees. Some national bodies allow games with only a single referee in special cases like illness on short notice. Should the referees disagree on any occasion, a decision is made on mutual agreement during a short timeout; or, in case of punishments, the more severe of the two comes into effect. The referees are obliged to make their decisions "on the basis of their observations of facts". Their judgements are final and can be appealed against only if not in compliance with the rules.

The referees position themselves in such a way that the team players are confined between them. They stand diagonally aligned so that each can observe one side line. Depending on their positions, one is called "field referee" and the other "goal referee". These positions automatically switch on ball turnover. They physically exchange their positions approximately every 10 minutes (long exchange), and change sides every five minutes (short exchange).

The IHF defines 18 hand signals for quick visual communication with players and officials. The signal for warning is accompanied by a yellow card. A disqualification for the game is indicated by a red card, followed by a blue card if the disqualification will be accompanied by a report.ref>Official rules 16:8</ref> The referees also use whistle blows to indicate infractions or to restart the play.

The referees are supported by a "scorekeeper" and a "timekeeper" who attend to formal things such as keeping track of goals and suspensions, or starting and stopping the clock, respectively. They also keep an eye on the benches and notify the referees on substitution errors. Their desk is located between the two substitution areas.

Each team consists of seven players on court and seven substitute players on the bench. One player on the court must be the designated goalkeeper, differing in his clothing from the rest of the field players. Substitution of players can be done in any number and at any time during game play. An exchange takes place over the substitution line. A prior notification of the referees is not necessary.

Some national bodies, such as the Deutsche Handball Bund (DHB, "German Handball Federation"), allow substitution in junior teams only when in ball possession or during timeouts. This restriction is intended to prevent early specialization of players to offence or defence.

Field players are allowed to touch the ball with any part of their bodies above and including the knee. As in several other team sports, a distinction is made between catching and dribbling. A player who is in possession of the ball may stand stationary for only three seconds, and may take only three steps. They must then either shoot, pass, or dribble the ball. Taking more than three steps at any time is considered travelling, and results in a turnover. A player may dribble as many times as they want (though, since passing is faster, it is the preferred method of attack), as long as during each dribble the hand contacts only the top of the ball. Therefore, carrying is completely prohibited, and results in a turnover. After the dribble is picked up, the player has the right to another three seconds or three steps. The ball must then be passed or shot, as further holding or dribbling will result in a "double dribble" turnover and a free throw for the other team. Other offensive infractions that result in a turnover include charging and setting an illegal screen. Carrying the ball into the six-meter zone results either in ball possession by the goalkeeper (by attacker) or turnover (by defender).

Only the goalkeepers are allowed to move freely within the goal perimeter, although they may not cross the goal perimeter line while carrying or dribbling the ball. Within the zone, they are allowed to touch the ball with all parts of their bodies, including their feet, with a defensive aim (for other actions, they are subject to the same restrictions as the field players). The goalkeepers may participate in the normal play of their teammates. They may be substituted by a regular field player if their team elects to use this scheme in order to outnumber the defending players. Earlier, this field player become the designated goalkeeper on the court; and had to wear some vest or bib to be identified as such. That shirt had to be equal in colour and form to the goalkeeper's shirt, to avoid confusion. A rule change meant to make the game more offensive now allows any player to substitute with the goalkeeper. The new rule resembles the one used in ice hockey. This rule was first used in the women's world championship in December 2015 and has since been used by the men's European championship in January 2016 and by both genders in the Olympic tournament in Rio in 2016.

If either goalkeeper deflects the ball over the outer goal line, their team stays in possession of the ball, in contrast to other sports like football. The goalkeeper resumes the play with a throw from within the zone ("goalkeeper throw"). Passing to one's own goalkeeper results in a turnover. In a penalty shot, throwing the ball against the head of a goalkeeper who is not moving risks a direct disqualification ("red card").

Outside of own D-zone, the goalkeeper is treated as a current field player, and has to follow field players' rules; holding or tackling an opponent player outside the area risks a direct disqualification. The goalkeeper may not return to the area with the ball.

Each team is allowed to have a maximum of four team officials seated on the benches. An official is anybody who is neither player nor substitute. One official must be the designated representative who is usually the team manager. Since 2012, representatives can call up to 3 team timeouts (up to twice per half), and may address the scorekeeper, timekeeper, and referees (before that, it was once per half); overtime and shootouts are considered extensions of the second half. Other officials typically include physicians or managers. Neither official is allowed to enter the playing court without the permission of the referees.

 The ball is spherical and must be made either of leather or a synthetic material. It is not allowed to have a shiny or slippery surface. As the ball is intended to be operated by a single hand, its official sizes vary depending on age and gender of the participating teams.
The referees may award a special throw to a team. This usually happens after certain events such as scored goals, off-court balls, turnovers and timeouts. All of these special throws require the thrower to obtain a certain position, and pose restrictions on the positions of all other players. Sometimes the execution must wait for a whistle blow by the referee.





Penalties are given to players, in progressive format, for fouls that require more punishment than just a free-throw. Actions directed mainly at the opponent and not the ball (such as reaching around, holding, pushing, tripping, and jumping into opponent) as well as contact from the side, from behind a player or impeding the opponent's counterattack are all considered illegal and are subject to penalty. Any infraction that prevents a clear scoring opportunity will result in a seven-meter penalty shot.

Typically the referee will give a warning yellow card for an illegal action; but, if the contact was particularly dangerous, like striking the opponent in the head, neck or throat, the referee can forego the warning for an immediate two-minute suspension. A player can get only one warning before receiving a two-minute suspension. One player is only permitted two two-minute suspensions; after the third time, they will be shown the red card.

A red card results in an ejection from the game and a two-minute penalty for the team. A player may receive a red card directly for particularly rough penalties. For instance, any contact from behind during a fast break is now being treated with a red card. A red-carded player has to leave the playing area completely. A player who is disqualified may be substituted with another player after the two-minute penalty is served. A coach or official can also be penalized progressively. Any coach or official who receives a two-minute suspension will have to pull out one of their players for two minutes; however, the player is not the one punished, and can be substituted in again, as the penalty consists of the team playing with a one player less than the opposing team.

After referees award the ball to the opponents for whatever reason, the player currently in possession of the ball has to lay it down quickly, or risk a two-minute suspension. Also, gesticulating or verbally questioning the referee's order, as well as arguing with the officials' decisions, will normally result in a two-minute suspension. If the suspended player protests further, does not walk straight off the field to the bench, or if the referee deems the tempo deliberately slow, the player can be given an additional two-minute suspension. Illegal substitution (outside of the dedicated area, or if the replacement player enters too early) is also punishable by a two-minute suspension.

Players are typically referred to by the positions they are playing. The positions are always denoted from the view of the respective goalkeeper, so that a defender on the right opposes an attacker on the left. However, not all of the following positions may be occupied depending on the formation or potential suspensions.


Sometimes, the offense uses formations with two pivot players.

There are many variations in defensive formations. Usually, they are described as "n:m" formations, where "n" is the number of players defending at the goal line and "m" the number of players defending more offensive. Exceptions are the 3:2:1 defense and n+m formation (e.g. 5+1), where m players defend some offensive player in man coverage (instead of the usual zone coverage).

Attacks are played with all field players on the side of the defenders. Depending on the speed of the attack, one distinguishes between three attack "waves" with a decreasing chance of success:



The third wave evolves into the normal offensive play when all defenders not only reach the zone, but gain their accustomed positions. Some teams then substitute specialised offence players. However, this implies that these players must play in the defence should the opposing team be able to switch quickly to offence. The latter is another benefit for fast playing teams.

If the attacking team does not make sufficient progress (eventually releasing a shot on goal), the referees can call passive play (since about 1995, the referee gives a passive warning some time before the actual call by holding one hand up in the air, signalling that the attacking team should release a shot soon), turning control over to the other team. A shot on goal or an infringement leading to a yellow card or two-minute penalty will mark the start of a new attack, causing the hand to be taken down; but a shot blocked by the defense or a normal free throw will not. If it were not for this rule, it would be easy for an attacking team to stall the game indefinitely, as it is difficult to intercept a pass without at the same time conceding dangerous openings towards the goal.

The usual formations of the defense are 6–0, when all the defense players line up between the and lines to form a wall; the 5–1, when one of the players cruises outside the perimeter, usually targeting the center forwards while the other 5 line up on the line; and the less common 4–2 when there are two such defenders out front. Very fast teams will also try a 3–3 formation which is close to a switching man-to-man style. The formations vary greatly from country to country, and reflect each country's style of play. 6–0 is sometimes known as "flat defense", and all other formations are usually called "offensive defense".

Handball teams are usually organised as clubs. On a national level, the clubs are associated in federations which organize matches in leagues and tournaments.

The International Handball Federation (IHF) is the administrative and controlling body for international handball. Handball is an Olympic sport played during the Summer Olympics.

The IHF organizes world championships, held in odd-numbered years, with separate competitions for men and women.
The IHF World Men's Handball Championship 2019 title holders are Denmark. The IHF 2017 Women's World Championship title holders are France.

The IHF is composed of five continental federations: Asian Handball Federation, African Handball Confederation, Pan-American Team Handball Federation, European Handball Federation and Oceania Handball Federation. These federations organize continental championships held every other second year. Handball is played during the Pan American Games, All-Africa Games, and Asian Games. It is also played at the Mediterranean Games. In addition to continental competitions between national teams, the federations arrange international tournaments between club teams.




The current worldwide attendance record for seven-a-side handball was set on September 6, 2014, during a neutral venue German league game between HSV Hamburg and the Mannheim-based Rhein-Neckar Lions. The matchup drew 44,189 spectators to Commerzbank Arena in Frankfurt, exceeding the previous record of 36,651 set at Copenhagen's Parken Stadium during the 2011 Danish Cup final.

Handball events have been selected as a main motif in numerous collectors' coins. One of the recent samples is the €10 Greek Handball commemorative coin, minted in 2003 to commemorate the 2004 Summer Olympics. On the coin, the modern athlete directs the ball in his hands towards his target, while in the background the ancient athlete is just about to throw a ball, in a game known as cheirosphaira, in a representation taken from a black-figure pottery vase of the Archaic period.

The most recent commemorative coin featuring handball is the British 50 pence coin, part of the series of coins commemorating the London 2012 Olympic Games. 

Notes


</doc>
<doc id="13733" url="https://en.wikipedia.org/wiki?curid=13733" title="Hilbert's basis theorem">
Hilbert's basis theorem

In mathematics, specifically commutative algebra, Hilbert's basis theorem says that a polynomial ring over a Noetherian ring is Noetherian.

If formula_1 is a ring, let formula_2 denote the ring of polynomials in the indeterminate formula_3 over formula_1. Hilbert proved that if formula_1 is "not too large", in the sense that if formula_1 is Noetherian, the same must be true for formula_2. Formally,

Hilbert's Basis Theorem. If formula_1 is a Noetherian ring, then formula_2 is a Noetherian ring.

Corollary. If formula_1 is a Noetherian ring, then formula_11 is a Noetherian ring.

This can be translated into algebraic geometry as follows: every algebraic set over a field can be described as the set of common roots of finitely many polynomial equations. proved the theorem (for the special case of polynomial rings over a field) in the course of his proof of finite generation of rings of invariants.

Hilbert produced an innovative proof by contradiction using mathematical induction; his method does not give an algorithm to produce the finitely many basis polynomials for a given ideal: it only shows that they must exist. One can determine basis polynomials using the method of Gröbner bases.

Remark. We will give two proofs, in both only the "left" case is considered, the proof for the right case is similar.

Suppose formula_14 is a non-finitely generated left-ideal. Then by recursion (using the axiom of dependent choice) there is a sequence formula_15 of polynomials such that if formula_16 is the left ideal generated by formula_17 then formula_18 is of minimal degree. It is clear that formula_19 is a non-decreasing sequence of naturals. Let formula_20 be the leading coefficient of formula_21 and let formula_22 be the left ideal in formula_1 generated by formula_24. Since formula_1 is Noetherian the chain of ideals

must terminate. Thus formula_27 for some integer formula_28. So in particular,

Now consider

whose leading term is equal to that of formula_31; moreover, formula_32. However, formula_33, which means that formula_34 has degree less than formula_31, contradicting the minimality.

Let formula_14 be a left-ideal. Let formula_37 be the set of leading coefficients of members of formula_38. This is obviously a left-ideal over formula_1, and so is finitely generated by the leading coefficients of finitely many members of formula_38; say formula_41. Let formula_42 be the maximum of the set formula_43, and let formula_44 be the set of leading coefficients of members of formula_38, whose degree is formula_46. As before, the formula_44 are left-ideals over formula_1, and so are finitely generated by the leading coefficients of finitely many members of formula_38, say

with degrees formula_46. Now let formula_52 be the left-ideal generated by:

We have formula_54 and claim also formula_55. Suppose for the sake of contradiction this is not so. Then let formula_56 be of minimal degree, and denote its leading coefficient by formula_57.

Thus our claim holds, and formula_73 which is finitely generated.

Note that the only reason we had to split into two cases was to ensure that the powers of formula_3 multiplying the factors, were non-negative in the constructions.

Let formula_1 be a Noetherian commutative ring. Hilbert's basis theorem has some immediate corollaries.


The Mizar project has completely formalized and automatically checked a proof of Hilbert's basis theorem in the HILBASIS file.



</doc>
<doc id="13734" url="https://en.wikipedia.org/wiki?curid=13734" title="Heterocyclic compound">
Heterocyclic compound

A heterocyclic compound or ring structure is a cyclic compound that has atoms of at least two different elements as members of its ring(s). Heterocyclic chemistry is the branch of organic chemistry dealing with the synthesis, properties, and applications of these heterocycles.

Examples of heterocyclic compounds include all of the nucleic acids, the majority of drugs, most biomass (cellulose and related materials), and many natural and synthetic dyes.

Although heterocyclic chemical compounds may be inorganic compounds or organic compounds, most contain at least one carbon. While atoms that are neither carbon nor hydrogen are normally referred to in organic chemistry as heteroatoms, this is usually in comparison to the all-carbon backbone. But this does not prevent a compound such as borazine (which has no carbon atoms) from being labelled "heterocyclic". IUPAC recommends the Hantzsch-Widman nomenclature for naming heterocyclic compounds.

Heterocyclic compounds can be usefully classified based on their electronic structure. The saturated heterocycles behave like the acyclic derivatives. Thus, piperidine and tetrahydrofuran are conventional amines and ethers, with modified steric profiles. Therefore, the study of heterocyclic chemistry focuses especially on unsaturated derivatives, and the preponderance of work and applications involves unstrained 5- and 6-membered rings. Included are pyridine, thiophene, pyrrole, and furan. Another large class of heterocycles are fused to benzene rings, which for pyridine, thiophene, pyrrole, and furan are quinoline, benzothiophene, indole, and benzofuran, respectively. Fusion of two benzene rings gives rise to a third large family of compounds, respectively the acridine, dibenzothiophene, carbazole, and dibenzofuran. The unsaturated rings can be classified according to the participation of the heteroatom in the conjugated system, pi system.

Heterocycles with three atoms in the ring are more reactive because of ring strain. Those containing one heteroatom are, in general, stable. Those with two heteroatoms are more likely to occur as reactive intermediates.<br>
Common 3-membered heterocycles with "one" heteroatom are:

Those with "two" heteroatoms include:

Compounds with one heteroatom:

Compounds with two heteroatoms:

With heterocycles containing five atoms, the unsaturated compounds are frequently more stable because of aromaticity.

The 5-membered ring compounds containing "two" heteroatoms, at least one of which is nitrogen, are collectively called the azoles. Thiazoles and isothiazoles contain a sulfur and a nitrogen atom in the ring. Dithiolanes have two sulfur atoms.

A large group of 5-membered ring compounds with "three" heteroatoms also exists. One example is dithiazoles that contain two sulfur and a nitrogen atom.

Six-membered rings with a "single" heteroatom:

With "two" heteroatoms:

With three heteroatoms:

With four heteroatoms:

With five heteroatoms:
The hypothetical compound with six nitrogen heteroatoms would be hexazine.

With 7-membered rings, the heteroatom must be able to provide an empty pi orbital (e.g., boron) for "normal" aromatic stabilization to be available; otherwise, homoaromaticity may be possible. Compounds with one heteroatom include:

Those with two heteroatoms include:

Heterocyclic rings systems that are formally derived by fusion with other rings, either carbocyclic or heterocyclic, have a variety of common and systematic names. For example, with the benzo-fused unsaturated nitrogen heterocycles, pyrrole provides indole or isoindole depending on the orientation. The pyridine analog is quinoline or isoquinoline. For azepine, benzazepine is the preferred name. Likewise, the compounds with two benzene rings fused to the central heterocycle are carbazole, acridine, and dibenzoazepine. Thienothiophene are the fusion of two thiophene rings. Phosphaphenalenes are a tricyclic phosphorus-containing heterocyclic system derived from the carbocycle phenalene.

The history of heterocyclic chemistry began in the 1800s, in step with the development of organic chemistry. Some noteworthy developments:
1818: Brugnatelli isolates alloxan from uric acid
1832: Dobereiner produces furfural (a furan) by treating starch with sulfuric acid
1834: Runge obtains pyrrole ("fiery oil") by dry distillation of bones
1906: Friedlander synthesizes indigo dye, allowing synthetic chemistry to displace a large agricultural industry
1936: Treibs isolates chlorophyl derivatives from crude oil, explaining the biological origin of petroleum.
1951: Chargaff's rules are described, highlighting the role of heterocyclic compounds (purines and pyrimidines) in the genetic code.

Heterocyclic compounds are pervasive in many areas of life sciences and technology. Many drugs are heterocyclic compounds.



</doc>
<doc id="13743" url="https://en.wikipedia.org/wiki?curid=13743" title="Harry Connick Jr.">
Harry Connick Jr.

Joseph Harry Fowler Connick Jr. (born September 11, 1967) is a Grammy and Emmy-award winning American singer, composer, actor, and television host. He has sold over 28million albums worldwide. Connick is ranked among the top60 best-selling male artists in the United States by the Recording Industry Association of America, with 16million in certified sales. He has had seven top20 US albums, and ten number-one US jazz albums, earning more number-one albums than any other artist in US jazz chart history.

Connick's best-selling album in the United States is his Christmas album "When My Heart Finds Christmas" (1993). His highest-charting album is his release "Only You" (2004), which reached No.5 in the US and No.6 in Britain. He has won three Grammy Awards and two Emmy Awards. He played Debra Messing's character Grace Adler’s husband, Leo Markus, on the NBC sitcom "Will & Grace" from 2002 to 2006.

Connick began his acting career as a tail gunner in the World War II film "Memphis Belle" (1990). He played a serial killer in "Copycat" (1995), before being cast as a fighter pilot in the blockbuster "Independence Day" (1996). Connick's first role as a leading man was in "Hope Floats" (1998) with Sandra Bullock. His first thriller film since "Copycat" came in the film "Basic" (2003) with John Travolta. Additionally, he played a violent ex-husband in "Bug", before two romantic comedies, "P.S. I Love You" (2007), and the leading man in "New in Town" (2009) with Renée Zellweger. In 2011, he appeared in the family film "Dolphin Tale" as Dr. Clay Haskett and in its 2014 sequel.

Harry Connick Jr. was born and raised in New Orleans, Louisiana. His mother, Anita Frances Livingston (née Levy) was a lawyer and judge in New Orleans and, later, a Louisiana Supreme Court justice. His father, Joseph Harry Fowler Connick Sr., was the district attorney of Orleans Parish from 1973 to 2003. His parents also owned a record store. Connick's father is a Catholic of Irish, English, and German ancestry. Connick's mother, who died from ovarian cancer, was Jewish (her parents had emigrated from Minsk and Vienna). Connick and his sister, Suzanna, were raised in the Lakeview neighborhood of New Orleans.

Connick's musical talents came to the fore when he started learning keyboards at age three, playing publicly at age five, and recording with a local jazz band at ten. When he was nine years old, Connick performed the Piano Concerto No. 3 Opus 37 of Beethoven with the New Orleans Symphony Orchestra (now the Louisiana Philharmonic). Later he played a duet with Eubie Blake at the Royal Orleans Esplanade Lounge in New Orleans. The song was "I'm Just Wild About Harry". This was recorded for a Japanese documentary called "Jazz Around the World". The clip was also shown in a Bravo special, called "Worlds of Harry Connick, Junior." in 1999. His musical talents were developed at the New Orleans Center for Creative Arts and under the tutelage of Ellis Marsalis Jr. and James Booker.

Connick attended Jesuit High School, Isidore Newman School, Lakeview School, and the New Orleans Center for Creative Arts, all in New Orleans. Following an unsuccessful attempt to study jazz academically, and having given recitals in the classical and jazz piano programs at Loyola University, Connick moved to the 92nd Street YMHA in New York City to study at Hunter College and the Manhattan School of Music. There he met Columbia Records executive, Dr. George Butler, who persuaded him to sign with Columbia. His first record, "Harry Connick Jr.", was a mainly instrumental album of standards. He soon acquired a reputation in jazz because of extended stays at high-profile New York venues. His next album, "20", featured his vocals and added to this reputation.

With Connick's reputation growing, director Rob Reiner asked him to provide a soundtrack for his romantic comedy, "When Harry Met Sally..." (1989), starring Meg Ryan and Billy Crystal. The soundtrack consisted of several standards, including "It Had to Be You", "Let's Call the Whole Thing Off" and "Don't Get Around Much Anymore", and achieved double-platinum status in the United States. He won his first Grammy Award for Best Jazz Male Vocal Performance for his work on the soundtrack.

Connick made his screen debut in "Memphis Belle" (1990), a fictional story about a B-17 Flying Fortress bomber crew in World War II. In that year he began a two-year world tour. In addition, he released two albums in July 1990: the instrumental jazz trio album "Lofty's Roach Souffle" and a big-band album of mostly original songs titled "We Are in Love", which also went double platinum. "We Are in Love" earned him his second consecutive Grammy for Best Jazz Male Vocal.

"Promise Me You'll Remember", his contribution to the "Godfather III" soundtrack, was nominated for both an Academy Award and a Golden Globe Award in 1991. In a year of recognition, he was also nominated for an Emmy Award for Best Performance in a Variety Special for his PBS special "Swingin' Out Live", which was also released as a video. In October 1991, he released his third consecutive multi-platinum album, "Blue Light, Red Light", on which he wrote and arranged the songs. Also in October 1991, he starred in "Little Man Tate", directed by Jodie Foster, playing the friend of a child prodigy who goes to college.

In November 1992, Connick released "25", a solo piano collection of standards that again went platinum. He also re-released the album "Eleven". Connick contributed "A Wink and a Smile" to the "Sleepless in Seattle" soundtrack, released in 1993. His multi-platinum album of holiday songs, "When My Heart Finds Christmas", was the best-selling Christmas album in 1993.

In 1994, Connick decided to branch out. He released "She", an album of New Orleans funk that also went platinum. In addition, he released a song called "(I Could Only) Whisper Your Name" for the soundtrack of "The Mask", starring Jim Carrey, which is his most successful single in the United States to date.

Connick took his funk music on a tour of the United Kingdom in 1994, an effort that did not please some of his fans, who were expecting a jazz crooner. Connick also went on a tour of the People's Republic of China in 1995, playing at the Shanghai Center Theatre. The performance was televised live in China for what became known as the Shanghai Gumbo special. In his third film "Copycat", Connick played a serial killer who terrorizes a psychiatrist (played by Sigourney Weaver). Released in 1995, "Copycat" also starred Holly Hunter and Sigourney Weaver. The following year, he released his second funk album, "Star Turtle", which did not sell as well as previous albums, although it did reach No. 38 on the charts. However, he appeared in the most successful movie of 1996, "Independence Day", with Will Smith and Jeff Goldblum.

For his 1997 release "To See You", Connick recorded original love songs, touring the United States and Europe with a full symphony orchestra backing him and his piano in each city. As part of his tour, he played at the Nobel Peace Prize Concert in Oslo, Norway, with his final concert of that tour in Paris being recorded for a Valentine's Day special on PBS in 1998. He also continued his film career, starring in "Excess Baggage" (1997) opposite Alicia Silverstone and Benicio del Toro.

In May 1998, he had his first leading role in director Forest Whitaker's "Hope Floats", with Sandra Bullock as his female lead. He released "Come By Me", his first album of big band music in eight years in 1999, and embarked on a world tour visiting the United States, Europe, Japan and Australia. In addition, he provided the voice of Dean McCoppin in the animated film "The Iron Giant".

Connick wrote the score for Susan Stroman's Broadway musical "Thou Shalt Not", based on Émile Zola's novel "Thérèse Raquin", in 2000; it premiered in 2001. His music and lyrics earned a Tony Award nomination. He was also the narrator of the film "My Dog Skip", released in that year.

In March 2001, Connick starred in a television production of "South Pacific" with Glenn Close, televised on the ABC network. He also starred in his twelfth movie, "Mickey", featuring a screenplay by John Grisham that same year. In October 2001, he again released two albums: "Songs I Heard", featuring big band re-workings of children's show themes, and "30", featuring Connick on piano with guest appearances by several other musical artists. "Songs I Heard" won Connick another Grammy for Best Traditional Pop Album and he toured performing songs from the album, holding matinees at which each parent had to be accompanied by a child.

In 2002, he received a for a "system and method for coordinating music display among players in an orchestra." Connick appeared as Grace Adler's boyfriend (and later husband) Leo Markus on the NBC sitcom "Will & Grace" from 2002 to 2006.

In July 2003, Connick released his first instrumental album in fifteen years, "Other Hours Connick on Piano Volume 1". It was released on Branford Marsalis' new label Marsalis Music and led to a short tour of nightclubs and small theaters. Connick appeared in the film "Basic". In October 2003, he released his second Christmas album, "Harry for the Holidays", which went gold and reached No. 12 on the "Billboard" 200 albums chart. He also had a television special on NBC featuring Whoopi Goldberg, Nathan Lane, Marc Anthony and Kim Burrell. "Only You", his seventeenth album for Columbia Records, was released in February 2004. A collection of 1950s and 1960s ballads, "Only You", went top ten on both sides of the Atlantic and was certified gold in the United States in March 2004. The "Only You" tour with big band went on in America, Australia and a short trip to Asia. "Harry for the Holidays" was certified platinum in November 2004. A music DVD "Harry Connick Jr."Only You" in Concert" was released in March 2004, after it had first aired as a "Great Performances" special on PBS. The special won him an Emmy Award for Outstanding Music Direction. The DVD received a Gold & Platinum Music VideoLong Form awards from the RIAA in November 2005.

An animated holiday special, "The Happy Elf", aired on NBC in December 2005, with Connick as the composer, the narrator, and one of the executive producers. Shortly after, it was released on DVD. The holiday special was based on his original song "The Happy Elf", from his 2003 album "Harry for the Holidays". Another album from Marsalis Music was recorded in 2005, "", a duo album with Harry Connick Jr. on piano together with Branford Marsalis on saxophone. A music DVD, "A Duo Occasion", was filmed at the Ottawa International Jazz Festival 2005 in Canada, and released in November 2005.

He appeared in another episode of NBC sitcom "Will & Grace" in November 2005, and appeared in an additional three episodes in 2006.

"Bug", a film directed by William Friedkin, is a psychological thriller filmed in 2005, starring Connick, Ashley Judd, and Michael Shannon. The film was released in 2007. He starred in the Broadway revival of "The Pajama Game", produced by the Roundabout Theater Company, along with Michael McKean and Kelli O'Hara, at the "American Airlines Theatre" in 2006. It ran from February 23 to June 17, 2006, including five benefit performances running from June 13 to 17. The "Pajama Game" cast recording was nominated for a Grammy, after being released as part of Connick's double disc album Harry on Broadway, Act I.

He hosted The Weather Channel's miniseries "100 Biggest Weather Moments" which aired in 2007. He was part of the documentary , released in November 2007. He sat in on piano on Bob French's 2007 album "Marsalis Music Honors Series: Bob French". He appeared in the film "P.S. I Love You", released in December 2007. A third album in the "Connick on Piano" series, "Chanson du Vieux Carré" was released in 2007, and Connick received two Grammy nominations for the track "Ash Wednesday", for the Grammy awards in 2008. "Chanson du Vieux Carré" was released simultaneously with the album "Oh, My NOLA". Connick toured North America and Europe in 2007, and toured Asia and Australia in 2008, as part of his My New Orleans Tour. Connick did the arrangements for, wrote a couple of songs, and sang a duet on Kelli O'Hara's album that was released in May 2008. He was also the featured singer at the Concert of Hope immediately preceding Pope Benedict XVI's Mass at Yankee Stadium in April 2008. He had the starring role of Dr. Dennis Slamon in the Lifetime television film "Living Proof" (2008). His third Christmas album, "What a Night!", was released in November 2008.

Harry has a vast knowledge of musical genres and vocalists, even Gospel music. One of his favorite Gospel artists is Stellar Award winner and Grammy nominated artist Kim Burrell of Houston, Texas. "And when Harry Connick Jr. assembled a symphony orchestra for Pope Benedict XVI’s appearance at Yankee Stadium in 2008, he wanted Burrell on vocals"

The film "New in Town" starring Connick and Renée Zellweger, began filming in January 2008, and was released in January 2009. Connick's album "Your Songs" was released on CD, September 22, 2009. In contrast to Connick's previous albums, this album is a collaboration with a record company producer, the multiple Grammy Award winning music executive Clive Davis.

Connick starred in the Broadway revival of "On a Clear Day You Can See Forever", which opened at the St. James Theatre in November 2011 in previews.

Connick appeared on May 4, 2010 episode of "American Idol" season 9, where he acted as a mentor for the top 5 finalists. He appeared again the next night on May 5 to perform "And I Love Her".

On January 6, 2012, NBC president Robert Greenblatt announced at the Television Critics Association winter press tour that Connick had been cast in a four-episode arc of NBC's long-running legal drama, "" as new Executive ADA, David Haden, a prosecutor who is assigned a case with Detective Olivia Benson (Mariska Hargitay).

On June 11, 2013, Connick released a new album of all original music titled "Every Man Should Know". Connick debuted the title track live on May 2, 2013 episode of "American Idol" and appeared on "The Ellen DeGeneres Show" the following week to discuss his new project. A 2013 US summer tour was announced in support of the album.

Connick returned to "American Idol" to mentor the top four of season 12. He performed "Every Man Should Know" on the results show the following night.

On September 3, 2013, the officials of "American Idol" officially announced that Connick would be a part of the judging panel for season 13 alongside former judge Jennifer Lopez and returning judge Keith Urban.

"Angels Sing", a family Christmas movie released in November 2013 by Lionsgate, afforded Connick an onscreen collaboration with fellow musician Willie Nelson. The two wrote a special song exclusively for the movie. Shot in Austin, Texas, "Angels Sing" features actor/musicians Connie Britton, Lyle Lovett, and Kris Kristofferson and is directed by Tim McCanlies, who previously worked with Connick in The Iron Giant.

A one-hour weekday daytime talk show both starring and named "Harry", debuted on September 12, 2016.

The following musicians have toured as the Harry Connick Jr. Big Band since its inception in 1990:

In January 2019, it was announced that Connick was hired by piano instruction software company Playground Sessions as a video instructor.

Connick, a New Orleans native, is a founder of the Krewe of Orpheus, a music-based New Orleans krewe, taking its name from Orpheus of classical mythology. The Krewe of Orpheus parades on St. Charles Avenue and Canal Street in New Orleans on Lundi Gras (Fat Monday)the day before Mardi Gras (Fat Tuesday).

On September 2, 2005, Connick helped to organize, and appeared in, the NBC-sponsored live telethon concert, "A Concert for Hurricane Relief", for relief in the wake of Hurricane Katrina. He spent several days touring the city to draw attention to the plight of citizens stranded at the Ernest N. Morial Convention Center and other places. At the concert he paired with host Matt Lauer, and entertainers including Tim McGraw, Faith Hill, Kanye West, Mike Myers, and John Goodman.

On September 6, 2005, Connick was made honorary chair of Habitat for Humanity's Operation Home Delivery, a long-term rebuilding plan for families who survived Hurricane Katrina in New Orleans and along the Gulf Coast. His actions in New Orleans earned him a Jefferson Award for Public Service.

Connick's album "Oh, My NOLA", and "" were released in 2007, with a following tour called the My New Orleans Tour.

Connick and Branford Marsalis devised an initiative to help restore New Orleans' musical heritage. Habitat for Humanity and New Orleans Area Habitat for Humanity, working with Connick and Marsalis announced December 6, 2005, plans for a Musicians' Village in New Orleans. The Musicians' Village includes Habitat-constructed homes, with an "Ellis Marsalis Center for Music", as the area's centerpiece. The Habitat-built homes provide musicians, and anyone else who qualifies, the opportunity to buy decent, affordable housing.

In 2012, Connick and Marsalis received the S. Roger Horchow Award for Greatest Public Service by a Private Citizen, an award given out annually by Jefferson Awards.

On April 16, 1994, Connick married former Victoria's Secret model Jill Goodacre, originally from Texas, at the St. Louis Cathedral, New Orleans. Jill is the daughter of sculptor Glenna Goodacre, originally from Lubbock, and now Santa Fe, New Mexico. The song "Jill", on the album "Blue Light, Red Light" (1991) is about her. They have three daughters: Georgia Tatum (born April 17, 1996), Sarah Kate (born September 12, 1997), and Charlotte (born June 26, 2002). The family currently resides in New Canaan, Connecticut and New Orleans, Louisiana. Connick is a practicing Roman Catholic. In 2011 Harry wrote Kate's debut song "A Lot Like Me". The song was released to celebrate the debut of American Girl's newest historical characters Cecile Rey and Marie Grace Gardner. "A Lot Like Me" is available on iTunes. The proceeds from "A Lot Like Me" went towards Ellis Marsalis Center for Music. In 2014, Cecile and Marie Grace were archived with Ruthie and Ivy to make room for the return of Samantha and BeForever.

Connick is a supporter of hometown NFL franchise New Orleans Saints. He was caught on camera at the Super Bowl XLIV, which the Saints won, in Miami by the television crew of "The Ellen DeGeneres Show" during the post-game celebrations. Ellen's mother Betty was on the sidelines watching the festivities when she spotted Connick in the stands sporting a Drew Brees jersey.

Connick was arrested by the Port Authority Police in December 1992 and charged with having a 9mm pistol in his possession at JFK International Airport. After spending a day in jail, he agreed to make a public-service television commercial warning against breaking gun laws. The court agreed to drop all charges if Connick stayed out of trouble for six months.





</doc>
<doc id="13744" url="https://en.wikipedia.org/wiki?curid=13744" title="List of humorists">
List of humorists

A humorist (American English) or humourist (British English) is an intellectual who uses humor in writing or public speaking. Humorists are distinct from comedians, who are show business entertainers whose business is to make an audience laugh, though it is possible for some persons to occupy both roles in the course of their careers.

Despite the fact that the Kennedy Center for the Performing Arts annually bestows a Mark Twain Prize for American Humor (usually on comedians) since 1998, this award does not by itself qualify the recipient as a humorist. only two recipients, Steve Martin and Neil Simon, are known as humorists, being humorous playwrights.

Notable humorists include:


</doc>
<doc id="13746" url="https://en.wikipedia.org/wiki?curid=13746" title="Hydrostatic shock (firearms)">
Hydrostatic shock (firearms)

Hydrostatic shock is the controversial concept that a penetrating projectile (such as a bullet) can produce a pressure wave that causes "remote neural damage", "subtle damage in neural tissues" and/or "rapid incapacitating effects" in living targets. It has also been suggested that pressure wave effects can cause indirect bone fractures at a distance from the projectile path, although it was later demonstrated that indirect bone fractures are caused by temporary cavity effects (strain placed on the bone by the radial tissue displacement produced by the temporary cavity formation).

Proponents of the concept argue that hydrostatic shock can produce remote neural damage and produce incapacitation more quickly than blood loss effects. In arguments about the differences in stopping power between calibers and between cartridge models, proponents of cartridges that are "light and fast" (such as the 9×19mm Parabellum) versus cartridges that are "slow and heavy" (such as the .45 ACP) often refer to this phenomenon.

Martin Fackler has argued that sonic pressure waves do not cause tissue disruption and that temporary cavity formation is the actual cause of tissue disruption mistakenly ascribed to sonic pressure waves. One review noted that strong opinion divided papers on whether the pressure wave contributes to wound injury. It ultimately concluded that no "conclusive evidence could be found for permanent pathological effects produced by the pressure wave".

In the scientific literature, the first discussion of pressure waves created when a bullet hits a living target is presented by E. Harvey Newton and his research group at Princeton University in 1947:

Frank Chamberlin, a World War II trauma surgeon and ballistics researcher, noted remote pressure wave effects. Col. Chamberlin described what he called "explosive effects" and "hydraulic reaction" of bullets in tissue. "...liquids are put in motion by ‘shock waves’ or hydraulic effects... with liquid filled tissues, the effects and destruction of tissues extend in all directions far beyond the wound axis". He avoided the ambiguous use of the term "shock" because it can refer to either a specific kind of pressure wave associated with explosions and supersonic projectiles or to a medical condition in the body.

Col. Chamberlin recognized that many theories have been advanced in wound ballistics. During World War II he commanded an 8,500-bed hospital center that treated over 67,000 patients during the fourteen months that he operated it. P.O. Ackley estimates that 85% of the patients were suffering from gunshot wounds. Col. Chamberlin spent many hours interviewing patients as to their reactions to bullet wounds. He conducted many live animal experiments after his tour of duty. On the subject of wound ballistics theories, he wrote:
Other World War II era scientists noted remote pressure wave effects in the peripheral nerves. There was support for the idea of remote neural effects of ballistic pressure waves in the medical and scientific communities, but the phrase "’hydrostatic shock’" and similar phrases including "shock" were used mainly by gunwriters (such as Jack O'Conner) and the small arms industry (such as Roy Weatherby, and Federal "Hydra-Shok.")

Dr. Martin Fackler, a Vietnam-era trauma surgeon, wound ballistics researcher, a Colonel in the U.S. Army and the head of the Wound Ballistics Laboratory for the U.S. Army’s Medical Training Center, Letterman Institute, claimed that hydrostatic shock had been disproved and that the assertion that a pressure wave plays a role in injury or incapacitation is a myth. Others expressed similar views.

Dr. Fackler based his argument on the lithotriptor, a tool commonly used to break up kidney stones. The lithotriptor uses sonic pressure waves which are stronger than those caused by most handgun bullets, yet it produces no damage to soft tissues whatsoever. Hence, Fackler argued, ballistic pressure waves cannot damage tissue either.

Dr. Fackler claimed that a study of rifle bullet wounds in Vietnam (Wound Data and Munitions Effectiveness Team) found "no cases of bones being broken, or major vessels torn, that were not hit by the penetrating bullet. In only two cases, an organ that was not hit (but was within a few cm of the projectile path), suffered some disruption." Dr. Fackler cited a personal communication with R. F. Bellamy. However, Bellamy’s published findings the following year estimated that 10% of fractures in the data set might be due to indirect injuries, and one specific case is described in detail (pp. 153–154). In addition, the published analysis documents five instances of abdominal wounding in cases where the bullet did not penetrate the abdominal cavity (pp. 149–152), a case of lung contusion resulting from a hit to the shoulder (pp. 146–149), and a case of indirect effects on the central nervous system (p. 155). Fackler's critics argue that Fackler's evidence does not contradict distant injuries, as Fackler claimed, but the WDMET data from Vietnam actually provides supporting evidence for it.

A summary of the debate was published in 2009 as part of a "Historical Overview of Wound Ballistics Research."

The Wound Data and Munitions Effectiveness Team (WDMET) gathered data on wounds sustained during the Vietnam War. In their analysis of this data published in the Textbook of Military Medicine, Ronald Bellamy and Russ Zajtchuck point out a number of cases which seem to be examples of distant injuries. Bellamy and Zajtchuck describe three mechanisms of distant wounding due to pressure transients: 1) stress waves 2) shear waves and 3) a vascular pressure impulse.

After citing Harvey's conclusion that "stress waves probably do not cause any tissue damage" (p. 136), Bellamy and Zajtchuck express their view that Harvey's interpretation might not be definitive because they write "the possibility that stress waves from a penetrating projectile might also cause tissue damage cannot be ruled out." (p. 136) The WDMET data includes a case of a lung contusion resulting from a hit to the shoulder. The caption to Figure 4-40 (p. 149) says, "The pulmonary injury may be the result of a stress wave." They describe the possibility that a hit to a soldier's trapezius muscle caused temporary paralysis due to "the stress wave passing through the soldier's neck indirectly [causing] cervical cord dysfunction." (p. 155)

In addition to stress waves, Bellamy and Zajtchuck describe shear waves as a possible mechanism of indirect injuries in the WDMET data. They estimate that 10% of bone fractures in the data may be the result of indirect injuries, that is, bones fractured by the bullet passing close to the bone without a direct impact. A Chinese experiment is cited which provides a formula estimating how pressure magnitude decreases with distance. Together with the difference between strength of human bones and strength of the animal bones in the Chinese experiment, Bellamy and Zajtchuck use this formula to estimate that assault rifle rounds "passing within a centimeter of a long bone might very well be capable of causing an indirect fracture." (p. 153) Bellamy and Zajtchuck suggest the fracture in Figures 4-46 and 4-47 is likely an indirect fracture of this type. Damage due to shear waves extends to even greater distances in abdominal injuries in the WDMET data. Bellamy and Zajtchuck write, "The abdomen is one body region in which damage from indirect effects may be common." (p. 150) Injuries to the liver and bowel shown in Figures 4-42 and 4-43 are described, "The damage shown in these examples extends far beyond the tissue that is likely to direct contact with the projectile." (p. 150)

In addition to providing examples from the WDMET data for indirect injury due to propagating shear and stress waves, Bellamy and Zajtchuck expresses an openness to the idea of pressure transients propagating via blood vessels can cause indirect injuries. "For example, pressure transients arising from an abdominal gunshot wound might propagate through the vena cavae and jugular venous system into the cranial cavity and cause a precipitous rise in intracranial pressure there, with attendant transient neurological dysfunction." (p. 154) However, no examples of this injury mechanism are presented from the WDMET data. However, the authors suggest the need for additional studies writing, "Clinical and experimental data need to be gathered before such indirect injuries can be confirmed." Distant injuries of this nature were later confirmed in the experimental data of Swedish and Chinese researchers, in the clinical findings of Krajsa and in autopsy findings from Iraq.

Proponents of the concept point to human autopsy results demonstrating brain hemorrhaging from fatal hits to the chest, including cases with handgun bullets. Thirty-three cases of fatal penetrating chest wounds by a single bullet were selected from a much larger set by excluding all other traumatic factors, including past history.
An 8-month study in Iraq performed in 2010 and published in 2011 reports on autopsies of 30 gunshot victims struck with high-velocity (greater than 2500 fps) rifle bullets. The authors determined that the lungs and chest are the most susceptible to distant wounding, followed by the abdomen. The study noted that the "sample size was so small [too small] to reach the level of statistical significance". Nevertheless, the authors conclude:

A shock wave can be created when fluid is rapidly displaced by an explosive or projectile. Tissue behaves similarly enough to water that a sonic pressure wave can be created by a bullet impact, generating pressures in excess of .

Duncan MacPherson, a former member of the International Wound Ballistics Association and author of the book, Bullet Penetration, claimed that shock waves cannot result from bullet impacts with tissue. In contrast, Brad Sturtevant, a leading researcher in shock wave physics at Caltech for many decades, found that shock waves can result from handgun bullet impacts in tissue. Other sources indicate that ballistic impacts can create shock waves in tissue.

Blast and ballistic pressure waves have physical similarities. Prior to wave reflection, they both are characterized by a steep wave front followed by a nearly exponential decay at close distances. They have similarities in how they cause neural effects in the brain. In tissue, both types of pressure waves have similar magnitudes, duration, and frequency characteristics. Both have been shown to cause damage in the hippocampus. It has been hypothesized that both reach the brain from the thoracic cavity via major blood vessels.

For example, Ibolja Cernak, a leading researcher in blast wave injury at the Applied Physics Laboratory at Johns Hopkins University, hypothesized, "alterations in brain function following blast exposure are induced by kinetic energy transfer of blast overpressure via great blood vessels in abdomen and thorax to the central nervous system." This hypothesis is supported by observations of neural effects in the brain from localized blast exposure focused on the lungs in experiments in animals.

"Hydrostatic shock" expresses the idea that organs can be damaged by the pressure wave in addition to damage from direct contact with the penetrating projectile. If one interprets the "shock" in the term "hydrostatic shock" to refer to the physiological effects rather than the physical wave characteristics, the question of whether the pressure waves satisfy the definition of "shock wave" is unimportant, and one can consider the weight of scientific evidence and various claims regarding the possibility of a ballistic pressure wave to create tissue damage and incapacitation in living targets.

A number of papers describe the physics of ballistic pressure waves created when a high-speed projectile enters a viscous medium. These results show that ballistic impacts produce pressure waves that propagate at close to the speed of sound.

Lee et al. present an analytical model showing that unreflected ballistic pressure waves are well approximated by an exponential decay, which is similar to blast pressure waves. Lee et al. note the importance of the energy transfer:

The rigorous calculations of Lee et al. require knowing the drag coefficient and frontal area of the penetrating projectile at every instant of the penetration. Since this is not generally possible with expanding handgun bullets, Courtney and Courtney developed a model for estimating the peak pressure waves of handgun bullets from the impact energy and penetration depth in ballistic gelatin. This model agrees with the more rigorous approach of Lee et al. for projectiles where they can both be applied. For expanding handgun bullets, the peak pressure wave magnitude is proportional to the bullet’s kinetic energy divided by the penetration depth.

Goransson et al. were the first contemporary researchers to present compelling evidence for remote cerebral effects of extremity bullet impact. They observed changes in EEG readings from pigs shot in the thigh. A follow-up experiment by Suneson et al. implanted high-speed pressure transducers into the brain of pigs and demonstrated that a significant pressure wave reaches the brain of pigs shot in the thigh. These scientists observed apnea, depressed EEG readings, and neural damage in the brain caused by the distant effects of the ballistic pressure wave originating in the thigh.

The results of Suneson et al. were confirmed and expanded upon by a later experiment in dogs
which "confirmed that distant effect exists in the central nervous system after a high-energy missile impact to an extremity. A high-frequency oscillating pressure wave with large amplitude and short duration was found in the brain after the extremity impact of a high-energy missile..." Wang et al. observed significant damage in both the hypothalamus and hippocampus regions of the brain due to remote effects of the ballistic pressure wave.

In a study of a handgun injury, Sturtevant found that pressure waves from a bullet impact in the torso can reach the spine and that a focusing effect from concave surfaces can concentrate the pressure wave on the spinal cord producing significant injury. This is consistent with other work showing remote spinal cord injuries from ballistic impacts.

Roberts et al. present both experimental work and finite element modeling showing that there can be considerable pressure wave magnitudes in the thoracic cavity for handgun projectiles stopped by a Kevlar vest. For example, an 8 gram projectile at 360 m/s impacting a NIJ level II vest over the sternum can produce an estimated pressure wave level of nearly 2.0 MPa (280 psi) in the heart and a pressure wave level of nearly 1.5 MPa (210 psi) in the lungs. Impacting over the liver can produce an estimated pressure wave level of 2.0 MPa (280 psi) in the liver.

The work of Courtney et al. supports the role of a ballistic pressure wave in incapacitation and injury. The work of Suneson et al. and Courtney et al. suggest that remote neural effects can occur with levels of energy transfer possible with handguns, about . Using sensitive biochemical techniques, the work of Wang et al. suggests even lower impact energy thresholds for remote neural injury to the brain. In analysis of experiments of dogs shot in the thigh they report highly significant (p < 0.01), easily detectable neural effects in the hypothalamus and hippocampus with energy transfer levels close to . Wang et al. reports less significant (p < 0.05) remote effects in the hypothalamus with energy transfer just under .

Even though Wang et al. document remote neural damage for low levels of energy transfer, roughly , these levels of neural damage are probably too small to contribute to rapid incapacitation. Courtney and Courtney believe that remote neural effects only begin to make significant contributions to rapid incapacitation for ballistic pressure wave levels above (corresponds to transferring roughly in of penetration) and become easily observable above (corresponds to transferring roughly in of penetration). Incapacitating effects in this range of energy transfer are consistent with observations of remote spinal injuries, observations of suppressed EEGs and apnea in pigs and with observations of incapacitating effects of ballistic pressure waves without a wound channel.

The scientific literature contains significant other findings regarding injury mechanisms of ballistic pressure waves. Ming et al. found that ballistic pressure waves can break bones. Tikka et al. reports abdominal pressure changes produced in pigs hit in one thigh. Akimov et al. report on injuries to the nerve trunk from gunshot wounds to the extremities.

In self-defense, military, and law enforcement communities, opinions vary regarding the importance of remote wounding effects in ammunition design and selection. In his book on hostage rescuers, Leroy Thompson discusses the importance of hydrostatic shock in choosing a specific design of .357 Magnum and 9×19mm Parabellum bullets. In "Armed and Female", Paxton Quigley explains that hydrostatic shock is the real source of "stopping power." Jim Carmichael, who served as shooting editor for Outdoor Life magazine for 25 years, believes that hydrostatic shock is important to "a more immediate disabling effect" and is a key difference in the performance of .38 Special and .357 Magnum hollow point bullets. In "The search for an effective police handgun," Allen Bristow describes that police departments recognize the importance of hydrostatic shock when choosing ammunition. A research group at West Point suggests handgun loads with at least of energy and of penetration and recommends:

A number of law enforcement and military agencies have adopted the 5.7×28mm cartridge. These agencies include the Navy SEALs and the Federal Protective Service branch of the ICE. In contrast, some defense contractors, law enforcement analysts, and military analysts say that hydrostatic shock is an unimportant factor when selecting cartridges for a particular use because any incapacitating effect it may have on a target is difficult to measure and inconsistent from one individual to the next. This is in contrast to factors such as proper shot placement and massive blood loss which are almost always eventually incapacitating for nearly every individual.

The FBI recommends that loads intended for self-defense and law enforcement applications meet a minimum penetration requirement of in ballistic gelatin and explicitly advises against selecting rounds based on hydrostatic shock effects.

Hydrostatic shock is commonly considered as a factor in the selection of hunting ammunition. Peter Capstick explains that hydrostatic shock may have value for animals up to the size of white-tailed deer, but the ratio of energy transfer to animal weight is an important consideration for larger animals. If the animal’s weight exceeds the bullet’s energy transfer, penetration in an undeviating line to a vital organ is a much more important consideration than energy transfer and hydrostatic shock. Jim Carmichael, in contrast, describes evidence that hydrostatic shock can affect animals as large as Cape Buffalo in the results of a carefully controlled study carried out by veterinarians in a buffalo culling operation.

Dr. Randall Gilbert describes hydrostatic shock as an important factor in bullet performance on whitetail deer, "When it [a bullet] enters a whitetail’s body, huge accompanying shock waves send vast amounts of energy through nearby organs, sending them into arrest or shut down." Dave Ehrig expresses the view that hydrostatic shock depends on impact velocities above per second. Sid Evans explains the performance of the Nosler Partition bullet and Federal Cartridge Company’s decision to load this bullet in terms of the large tissue cavitation and hydrostatic shock produced from the frontal diameter of the expanded bullet. The North American Hunting Club suggests big game cartridges that create enough hydrostatic shock to quickly bring animals down.




</doc>
<doc id="13749" url="https://en.wikipedia.org/wiki?curid=13749" title="Hadith">
Hadith

Ḥadīth ( or ; , pl. Aḥādīth, , , also "Traditions") in Islam are the record of the words, actions, and silent approval, traditionally attributed to the Islamic prophet Muhammad. Within Islam the authority of Ḥadīth as a source for religious law and moral guidance ranks second only to that of the Qur'an (which Muslims hold to be the word of Allah revealed to his messenger Muhammad). Quranic verses (such as 24:54, 33:21) enjoin Muslims to emulate Muhammad and obey his judgements, providing scriptural authority for ahadith. While the number of verses pertaining to law in the Quran is relatively few, ahadith give direction on everything from details of religious obligations (such as "Ghusl" or "Wudu", ablutions for salat prayer), to the correct forms of salutations and the importance of benevolence to slaves. Thus the "great bulk" of the rules of Sharia (Islamic law) are derived from ahadith, rather than the Qur'an.

Ḥadīth is the Arabic word for speech, report, account, narrative. Unlike the Qur'an, not all Muslims believe Ahadith accounts (or at least not all ahadith accounts) are divine revelation. Ahadith were not written down by Muhammad's followers immediately after his death but several generations later when they were collected, collated and compiled into a great corpus of Islamic literature. Different collections of "Aḥādīth" would come to differentiate the different branches of the Islamic faith. A small minority of Muslims called Quranists reject all Ḥadīth, even if Hadith is apart of sunnah.

Because some ahadith include questionable and even contradictory statements, the authentication of ahadith became a major field of study in Islam. In its classic form a hadith has two parts — the chain of narrators who have transmitted the report (the "isnad"), and the main text of the report (the "matn"). Individual hadith are classified by Muslim clerics and jurists into categories such as "sahih" ("authentic"), "hasan" ("good") or "da'if" ("weak"). However, different groups and different scholars may classify a hadith differently. 
Among some scholars of Sunni Islam, the term hadith may include not only the supposed words, advice, practices, etc. of Muhammad, but also those of his companions. In Shia Islam, "Ḥadīth" is the embodiment of the sunnah, the words and actions of the Prophet and his family the "Ahl al-Bayt" (The Twelve Imams and the Prophet's daughter, Fatimah).

In Arabic, the noun ' (  ) means "report", "account", or "narrative". Its Arabic plural is ' ( ). "Hadith" also refers to the speech of a person.

In Islamic terminology, according to Juan Campo, the term "hadith" refers to reports of statements or actions of Muhammad, or of his tacit approval or criticism of something said or done in his presence.

Classical hadith specialist Ibn Hajar al-Asqalani says that the intended meaning of "hadith" in religious tradition is something attributed to Muhammad but that is not found in the Quran.

Scholar Patricia Crone includes reports by others than Muhammad in her definition of hadith — "short reports (sometimes just a line or two) recording what an early figure, such as a companion of the prophet (known as "sahabah") or Mohammed himself, said or did on a particular occasion, prefixed by a chain of transmitters". But she adds that "nowadays, hadith almost always means hadith from Mohammed himself."

Other associated words possess similar meanings including: "khabar" (news, information) often refers to reports about Muhammad, but sometimes refers to traditions about his companions and their successors from the following generation; conversely, "athar" (trace, vestige) usually refers to traditions about the companions and successors, though sometimes connotes traditions about Muhammad.

However, according to the Shia Islam Ahlul Bayt Digital Library Project, "... when there is no clear Qur’anic statement, nor is there a Hadith upon which Muslim schools have agreed. ... Shi’a ... refer to Ahlul-Bayt for deriving the Sunnah of Prophet" — implying that while Hadith in limited to the "Traditions" of Muhammad, the Shia Sunna draws on the sayings, etc. of the "Ahlul-Bayt" i.e. the Imams of Shia Islam.

The word "sunnah" (custom or "all the traditions and practices" of the Islamic prophet that "have become models to be followed" by Muslims) is also used in reference to a normative custom of Muhammad or the early Muslim community.

Joseph Schacht describes hadith as providing "the documentation" of the Sunnah.

Another source (Joseph A. Islam) distinguishes between the two saying: 
Whereas the 'Hadith' is an oral communication that is allegedly derived from the Prophet or his teachings, the 'Sunna' (quite literally: mode of life, behaviour or example) signifies the prevailing customs of a particular community or people. ... A 'Sunna' is a practice which has been passed on by a community from generation to generation en masse, whereas the Ahadith are reports collected by later compilers often centuries removed from the source. ... A practice which is contained within the Hadith may well be regarded as Sunna, but it is not necessary that a Sunna would have a supporting hadith sanctioning it.

Some sources (Khaled Abou El Fadl) limit hadith to verbal reports, with the deeds of Muhammad and reports about his companions being part of the "Sunnah", but not hadith.

Joseph Schacht quotes a Hadith by Muhammad that is used "to justify reference" in Islamic law to companions of Muhammad as religious authorities — "My companions are like lodestars." According to Schacht, (and other scholars) in the very first generations after the death of Muhammad, use of hadith from "Sahabah" ("companions" of Muhammad) and "Tabi‘un" ("successors" of the companions) "was the rule", while use of hadith of Muhammad himself by Muslims was "the exception". Schacht credits Al-Shafi‘i — founder of the Shafi'i school of "fiqh" (or "madh'hab") — with establishing the principle of the use of the ahadith of the Muhammad for Islamic law, and emphasizing the inferiority of hadith of anyone else, saying ahadith 
"from other persons are of no account in the face of a tradition from the Prophet, whether they confirm or contradict it;
if the other persons had been aware of the tradition from the Prophet, they would have followed it". This led to "the almost complete neglect" of traditions from Companions and others.

Collections of ahadith sometimes mix those of Muhammad with the reports of others. Muwatta Imam Malik is usually described as "the earliest written collection of hadith" but sayings of Muhammad are “blended with the sayings of the companions”, (822 hadith from Muhammad and 898 from others, according to the count of one edition).
In "Introduction to Hadith" by Abd al-Hadi al-Fadli, "Kitab Ali" is referred to as "the first hadith book of the "Ahl al-Bayt" (family of Muhammad) to be written on the authority of the Prophet".

The theological importance of ahadith comes from several verses in the Quran such as:
Say: Obey Allah and obey the Messenger, but if you turn away, he (the Prophet) is only responsible for the duty placed on him (i.e. to convey Allah’s Message) and you for that placed on you. If you obey him, you shall be on the right guidance. The Messenger’s duty is only to convey (the message) in a clear way. (An-Nur 24:54)

In God's messenger you have indeed a good example for everyone who looks forward with hope to God and the Last Day, and remembers God unceasingly. (Al-Ahzab 33: 21)

The hadith literature is based on spoken reports in circulation after the death of Muhammad. Unlike the Qur'an, ahadith were not promptly written down during Muhammad's life or immediately after his death. Hadith were evaluated and gathered into large collections during the 8th and 9th centuries, generations after the death of Muhammad, after the end of the era of the "rightful" Rashidun Caliphate, over from where Muhammad lived. "Many thousands of times" more numerous than Quranic verse, ahadith have been described as resembling layers surrounding the “core” of the Islamic belief (the Quran). Well-known, widely accepted Hadiths make up the narrow inner layer, with ahadith becoming less reliable and accepted with each layer stretching outward.

Unlike the Quran, ahadith are “grounded in the prosaic moments of everyday life". The reports of behavior to be emulated that were collected include details of ritual religious practice such as the five salat (obligatory Islamic prayers) that are not found in the Quran, but also everyday behavior such as table manners, dress, posture. Hadith are also regarded by Muslims as important tools for understanding things mentioned in the Quran but not explained, a source for "tafsir" (commentaries written on the Quran).

Some important elements, which are today taken to be a long-held part of Islamic practice and belief are not mentioned in the Qur'an at all, but are derived solely from the hadith. Almost all Muslims, therefore, can be called Hadithists (i.e. believers in hadith), and maintain that the ahadith are a necessary requirement for the true and proper practice of Islam, as it gives Muslims the nuanced details of Islamic practice and belief in areas where the Qur'an is silent. Quranists, on the contrary, hold that if the Qur'an is silent on some matter, it is because Allah did not hold its detail to be of consequence; and that some ahadith contradict the Qur'an, evidence that some ahadith are a source of corruption and not a compliment to the Quran.

A classical example is salat (the five daily prayers of Islam), which is commanded in the Qur'an, and considered by all Muslims to be an obligatory part of Islamic religious practice—one of the five pillars of Islam. Details of prescribed movements and words of the prayer (known as rakat) and how many times they are to be performed, are found in ahadith, demonstrating to Hadithists that ahadith "validly" fulfill the Qur'anic command of ritual prayer. However, ahadith differ on these details and consequently salat is performed differently by different hadithist Islamic sects. (Quranists, for their part, believe if Allah thought the details of salat to be consequence, would have included them in the Quran and that the details of salat are a matter between each individual Muslim and Allah, with "correctly" performed salat depending on a correct intention to perform the prayers, valid however it may be individually performed.)
Among most hadithists, the importance of ahadith is secondary to Qur'an given that, at least in theory, an Islamic conflict of laws doctrine holds Qur'anic supremacy above ahadith in developing Islamic jurisprudence. However, a minority of hadithists have historically placed ahadith on a par with the Qur'an. A smaller minority have upheld ahadith in contradiction to the Qur'an, thereby placing ahadith above Qur'an and claiming that contradictory ahadith abrogate the parts of the Qur'an where they conflict.

It has been narrated through a chain of narrators, including Muhammad ibn Isma'il and originating with Imam Ja'far al-Sadiq, that the Prophet Muhammad once addressed his people in Mina saying ‘O people, whatever comes to you in the form of my Hadith, if it agrees with the Holy Book of Allah, it is genuine, but whatever comes to you that does not agree with the book of Allah you must know that I have not said it.' 

The hadith had a profound and controversial influence on "tafsir" (commentaries of the Quran). The earliest commentary of the Quran known as Tafsir Ibn Abbas is sometimes attributed to the companion Ibn Abbas.

The hadith were used in forming the basis of "Sharia" (the religious law system forming part of the Islamic tradition), and "fiqh" (Islamic jurisprudence). The hadith are at the root of why there is no single "fiqh" system, but rather a collection of parallel systems within Islam.

Much of early Islamic history available today is also based on the hadith, although it has been challenged for its lack of basis in primary source material and the internal contradictions of the secondary material available.

 Hadith may be "hadith qudsi" (sacred hadith) — which some Muslims regard as the words of God (Arabic: Allah) — or "hadith sharif" (noble hadith), which are Muhammad's own utterances.

According to as-Sayyid ash-Sharif al-Jurjani, the hadith qudsi differ from the Quran in that the former are "expressed in Muhammad's words", whereas the latter are the "direct words of God". A "hadith qudsi" need not be a "sahih" (sound hadith), but may be "da‘if" or even "mawdu‘".

An example of a "hadith qudsi" is the hadith of Abu Hurairah who said that Muhammad said:
When God decreed the Creation He pledged Himself by writing in His book which is laid down with Him: My mercy prevails over My wrath.

In the Shia school of thought, there are two fundamental viewpoints of Hadith: The Akhbari view and the Usuli view. The Usuli scholars stress the importance of scientific examination of Ahadith using ijtihad while the Akhbari scholars take all Ahadith from the four Shia books as authentic

The two major aspects of a hadith are the text of the report (the "matn"), which contains the actual narrative, and the chain of narrators (the "isnad"), which documents the route by which the report has been transmitted. The isnad was an effort to document that a hadith had actually come from Muhammad, and Muslim scholars from the eighth century until today have never ceased repeating the mantra "The isnad is part of the religion — if not for the isnad, whoever wanted could say whatever they wanted." The "isnad" means literally 'support', and it is so named due to the reliance of the hadith specialists upon it in determining the authenticity or weakness of a hadith. The "isnad" consists of a chronological list of the narrators, each mentioning the one from whom they heard the hadith, until mentioning the originator of the "matn" along with the "matn" itself.

The first people to hear hadith were the companions who preserved it and then conveyed it to those after them. Then the generation following them received it, thus conveying it to those after them and so on. So a companion would say, "I heard the Prophet say such and such." The Follower would then say, "I heard a companion say, 'I heard the Prophet.'" The one after him would then say, "I heard someone say, 'I heard a Companion say, 'I heard the Prophet..."" and so on.

Different branches of Islam refer to different collections of hadith, though the same incident may be found in hadith in different collections:

In general, the difference between Shi'a and Sunni collections is that Shia give preference to ahadith credited to the Prophet's family and close associates ("Ahl al-Bayt"), while Sunnis do not consider family lineage in evaluating aHadith and Sunnah narrated by any of twelve thousand companions of Muhammad.

Traditions of the life of Muhammad and the early history of Islam were passed down mostly orally for more than a hundred years after Muhammad's death in AD 632. Muslim historians say that Caliph Uthman ibn Affan (the third khalifa (caliph) of the Rashidun Caliphate, or third successor of Muhammad, who had formerly been Muhammad's secretary), is generally believed to urge Muslims to record the hadith just as Muhammad suggested to some of his followers to write down his words and actions.

Uthman's labours were cut short by his assassination, at the hands of aggrieved soldiers, in 656. No sources survive directly from this period so we are dependent on what later writers tell us about this period.

According to British historian of Arab world Alfred Guillaume, it is "certain" that "several small collections" of hadith were "assembled in Umayyad times."

In Islamic law, the use of hadith as now understood (hadith of Muhammad with documentation, isnads, etc.) came gradually. According to scholars such as Joseph Schacht, Ignaz Goldziher, and Daniel W. Brown, early schools of Islamic jurisprudence used rulings of the Prophet’s Companions, the rulings of the Caliphs, and practices that “had gained general acceptance among the jurists of that school”. On his deathbed, Caliph Umar instructed Muslims to seek guidance from the Qur’an, the early Muslims ("muhajirun") who emigrated to Medina with Muhammad, the Medina residents who welcomed and supported the "muhajirun" (the "ansar"), the people of the desert, and the protected communities of Jews and Christians ("ahl al-dhimma"). But did not mention Muhammad

According to scholars Harald Motzki and Daniel W. Brown. The earliest Islamic legal reasonings that have come down to us were "virtually hadith-free", but gradually, over the course of second century A.H. "the infiltration and incorporation of Prophetic ahadith into Islamic jurisprudence" took place.

It was Abū ʿAbdullāh Muhammad ibn Idrīs al-Shāfiʿī (150-204 AH), known as al-Shafi'i who emphasized the final authority of a hadith of Muhammad, so that even the Qur'an was "to be interpreted in the light of traditions (i.e. hadith), and not vice versa." While traditionally the Quran is considered above the Sunna in authority, Al-Shafi'i "forcefully argued" that the sunna stands "on equal footing with the Quran", (according to scholar Daniel Brown) for (as Al-Shafi'i put it) “the command of the Prophet is the command of God.” 

In 851 the rationalist Mu`tazila school of thought fell from favor in the Abbasid Caliphate. The Mu`tazila, for whom the "judge of truth ... was human reason," had clashed with traditionists who looked to the literal meaning of the Quran and hadith for truth. While the Quran had been officially compiled and approved, hadiths had not. 
One result was the number of hadiths began "multiplying in suspiciously direct correlation to their utility" to the quoter of the hadith (Traditionists quoted hadith warning against listening to human opinion instead of Sharia; Hanafites quoted a hadith stating that "In my community there will rise a man called Abu Hanifa [the Hanafite founder] who will be its guiding light". In fact one agreed upon hadith warned that, "There will be forgers, liars who will bring you hadiths which neither you nor your forefathers have heard, Beware of them." In addition the number of hadith grew enormously. While Malik ibn Anas had attributed just 1720 statements or deeds to the Muhammad, it was no longer unusual to find people who had collected a hundred times that number of hadith.

Faced with a huge corpus of miscellaneous traditions supported differing views on a variety of controversial matters—some of them flatly contradicting each other—Islamic scholars of the Abbasid sought to authenticate hadith. Scholars had to decide which hadith were to be trusted as authentic and which had been invented for political or theological purposes. To do this, they used a number of techniques which Muslims now call the science of hadith.

Sunni and Shia hadith collections differ because scholars from the two traditions differ as to the reliability of the narrators and transmitters. Narrators who took the side of Abu Bakr and Umar rather than Ali, in the disputes over leadership that followed the death of Muhammad, are seen as unreliable by the Shia; narrations sourced to Ali and the family of Muhammad, and to their supporters, are preferred. Sunni scholars put trust in narrators, such as Aisha, whom Shia reject. Differences in hadith collections have contributed to differences in worship practices and shari'a law and have hardened the dividing line between the two traditions.

In the Sunni tradition, the number of such texts is somewhere between seven and thirteen thousand, but the number of "ahadith" is far greater because several "isnad" sharing the same text are each counted as individual ahadith. If, say, ten companions record a text reporting a single incident in the life of Muhammad, hadith scholars can count this as ten hadiths. So Musnad Ahmad, for example, has over 30,000 hadiths—but this count includes texts that are repeated in order to record slight variations within the text or within the chains of narrations. Identifying the narrators of the various texts, comparing their narrations of the same texts to identify both the soundest reporting of a text and the reporters who are most sound in their reporting occupied experts of hadith throughout the 2nd century. In the 3rd century of Islam (from 225/840 to about 275/889), hadith experts composed brief works recording a selection of about two- to five-thousand such texts which they felt to have been most soundly documented or most widely referred to in the Muslim scholarly community. The 4th and 5th century saw these six works being commented on quite widely. This auxiliary literature has contributed to making their study the place of departure for any serious study of hadith. In addition, Bukhari and Muslim in particular, claimed that they were collecting only the soundest of sound hadiths. These later scholars tested their claims and agreed to them, so that today, they are considered the most reliable collections of hadith. Toward the end of the 5th century, Ibn al-Qaisarani formally standardized the Sunni canon into six pivotal works, a delineation which remains to this day.

Over the centuries, several different categories of collections came into existence. Some are more general, like the "muṣannaf", the "muʿjam", and the "jāmiʿ", and some more specific, either characterized by the topics treated, like the "sunan" (restricted to legal-liturgical traditions), or by its composition, like the "arbaʿīniyyāt" (collections of forty hadiths).

Shi'a Muslims hardly ever use the six major hadith collections followed by the Sunni, as they never usually trust many of the Sunni narrators and transmitters. They have their own extensive hadith literature. The best-known hadith collections are The Four Books, which were compiled by three authors who are known as the 'Three Muhammads'. The Four Books are: "Kitab al-Kafi" by Muhammad ibn Ya'qub al-Kulayni al-Razi (329 AH), "Man la yahduruhu al-Faqih" by Muhammad ibn Babuya and "Al-Tahdhib" and "Al-Istibsar" both by Shaykh Muhammad Tusi. Shi'a clerics also make use of extensive collections and commentaries by later authors.

Unlike Sunnis, the majority of Shia do not consider any of their hadith collections to be sahih (authentic) in their entirety. Therefore, every individual hadith in a specific collection must be investigated separately to determine its authenticity. However, the Akhbari school does take all hadith from the four books as authentic.

The importance of Hadith in the Shia school of thought is well documented. This can be captured by Ali ibn Abi Talib, cousin of Muhammad, when he narrated that "Whoever of our Shia (followers) knows our Shariah and takes out the weak of our followers from the darkness of ignorance to the light of knowledge (Hadith) which we (Ahl al-Bayt) have gifted to them, he on the day of judgement will come with a crown on his head. It will shine among the people gathered on the plain of resurrection." Hassan al-Askari, a descendent of Muhammad, gave support to this narration, stating "Whoever he had taken out in the worldly life from the darkness of ignorance can hold to his light to be taken out of the darkness of the plain of resurrection to the garden (paradise). Then all those whomever he had taught in the worldly life anything of goodness, or had opened from his heart a lock of ignorance or had removed his doubts will come out."

Regarding the importance of maintaining accuracy in recording Hadith, it has been documented that Muhammad al-Baqir, the great grandson of Muhammad, has said that "Holding back in a doubtful issue is better than entering destruction. Your not narrating a Hadith is better than you narrating a Hadith in which you have not studied thoroughly. On every truth, there is a reality. Above every right thing, there is a light. Whatever agrees with the book of Allah you must take it and whatever disagrees you must leave it alone." Al-Baqir also emphasized the selfless devotion of Ahl al-Bayt to preserving the traditions of the Islamic Prophet through his conversation with Jabir ibn Abd Allah, an old companion of Muhammad. He (Al-Baqir) said, "Oh Jabir, had we spoken to you from our opinions and desires, we would be counted among those who are destroyed. We speak to you of the Ahadith which we treasure from the Messenger of Allah, Oh Allah grant compensation to Muhammad and his family worthy of their services to your cause, just as they treasure their gold and silver." Further, it has been narrated that Ja'far al-Sadiq, the son of al-Baqir, has said the following regarding hadith: "You must write it down; you will not memorize until you write it down."

The mainstream sects consider hadith to be essential supplements to, and clarifications of, the Quran, Islam's holy book, as well as for clarifying issues pertaining to Islamic jurisprudence. Ibn al-Salah, a hadith specialist, described the relationship between hadith and other aspect of the religion by saying: "It is the science most pervasive in respect to the other sciences in their various branches, in particular to jurisprudence being the most important of them." "The intended meaning of 'other sciences' here are those pertaining to religion," explains Ibn Hajar al-Asqalani, "Quranic exegesis, hadith, and jurisprudence. The science of hadith became the most pervasive due to the need displayed by each of these three sciences. The need hadith has of its science is apparent. As for Quranic exegesis, then the preferred manner of explaining the speech of God is by means of what has been accepted as a statement of Muhammad. The one looking to this is in need of distinguishing the acceptable from the unacceptable. Regarding jurisprudence, then the jurist is in need of citing as an evidence the acceptable to the exception of the later, something only possible utilizing the science of hadith."

According to Bernard Lewis, "in the early Islamic centuries there could be no better way of promoting a cause, an opinion, or a faction than to cite an appropriate action or utterance of the Prophet." To fight these forgeries, the elaborate science of hadith studies was devised. 
Hadith studies use a number of methods of evaluation developed by early Muslim scholars in determining the veracity of reports attributed to Muhammad. This is achieved by analyzing the text of the report, the scale of the report's transmission, the routes through which the report was transmitted, and the individual narrators involved in its transmission. On the basis of these criteria, various classifications were devised for hadith. The earliest comprehensive work in hadith studies was Abu Muhammad al-Ramahurmuzi's "al-Muhaddith al-Fasil", while another significant work was al-Hakim al-Naysaburi's "Ma‘rifat ‘ulum al-hadith". Ibn al-Salah's "ʻUlum al-hadith" is considered the standard classical reference on hadith studies.

By means of hadith terminology, hadith are categorized as "ṣaḥīḥ" (sound, authentic), "ḍaʿīf" (weak), or "mawḍūʿ" (fabricated). Other classifications used also include: "ḥasan" (good), which refers to an otherwise "ṣaḥīḥ" report suffering from minor deficiency, or a weak report strengthened due to numerous other corroborating reports; and "munkar" (denounced) which is a report that is rejected due to the presence of an unreliable transmitter contradicting another more reliable narrator. Both "sahīh" and "hasan" reports are considered acceptable for usage in Islamic legal discourse. Classifications of hadith may also be based upon the scale of transmission. Reports that pass through many reliable transmitters at each point in the "isnad" up until their collection and transcription are known as "mutawātir". These reports are considered the most authoritative as they pass through so many different routes that collusion between all of the transmitters becomes an impossibility. Reports not meeting this standard are known as "aahad", and are of several different types.

Another area of focus in the study of hadith is biographical analysis ("‘ilm al-rijāl", lit. "science of people"), in which details about the transmitter are scrutinized. This includes analyzing their date and place of birth; familial connections; teachers and students; religiosity; moral behaviour; literary output; their travels; as well as their date of death. Based upon these criteria, the reliability ("thiqāt") of the transmitter is assessed. Also determined is whether the individual was actually able to transmit the report, which is deduced from their contemporaneity and geographical proximity with the other transmitters in the chain. Examples of biographical dictionaries include: Abd al-Ghani al-Maqdisi's "Al-Kamal fi Asma' al-Rijal", Ibn Hajar al-Asqalani's "Tahdhīb al-Tahdhīb" and al-Dhahabi's "Tadhkirat al-huffaz".

Related article: "Goldziher on Hadīth"

The major points of intra-Muslim criticism of the Hadith literature is based in questions regarding its authenticity. However, Muslim criticism of ahadith is also based on theological and philosophical Islamic grounds of argument and critique.

With regard to clarity, Imam Ali al-Ridha has narrated that "In our Hadith there are Mutashabih (unclear ones) like those in al-Quran as well as Muhkam (clear ones) like those of al-Quran. You must refer the unclear ones to the clear ones.”.

Muslim scholars have a long history of questioning the Hadith literature throughout Islamic history. Western academics also became active in the field later on.






</doc>
<doc id="13755" url="https://en.wikipedia.org/wiki?curid=13755" title="Hull (watercraft)">
Hull (watercraft)

A hull is the watertight body of a ship or boat. The hull may open at the top (such as a dinghy), or it may be fully or partially covered with a deck. Atop the deck may be a deckhouse and other superstructures, such as a funnel, derrick, or mast. The line where the hull meets the water surface is called the waterline.

There is a wide variety of hull types that are chosen for suitability for different usages, the hull shape being dependent upon the needs of the design. Shapes range from a nearly perfect box in the case of scow barges, to a needle-sharp surface of revolution in the case of a racing multihull sailboat. The shape is chosen to strike a balance between cost, hydrostatic considerations (accommodation, load carrying and stability), hydrodynamics (speed, power requirements, and motion and behavior in a seaway) and special considerations for the ship's role, such as the rounded bow of an icebreaker or the flat bottom of a landing craft.

In a typical modern steel ship, the hull will have watertight decks, and major transverse members called bulkheads. There may also be intermediate members such as girders, stringers and webs, and minor members called ordinary transverse frames, frames, or longitudinals, depending on the structural arrangement. The uppermost continuous deck may be called the "upper deck", "weather deck", "spar deck", "main deck", or simply "deck". The particular name given depends on the context—the type of ship or boat, the arrangement, or even where it sails.

In a typical wooden sailboat, the hull is constructed of wooden planking, supported by transverse frames (often referred to as ribs) and bulkheads, which are further tied together by longitudinal stringers or ceiling. Often but not always there is a centerline longitudinal member called a keel. In fiberglass or composite hulls, the structure may resemble wooden or steel vessels to some extent, or be of a monocoque arrangement. In many cases, composite hulls are built by sandwiching thin fiber-reinforced skins over a lightweight but reasonably rigid core of foam, balsa wood, impregnated paper honeycomb or other material.

Perhaps the earliest proper hulls were built by the Ancient Egyptians, who by 3000 BC knew how to assemble wooden planks into ahull.

"See also: Hull (ship)"

Hulls come in many varieties and can have composite shape, (e.g., a fine entry forward and inverted bell shape aft), but are grouped primarily as follows:


At present, the most widely used form is the round bilge hull.

In the inverted bell shape of the hull, with a smaller payload the waterline cross-section is less, hence the resistance is less and the speed is higher. With a higher payload the outward bend provides smoother performance in waves. As such, the inverted bell shape is a popular form used with planing hulls.
A chined hull consists of straight, smooth, tall, long, or short plates, timbers or sheets of ply, which are set at an angle to each other when viewed in transverse section. The traditional chined hull is a simple hull shape because it works with only straight planks bent into a curve. These boards are often bent lengthwise. Plywood chined boats made of 8'x4' sheets have most bend along the long axis of the sheet. Only thin ply 3–6 mm can easily be shaped into a compound bend. Most home-made constructed boats are chined hull boats. Mass-produced chine powerboats are usually made of sprayed chop strand fibreglass over a wooden mold. The Cajun "pirogue" is an example of a craft with hard chines. Benefits of this type of hull is the low production cost and the (usually) fairly flat bottom, making the boat faster at planing. Sail boats with chined hull make use of a dagger board or keel.

Chined hulls may have one of three shapes:

Each of these chine hulls has its own unique characteristics and use. The flat bottom hull has high initial stability but high drag. To counter the high drag hull forms are narrow and sometimes severely tapered at bow and stern. This leads to poor stability when heeled in a sail boat. This is often countered by using heavy interior ballast on sailing versions. They are best suited to sheltered inshore waters. Early racing power boats were fine forward and flat aft. This produced maximum lift and a smooth,fast ride in flat water but this hull form is easily unsettled in waves. The multi chine hull approximates a curved hull form. It has less drag than a flat bottom boat. Multi chines are more complex to build but produce a more seaworthy hull form. They are usually displacement hulls. V or arc bottom chine boats have a Vshape between 6and 23degrees. This is called the deadrise angle. The flatter shape of a 6degrees hull will plane with less wind or a lower horse power engine but will pound more in waves. The deep Vform (between 18and 23degrees) is only suited to high power planing boats. They require more powerful engines to lift the boat onto the plane but give a faster smoother ride in waves.
Displacement chined hulls have more wetted surface area, hence more drag, than an equivalent round hull form, for any given displacement.

Smooth curve hulls are hulls which use, just like the curved hulls, a sword or an attached keel.

Semi round bilge hulls are somewhat less round. The advantage of the semi-round is that it is a nice middle between the S-bottom and chined hull. Typical examples of a semi-round bilge hull can be found in the Centaur and Laser cruising dinghies.

S-bottom hulls are hulls shaped like an "s". In the s-bottom, the hull runs smooth to the keel. As there are no sharp corners in the fuselage. Boats with this hull have a fixed keel, or a "kielmidzwaard" (literally "keel with sword"). This is a short fixed keel, with a swing keel inside. Examples of cruising dinghies that use this s-shape are the Yngling and Randmeer.

"See also: Boat building"

"See also: Naval architecture"


Hull forms are defined as follows:

Use of computer-aided design has superseded paper-based methods of ship design that relied on manual calculations and lines drawing. Since the early 1990s, a variety of commercial and freeware software packages specialized for naval architecture have been developed that provide 3D drafting capabilities combined with calculation modules for hydrostatics and hydrodynamics. These may be referred to as geometric modeling systems for naval architecture.





</doc>
<doc id="13756" url="https://en.wikipedia.org/wiki?curid=13756" title="Hymn">
Hymn

A hymn is a type of song, usually religious, specifically written for the purpose of adoration or prayer, and typically addressed to a deity or deities, or to a prominent figure or personification. The word "hymn" derives from Greek ("hymnos"), which means "a song of praise". A writer of hymns is known as a hymnodist. The singing or composition of hymns is called hymnody. Collections of hymns are known as hymnals or hymn books. Hymns may or may not include instrumental accompaniment.

Although most familiar to speakers of English in the context of Christianity, hymns are also a fixture of other world religions, especially on the Indian subcontinent. Hymns also survive from antiquity, especially from Egyptian and Greek cultures. Some of the oldest surviving examples of notated music are hymns with Greek texts.

Ancient hymns include the Egyptian "Great Hymn to the Aten", composed by Pharaoh Akhenaten; the Hurrian Hymn to Nikkal; the "Vedas", a collection of hymns in the tradition of Hinduism; and the Psalms, a collection of songs from Judaism. The Western tradition of hymnody begins with the Homeric Hymns, a collection of ancient Greek hymns, the oldest of which were written in the 7th century BC, praising deities of the ancient Greek religions. Surviving from the 3rd century BC is a collection of six literary hymns () by the Alexandrian poet Callimachus.

Patristic writers began applying the term , or "hymnus" in Latin, to Christian songs of praise, and frequently used the word as a synonym for "psalm".

Originally modeled on the Book of Psalms and other poetic passages (commonly referred to as "canticles") in the Scriptures, Christian hymns are generally directed as praise to the Christian God. Many refer to Jesus Christ either directly or indirectly.

Since the earliest times, Christians have sung "psalms and hymns and spiritual songs", both in private devotions and in corporate worship (; ; ; ; ; ; ; cf. ; ).

Non-scriptural hymns (i.e. not psalms or canticles) from the Early Church still sung today include 'Phos Hilaron', 'Sub tuum praesidium', and 'Te Deum'.

One definition of a hymn is "...a lyric poem, reverently and devotionally conceived, which is designed to be sung and which expresses the worshipper's attitude toward God or God's purposes in human life. It should be simple and metrical in form, genuinely emotional, poetic and literary in style, spiritual in quality, and in its ideas so direct and so immediately apparent as to unify a congregation while singing it."

Christian hymns are often written with special or seasonal themes and these are used on holy days such as Christmas, Easter and the Feast of All Saints, or during particular seasons such as Advent and Lent. Others are used to encourage reverence for the Bible or to celebrate Christian practices such as the eucharist or baptism. Some hymns praise or address individual saints, particularly the Blessed Virgin Mary; such hymns are particularly prevalent in Catholicism, Eastern Orthodoxy and to some extent High Church Anglicanism.

A writer of hymns is known as a hymnodist, and the practice of singing hymns is called "hymnody"; the same word is used for the collectivity of hymns belonging to a particular denomination or period (e.g. "nineteenth century Methodist hymnody" would mean the body of hymns written and/or used by Methodists in the 19th century). A collection of hymns is called a "hymnal" or "hymnary". These may or may not include music. A student of hymnody is called a "hymnologist", and the scholarly study of hymns, hymnists and hymnody is hymnology. The music to which a hymn may be sung is a hymn tune.

In many Evangelical churches, traditional songs are classified as hymns while more contemporary worship songs are not considered hymns. The reason for this distinction is unclear, but according to some it is due to the radical shift of style and devotional thinking that began with the Jesus movement and Jesus music. Of note, in recent years, Christian traditional hymns "have" seen a revival in some churches, usually more Reformed or Calvinistic in nature, as modern hymn writers such as Keith and Kristyn Getty and Sovereign Grace Music have reset old lyrics to new melodies, revised old hymns and republished them, or simply written a song in accordance with Christian hymn standards such as the hymn, "In Christ Alone".

In ancient and medieval times, string instruments such as the harp, lyre and lute were used with psalms and hymns.

Since there is a lack of musical notation in early writings, the actual musical forms in the early church can only be surmised. During the Middle Ages a rich hymnody developed in the form of Gregorian chant or plainsong. This type was sung in unison, in one of eight church modes, and most often by monastic choirs. While they were written originally in Latin, many have been translated; a familiar example is the 4th century "Of the Father's Heart Begotten" sung to the 11th century plainsong "Divinum Mysterium".

Later hymnody in the Western church introduced four-part vocal harmony as the norm, adopting major and minor keys, and came to be led by organ and choir. It shares many elements with classical music.

Today, except for choirs, more musically inclined congregations and "a cappella" congregations, hymns are typically sung in unison. In some cases complementary full settings for organ are also published, in others organists and other accompanists are expected to transcribe the four-part vocal score for their instrument of choice.

To illustrate Protestant usage, in the traditional services and liturgies of the Methodist churches, which are based upon Anglican practice, hymns are sung (often accompanied by an organ) during the processional to the altar, during the receiving of communion, during the recessional, and sometimes at other points during the service. These hymns can be found in a common book such as the United Methodist Hymnal. The Doxology is also sung after the tithes and offerings are brought up to the altar.

Contemporary Christian worship, as often found in Evangelicalism and Pentecostalism, may include the use of contemporary worship music played with electric guitars and the drum kit, sharing many elements with rock music.

Other groups of Christians have historically excluded instrumental accompaniment, citing the absence of instruments in worship by the church in the first several centuries of its existence, and adhere to an unaccompanied "a cappella" congregational singing of hymns. These groups include the 'Brethren' (often both 'Open' and 'Exclusive'), the Churches of Christ, Mennonites, Primitive Baptists, and certain Reformed churches, although during the last century or so, several of these, such as the Free Church of Scotland have abandoned this stance.

Eastern Christianity (the Eastern Orthodox, Oriental Orthodox and Eastern Catholic churches) has a variety of ancient hymnographical traditions.

Byzantine chant is almost always a cappella, and instrumental accompaniment is rare. It is used to chant all forms of liturgical worship.

Instruments are common in Oriental traditions. The Coptic tradition which makes use of the cymbals and the Triangle (musical instrument). The Indian Orthodox (Malankara Orthodox Syrian Church) use of the organ. The Tewahedo Churches use drums, cymbals and other instruments on certain occasions.

Thomas Aquinas, in the introduction to his commentary on the Psalms, defined the Christian hymn thus: ""Hymnus est laus Dei cum cantico; canticum autem exultatio mentis de aeternis habita, prorumpens in vocem"." ("A hymn is the praise of God with song; a song is the exultation of the mind dwelling on eternal things, bursting forth in the voice.")

The Protestant Reformation resulted in two conflicting attitudes towards hymns. One approach, the regulative principle of worship, favoured by many Zwinglians, Calvinists and some radical reformers, considered anything that was not directly authorised by the Bible to be a novel and Catholic introduction to worship, which was to be rejected. All hymns that were not direct quotations from the Bible fell into this category. Such hymns were banned, along with any form of instrumental musical accompaniment, and organs were removed from churches. Instead of hymns, biblical psalms were chanted, most often without accompaniment, to very basic melodies. This was known as exclusive psalmody. Examples of this may still be found in various places, including in some of the Presbyterian churches of western Scotland.
The other Reformation approach, the normative principle of worship, produced a burst of hymn writing and congregational singing. Martin Luther is notable not only as a reformer, but as the author of many hymns including "Ein feste Burg ist unser Gott" ("A Mighty Fortress Is Our God"), which is sung today even by Catholics, and "Gelobet seist du, Jesu Christ" ("Praise be to You, Jesus Christ") for Christmas. Luther and his followers often used their hymns, or chorales, to teach tenets of the faith to worshipers. The first Protestant hymnal was published in Bohemia in 1532 by the Unitas Fratrum. Count Zinzendorf, the Lutheran leader of the Moravian Church in the 18th century wrote some 2,000 hymns. The earlier English writers tended to paraphrase biblical texts, particularly Psalms; Isaac Watts followed this tradition, but is also credited as having written the first English hymn which was not a direct paraphrase of Scripture.
Watts (1674–1748), whose father was an Elder of a dissenter congregation, complained at age 16, that when allowed only psalms to sing, the faithful could not even sing about their Lord, Christ Jesus. His father invited him to see what he could do about it; the result was Watts' first hymn, "Behold the glories of the Lamb".
Found in few hymnals today, the hymn has eight stanzas in common meter and is based on Revelation 5:6, 8, 9, 10, 12.

Relying heavily on Scripture, Watts wrote metered texts based on New Testament passages that brought the Christian faith into the songs of the church. Isaac Watts has been called "the father of English hymnody", but Erik Routley sees him more as "the liberator of English hymnody", because his hymns, and hymns like them, moved worshipers beyond singing only Old Testament psalms, inspiring congregations and revitalizing worship.

Later writers took even more freedom, some even including allegory and metaphor in their texts.

Charles Wesley's hymns spread Methodist theology, not only within Methodism, but in most Protestant churches. He developed a new focus: expressing one's personal feelings in the relationship with God as well as the simple worship seen in older hymns. Wesley wrote:

Wesley's contribution, along with the Second Great Awakening in America led to a new style called gospel, and a new explosion of sacred music writing with Fanny Crosby, Lina Sandell, Philip Bliss, Ira D. Sankey, and others who produced testimonial music for revivals, camp meetings, and evangelistic crusades. The tune style or form is technically designated "gospel songs" as distinct from hymns. Gospel songs generally include a refrain (or chorus) and usually (though not always) a faster tempo than the hymns. As examples of the distinction, "Amazing Grace" is a hymn (no refrain), but "How Great Thou Art" is a gospel song. During the 19th century, the gospel-song genre spread rapidly in Protestantism and to a lesser but still definite extent, in Roman Catholicism; the gospel-song genre is unknown in the worship "per se" by Eastern Orthodox churches, which rely exclusively on traditional chants (a type of hymn).

The Methodist Revival of the 18th century created an explosion of hymn-writing in Welsh, which continued into the first half of the 19th century. The most prominent names among Welsh hymn-writers are William Williams Pantycelyn and Ann Griffiths. The second half of the 19th century witnessed an explosion of hymn tune composition and congregational four-part singing in Wales.

Along with the more classical sacred music of composers ranging from Mozart to Monteverdi, the Catholic Church continued to produce many popular hymns such as Lead, Kindly Light, Silent Night, O Sacrament Divine and Faith of our Fathers.

Many churches today use contemporary worship music which includes a range of styles often influenced by popular music. This often leads to some conflict between older and younger congregants (see contemporary worship). This is not new; the Christian pop music style began in the late 1960s and became very popular during the 1970s, as young hymnists sought ways in which to make the music of their religion relevant for their generation.

This long tradition has resulted in a wide variety of hymns. Some modern churches include within hymnody the traditional hymn (usually describing God), contemporary worship music (often directed to God) and gospel music (expressions of one's personal experience of God). This distinction is not perfectly clear; and purists remove the second two types from the classification as hymns. It is a matter of debate, even sometimes within a single congregation, often between revivalist and traditionalist movements.

African-Americans developed a rich hymnody from spirituals during times of slavery to the modern, lively black gospel style. The first influences of African American Culture into hymns came from Slave Songs of the United States a collection of slave hymns compiled by William Francis Allen who had difficulty pinning them down from the oral tradition, and though he succeeded, he points out the awe inspiring effect of the hymns when sung in by their originators.

Hymn writing, composition, performance and the publishing of Christian hymnals were prolific in the 19th-century and were often linked to the abolitionist movement by many hymn writers. Surprisingly, Stephen Foster wrote a number of hymns that were used during church services during this era of publishing.

Thomas Symmes spread throughout churches a new idea of how to sing hymns, in which anyone could sing a hymn any way they felt led to; this idea was opposed by the views of Symmes' colleagues who felt it was "like Five Hundred different Tunes roared out at the same time". William Billings, a singing school teacher, created the first tune book with only American born compositions. Within his books, Billings did not put as much emphasis on "common measure" which was the typical way hymns were sung, but he attempted "to have a Sufficiency in each measure". Boston's Handel and Haydn Society aimed at raising the level of church music in America, publishing their "Collection of Church Music". In the late 19th century Ira D. Sankey and Dwight L. Moody developed the relatively new subcategory of gospel hymns.

Earlier in the 19th century, the use of musical notation, especially shape notes, exploded in America, and professional singing masters went from town to town teaching the population how to sing from sight, instead of the more common lining out that had been used before that. During this period hundreds of tune books were published, including B.F. White's "Sacred Harp", and earlier works like the "Missouri Harmony", "Kentucky Harmony", "Hesperian Harp", D.H. Mansfield's "The American Vocalist", "The Social Harp", the "Southern Harmony", William Walker's "Christian Harmony", Jeremiah Ingalls' "Christian Harmony", and literally many dozens of others. Shape notes were important in the spread of (then) more modern singing styles, with tenor-led 4-part harmony (based on older English West Gallery music), fuging sections, anthems and other more complex features. During this period, hymns were incredibly popular in the United States, and one or more of the above-mentioned tunebooks could be found in almost every household. It isn't uncommon to hear accounts of young people and teenagers gathering together to spend an afternoon singing hymns and anthems from tune books, which was considered great fun, and there are surviving accounts of Abraham Lincoln and his sweetheart singing together from the "Missouri Harmony" during his youth. 

By the 1860s musical reformers like Lowell Mason (the so-called "better music boys") were actively campaigning for the introduction of more "refined" and modern singing styles, and eventually these American tune books were replaced in many churches, starting in the Northeast and urban areas, and spreading out into the countryside as people adopted the gentler, more soothing tones of Victorian hymnody, and even adopted dedicated, trained choirs to do their church's singing, rather than having the entire congregation participate. But in many rural areas the old traditions lived on, not in churches, but in weekly, monthly or annual conventions were people would meet to sing from their favorite tunebooks. The most popular one, and the only one that survived continuously in print, was the "Sacred Harp", which could be found in the typical rural Southern home right up until the living tradition was "re-discovered" by Alan Lomax in the 1960s (although it had been well-documented by musicologist George Pullen Jackson prior to this). Indeed, "the most common book on . Since then there has been a renaissance in "Sacred Harp singing", with annual conventions popping up in all 50 states and in a number of European countries recently, including the UK, Germany, Ireland and Poland, as well as in Australia. Today "Sacred Harp singing" is a vibrant and living tradition with thousands of enthusiastic participants all around the globe, drawn to the democratic principles of the tradition and exotic, beautiful sound of the music. Although the lyrics tend to be highly religious in nature, the tradition is largely secular, and participation if open to all who care to attend.

The meter indicates the number of syllables for the lines in each stanza of a hymn. This provides a means of marrying the hymn's text with an appropriate hymn tune for singing. In practice many hymns conform to one of a relatively small number of meters (syllable count and stress patterns). Care must be taken, however, to ensure that not only the metre of words and tune match, but also the stresses on the words in each line. Technically speaking an iambic tune, for instance, cannot be used with words of, say, trochaic metre.

The meter is often denoted by a row of figures besides the name of the tune, such as "87.87.87", which would inform the reader that each verse has six lines, and that the first line has eight syllables, the second has seven, the third line eight, etc. The meter can also be described by initials; L.M. indicates long meter, which is 88.88 (four lines, each eight syllables long); S.M. is short meter (66.86); C.M. is common metre (86.86), while D.L.M., D.S.M. and D.C.M. (the "D" stands for double) are similar to their respective single meters except that they have eight lines in a verse instead of four.

Also, if the number of syllables in one verse differ from another verse in the same hymn (e.g., the hymn "I Sing a Song of the Saints of God"), the meter is called Irregular.

The Sikh holy book, the Guru Granth Sahib Ji ( ), is a collection of hymns (Shabad) or "Gurbani" describing the qualities of God and why one should meditate on God's name. The "Guru Granth Sahib" is divided by their musical setting in different ragas into fourteen hundred and thirty pages known as "Angs" (limbs) in Sikh tradition. Guru Gobind Singh (1666–1708), the tenth guru, after adding Guru Tegh Bahadur's bani to the Adi Granth affirmed the sacred text as his successor, elevating it to "Guru Granth Sahib". The text remains the holy scripture of the Sikhs, regarded as the teachings of the Ten Gurus. The role of Guru Granth Sahib, as a source or guide of prayer, is pivotal in Sikh worship.



The links below are restricted to either material that is historical or resources that are non-denominational or inter-denominational. Denomination-specific resources are mentioned from the relevant denomination-specific articles. 


</doc>
<doc id="13758" url="https://en.wikipedia.org/wiki?curid=13758" title="History of physics">
History of physics

Physics (from the Ancient Greek φύσις "physis" meaning "nature") is the fundamental branch of science. The primary objects of study are matter and energy. Physics is, in one sense, the oldest and most basic academic pursuit; its discoveries find applications throughout the natural sciences, since matter and energy are the basic constituents of the natural world. The other sciences are generally more limited in their scope and may be considered branches that have split off from physics to become sciences in their own right. Physics today may be divided loosely into classical physics and modern physics.

Elements of what became physics were drawn primarily from the fields of astronomy, optics, and mechanics, which were methodologically united through the study of geometry. These mathematical disciplines began in antiquity with the Babylonians and with Hellenistic writers such as Archimedes and Ptolemy. Ancient philosophy, meanwhile – including what was called "physics" – focused on explaining nature through ideas such as Aristotle's four types of "cause".

The move towards a rational understanding of nature began at least since the Archaic period in Greece (650–480 BCE) with the Pre-Socratic philosophers. The philosopher Thales of Miletus (7th and 6th centuries BCE), dubbed "the Father of Science" for refusing to accept various supernatural, religious or mythological explanations for natural phenomena, proclaimed that every event had a natural cause. Thales also made advancements in 580 BCE by suggesting that water is the basic element, experimenting with the attraction between magnets and rubbed amber and formulating the first recorded cosmologies. Anaximander, famous for his proto-evolutionary theory, disputed the Thales' ideas and proposed that rather than water, a substance called "apeiron" was the building block of all matter. Around 500 BCE, Heraclitus proposed that the only basic law governing the Universe was the principle of change and that nothing remains in the same state indefinitely. This observation made him one of the first scholars in ancient physics to address the role of time in the universe, a key and sometimes contentious concept in modern and present-day physics. The early physicist Leucippus (fl. first half of the 5th century BCE) adamantly opposed the idea of direct divine intervention in the universe, proposing instead that natural phenomena had a natural cause. Leucippus and his student Democritus were the first to develop the theory of atomism, the idea that everything is composed entirely of various imperishable, indivisible elements called atoms.

During the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy slowly developed into an exciting and contentious field of study. Aristotle (, "Aristotélēs") (384 – 322 BCE), a student of Plato, promoted the concept that observation of physical phenomena could ultimately lead to the discovery of the natural laws governing them. Aristotle's writings cover physics, metaphysics, poetry, theater, music, logic, rhetoric, linguistics, politics, government, ethics, biology and zoology. He wrote the first work which refers to that line of study as "Physics" – in the 4th century BCE, Aristotle founded the system known as Aristotelian physics. He attempted to explain ideas such as motion (and gravity) with the theory of four elements. Aristotle believed that all matter was made up of aether, or some combination of four elements: earth, water, air, and fire. According to Aristotle, these four terrestrial elements are capable of inter-transformation and move toward their natural place, so a stone falls downward toward the center of the cosmos, but flames rise upward toward the circumference. Eventually, Aristotelian physics became enormously popular for many centuries in Europe, informing the scientific and scholastic developments of the Middle Ages. It remained the mainstream scientific paradigm in Europe until the time of Galileo Galilei and Isaac Newton.

Early in Classical Greece, knowledge that the Earth is spherical ("round") was common. Around 240 BCE, as the result a seminal experiment, Eratosthenes (276–194 BCE) accurately estimated its circumference. In contrast to Aristotle's geocentric views, Aristarchus of Samos (; c.310 – c.230 BCE) presented an explicit argument for a heliocentric model of the Solar system, i.e. for placing the Sun, not the Earth, at its centre. Seleucus of Seleucia, a follower of Aristarchus' heliocentric theory, stated that the Earth rotated around its own axis, which, in turn, revolved around the Sun. Though the arguments he used were lost, Plutarch stated that Seleucus was the first to prove the heliocentric system through reasoning.

In the 3rd century BCE, the Greek mathematician Archimedes of Syracuse ( (287–212 BCE) – generally considered to be the greatest mathematician of antiquity and one of the greatest of all time – laid the foundations of hydrostatics, statics and calculated the underlying mathematics of the lever. A leading scientist of classical antiquity, Archimedes also developed elaborate systems of pulleys to move large objects with a minimum of effort. The Archimedes' screw underpins modern hydroengineering, and his machines of war helped to hold back the armies of Rome in the First Punic War. Archimedes even tore apart the arguments of Aristotle and his metaphysics, pointing out that it was impossible to separate mathematics and nature and proved it by converting mathematical theories into practical inventions. Furthermore, in his work "On Floating Bodies", around 250 BCE, Archimedes developed the law of buoyancy, also known as Archimedes' principle. In mathematics, Archimedes used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers. He also developed the principles of equilibrium states and centers of gravity, ideas that would influence the well known scholars, Galileo, and Newton.

Hipparchus (190–120 BCE), focusing on astronomy and mathematics, used sophisticated geometrical techniques to map the motion of the stars and planets, even predicting the times that Solar eclipses would happen. In addition, he added calculations of the distance of the Sun and Moon from the Earth, based upon his improvements to the observational instruments used at that time. Another of the most famous of the early physicists was Ptolemy (90–168 CE), one of the leading minds during the time of the Roman Empire. Ptolemy was the author of several scientific treatises, at least three of which were of continuing importance to later Islamic and European science. The first is the astronomical treatise now known as the "Almagest" (in Greek, Ἡ Μεγάλη Σύνταξις, "The Great Treatise", originally Μαθηματικὴ Σύνταξις, "Mathematical Treatise"). The second is the "Geography", which is a thorough discussion of the geographic knowledge of the Greco-Roman world.

Much of the accumulated knowledge of the ancient world was lost. Even of the works of the better known thinkers, few fragments survived. Although he wrote at least fourteen books, almost nothing of Hipparchus' direct work survived. Of the 150 reputed Aristotelian works, only 30 exist, and some of those are "little more than lecture notes".

Important physical and mathematical traditions also existed in ancient Chinese and Indian sciences.

In Indian philosophy, Maharishi Kanada was the first to systematically develop a theory of atomism around 200 BCE though some authors have allotted him an earlier era in the 6th century BCE. It was further elaborated by the Buddhist atomists Dharmakirti and Dignāga during the 1st millennium CE. Pakudha Kaccayana, a 6th-century BCE Indian philosopher and contemporary of Gautama Buddha, had also propounded ideas about the atomic constitution of the material world. These philosophers believed that other elements (except ether) were physically palpable and hence comprised minuscule particles of matter. The last minuscule particle of matter that could not be subdivided further was termed Parmanu. These philosophers considered the atom to be indestructible and hence eternal. The Buddhists thought atoms to be minute objects unable to be seen to the naked eye that come into being and vanish in an instant. The Vaisheshika school of philosophers believed that an atom was a mere point in space. It was also first to depict relations between motion and force applied. Indian theories about the atom are greatly abstract and enmeshed in philosophy as they were based on logic and not on personal experience or experimentation. In Indian astronomy, Aryabhata's "Aryabhatiya" (499 CE) proposed the Earth's rotation, while Nilakantha Somayaji (1444–1544) of the Kerala school of astronomy and mathematics proposed a semi-heliocentric model resembling the Tychonic system.

The study of magnetism in Ancient China dates back to the 4th century BCE. (in the "Book of the Devil Valley Master"), A main contributor to this field was Shen Kuo (1031–1095), a polymath and statesman who was the first to describe the magnetic-needle compass used for navigation, as well as establishing the concept of true north. In optics, Shen Kuo independently developed a camera obscura.

In the 7th to 15th centuries, scientific progress occurred in the Muslim world. Many classic works in Indian, Assyrian, Sassanian (Persian) and Greek, including the works of Aristotle, were translated into Arabic. Ibn Sīnā (980–1037), known as "Avicenna", was a polymath from Bukhara (in present-day Uzbekistan) responsible for important contributions to physics, optics, philosophy and medicine. He is most famous for writing "The Canon of Medicine", a text that was used to teach student doctors in Europe until the 1600s.

Important contributions were made by Ibn al-Haytham (965–1040), an Arab scientist, considered as one of the founders of modern optics. Ptolemy and Aristotle theorised that light either shone from the eye to illuminate objects or that light emanated from objects themselves, whereas al-Haytham (known by the Latin name Alhazen) suggested that light travels to the eye in rays from different points on an object. The works of Ibn al-Haytham and Abū Rayhān Bīrūnī, a Persian scientist, eventually passed on to Western Europe where they were studied by scholars such as Roger Bacon and Witelo. Omar Khayyám (1048–1131), a Persian scientist, calculated the length of a solar year and was only out by a fraction of a second when compared to our modern day calculations. He used this to compose a calendar considered more accurate than the Gregorian calendar that came along 500 years later. He is classified as one of the world's first great science communicators, said, for example to have convinced a Sufi theologian that the world turns on an axis.

Nasir al-Din al-Tusi (1201–1274), a Persian astronomer and mathematician who died in Baghdad, authored the "Treasury of Astronomy", a remarkably accurate table of planetary movements that reformed the existing planetary model of Roman astronomer Ptolemy by describing a uniform circular motion of all planets in their orbits. This work led to the later discovery, by one of his students, that planets actually have an elliptical orbit. Copernicus later drew heavily on the work of al-Din al-Tusi and his students, but without acknowledgment. The gradual chipping away of the Ptolemaic system paved the way for the revolutionary idea that the Earth actually orbited the Sun (heliocentrism).

Awareness of ancient works re-entered the West through translations from Arabic to Latin. Their re-introduction, combined with Judeo-Islamic theological commentaries, had a great influence on Medieval philosophers such as Thomas Aquinas. Scholastic European scholars, who sought to reconcile the philosophy of the ancient classical philosophers with Christian theology, proclaimed Aristotle the greatest thinker of the ancient world. In cases where they didn't directly contradict the Bible, Aristotelian physics became the foundation for the physical explanations of the European Churches. Quantification became a core element of medieval physics.

Based on Aristotelian physics, Scholastic physics described things as moving according to their essential nature. Celestial objects were described as moving in circles, because perfect circular motion was considered an innate property of objects that existed in the uncorrupted realm of the celestial spheres. The theory of impetus, the ancestor to the concepts of inertia and momentum, was developed along similar lines by medieval philosophers such as John Philoponus and Jean Buridan. Motions below the lunar sphere were seen as imperfect, and thus could not be expected to exhibit consistent motion. More idealized motion in the "sublunary" realm could only be achieved through artifice, and prior to the 17th century, many did not view artificial experiments as a valid means of learning about the natural world. Physical explanations in the sublunary realm revolved around tendencies. Stones contained the element earth, and earthly objects tended to move in a straight line toward the centre of the earth (and the universe in the Aristotelian geocentric view) unless otherwise prevented from doing so.

During the 16th and 17th centuries, a large advancement of scientific progress known as the Scientific revolution took place in Europe. Dissatisfaction with older philosophical approaches had begun earlier and had produced other changes in society, such as the Protestant Reformation, but the revolution in science began when natural philosophers began to mount a sustained attack on the Scholastic philosophical programme and supposed that mathematical descriptive schemes adopted from such fields as mechanics and astronomy could actually yield universally valid characterizations of motion and other concepts.

A breakthrough in astronomy was made by Polish astronomer Nicolaus Copernicus (1473–1543) when, in 1543, he gave strong arguments for the heliocentric model of the Solar system, ostensibly as a means to render tables charting planetary motion more accurate and to simplify their production. In heliocentric models of the Solar system, the Earth orbits the Sun along with other bodies in Earth's galaxy, a contradiction according to the Greek-Egyptian astronomer Ptolemy (2nd century CE; see above), whose system placed the Earth at the center of the Universe and had been accepted for over 1,400 years. The Greek astronomer Aristarchus of Samos (c.310 – c.230 BCE) had suggested that the Earth revolves around the Sun, but Copernicus' reasoning led to lasting general acceptance of this "revolutionary" idea. Copernicus' book presenting the theory ("De revolutionibus orbium coelestium", "On the Revolutions of the Celestial Spheres") was published just before his death in 1543 and, as it is now generally considered to mark the beginning of modern astronomy, is also considered to mark the beginning of the Scientific revolution. Copernicus' new perspective, along with the accurate observations made by Tycho Brahe, enabled German astronomer Johannes Kepler (1571–1630) to formulate his laws regarding planetary motion that remain in use today.

The Italian mathematician, astronomer, and physicist Galileo Galilei (1564–1642) was the central figure in the Scientific revolution and famous for his support for Copernicanism, his astronomical discoveries, empirical experiments and his improvement of the telescope. As a mathematician, Galileo's role in the university culture of his era was subordinated to the three major topics of study: law, medicine, and theology (which was closely allied to philosophy). Galileo, however, felt that the descriptive content of the technical disciplines warranted philosophical interest, particularly because mathematical analysis of astronomical observations – notably, Copernicus' analysis of the relative motions of the Sun, Earth, Moon, and planets – indicated that philosophers' statements about the nature of the universe could be shown to be in error. Galileo also performed mechanical experiments, insisting that motion itself – regardless of whether it was produced "naturally" or "artificially" (i.e. deliberately) – had universally consistent characteristics that could be described mathematically.

Galileo's early studies at the University of Pisa were in medicine, but he was soon drawn to mathematics and physics. At 19, he discovered (and, subsequently, verified) the isochronal nature of the pendulum when, using his pulse, he timed the oscillations of a swinging lamp in Pisa's cathedral and found that it remained the same for each swing regardless of the swing's amplitude. He soon became known through his invention of a hydrostatic balance and for his treatise on the center of gravity of solid bodies. While teaching at the University of Pisa (1589–92), he initiated his experiments concerning the laws of bodies in motion that brought results so contradictory to the accepted teachings of Aristotle that strong antagonism was aroused. He found that bodies do not fall with velocities proportional to their weights. The famous story in which Galileo is said to have dropped weights from the Leaning Tower of Pisa is apocryphal, but he did find that the path of a projectile is a parabola and is credited with conclusions that anticipated Newton's laws of motion (e.g. the notion of inertia). Among these is what is now called Galilean relativity, the first precisely formulated statement about properties of space and time outside three-dimensional geometry.

Galileo has been called the "father of modern observational astronomy", the "father of modern physics", the "father of science", and "the father of modern science". According to Stephen Hawking, "Galileo, perhaps more than any other single person, was responsible for the birth of modern science." As religious orthodoxy decreed a geocentric or Tychonic understanding of the Solar system, Galileo's support for heliocentrism provoked controversy and he was tried by the Inquisition. Found "vehemently suspect of heresy", he was forced to recant and spent the rest of his life under house arrest.

The contributions that Galileo made to observational astronomy include the telescopic confirmation of the phases of Venus; his discovery, in 1609, of Jupiter's four largest moons (subsequently given the collective name of the "Galilean moons"); and the observation and analysis of sunspots. Galileo also pursued applied science and technology, inventing, among other instruments, a military compass. His discovery of the Jovian moons was published in 1610 and enabled him to obtain the position of mathematician and philosopher to the Medici court. As such, he was expected to engage in debates with philosophers in the Aristotelian tradition and received a large audience for his own publications such as the "Discourses and Mathematical Demonstrations Concerning Two New Sciences" (published abroad following his arrest for the publication of "Dialogue Concerning the Two Chief World Systems") and "The Assayer". Galileo's interest in experimenting with and formulating mathematical descriptions of motion established experimentation as an integral part of natural philosophy. This tradition, combining with the non-mathematical emphasis on the collection of "experimental histories" by philosophical reformists such as William Gilbert and Francis Bacon, drew a significant following in the years leading up to and following Galileo's death, including Evangelista Torricelli and the participants in the Accademia del Cimento in Italy; Marin Mersenne and Blaise Pascal in France; Christiaan Huygens in the Netherlands; and Robert Hooke and Robert Boyle in England.

The French philosopher René Descartes (1596–1650) was well-connected to, and influential within, the experimental philosophy networks of the day. Descartes had a more ambitious agenda, however, which was geared toward replacing the Scholastic philosophical tradition altogether. Questioning the reality interpreted through the senses, Descartes sought to re-establish philosophical explanatory schemes by reducing all perceived phenomena to being attributable to the motion of an invisible sea of "corpuscles". (Notably, he reserved human thought and God from his scheme, holding these to be separate from the physical universe). In proposing this philosophical framework, Descartes supposed that different kinds of motion, such as that of planets versus that of terrestrial objects, were not fundamentally different, but were merely different manifestations of an endless chain of corpuscular motions obeying universal principles. Particularly influential were his explanations for circular astronomical motions in terms of the vortex motion of corpuscles in space (Descartes argued, in accord with the beliefs, if not the methods, of the Scholastics, that a vacuum could not exist), and his explanation of gravity in terms of corpuscles pushing objects downward.

Descartes, like Galileo, was convinced of the importance of mathematical explanation, and he and his followers were key figures in the development of mathematics and geometry in the 17th century. Cartesian mathematical descriptions of motion held that all mathematical formulations had to be justifiable in terms of direct physical action, a position held by Huygens and the German philosopher Gottfried Leibniz, who, while following in the Cartesian tradition, developed his own philosophical alternative to Scholasticism, which he outlined in his 1714 work, "The Monadology". Descartes has been dubbed the 'Father of Modern Philosophy', and much subsequent Western philosophy is a response to his writings, which are studied closely to this day. In particular, his "Meditations on First Philosophy" continues to be a standard text at most university philosophy departments. Descartes' influence in mathematics is equally apparent; the Cartesian coordinate system — allowing algebraic equations to be expressed as geometric shapes in a two-dimensional coordinate system — was named after him. He is credited as the father of analytical geometry, the bridge between algebra and geometry, important to the discovery of calculus and analysis.

The late 17th and early 18th centuries saw the achievements of the greatest figure of the Scientific revolution: Cambridge University physicist and mathematician Sir Isaac Newton (1642-1727), considered by many to be the greatest and most influential scientist who ever lived. Newton, a fellow of the Royal Society of England, combined his own discoveries in mechanics and astronomy to earlier ones to create a single system for describing the workings of the universe. Newton formulated three laws of motion which formulated the relationship between motion and objects and also the law of universal gravitation, the latter of which could be used to explain the behavior not only of falling bodies on the earth but also planets and other celestial bodies. To arrive at his results, Newton invented one form of an entirely new branch of mathematics: calculus (also invented independently by Gottfried Leibniz), which was to become an essential tool in much of the later development in most branches of physics. Newton's findings were set forth in his "Philosophiæ Naturalis Principia Mathematica" ("Mathematical Principles of Natural Philosophy"), the publication of which in 1687 marked the beginning of the modern period of mechanics and astronomy.

Newton was able to refute the Cartesian mechanical tradition that all motions should be explained with respect to the immediate force exerted by corpuscles. Using his three laws of motion and law of universal gravitation, Newton removed the idea that objects followed paths determined by natural shapes and instead demonstrated that not only regularly observed paths, but all the future motions of any body could be deduced mathematically based on knowledge of their existing motion, their mass, and the forces acting upon them. However, observed celestial motions did not precisely conform to a Newtonian treatment, and Newton, who was also deeply interested in theology, imagined that God intervened to ensure the continued stability of the solar system.

Newton's principles (but not his mathematical treatments) proved controversial with Continental philosophers, who found his lack of metaphysical explanation for movement and gravitation philosophically unacceptable. Beginning around 1700, a bitter rift opened between the Continental and British philosophical traditions, which were stoked by heated, ongoing, and viciously personal disputes between the followers of Newton and Leibniz concerning priority over the analytical techniques of calculus, which each had developed independently. Initially, the Cartesian and Leibnizian traditions prevailed on the Continent (leading to the dominance of the Leibnizian calculus notation everywhere except Britain). Newton himself remained privately disturbed at the lack of a philosophical understanding of gravitation while insisting in his writings that none was necessary to infer its reality. As the 18th century progressed, Continental natural philosophers increasingly accepted the Newtonians' willingness to forgo ontological metaphysical explanations for mathematically described motions.

Newton built the first functioning reflecting telescope and developed a theory of color, published in "Opticks", based on the observation that a prism decomposes white light into the many colours forming the visible spectrum. While Newton explained light as being composed of tiny particles, a rival theory of light which explained its behavior in terms of waves was presented in 1690 by Christiaan Huygens. However, the belief in the mechanistic philosophy coupled with Newton's reputation meant that the wave theory saw relatively little support until the 19th century. Newton also formulated an empirical law of cooling, studied the speed of sound, investigated power series, demonstrated the generalised binomial theorem and developed a method for approximating the roots of a function. His work on infinite series was inspired by Simon Stevin's decimals. Most importantly, Newton showed that the motions of objects on Earth and of celestial bodies are governed by the same set of natural laws, which were neither capricious nor malevolent. By demonstrating the consistency between Kepler's laws of planetary motion and his own theory of gravitation, Newton also removed the last doubts about heliocentrism. By bringing together all the ideas set forth during the Scientific revolution, Newton effectively established the foundation for modern society in mathematics and science.

Other branches of physics also received attention during the period of the Scientific revolution. William Gilbert, court physician to Queen Elizabeth I, published an important work on magnetism in 1600, describing how the earth itself behaves like a giant magnet. Robert Boyle (1627–91) studied the behavior of gases enclosed in a chamber and formulated the gas law named for him; he also contributed to physiology and to the founding of modern chemistry. Another important factor in the scientific revolution was the rise of learned societies and academies in various countries. The earliest of these were in Italy and Germany and were short-lived. More influential were the Royal Society of England (1660) and the Academy of Sciences in France (1666). The former was a private institution in London and included such scientists as John Wallis, William Brouncker, Thomas Sydenham, John Mayow, and Christopher Wren (who contributed not only to architecture but also to astronomy and anatomy); the latter, in Paris, was a government institution and included as a foreign member the Dutchman Huygens. In the 18th century, important royal academies were established at Berlin (1700) and at St. Petersburg (1724). The societies and academies provided the principal opportunities for the publication and discussion of scientific results during and after the scientific revolution. In 1690, James Bernoulli showed that the cycloid is the solution to the tautochrone problem; and the following year, in 1691, Johann Bernoulli showed that a chain freely suspended from two points will form a catenary, the curve with the lowest possible center of gravity available to any chain hung between two fixed points. He then showed, in 1696, that the cycloid is the solution to the brachistochrone problem.

A precursor of the engine was designed by the German scientist Otto von Guericke who, in 1650, designed and built the world's first vacuum pump and created the world's first ever vacuum known as the Magdeburg hemispheres experiment. He was driven to make a vacuum to disprove Aristotle's long-held supposition that 'Nature abhors a vacuum'. Shortly thereafter, Irish physicist and chemist Boyle had learned of Guericke's designs and in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed the pressure-volume correlation for a gas: "PV" = "k", where "P" is pressure, "V" is volume and "k" is a constant: this relationship is known as Boyle's Law. In that time, air was assumed to be a system of motionless particles, and not interpreted as a system of moving molecules. The concept of thermal motion came two centuries later. Therefore, Boyle's publication in 1660 speaks about a mechanical concept: the air spring. Later, after the invention of the thermometer, the property temperature could be quantified. This tool gave Gay-Lussac the opportunity to derive his law, which led shortly later to the ideal gas law. But, already before the establishment of the ideal gas law, an associate of Boyle's named Denis Papin built in 1679 a bone digester, which is a closed vessel with a tightly fitting lid that confines steam until a high pressure is generated.

Later designs implemented a steam release valve to keep the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and cylinder engine. He did not however follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time. Hence, prior to 1698 and the invention of the Savery Engine, horses were used to power pulleys, attached to buckets, which lifted water out of flooded salt mines in England. In the years to follow, more variations of steam engines were built, such as the Newcomen Engine, and later the Watt Engine. In time, these early engines would eventually be utilized in place of horses. Thus, each engine began to be associated with a certain amount of "horse power" depending upon how many horses it had replaced. The main problem with these first engines was that they were slow and clumsy, converting less than 2% of the input fuel into useful work. In other words, large quantities of coal (or wood) had to be burned to yield only a small fraction of work output. Hence the need for a new science of engine dynamics was born.

During the 18th century, the mechanics founded by Newton was developed by several scientists as more mathematicians learned calculus and elaborated upon its initial formulation. The application of mathematical analysis to problems of motion was known as rational mechanics, or mixed mathematics (and was later termed classical mechanics).

In 1714, Brook Taylor derived the fundamental frequency of a stretched vibrating string in terms of its tension and mass per unit length by solving a differential equation. The Swiss mathematician Daniel Bernoulli (1700–1782) made important mathematical studies of the behavior of gases, anticipating the kinetic theory of gases developed more than a century later, and has been referred to as the first mathematical physicist. In 1733, Daniel Bernoulli derived the fundamental frequency and harmonics of a hanging chain by solving a differential equation. In 1734, Bernoulli solved the differential equation for the vibrations of an elastic bar clamped at one end. Bernoulli's treatment of fluid dynamics and his examination of fluid flow was introduced in his 1738 work "Hydrodynamica".

Rational mechanics dealt primarily with the development of elaborate mathematical treatments of observed motions, using Newtonian principles as a basis, and emphasized improving the tractability of complex calculations and developing of legitimate means of analytical approximation. A representative contemporary textbook was published by Johann Baptiste Horvath. By the end of the century analytical treatments were rigorous enough to verify the stability of the solar system solely on the basis of Newton's laws without reference to divine intervention—even as deterministic treatments of systems as simple as the three body problem in gravitation remained intractable. In 1705, Edmond Halley predicted the periodicity of Halley's Comet, William Herschel discovered Uranus in 1781, and Henry Cavendish measured the gravitational constant and determined the mass of the Earth in 1798. In 1783, John Michell suggested that some objects might be so massive that not even light could escape from them.

In 1739, Leonhard Euler solved the ordinary differential equation for a forced harmonic oscillator and noticed the resonance phenomenon. In 1742, Colin Maclaurin discovered his uniformly rotating self-gravitating spheroids. In 1742, Benjamin Robins published his "New Principles in Gunnery", establishing the science of aerodynamics. British work, carried on by mathematicians such as Taylor and Maclaurin, fell behind Continental developments as the century progressed. Meanwhile, work flourished at scientific academies on the Continent, led by such mathematicians as Bernoulli, Euler, Lagrange, Laplace, and Legendre. In 1743, Jean le Rond d'Alembert published his "Traite de Dynamique", in which he introduced the concept of generalized forces for accelerating systems and systems with constraints, and applied the new idea of virtual work to solve dynamical problem, now known as D'Alembert's principle, as a rival to Newton's second law of motion. In 1747, Pierre Louis Maupertuis applied minimum principles to mechanics. In 1759, Euler solved the partial differential equation for the vibration of a rectangular drum. In 1764, Euler examined the partial differential equation for the vibration of a circular drum and found one of the Bessel function solutions. In 1776, John Smeaton published a paper on experiments relating power, work, momentum and kinetic energy, and supporting the conservation of energy. In 1788, Joseph Louis Lagrange presented Lagrange's equations of motion in "Mécanique Analytique", in which the whole of mechanics was organized around the principle of virtual work. In 1789, Antoine Lavoisier states the law of conservation of mass. The rational mechanics developed in the 18th century received a brilliant exposition in both Lagrange's 1788 work and the "Celestial Mechanics" (1799–1825) of Pierre-Simon Laplace.

During the 18th century, thermodynamics was developed through the theories of weightless "imponderable fluids", such as heat ("caloric"), electricity, and phlogiston (which was rapidly overthrown as a concept following Lavoisier's identification of oxygen gas late in the century). Assuming that these concepts were real fluids, their flow could be traced through a mechanical apparatus or chemical reactions. This tradition of experimentation led to the development of new kinds of experimental apparatus, such as the Leyden Jar; and new kinds of measuring instruments, such as the calorimeter, and improved versions of old ones, such as the thermometer. Experiments also produced new concepts, such as the University of Glasgow experimenter Joseph Black's notion of latent heat and Philadelphia intellectual Benjamin Franklin's characterization of electrical fluid as flowing between places of excess and deficit (a concept later reinterpreted in terms of positive and negative charges). Franklin also showed that lightning is electricity in 1752.

The accepted theory of heat in the 18th century viewed it as a kind of fluid, called caloric; although this theory was later shown to be erroneous, a number of scientists adhering to it nevertheless made important discoveries useful in developing the modern theory, including Joseph Black (1728–99) and Henry Cavendish (1731–1810). Opposed to this caloric theory, which had been developed mainly by the chemists, was the less accepted theory dating from Newton's time that heat is due to the motions of the particles of a substance. This mechanical theory gained support in 1798 from the cannon-boring experiments of Count Rumford (Benjamin Thompson), who found a direct relationship between heat and mechanical energy.

While it was recognized early in the 18th century that finding absolute theories of electrostatic and magnetic force akin to Newton's principles of motion would be an important achievement, none were forthcoming. This impossibility only slowly disappeared as experimental practice became more widespread and more refined in the early years of the 19th century in places such as the newly established Royal Institution in London. Meanwhile, the analytical methods of rational mechanics began to be applied to experimental phenomena, most influentially with the French mathematician Joseph Fourier's analytical treatment of the flow of heat, as published in 1822. Joseph Priestley proposed an electrical inverse-square law in 1767, and Charles-Augustin de Coulomb introduced the inverse-square law of electrostatics in 1798.

At the end of the century, the members of the French Academy of Sciences had attained clear dominance in the field. At the same time, the experimental tradition established by Galileo and his followers persisted. The Royal Society and the French Academy of Sciences were major centers for the performance and reporting of experimental work. Experiments in mechanics, optics, magnetism, static electricity, chemistry, and physiology were not clearly distinguished from each other during the 18th century, but significant differences in explanatory schemes and, thus, experiment design were emerging. Chemical experimenters, for instance, defied attempts to enforce a scheme of abstract Newtonian forces onto chemical affiliations, and instead focused on the isolation and classification of chemical substances and reactions.

In 1800, Alessandro Volta invented the electric battery (known of the voltaic pile) and thus improved the way electric currents could also be studied. A year later, Thomas Young demonstrated the wave nature of light—which received strong experimental support from the work of Augustin-Jean Fresnel—and the principle of interference. In 1813, Peter Ewart supported the idea of the conservation of energy in his paper "On the measure of moving force". In 1820, Hans Christian Ørsted found that a current-carrying conductor gives rise to a magnetic force surrounding it, and within a week after Ørsted's discovery reached France, André-Marie Ampère discovered that two parallel electric currents will exert forces on each other. In 1821, William Hamilton began his analysis of Hamilton's characteristic function. In 1821, Michael Faraday built an electricity-powered motor, while Georg Ohm stated his law of electrical resistance in 1826, expressing the relationship between voltage, current, and resistance in an electric circuit. A year later, botanist Robert Brown discovered Brownian motion: pollen grains in water undergoing movement resulting from their bombardment by the fast-moving atoms or molecules in the liquid. In 1829, Gaspard Coriolis introduced the terms of work (force times distance) and kinetic energy with the meanings they have today.

In 1831, Faraday (and independently Joseph Henry) discovered the reverse effect, the production of an electric potential or current through magnetism – known as electromagnetic induction; these two discoveries are the basis of the electric motor and the electric generator, respectively. In 1834, Carl Jacobi discovered his uniformly rotating self-gravitating ellipsoids (the Jacobi ellipsoid). In 1834, John Russell observed a nondecaying solitary water wave (soliton) in the Union Canal near Edinburgh and used a water tank to study the dependence of solitary water wave velocities on wave amplitude and water depth. In 1835, William Hamilton stated Hamilton's canonical equations of motion. In the same year, Gaspard Coriolis examined theoretically the mechanical efficiency of waterwheels, and deduced the Coriolis effect. In 1841, Julius Robert von Mayer, an amateur scientist, wrote a paper on the conservation of energy but his lack of academic training led to its rejection. In 1842, Christian Doppler proposed the Doppler effect. In 1847, Hermann von Helmholtz formally stated the law of conservation of energy. In 1851, Léon Foucault showed the Earth's rotation with a huge pendulum (Foucault pendulum).

There were important advances in continuum mechanics in the first half of the century, namely formulation of laws of elasticity for solids and discovery of Navier–Stokes equations for fluids.

In the 19th century, the connection between heat and mechanical energy was established quantitatively by Julius Robert von Mayer and James Prescott Joule, who measured the mechanical equivalent of heat in the 1840s. In 1849, Joule published results from his series of experiments (including the paddlewheel experiment) which show that heat is a form of energy, a fact that was accepted in the 1850s. The relation between heat and energy was important for the development of steam engines, and in 1824 the experimental and theoretical work of Sadi Carnot was published. Carnot captured some of the ideas of thermodynamics in his discussion of the efficiency of an idealized engine. Sadi Carnot's work provided a basis for the formulation of the first law of thermodynamics—a restatement of the law of conservation of energy—which was stated around 1850 by William Thomson, later known as Lord Kelvin, and Rudolf Clausius. Lord Kelvin, who had extended the concept of absolute zero from gases to all substances in 1848, drew upon the engineering theory of Lazare Carnot, Sadi Carnot, and Émile Clapeyron–as well as the experimentation of James Prescott Joule on the interchangeability of mechanical, chemical, thermal, and electrical forms of work—to formulate the first law.

Kelvin and Clausius also stated the second law of thermodynamics, which was originally formulated in terms of the fact that heat does not spontaneously flow from a colder body to a hotter. Other formulations followed quickly (for example, the second law was expounded in Thomson and Peter Guthrie Tait's influential work "Treatise on Natural Philosophy") and Kelvin in particular understood some of the law's general implications. The second Law was the idea that gases consist of molecules in motion had been discussed in some detail by Daniel Bernoulli in 1738, but had fallen out of favor, and was revived by Clausius in 1857. In 1850, Hippolyte Fizeau and Léon Foucault measured the speed of light in water and find that it is slower than in air, in support of the wave model of light. In 1852, Joule and Thomson demonstrated that a rapidly expanding gas cools, later named the Joule–Thomson effect or Joule–Kelvin effect. Hermann von Helmholtz puts forward the idea of the heat death of the universe in 1854, the same year that Clausius established the importance of "dQ/T" (Clausius's theorem) (though he did not yet name the quantity).

In 1859, James Clerk Maxwell discovered the distribution law of molecular velocities. Maxwell showed that electric and magnetic fields are propagated outward from their source at a speed equal to that of light and that light is one of several kinds of electromagnetic radiation, differing only in frequency and wavelength from the others. In 1859, Maxwell worked out the mathematics of the distribution of velocities of the molecules of a gas. The wave theory of light was widely accepted by the time of Maxwell's work on the electromagnetic field, and afterward the study of light and that of electricity and magnetism were closely related. In 1864 James Maxwell published his papers on a dynamical theory of the electromagnetic field, and stated that light is an electromagnetic phenomenon in the 1873 publication of Maxwell's "Treatise on Electricity and Magnetism". This work drew upon theoretical work by German theoreticians such as Carl Friedrich Gauss and Wilhelm Weber. The encapsulation of heat in particulate motion, and the addition of electromagnetic forces to Newtonian dynamics established an enormously robust theoretical underpinning to physical observations.

The prediction that light represented a transmission of energy in wave form through a "luminiferous ether", and the seeming confirmation of that prediction with Helmholtz student Heinrich Hertz's 1888 detection of electromagnetic radiation, was a major triumph for physical theory and raised the possibility that even more fundamental theories based on the field could soon be developed. Experimental confirmation of Maxwell's theory was provided by Hertz, who generated and detected electric waves in 1886 and verified their properties, at the same time foreshadowing their application in radio, television, and other devices. In 1887, Heinrich Hertz discovered the photoelectric effect. Research on the electromagnetic waves began soon after, with many scientists and inventors conducting experiments on their properties. In the mid to late 1890s Guglielmo Marconi developed a radio wave based wireless telegraphy system (see invention of radio).

The atomic theory of matter had been proposed again in the early 19th century by the chemist John Dalton and became one of the hypotheses of the kinetic-molecular theory of gases developed by Clausius and James Clerk Maxwell to explain the laws of thermodynamics.

The kinetic theory in turn led to a revolutionary approach to science, the statistical mechanics of Ludwig Boltzmann (1844–1906) and Josiah Willard Gibbs (1839–1903), which studies the statistics of microstates of a system and uses statistics to determine the state of a physical system. Interrelating the statistical likelihood of certain states of organization of these particles with the energy of those states, Clausius reinterpreted the dissipation of energy to be the statistical tendency of molecular configurations to pass toward increasingly likely, increasingly disorganized states (coining the term "entropy" to describe the disorganization of a state). The statistical versus absolute interpretations of the second law of thermodynamics set up a dispute that would last for several decades (producing arguments such as "Maxwell's demon"), and that would not be held to be definitively resolved until the behavior of atoms was firmly established in the early 20th century. In 1902, James Jeans found the length scale required for gravitational perturbations to grow in a static nearly homogeneous medium.

At the end of the 19th century, physics had evolved to the point at which classical mechanics could cope with highly complex problems involving macroscopic situations; thermodynamics and kinetic theory were well established; geometrical and physical optics could be understood in terms of electromagnetic waves; and the conservation laws for energy and momentum (and mass) were widely accepted. So profound were these and other developments that it was generally accepted that all the important laws of physics had been discovered and that, henceforth, research would be concerned with clearing up minor problems and particularly with improvements of method and measurement. However, around 1900 serious doubts arose about the completeness of the classical theories—the triumph of Maxwell's theories, for example, was undermined by inadequacies that had already begun to appear—and their inability to explain certain physical phenomena, such as the energy distribution in blackbody radiation and the photoelectric effect, while some of the theoretical formulations led to paradoxes when pushed to the limit. Prominent physicists such as Hendrik Lorentz, Emil Cohn, Ernst Wiechert and Wilhelm Wien believed that some modification of Maxwell's equations might provide the basis for all physical laws. These shortcomings of classical physics were never to be resolved and new ideas were required. At the beginning of the 20th century a major revolution shook the world of physics, which led to a new era, generally referred to as modern physics.

In the 19th century, experimenters began to detect unexpected forms of radiation: Wilhelm Röntgen caused a sensation with his discovery of X-rays in 1895; in 1896 Henri Becquerel discovered that certain kinds of matter emit radiation on their own accord. In 1897, J. J. Thomson discovered the electron, and new radioactive elements found by Marie and Pierre Curie raised questions about the supposedly indestructible atom and the nature of matter. Marie and Pierre coined the term "radioactivity" to describe this property of matter, and isolated the radioactive elements radium and polonium. Ernest Rutherford and Frederick Soddy identified two of Becquerel's forms of radiation with electrons and the element helium. Rutherford identified and named two types of radioactivity and in 1911 interpreted experimental evidence as showing that the atom consists of a dense, positively charged nucleus surrounded by negatively charged electrons. Classical theory, however, predicted that this structure should be unstable. Classical theory had also failed to explain successfully two other experimental results that appeared in the late 19th century. One of these was the demonstration by Albert A. Michelson and Edward W. Morley—known as the Michelson–Morley experiment—which showed there did not seem to be a preferred frame of reference, at rest with respect to the hypothetical luminiferous ether, for describing electromagnetic phenomena. Studies of radiation and radioactive decay continued to be a preeminent focus for physical and chemical research through the 1930s, when the discovery of nuclear fission opened the way to the practical exploitation of what came to be called "atomic" energy.

In 1905 a young, 26-year-old German physicist (then a Bern patent clerk) named Albert Einstein (1879–1955), showed how measurements of time and space are affected by motion between an observer and what is being observed. To say that Einstein's radical theory of relativity revolutionized science is no exaggeration. Although Einstein made many other important contributions to science, the theory of relativity alone represents one of the greatest intellectual achievements of all time. Although the concept of relativity was not introduced by Einstein, his major contribution was the recognition that the speed of light in a vacuum is constant, i.e. the same for all observers, and an absolute physical boundary for motion. This does not impact a person's day-to-day life since most objects travel at speeds much slower than light speed. For objects travelling near light speed, however, the theory of relativity shows that clocks associated with those objects will run more slowly and that the objects shorten in length according to measurements of an observer on Earth. Einstein also derived the famous equation, "E" = "mc", which expresses the equivalence of mass and energy.

Einstein argued that the speed of light was a constant in all inertial reference frames and that electromagnetic laws should remain valid independent of reference frame—assertions which rendered the ether "superfluous" to physical theory, and that held that observations of time and length varied relative to how the observer was moving with respect to the object being measured (what came to be called the "special theory of relativity"). It also followed that mass and energy were interchangeable quantities according to the equation "E"="mc". In another paper published the same year, Einstein asserted that electromagnetic radiation was transmitted in discrete quantities ("quanta"), according to a constant that the theoretical physicist Max Planck had posited in 1900 to arrive at an accurate theory for the distribution of blackbody radiation—an assumption that explained the strange properties of the photoelectric effect.

The special theory of relativity is a formulation of the relationship between physical observations and the concepts of space and time. The theory arose out of contradictions between electromagnetism and Newtonian mechanics and had great impact on both those areas. The original historical issue was whether it was meaningful to discuss the electromagnetic wave-carrying "ether" and motion relative to it and also whether one could detect such motion, as was unsuccessfully attempted in the Michelson–Morley experiment. Einstein demolished these questions and the ether concept in his special theory of relativity. However, his basic formulation does not involve detailed electromagnetic theory. It arises out of the question: "What is time?" Newton, in the "Principia" (1686), had given an unambiguous answer: "Absolute, true, and mathematical time, of itself, and from its own nature, flows equably without relation to anything external, and by another name is called duration." This definition is basic to all classical physics.

Einstein had the genius to question it, and found that it was incomplete. Instead, each "observer" necessarily makes use of his or her own scale of time, and for two observers in relative motion, their time-scales will differ. This induces a related effect on position measurements. Space and time become intertwined concepts, fundamentally dependent on the observer. Each observer presides over his or her own space-time framework or coordinate system. There being no absolute frame of reference, all observers of given events make different but equally valid (and reconcilable) measurements. What remains absolute is stated in Einstein's relativity postulate: "The basic laws of physics are identical for two observers who have a constant relative velocity with respect to each other."

Special Relativity had a profound effect on physics: started as a rethinking of the theory of electromagnetism, it found a new symmetry law of nature, now called "Poincaré symmetry", that replaced the old Galilean (see above) symmetry.

Special Relativity exerted another long-lasting effect on dynamics. Although initially it was credited with the "unification of mass and energy", it became evident that relativistic dynamics established a firm "distinction" between rest mass, which is an invariant (observer independent) property of a particle or system of particles, and the energy and momentum of a system. The latter two are separately conserved in all situations but not invariant with respect to different observers. The term "mass" in particle physics underwent a semantic change, and since the late 20th century it almost exclusively denotes the rest (or "invariant") mass. See mass in special relativity for additional discussion.

By 1916, Einstein was able to generalize this further, to deal with all states of motion including non-uniform acceleration, which became the general theory of relativity. In this theory Einstein also specified a new concept, the curvature of space-time, which described the gravitational effect at every point in space. In fact, the curvature of space-time completely replaced Newton's universal law of gravitation. According to Einstein, gravitational force in the normal sense is a kind of illusion caused by the geometry of space. The presence of a mass causes a curvature of space-time in the vicinity of the mass, and this curvature dictates the space-time path that all freely-moving objects must follow. It was also predicted from this theory that light should be subject to gravity - all of which was verified experimentally. This aspect of relativity explained the phenomena of light bending around the sun, predicted black holes as well as properties of the Cosmic microwave background radiation — a discovery rendering fundamental anomalies in the classic Steady-State hypothesis. For his work on relativity, the photoelectric effect and blackbody radiation, Einstein received the Nobel Prize in 1921.

The gradual acceptance of Einstein's theories of relativity and the quantized nature of light transmission, and of Niels Bohr's model of the atom created as many problems as they solved, leading to a full-scale effort to reestablish physics on new fundamental principles. Expanding relativity to cases of accelerating reference frames (the "general theory of relativity") in the 1910s, Einstein posited an equivalence between the inertial force of acceleration and the force of gravity, leading to the conclusion that space is curved and finite in size, and the prediction of such phenomena as gravitational lensing and the distortion of time in gravitational fields.

Although relativity resolved the electromagnetic phenomena conflict demonstrated by Michelson and Morley, a second theoretical problem was the explanation of the distribution of electromagnetic radiation emitted by a black body; experiment showed that at shorter wavelengths, toward the ultraviolet end of the spectrum, the energy approached zero, but classical theory predicted it should become infinite. This glaring discrepancy, known as the ultraviolet catastrophe, was solved by the new theory of quantum mechanics. Quantum mechanics is the theory of atoms and subatomic systems. Approximately the first 30 years of the 20th century represent the time of the conception and evolution of the theory. The basic ideas of quantum theory were introduced in 1900 by Max Planck (1858–1947), who was awarded the Nobel Prize for Physics in 1918 for his discovery of the quantified nature of energy. The quantum theory (which previously relied in the "correspondence" at large scales between the quantized world of the atom and the continuities of the "classical" world) was accepted when the Compton Effect established that light carries momentum and can scatter off particles, and when Louis de Broglie asserted that matter can be seen as behaving as a wave in much the same way as electromagnetic waves behave like particles (wave–particle duality).

In 1905, Einstein used the quantum theory to explain the photoelectric effect, and in 1913 the Danish physicist Niels Bohr used the same constant to explain the stability of Rutherford's atom as well as the frequencies of light emitted by hydrogen gas. The quantized theory of the atom gave way to a full-scale quantum mechanics in the 1920s. New principles of a "quantum" rather than a "classical" mechanics, formulated in matrix-form by Werner Heisenberg, Max Born, and Pascual Jordan in 1925, were based on the probabilistic relationship between discrete "states" and denied the possibility of causality. Quantum mechanics was extensively developed by Heisenberg, Wolfgang Pauli, Paul Dirac, and Erwin Schrödinger, who established an equivalent theory based on waves in 1926; but Heisenberg's 1927 "uncertainty principle" (indicating the impossibility of precisely and simultaneously measuring position and momentum) and the "Copenhagen interpretation" of quantum mechanics (named after Bohr's home city) continued to deny the possibility of fundamental causality, though opponents such as Einstein would metaphorically assert that "God does not play dice with the universe". The new quantum mechanics became an indispensable tool in the investigation and explanation of phenomena at the atomic level. Also in the 1920s, the Indian scientist Satyendra Nath Bose's work on photons and quantum mechanics provided the foundation for Bose–Einstein statistics, the theory of the Bose–Einstein condensate.

Fermions are particles "like electrons and nucleons" and are the usual constituents of matter. Fermi–Dirac statistics later found numerous other uses, from astrophysics (see Degenerate matter) to semiconductor design.

As the philosophically inclined continued to debate the fundamental nature of the universe, quantum theories continued to be produced, beginning with Paul Dirac's formulation of a relativistic quantum theory in 1928. However, attempts to quantize electromagnetic theory entirely were stymied throughout the 1930s by theoretical formulations yielding infinite energies. This situation was not considered adequately resolved until after World War II ended, when Julian Schwinger, Richard Feynman and Sin-Itiro Tomonaga independently posited the technique of renormalization, which allowed for an establishment of a robust quantum electrodynamics (QED).

Meanwhile, new theories of fundamental particles proliferated with the rise of the idea of the quantization of fields through "exchange forces" regulated by an exchange of short-lived "virtual" particles, which were allowed to exist according to the laws governing the uncertainties inherent in the quantum world. Notably, Hideki Yukawa proposed that the positive charges of the nucleus were kept together courtesy of a powerful but short-range force mediated by a particle with a mass between that of the electron and proton. This particle, the "pion", was identified in 1947 as part of what became a slew of particles discovered after World War II. Initially, such particles were found as ionizing radiation left by cosmic rays, but increasingly came to be produced in newer and more powerful particle accelerators.

Outside particle physics, significant advances of the time were:

Einstein deemed that all fundamental interactions in nature can be explained in a single theory. Unified field theories were numerous attempts to "merge" several interactions. One of formulations of such theories (as well as field theories in general) is a "gauge theory", a generalization of the idea of symmetry. Eventually the Standard Model (see below) succeeded in unification of strong, weak, and electromagnetic interactions. All attempts to unify gravitation with something else failed.
The interaction of these particles by scattering and decay provided a key to new fundamental quantum theories. Murray Gell-Mann and Yuval Ne'eman brought some order to these new particles by classifying them according to certain qualities, beginning with what Gell-Mann referred to as the "Eightfold Way". While its further development, the quark model, at first seemed inadequate to describe strong nuclear forces, allowing the temporary rise of competing theories such as the S-Matrix, the establishment of quantum chromodynamics in the 1970s finalized a set of fundamental and exchange particles, which allowed for the establishment of a "standard model" based on the mathematics of gauge invariance, which successfully described all forces except for gravitation, and which remains generally accepted within its domain of application.

The Standard Model groups the electroweak interaction theory and quantum chromodynamics into a structure denoted by the gauge group SU(3)×SU(2)×U(1). The formulation of the unification of the electromagnetic and weak interactions in the standard model is due to Abdus Salam, Steven Weinberg and, subsequently, Sheldon Glashow. Electroweak theory was later confirmed experimentally (by observation of neutral weak currents), and distinguished by the 1979 Nobel Prize in Physics.

Since the 1970s, fundamental particle physics has provided insights into early universe cosmology, particularly the Big Bang theory proposed as a consequence of Einstein's general theory of relativity. However, starting in the 1990s, astronomical observations have also provided new challenges, such as the need for new explanations of galactic stability ("dark matter") and the apparent acceleration in the expansion of the universe ("dark energy").

While accelerators have confirmed most aspects of the Standard Model by detecting expected particle interactions at various collision energies, no theory reconciling general relativity with the Standard Model has yet been found, although supersymmetry and string theory were believed by many theorists to be a promising avenue forward. The Large Hadron Collider, however, which began operating in 2008, has failed to find any evidence whatsoever that is supportive of supersymmetry and string theory.

Cosmology may be said to have become a serious research question with the publication of Einstein's General Theory of Relativity in 1915 although it did not enter the scientific mainstream until the period known as the "Golden age of general relativity".

About a decade later, in the midst of what was dubbed the "Great Debate", Hubble and Slipher discovered the expansion of universe in the 1920s measuring the redshifts of Doppler spectra from galactic nebulae. Using Einstein's general relativity, Lemaître and Gamow formulated what would become known as the big bang theory. A rival, called the steady state theory was devised by Hoyle, Gold, Narlikar and Bondi.

Cosmic background radiation was verified in the 1960s by Penzias and Wilson, and this discovery favoured the big bang at the expense of the steady state scenario. Later work was by Smoot et al. (1989), among other contributors, using data from the Cosmic Background explorer (CoBE) and the Wilkinson Microwave Anisotropy Probe (WMAP) satellites that refined these observations. The 1980s (the same decade of the COBE measurements) also saw the proposal of inflation theory by Guth.

Recently the problems of dark matter and dark energy have risen to the top of the cosmology agenda.

On July 4, 2012, physicists working at CERN's Large Hadron Collider announced that they had discovered a new subatomic particle greatly resembling the Higgs boson, a potential key to an understanding of why elementary particles have mass and indeed to the existence of diversity and life in the universe. For now, some physicists are calling it a "Higgslike" particle. Joe Incandela, of the University of California, Santa Barbara, said, "It's something that may, in the end, be one of the biggest observations of any new phenomena in our field in the last 30 or 40 years, going way back to the discovery of quarks, for example." Michael Turner, a cosmologist at the University of Chicago and the chairman of the physics center board, said:

Peter Higgs was one of six physicists, working in three independent groups, who, in 1964, invented the notion of the Higgs field ("cosmic molasses"). The others were Tom Kibble of Imperial College, London; Carl Hagen of the University of Rochester; Gerald Guralnik of Brown University; and François Englert and Robert Brout, both of Université libre de Bruxelles.

Although they have never been seen, Higgslike fields play an important role in theories of the universe and in string theory. Under certain conditions, according to the strange accounting of Einsteinian physics, they can become suffused with energy that exerts an antigravitational force. Such fields have been proposed as the source of an enormous burst of expansion, known as inflation, early in the universe and, possibly, as the secret of the dark energy that now seems to be speeding up the expansion of the universe.

With increased accessibility to and elaboration upon advanced analytical techniques in the 19th century, physics was defined as much, if not more, by those techniques than by the search for universal principles of motion and energy, and the fundamental nature of matter. Fields such as acoustics, geophysics, astrophysics, aerodynamics, plasma physics, low-temperature physics, and solid-state physics joined optics, fluid dynamics, electromagnetism, and mechanics as areas of physical research. In the 20th century, physics also became closely allied with such fields as electrical, aerospace and materials engineering, and physicists began to work in government and industrial laboratories as much as in academic settings. Following World War II, the population of physicists increased dramatically, and came to be centered on the United States, while, in more recent decades, physics has become a more international pursuit than at any time in its previous history.




</doc>
<doc id="13761" url="https://en.wikipedia.org/wiki?curid=13761" title="Hydrofoil">
Hydrofoil

A hydrofoil is a lifting surface, or foil, that operates in water. They are similar in appearance and purpose to aerofoils used by aeroplanes. Boats that use hydrofoil technology are also simply termed hydrofoils. As a hydrofoil craft gains speed, the hydrofoils lift the boat's hull out of the water, decreasing drag and allowing greater speeds.

The hydrofoil usually consists of a wing like structure mounted on struts below the hull, or across the keels of a catamaran in a variety of boats (see illustration). As a hydrofoil-equipped watercraft increases in speed, the hydrofoil elements below the hull(s) develop enough lift to raise the hull out of the water, which greatly reduces hull drag. This provides a corresponding increase in speed and fuel efficiency.

Wider adoption of hydrofoils is prevented by the increased complexity of building and maintaining them. Hydrofoils are generally prohibitively more expensive than conventional watercraft above the certain displacement, so most hydrofoil craft are relatively small, and are mainly used as high-speed passenger ferries, where the relatively high passenger fees can offset the high cost of the craft itself. However, the design is simple enough that there are many human-powered hydrofoil designs. Amateur experimentation and development of the concept is popular.

Since air and water are governed by similar fluid equations—albeit with different levels of viscosity, density, and compressibility—the hydrofoil and airfoil (both types of foil) create lift in identical ways. The foil shape moves smoothly through the water, deflecting the flow downward, which, following the Euler equations, exerts an upward force on the foil. This turning of the water creates higher pressure on the bottom of the foil and reduced pressure on the top. This pressure difference is accompanied by a velocity difference, via Bernoulli's principle, so the resulting flow field about the foil has a higher average velocity on one side than the other.

When used as a lifting element on a hydrofoil boat, this upward force lifts the body of the vessel, decreasing drag and increasing speed. The lifting force eventually balances with the weight of the craft, reaching a point where the hydrofoil no longer lifts out of the water but remains in equilibrium. Since wave resistance and other impeding forces such as various types of drag (physics) on the hull are eliminated as the hull lifts clear, turbulence and drag act increasingly on the much smaller surface area of the hydrofoil, and decreasingly on the hull, creating a marked increase in speed.

Early hydrofoils used V-shaped foils. Hydrofoils of this type are known as "surface-piercing" since portions of the V-shape hydrofoils rise above the water surface when foilborne. Some modern hydrofoils use fully submerged inverted T-shape foils. Fully submerged hydrofoils are less subject to the effects of wave action, and, therefore, more stable at sea and more comfortable for crew and passengers. This type of configuration, however, is not self-stabilizing. The angle of attack on the hydrofoils must be adjusted continuously to changing conditions, a control process performed by sensors, a computer, and active surfaces.

The first evidence of a hydrofoil on a vessel appears on a British patent granted in 1869 to Emmanuel Denis Farcot, a Parisian. He claimed that "adapting to the sides and bottom of the vessel a series or inclined planes or wedge formed pieces, which as the vessel is driven forward will have the effect of lifting it in the water and reducing the draught.". Italian inventor Enrico Forlanini began work on hydrofoils in 1898 and used a "ladder" foil system. Forlanini obtained patents in Britain and the United States for his ideas and designs.

Between 1899 and 1901, British boat designer John Thornycroft worked on a series of models with a stepped hull and single bow foil. In 1909 his company built the full scale long boat, "Miranda III". Driven by a engine, it rode on a bowfoil and flat stern. The subsequent "Miranda IV" was credited with a speed of .

A March 1906 Scientific American article by American hydrofoil pioneer William E. Meacham explained the basic principle of hydrofoils. Alexander Graham Bell considered the invention of the hydroplane a very significant achievement, and after reading the article began to sketch concepts of what is now called a hydrofoil boat. With his chief engineer Casey Baldwin, Bell began hydrofoil experiments in the summer of 1908. Baldwin studied the work of the Italian inventor Enrico Forlanini and began testing models based on those designs, which led to the development of hydrofoil watercraft. During Bell's world tour of 1910–1911, Bell and Baldwin met with Forlanini in Italy, where they rode in his hydrofoil boat over Lake Maggiore. Baldwin described it as being as smooth as flying.

On returning to Bell's large laboratory at his Beinn Bhreagh estate near Baddeck, Nova Scotia, they experimented with a number of designs, culminating in Bell's "HD-4". Using Renault engines, a top speed of was achieved, accelerating rapidly, taking waves without difficulty, steering well and showing good stability. Bell's report to the United States Navy permitted him to obtain two 260 kW (350 hp) engines. On 9 September 1919 the "HD-4" set a world marine speed record of , which stood for two decades. A full-scale replica of the "HD-4" is viewable at the Alexander Graham Bell National Historic Site museum in Baddeck, Nova Scotia.

In the early 1950s an English couple built the "White Hawk", a jet-powered hydrofoil water craft, in an attempt to beat the absolute water speed record. However, in tests, "White Hawk" could barely top the record breaking speed of the 1919 "HD-4". The designers had faced an engineering phenomenon that limits the top speed of even modern hydrofoils: cavitation disturbs the lift created by the foils as they move through the water at speed above , bending the lifting foil.

German engineer Hanns von Schertel worked on hydrofoils prior to and during World War II in Germany. After the war, the Russians captured Schertel's team. As Germany was not authorized to build fast boats, Schertel went to Switzerland, where he established the Supramar company. In 1952, Supramar launched the first commercial hydrofoil, PT10 "Freccia d'Oro" (Golden Arrow), in Lake Maggiore, between Switzerland and Italy. The PT10 is of surface-piercing type, it can carry 32 passengers and travel at . In 1968, the Bahraini born banker Hussain Najadi acquired the Supramar AG and expanded its operations into Japan, Hong Kong, Singapore, the UK, Norway and the US. General Dynamics of the United States became its licensee, and the Pentagon awarded its first R&D naval research project in the field of supercavitation. Hitachi Shipbuilding of Osaka, Japan, was another licensee of Supramar, as well as many leading ship owners and shipyards in the OECD countries.

From 1952 to 1971, Supramar designed many models of hydrofoils: PT20, PT50, PT75, PT100 and PT150. All are of surface-piercing type, except the PT150 combining a surface-piercing foil forward with a fully submerged foil in the aft location. Over 200 of Supramar's design were built, most of them by Rodriquez in Sicily, Italy.

During the same period the Soviet Union experimented extensively with hydrofoils, constructing hydrofoil river boats and ferries with streamlined designs during the cold war period and into the 1980s. Such vessels include the Raketa (1957) type, followed by the larger Meteor type and the smaller Voskhod type. One of the most successful Soviet designer/inventor in this area was Rostislav Alexeyev, who some consider the 'father' of the modern hydrofoil due to his 1950s era high speed hydrofoil designs. Later, circa 1970s, Alexeyev combined his hydrofoil experience with the surface effect principle to create the Ekranoplan.

In 1961, SRI International issued a study on "The Economic Feasibility of Passenger Hydrofoil Craft in US Domestic and Foreign Commerce". Commercial use of hydrofoils in the US first appeared in 1961 when two commuter vessels were commissioned by Harry Gale Nye, Jr.'s North American Hydrofoils to service the route from Atlantic Highlands, New Jersey to the financial district of Lower Manhattan.

A 17-ton German craft "VS-6 Hydrofoil" was designed and constructed in 1940, completed in 1941 for use as a mine layer, it was tested in the Baltic Sea, producing speeds of 47 knots. Tested against a standard E-boat over the next three years it performed well but was not brought into production. Being faster it could carry a higher payload and was capable of travelling over minefields but was prone to damage and noisier.

In Canada during World War II, Baldwin worked on an experimental smoke laying hydrofoil (later called the Comox Torpedo) that was later superseded by other smoke-laying technology and an experimental target-towing hydrofoil. The forward two foil assemblies of what is believed to be the latter hydrofoil were salvaged in the mid-1960s from a derelict hulk in Baddeck, Nova Scotia by Colin MacGregor Stevens. These were donated to the Maritime Museum in Halifax, Nova Scotia. The Canadian Armed Forces built and tested a number of hydrofoils (e.g., Baddeck and two vessels named "Bras d'Or"), which culminated in the high-speed anti-submarine hydrofoil HMCS "Bras d'Or" in the late 1960s. However, the program was cancelled in the early 1970s due to a shift away from anti-submarine warfare by the Canadian military. The "Bras d'Or" was a surface-piercing type that performed well during her trials, reaching a maximum speed of .

The USSR introduced several hydrofoil-based fast attack craft into their navy, principally:

The US Navy began experiments with hydrofoils in the mid-1950s by funding a sailing vessel that used hydrofoils to reach speeds in the 30 mph range. The "XCH-4" (officially, "Experimental Craft, Hydrofoil No. 4"), designed by William P. Carl, exceeded speeds of and was mistaken for a seaplane due to its shape. The US Navy implemented a small number of combat hydrofoils, such as the "Pegasus" class, from 1977 through 1993. These hydrofoils were fast and well armed.

The Italian Navy has used six hydrofoils of the "Sparviero" class since the late 1970s. These were armed with a 76 mm gun and two missiles, and were capable of speeds up to . Three similar boats were built for the Japan Maritime Self-Defense Force.

The French experimental sail powered hydrofoil "Hydroptère" is the result of a research project that involves advanced engineering skills and technologies. In September 2009, the "Hydroptère" set new sailcraft world speed records in the 500 m category, with a speed of and in the one nautical mile (1.9 km) category with a speed of .

Another trimaran sailboat is the Windrider Rave. The Rave is a commercially available , two person, hydrofoil trimaran, capable of reaching speeds of . The boat was designed by Jim Brown.

The Moth dinghy has evolved into some radical foil configurations.

Hobie Sailboats produced a production foiling trimaran, the Hobie Trifoiler, the fastest production sailboat. Trifoilers have clocked speeds upward of thirty knots.

A new kayak design, called Flyak, has hydrofoils that lift the kayak enough to significantly reduce drag, allowing speeds of up to . Some surfers have developed surfboards with hydrofoils called foilboards, specifically aimed at surfing big waves further out to sea.
Quadrofoil Q2 is a two-seater, four-foiled hydrofoil electrical leisure watercraft. Its initial design was set in 2012 and it has been available commercially since the end of 2016. Powered by a 5.2-kWh lithium-ion battery pack and propelled by a 5.5 kW motor, it reaches the top speed of 40 km/h and has 80 km of range.

The Manta5 Hydrofoiler XE-1 is a Hydrofoil E-bike, designed and built in New Zealand. Initially designed in secret by Guy Howard-Willis in 2011, it has since been available commercially for pre-order in New Zealand since late 2017 Propelled by a 400 watt motor, it can reach speeds exceeding 14 km/h with a weight of 22 kg. A single charge of the battery lasts an hour for a rider weighing 85kg. 

Soviet-built Voskhods are one of the most successful passenger hydrofoil designs. Manufactured in Russia and Ukraine, they are in service in more than 20 countries. The most recent model, Voskhod-2M FFF, also known as Eurofoil, was built in Feodosiya for the Dutch public transport operator Connexxion.

The Boeing 929 is widely used in Asia for passenger services between the many islands of Japan, between Hong Kong and Macau and on the Korean peninsula.

Current operators of hydrofoils include:



See also the history of Condor Ferries, which operated six hydrofoil ferries over a 29-year period between the Channel Islands, south coast of England and Saint-Malo.

Hydrofoils had their peak in popularity in the 1960s and 70s. Since then there has been a steady decline in their use and popularity for leisure, military and commercial passenger transport use. There are a number of reasons for this:




</doc>
<doc id="13763" url="https://en.wikipedia.org/wiki?curid=13763" title="Henri Chopin">
Henri Chopin

Henri Chopin (18 June 1922 – 3 January 2008) was an avant-garde poet and musician.

Henri Chopin was born in Paris,18 June 1922, one of three brothers, and the son of an accountant. Both his siblings died during the war. One was shot by a German soldier the day after an armistice was declared in Paris, the other while sabotaging a train .

Chopin was a French practitioner of concrete and sound poetry, well known throughout the second half of the 20th century. His work, though iconoclastic, remained well within the historical spectrum of poetry as it moved from a spoken tradition to the printed word and now back to the spoken word again . He created a large body of pioneering recordings using early tape recorders, studio technologies and the sounds of the manipulated human voice. His emphasis on sound is a reminder that language stems as much from oral traditions as from classic literature, of the relationship of balance between order and chaos.

Chopin is significant above all for his diverse spread of creative achievement, as well as for his position as a focal point of contact for the international arts. As poet, painter, graphic artist and designer, typographer, independent publisher, filmmaker, broadcaster and arts promoter, Chopin's work is a barometer of the shifts in European media between the 1950s and the 1970s.

In 1966 he was with Gustav Metzger, Otto Muehl, Wolf Vostell, Peter Weibel and others a participant of the Destruction in Art Symposium ("DIAS") in London .

In 1964 he created "OU", one of the most notable reviews of the second half of the 20th century, and he ran it until 1974. "OU"'s contributors included William S. Burroughs, Brion Gysin, Gil J Wolman, François Dufrêne, Bernard Heidsieck, John Furnival, Tom Phillips, and the Austrian sculptor, writer and Dada pioneer Raoul Hausmann.

His books included "Le Dernier Roman du Monde" (1971), "Portrait des 9" (1975), "The Cosmographical Lobster" (1976), "Poésie Sonore Internationale" (1979), "Les Riches Heures de l'Alphabet" (1992) and "Graphpoemesmachine" (2006). Henri also created many graphic works on his typewriter: the typewriter poems (also known as dactylopoèmes) feature in international art collections such as those of Francesco Conz in Verona, the Morra Foundation in Naples and Ruth and Marvin Sackner in Miami, and have been the subject of Australian, British and French retrospectives .

His publication and design of the classic audio-visual magazines "Cinquième Saison" and "OU" between 1958 and 1974, each issue containing recordings as well as texts, images, screenprints and multiples, brought together international contemporary writers and artists such as members of Lettrisme and Fluxus, Jiri Kolar, Ian Hamilton Finlay, Tom Phillips, Brion Gysin, William S. Burroughs and many others, as well as bringing the work of survivors from earlier generations such as Raoul Hausmann and Marcel Janco to a fresh audience.

From 1968 to 1986 Henri Chopin lived in Ingatestone, Essex, but with the death of his wife Jean in 1985, he moved back to France.

In 2001 with his health failing, he returned to England, living with his daughter and family at Dereham, Norfolk .

Chopin's "poesie sonore" aesthetics included a deliberate cultivation of a "barbarian" approach in production, using raw or crude sound manipulations to explore the area between
distortion and intelligibility. He avoided high-quality, professional recording machines, preferring to use very basic equipment and "bricolage" methods, such as sticking matchsticks in the erase heads of a second-hand tape recorder, or manually interfering with the tape path .









</doc>
<doc id="13764" url="https://en.wikipedia.org/wiki?curid=13764" title="Hassium">
Hassium

Hassium is a synthetic chemical element with symbol Hs and atomic number 108. It is named after the German state of Hesse. It is a synthetic element and radioactive; the most stable known isotope, Hs, has a half-life of approximately 10 seconds.

In the periodic table of the elements, it is a d-block transactinide element. Hassium is a member of the 7th period and belongs to the group 8 elements: it is thus the sixth member of the 6d series of transition metals. Chemistry experiments have confirmed that hassium behaves as the heavier homologue to osmium in group 8. The chemical properties of hassium are characterized only partly, but they compare well with the chemistry of the other group 8 elements. In bulk quantities, hassium is expected to be a silvery metal that reacts readily with oxygen in the air, forming a volatile tetroxide.

The synthesis of element 108 was first attempted in 1978 by a research team led by Yuri Oganessian at the Joint Institute for Nuclear Research (JINR) in Dubna, Moscow Oblast, Russian SFSR, Soviet Union. The team used a reaction that would generate 108 from radium and calcium. The researchers were uncertain in interpreting result data; the paper did not unambiguously claim discovery. That same year, another team at JINR investigated the possibility of synthesis of element 108 in reactions between lead and iron; they were uncertain in interpreting result data as well, openly suggesting a possibility that element 108 had not been created.

New experiments were performed in 1983. In each experiment, a thin layer of a target material was installed on a rotating wheel and bombarded at a shallow angle. This was made so that fission fragments from spontaneously fissioning nuclides formed could escape the target and be detected in a number of fission track detectors surrounding the wheel. The experiments probably resulted in synthesis of element 108: bismuth was bombarded with manganese to obtain 108, lead was bombarded with iron to obtain 108, and californium was bombarded with neon to obtain 108. These experiments were not claimed as a discovery and were only announced by Oganessian in a conference rather than in a written report.

In 1984, researchers in Dubna published a written report. The researchers performed a number of experiments set up as the previous ones with, bombarding target materials (bismuth and lead) with ions of lighter element (manganese and iron, correspondingly).

Also in 1984, a research team led by Peter Armbruster and Gottfried Münzenberg at the Gesellschaft für Schwerionenforschung (GSI; "Institute for Heavy Ion Research") in Darmstadt, Hesse, West Germany, attempted to create element 108. The team bombarded a target of lead with accelerated nuclei of iron. They reported synthesis of 3 atoms of 108.

In 1985, the International Union of Pure and Applied Chemistry (IUPAC) and the International Union of Pure and Applied Physics (IUPAP) formed a Joint Working Party (JWP) to assess discoveries and establish final names for the elements with atomic number greater than 100. The party held meetings with delegates from the three competing institutes; in 1990, they established criteria on recognition of an element, and in 1991, they finished the work on assessing discoveries, and disbanded. These results were published in 1993.

According to the report, the 1984 works from JINR and GSI simultaneously independently established synthesis of element 108. Of the two 1984 works, the one from GSI was said to be sufficient as a discovery on its own, while the JINR work, which preceded the GSI one, "very probably" displayed synthesis of element 108, but that is determined in retrospect given the work from Darmstadt. The report concluded that the major credit should be awarded to GSI.

According to Mendeleev's nomenclature for unnamed and undiscovered elements, hassium should be known as "eka-osmium". In 1979, IUPAC published recommendations according to which the element was to be called "unniloctium" (with the corresponding symbol of "Uno"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it "element 108", with the symbol of "E108", "(108)", or even simply "108", or used the proposed name "hassium".

The name "hassium" was proposed by Peter Armbruster and his colleagues, the officially recognised German discoverers, in September 1992. It is derived from the Latin name "Hassia" for the German state of Hesse where the institute is located. In 1994, IUPAC Commission on Nomenclature of Inorganic Chemistry recommended that element 108 be named "hahnium" (Hn) after the German physicist Otto Hahn so that elements named after Hahn and Lise Meitner (meitnerium) would be next to each other, honoring their joint discovery of nuclear fission. This was because they felt that Hesse did not merit an element being named after it. GSI protested saying that this contradicted the long-standing convention to give the discoverer the right to suggest a name; the American Chemical Society supported the GSI. IUPAC relented and the name "hassium" (Hs) was adopted internationally in 1997.

Hassium is not known to occur naturally on Earth; the half-lives of all its known isotopes are short enough that no primordial hassium would have survived to the present day. This does not rule out the possibility of unknown longer-lived isotopes or nuclear isomers existing, some of which could still exist in trace quantities today if they are long-lived enough. In the early 1960s, it was predicted that long-lived deformed isomers of hassium might occur naturally on Earth in trace quantities. This was theorized in order to explain the extreme radiation damage in some minerals that could not have been caused by any known natural radioisotopes, but could have been caused by superheavy elements.

In 1963, Soviet scientist Viktor Cherdyntsev, who had previously claimed the existence of primordial curium-247, claimed to have discovered element 108 (specifically, the Hs isotope, which supposedly had a half-life of 400 to 500 million years) in natural molybdenite and suggested the name "sergenium" (symbol Sg; at the time, this symbol had not yet been taken by seaborgium) for it; this name takes its origin in the name for the Silk Road and was explained as "coming from Kazakhstan". His rationale for claiming that sergenium was the heavier homologue to osmium was that minerals supposedly containing sergenium formed volatile oxides when boiled in nitric acid, similarly to osmium. His findings were criticized by Soviet physicist Vladimir Kulakov on the grounds that some of the properties Cherdyntsev claimed sergenium had were inconsistent with the then-current nuclear physics.

The chief questions raised by Kulakov were that the claimed alpha decay energy of sergenium was many orders of magnitude lower than expected and the half-life given was eight orders of magnitude shorter than what would be predicted for a nuclide alpha decaying with the claimed decay energy, but at the same time a corrected half-life in the region of 10 years would be impossible as it would imply that the samples contained about 100 milligrams of sergenium. In 2003 it was suggested that the observed alpha decay with energy 4.5 MeV could be due to a low-energy and strongly enhanced transition between different hyperdeformed states of a hassium isotope around Hs, thus suggesting that the existence of superheavy elements in nature was at least possible, although unlikely.

In 2004, the Joint Institute for Nuclear Research conducted a search for natural hassium. This was done underground to avoid interference and false positives from cosmic rays, but no results have been released, strongly implying that no natural hassium was found. The possible extent of primordial hassium on Earth is uncertain; it might now only exist in traces, or could even have completely decayed by now after having caused the radiation damage long ago.

In 2006, it was hypothesized that an isomer of Hs might have a half-life of around years, which would explain the observation of alpha particles with energies of around 4.4 MeV in some samples of molybdenite and osmiridium. This isomer of Hs could be produced from the beta decay of Bh and Sg, which, being homologous to rhenium and molybdenum respectively, should occur in molybdenite along with rhenium and molybdenum if they occurred in nature. Since hassium is homologous to osmium, it should also occur along with osmium in osmiridium if it occurred in nature. The decay chains of Bh and Sg are very hypothetical and the predicted half-life of this hypothetical hassium isomer is not long enough for any sufficient quantity to remain on Earth. It is possible that more Hs may be deposited on the Earth as the Solar System travels through the spiral arms of the Milky Way, which would also explain excesses of plutonium-239 found on the floors of the Pacific Ocean and the Gulf of Finland, but minerals enriched with Hs are predicted to also have excesses of uranium-235 and lead-207, and would have different proportions of elements that are formed during spontaneous fission, such as krypton, zirconium, and xenon. Thus, the occurrence of hassium in nature in minerals such as molybdenite and osmiride is theoretically possible, but highly unlikely.

A 2007 calculation on the decay properties of unknown neutron-rich isotopes of superheavy elements suggested that the isotope Hs may be the most stable superheavy nucleus against alpha decay and spontaneous fission, as a consequence of the shell closure at "N" = 184. As such, it was considered as a candidate to exist in nature. However, this nucleus is predicted to be highly unstable toward beta decay, and any beta-stable isotopes of hassium (such as Hs) would be too unstable in the other decay channels to possibly be observed in nature. Indeed, a subsequent search for Hs in nature along with its congener osmium was unsuccessful, setting an upper limit to its abundance at of hassium per gram of osmium.

Hassium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes have been reported with atomic masses from 263 to 277 (with the exceptions of 272, 274, and 276), three of which, hassium-265, hassium-267, hassium-269, have known metastable states. Most of these decay predominantly through alpha decay, but some also undergo spontaneous fission.

The lightest isotopes, which usually have shorter half-lives were synthesized by direct fusion between two lighter nuclei and as decay products. The heaviest isotope produced by direct fusion is Hs; heavier isotopes have only been observed as decay products of elements with larger atomic numbers. In 1999, scientists at University of California in Berkeley, California, United States, announced that they had succeeded in synthesizing three atoms of Og. These parent nuclei were reported to have successively emitted three alpha particles to form hassium-273 nuclei, which were claimed to have undergone an alpha decay, emitting alpha particles with decay energies of 9.78 and 9.47 MeV and half-life 1.2 s, but their claim was retracted in 2001, as the data was found to be fabricated. The isotope was successfully produced in 2010 by the same team. The new data matched the previous (fabricated) data.

According to calculations, 108 is a proton magic number for deformed nuclei (nuclei that are far from spherical), and 162 is a neutron magic number for deformed nuclei. This means that such nuclei are permanently deformed in their ground state but have high, narrow fission barriers to further deformation and hence relatively long life-times to spontaneous fission. The spontaneous fission half-lives in this region are typically reduced by a factor of 10 in comparison with those in the vicinity of the spherical doubly magic nucleus Fl, caused by the narrower fission barrier for such deformed nuclei. Hence, the nucleus Hs has promise as a deformed doubly magic nucleus. Experimental data from the decay of the darmstadtium (Z=110) isotopes Ds and Ds provides strong evidence for the magic nature of the N=162 sub-shell. The syntheses of Hs, Hs, and Hs also fully support the assignment of N=162 as a magic number. In particular, the low decay energy for Hs is in complete agreement with calculations.

Evidence for the magicity of the Z=108 proton shell can be obtained from two sources: the variation in the partial spontaneous fission half-lives for isotones and the large gap in the alpha Q value for isotonic nuclei of hassium and darmstadtium. For spontaneous fission, it is necessary to measure the half-lives for the isotonic nuclei Sg, Hs and Ds. Since the isotopes Sg and Ds are not currently known, and fission of Hs has not been measured, this method cannot yet be used to confirm the stabilizing nature of the Z=108 shell. Good evidence for the magicity of the Z=108 shell can nevertheless be found from the large differences in the alpha decay energies measured for Hs, Ds and Ds. More conclusive evidence would come from the determination of the decay energy for the unknown nucleus Ds.
Various calculations show that hassium should be the heaviest known group 8 element, consistent with the periodic law. Its properties should generally match those expected for a heavier homologue of osmium, with a few deviations arising from relativistic effects.

The previous members of group 8 have relatively high melting points (Fe, 1538 °C; Ru, 2334 °C; Os, 3033 °C). Much like them, hassium is predicted to be a solid at room temperature, although the melting point of hassium has not been precisely calculated. Hassium should crystallize in the hexagonal close-packed structure (/ = 1.59), similarly to its lighter congener osmium. Pure metallic hassium is calculated to have a bulk modulus (resistance to uniform compression) comparable to that of diamond (442 GPa). Hassium is expected to have a bulk density of 40.7 g/cm, the highest of any of the 118 known elements and nearly twice the density of osmium, the most dense measured element, at 22.61 g/cm. This results from hassium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough hassium to measure this quantity would be impractical, and the sample would quickly decay. Osmium is the densest element of the first 6 periods, and its heavier congener hassium is expected to be the densest element of the first 7 periods.

The atomic radius of hassium is expected to be around 126 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Hs ion is predicted to have an electron configuration of [Rn] 5f 6d 7s, giving up a 6d electron instead of a 7s electron, which is the opposite of the behavior of its lighter homologues. On the other hand, the Hs ion is expected to have an electron configuration of [Rn] 5f 6d 7s, analogous to that calculated for the Os ion.

Hassium is the sixth member of the 6d series of transition metals and is expected to be much like the platinum group metals. Calculations on its ionization potentials, atomic radius, as well as radii, orbital energies, and ground levels of its ionized states are similar to that of osmium, implying that hassium's properties would resemble those of the other group 8 elements, iron, ruthenium, and osmium. Some of these properties were confirmed by gas-phase chemistry experiments. The group 8 elements portray a wide variety of oxidation states, but ruthenium and osmium readily portray their group oxidation state of +8 (the second-highest known oxidation state for any element, which is very rare for other elements) and this state becomes more stable as the group is descended. Thus hassium is expected to form a stable +8 state. Analogously to its lighter congeners, hassium is expected to also show other stable lower oxidation states, such as +6, +5, +4, +3, and +2. Indeed, hassium(IV) is expected to be more stable than hassium(VIII) in aqueous solution.

The group 8 elements show a very distinctive oxide chemistry which allows extrapolations to be made easily for hassium. All the lighter members have known or hypothetical tetroxides, MO. Their oxidising power decreases as one descends the group. FeO is not known due to its extraordinarily large electron affinity (the amount of energy released when an electron is added to a neutral atom or molecule to form a negative ion) which results in the formation of the well-known oxoanion ferrate(VI), . Ruthenium tetroxide, RuO, formed by oxidation of ruthenium(VI) in acid, readily undergoes reduction to ruthenate(VI), . Oxidation of ruthenium metal in air forms the dioxide, RuO. In contrast, osmium burns to form the stable tetroxide, OsO, which complexes with the hydroxide ion to form an osmium(VIII) -"ate" complex, [OsO(OH)]. Therefore, eka-osmium properties for hassium should be demonstrated by the formation of a stable, very volatile tetroxide HsO, which undergoes complexation with hydroxide to form a hassate(VIII), [HsO(OH)]. Ruthenium tetroxide and osmium tetroxide are both volatile, due to their symmetrical tetrahedral molecular geometry and their being charge-neutral; hassium tetroxide should similarly be a very volatile solid. The trend of the volatilities of the group 8 tetroxides is known to be RuO < OsO > HsO, which completely confirms the calculated results. In particular, the calculated enthalpies of adsorption (the energy required for the adhesion of atoms, molecules, or ions from a gas, liquid, or dissolved solid to a surface) of HsO, −(45.4 ± 1) kJ/mol on quartz, agrees very well with the experimental value of .

Despite the fact that the selection of a volatile hassium compound (hassium tetroxide) for gas-phase chemical studies was clear from the beginning, the chemical characterization of hassium was considered a difficult task for a long time. Although hassium isotopes were first synthesized in 1984, it was not until 1996 that a hassium isotope long-lived enough to allow chemical studies to be performed was synthesized. Unfortunately, this hassium isotope, Hs, was then synthesized indirectly from the decay of Cn; not only are indirect synthesis methods not favourable for chemical studies, but also the reaction that produced the isotope Cn had a low yield (its cross-section was only 1 pb), and thus did not provide enough hassium atoms for a chemical investigation. The direct synthesis of Hs and Hs in the reaction Cm(Mg,xn)Hs (x = 4 or 5) appeared more promising, as the cross-section for this reaction was somewhat larger, at 7 pb. This yield was still around ten times lower than that for the reaction used for the chemical characterization of bohrium. New techniques for irradiation, separation, and detection had to be introduced before hassium could be successfully characterized chemically as a typical member of group 8 in early 2001.

Ruthenium and osmium have very similar chemistry due to the lanthanide contraction, but iron shows some differences from them: for example, although ruthenium and osmium form stable tetroxides in which the metal is in the +8 oxidation state, iron does not. Consequently, in preparation for the chemical characterization of hassium, researches focused on ruthenium and osmium rather than iron, as hassium was expected to also be similar to ruthenium and osmium due to the actinide contraction. Nevertheless, in the planned experiment to study hassocene (Hs(CH)), ferrocene may also be used for comparison along with ruthenocene and osmocene.
The first chemistry experiments were performed using gas thermochromatography in 2001, using Os and Os as a reference. During the experiment, 5 hassium atoms were synthesized using the reaction Cm(Mg,5n)Hs. They were then thermalized and oxidized in a mixture of helium and oxygen gas to form the tetroxide.

The measured deposition temperature indicated that hassium(VIII) oxide is less volatile than osmium tetroxide, OsO, and places hassium firmly in group 8. However, the enthalpy of adsorption for HsO measured, , was significantly lower than what was predicted, , indicating that OsO was more volatile than HsO, contradicting earlier calculations, which implied that they should have very similar volatilities. For comparison, the value for OsO is . It is possible that hassium tetroxide interacts differently with the different chemicals (silicon nitride and silicon dioxide) used for the detector; further research is required, including more accurate measurements of the nuclear properties of Hs and comparisons with RuO in addition to OsO.

In 2004 scientists reacted hassium tetroxide and sodium hydroxide to form sodium hassate(VIII), a reaction well known with osmium. This was the first acid-base reaction with a hassium compound, forming sodium hassate(VIII):

The team from the University of Mainz were planning to study the electrodeposition of hassium atoms using the new TASCA facility at the GSI. Their aim was to use the reaction Ra(Ca,4n)Hs. In addition, scientists at the GSI were hoping to utilize TASCA to study the synthesis and properties of the hassium(II) compound hassocene, Hs(CH), using the reaction Ra(Ca,xn). This compound is analogous to the lighter ferrocene, ruthenocene, and osmocene, and is expected to have the two cyclopentadienyl rings in an eclipsed conformation like ruthenocene and osmocene and not in a staggered conformation like ferrocene. Hassocene was chosen because it has hassium in the low formal oxidation state of +2 (although the bonding between the metal and the rings is mostly covalent in metallocenes) rather than the high +8 state which had previously been investigated, and relativistic effects were expected to be stronger in the lower oxidation state. Many metals in the periodic table form metallocenes, so that trends could be more easily determined, and the highly symmetric structure of hassocene and its low number of atoms also make relativistic calculations easier. Hassocene should be a stable and highly volatile compound.


</doc>
<doc id="13765" url="https://en.wikipedia.org/wiki?curid=13765" title="Henry Kissinger">
Henry Kissinger

Henry Alfred Kissinger (; ; born Heinz Alfred Kissinger; May 27, 1923) is an American elder statesman, political scientist, diplomat, and geopolitical consultant who served as United States Secretary of State and National Security Advisor under the presidential administrations of Richard Nixon and Gerald Ford. A Jewish refugee who fled Nazi Germany with his family in 1938, he became National Security Advisor in 1969 and U.S. Secretary of State in 1973. For his actions negotiating a ceasefire in Vietnam, Kissinger received the 1973 Nobel Peace Prize under controversial circumstances, with two members of the committee resigning in protest. Kissinger later sought, unsuccessfully, to return the prize after the ceasefire failed.

A practitioner of "Realpolitik", Kissinger played a prominent role in United States foreign policy between 1969 and 1977. During this period, he pioneered the policy of "détente" with the Soviet Union, orchestrated the opening of relations with the People's Republic of China, engaged in what became known as shuttle diplomacy in the Middle East to end the Yom Kippur War, and negotiated the Paris Peace Accords, ending American involvement in the Vietnam War. Kissinger has also been associated with such controversial policies as U.S. involvement in the 1973 Chilean military coup, a "green light" to Argentina's military junta for their Dirty War, and U.S. support for Pakistan during the Bangladesh War despite the genocide being perpetrated by his allies. After leaving government, he formed Kissinger Associates, an international geopolitical consulting firm. Kissinger has written over one dozen books on diplomatic history and international relations.

Kissinger remains widely regarded as a controversial figure in American politics, and has been condemned as a war criminal by journalists, political activists, and human rights lawyers. According to a 2014 survey by "Foreign Policy" magazine 32.21% of "America's top International Relations scholars" considered Henry Kissinger the most effective U.S. Secretary of State since 1965.

Kissinger was born Heinz Alfred Kissinger in Fürth, Bavaria, Germany in 1923 to a family of German Jews. His father, Louis Kissinger (1887–1982), was a schoolteacher. His mother, Paula (Stern) Kissinger (1901–1998), from Leutershausen, was a homemaker. Kissinger has a younger brother, Walter Kissinger (born 1924). The surname Kissinger was adopted in 1817 by his great-great-grandfather Meyer Löb, after the Bavarian spa town of Bad Kissingen. In youth, Kissinger enjoyed playing soccer, and played for the youth wing of his favorite club, SpVgg Fürth, which was one of the nation's best clubs at the time. In 1938, when Kissinger was 15 years old, fleeing Nazi persecution, his family briefly emigrated to London, England, before arriving in New York on September 5.

Kissinger spent his high school years in the Washington Heights section of Upper Manhattan as part of the German Jewish immigrant community that resided there at the time. Although Kissinger assimilated quickly into American culture, he never lost his pronounced German accent, due to childhood shyness that made him hesitant to speak. Following his first year at George Washington High School, he began attending school at night and worked in a shaving brush factory during the day.

Following high school, Kissinger enrolled in the City College of New York, studying accounting. He excelled academically as a part-time student, continuing to work while enrolled. His studies were interrupted in early 1943, when he was drafted into the U.S. Army.

Kissinger underwent basic training at Camp Croft in Spartanburg, South Carolina. On June 19, 1943, while stationed in South Carolina, at the age of 20 years, he became a naturalized U.S. citizen. The army sent him to study engineering at Lafayette College, Pennsylvania, but the program was canceled, and Kissinger was reassigned to the 84th Infantry Division. There, he made the acquaintance of Fritz Kraemer, a fellow Jewish immigrant from Germany who noted Kissinger's fluency in German and his intellect, and arranged for him to be assigned to the military intelligence section of the division. Kissinger saw combat with the division, and volunteered for hazardous intelligence duties during the Battle of the Bulge.

During the American advance into Germany, Kissinger, only a private, was put in charge of the administration of the city of Krefeld, owing to a lack of German speakers on the division's intelligence staff. Within eight days he had established a civilian administration. Kissinger was then reassigned to the Counter Intelligence Corps (CIC), where he became a CIC Special Agent holding the enlisted rank of sergeant. He was given charge of a team in Hanover assigned to tracking down Gestapo officers and other saboteurs, for which he was awarded the Bronze Star. In June 1945, Kissinger was made commandant of the Bensheim metro CIC detachment, Bergstrasse district of Hesse, with responsibility for de-Nazification of the district. Although he possessed absolute authority and powers of arrest, Kissinger took care to avoid abuses against the local population by his command.

In 1946, Kissinger was reassigned to teach at the European Command Intelligence School at Camp King and, as a civilian employee following his separation from the army, continued to serve in this role.

Henry Kissinger received his BA degree "summa cum laude," Phi Beta Kappa in political science from Harvard College in 1950, where he lived in Adams House and studied under William Yandell Elliott. He received his MA and PhD degrees at Harvard University in 1951 and 1954, respectively. In 1952, while still a graduate student at Harvard, he served as a consultant to the director of the Psychological Strategy Board. His doctoral dissertation was titled "Peace, Legitimacy, and the Equilibrium (A Study of the Statesmanship of Castlereagh and Metternich)".

Kissinger remained at Harvard as a member of the faculty in the Department of Government and, with Robert R. Bowie, co-founded the Center for International Affairs in 1958 where he served as associate director. In 1955, he was a consultant to the National Security Council's Operations Coordinating Board. During 1955 and 1956, he was also study director in nuclear weapons and foreign policy at the Council on Foreign Relations. He released his book "Nuclear Weapons and Foreign Policy" the following year. From 1956 to 1958 he worked for the Rockefeller Brothers Fund as director of its Special Studies Project. He was director of the Harvard Defense Studies Program between 1958 and 1971. He was also director of the Harvard International Seminar between 1951 and 1971. Outside of academia, he served as a consultant to several government agencies and think tanks, including the Operations Research Office, the Arms Control and Disarmament Agency, Department of State, and the RAND Corporation.

Keen to have a greater influence on U.S. foreign policy, Kissinger became foreign policy advisor to the presidential campaigns of Nelson Rockefeller, supporting his bids for the Republican nomination in 1960, 1964, and 1968. After Richard Nixon became president in 1968, Kissinger was appointed as National Security Advisor.

Kissinger served as National Security Advisor and Secretary of State under President Richard Nixon, and continued as Secretary of State under Nixon's successor Gerald Ford. On Nixon's last full day in office, in the meeting where he informed Ford of his intention to resign the next day, he advised Ford that he felt it was very important that he keep Kissinger in his new administration, to which Ford agreed.

The relationship between Nixon and Kissinger was unusually close, and has been compared to the relationships of Woodrow Wilson and Colonel House, or Franklin D. Roosevelt and Harry Hopkins. In all three cases, State Department was relegated to a backseat role in developing foreign-policy. Historian David Rothkopf has looked at the personalities of Nixon and Kissinger:

A proponent of "Realpolitik", Kissinger played a dominant role in United States foreign policy between 1969 and 1977. In that period, he extended the policy of "détente". This policy led to a significant relaxation in US–Soviet tensions and played a crucial role in 1971 talks with Chinese Premier Zhou Enlai. The talks concluded with a rapprochement between the United States and the People's Republic of China, and the formation of a new strategic anti-Soviet Sino-American alignment. He was jointly awarded the 1973 Nobel Peace Prize with Lê Đức Thọ for helping to establish a ceasefire and U.S. withdrawal from Vietnam. The ceasefire, however, was not durable. Thọ declined to accept the award and Kissinger appeared deeply ambivalent about it (donating his prize money to charity, not attending the award ceremony and later offering to return his prize medal). As National Security Advisor, in 1974 Kissinger directed the much-debated National Security Study Memorandum 200.

Kissinger and Nixon shared a penchant for secrecy and conducted numerous "backchannel" negotiations that excluded State Department experts. One such years-long backchannel was conducted through the Soviet Ambassador to the United States, Anatoly Dobrynin. One historian argues that Kissinger formed such a strong "bond of affection, trust, and mutual interest" with the ambassador that he came to see U.S.-Soviet relations as holding exaggerated significance. He typically met with or talked to Dobrynin about four times a week, and they had a direct line to each other's offices.

As National Security Advisor under Nixon, Kissinger pioneered the policy of "détente" with the Soviet Union, seeking a relaxation in tensions between the two superpowers. As a part of this strategy, he negotiated the Strategic Arms Limitation Talks (culminating in the SALT I treaty) and the Anti-Ballistic Missile Treaty with Leonid Brezhnev, General Secretary of the Soviet Communist Party. Negotiations about strategic disarmament were originally supposed to start under the Johnson Administration but were postponed in protest upon the invasion by Warsaw Pact troops of Czechoslovakia in August 1968.

Kissinger sought to place diplomatic pressure on the Soviet Union. He made two trips to the People's Republic of China in July and October 1971 (the first of which was made in secret) to confer with Premier Zhou Enlai, then in charge of Chinese foreign policy. According to Kissinger's book, "The White House Years" and "On China", the first secret China trip was arranged through Pakistani and Romanian diplomatic and Presidential involvement, as there were no direct communication channels between the states. His trips paved the way for the groundbreaking 1972 summit between Nixon, Zhou, and Communist Party of China Chairman Mao Zedong, as well as the formalization of relations between the two countries, ending 23 years of diplomatic isolation and mutual hostility. The result was the formation of a tacit strategic anti-Soviet alliance between China and the United States.

While Kissinger's diplomacy led to economic and cultural exchanges between the two sides and the establishment of Liaison Offices in the Chinese and American capitals, with serious implications for Indochinese matters, full normalization of relations with the People's Republic of China would not occur until 1979, because the Watergate scandal overshadowed the latter years of the Nixon presidency and because the United States continued to recognize the Republic of China on Taiwan.

In September 1989, the Wall Street Journal's John Fialka disclosed that Kissinger took a direct economic interest in US-China relations in March 1989 with the establishment of China Ventures, Inc., a Delaware limited partnership, of which he was chairman of the board and chief executive officer. A US$75 million investment in a joint venture with the Communist Party government's primary commercial vehicle at the time, China International Trust & Investment Corporation (CITIC), was its purpose. Board members were major clients of Kissinger Associates. Kissinger was criticised for not disclosing his role in the venture when called upon by ABC's Peter Jennings to comment the morning after the June 4, 1989, Tiananmen crackdown. Kissinger's position was generally supportive of Deng Xiaoping's clearance of the square and he opposed economic sanctions.

Kissinger's involvement in Indochina started prior to his appointment as National Security Adviser to Nixon. While still at Harvard, he had worked as a consultant on foreign policy to both the White House and State Department. Kissinger says that "In August 1965 ... <nowiki>[</nowiki>Henry Cabot Lodge, Jr.<nowiki>]</nowiki>, an old friend serving as Ambassador to Saigon, had asked me to visit Vietnam as his consultant. I toured Vietnam first for two weeks in October and November 1965, again for about ten days in July 1966, and a third time for a few days in October 1966 ... Lodge gave me a free hand to look into any subject of my choice". He became convinced of the meaninglessness of military victories in Vietnam, "... unless they brought about a political reality that could survive our ultimate withdrawal". In a 1967 peace initiative, he would mediate between Washington and Hanoi.

Nixon had been elected in 1968 on the promise of achieving "peace with honor" and ending the Vietnam War. In office, and assisted by Kissinger, Nixon implemented a policy of Vietnamization that aimed to gradually withdraw U.S. troops while expanding the combat role of the South Vietnamese Army so that it would be capable of independently defending its government against the National Front for the Liberation of South Vietnam, a Communist guerrilla organization, and the North Vietnamese army (Vietnam People's Army or PAVN). Kissinger played a key role in bombing Cambodia to disrupt PAVN and Viet Cong units launching raids into South Vietnam from within Cambodia's borders and resupplying their forces by using the Ho Chi Minh trail and other routes, as well as the 1970 Cambodian Incursion and subsequent widespread bombing of Khmer Rouge targets in Cambodia. The bombing campaign contributed to the chaos of the Cambodian Civil War, which saw the forces of leader Lon Nol unable to retain foreign support to combat the growing Khmer Rouge insurgency that would overthrow him in 1975. Documents uncovered from the Soviet archives after 1991 reveal that the North Vietnamese invasion of Cambodia in 1970 was launched at the explicit request of the Khmer Rouge and negotiated by Pol Pot's then second in command, Nuon Chea. The American bombing of Cambodia resulted in 40,000–150,000 deaths from 1969 to 1973, including at least 5,000 civilians. Pol Pot biographer David P. Chandler argues that the bombing "had the effect the Americans wanted—it broke the Communist encirclement of Phnom Penh." However, Ben Kiernan and Taylor Owen suggest that "the bombs drove ordinary Cambodians into the arms of the Khmer Rouge, a group that seemed initially to have slim prospects of revolutionary success." Kissinger himself defers to others on the subject of casualty estimates. "...since I am in no position to make an accurate estimate of my own, I consulted the OSD Historian, who gave me an estimate of 50,000 based on the tonnage of bombs delivered over the period of four and a half years."

Along with North Vietnamese Politburo Member Le Duc Tho, Kissinger was awarded the Nobel Peace Prize on December 10, 1973, for their work in negotiating the ceasefires contained in the Paris Peace Accords on "Ending the War and Restoring Peace in Vietnam", signed the previous January. According to Irwin Abrams, this prize was the most controversial to date. For the first time in the history of the Peace Prize, two members left the Nobel Committee in protest. Tho rejected the award, telling Kissinger that peace had not been restored in South Vietnam. Kissinger wrote to the Nobel Committee that he accepted the award "with humility," and "donated the entire proceeds to the children of American servicemembers killed or missing in action in Indochina." After the Fall of Saigon in 1975, Kissinger attempted to return the award.

Under Kissinger's guidance, the United States government supported Pakistan in the Bangladesh Liberation War in 1971. Kissinger was particularly concerned about the expansion of Soviet influence in the Indian Subcontinent as a result of a treaty of friendship recently signed by India and the USSR, and sought to demonstrate to the People's Republic of China (Pakistan's ally and an enemy of both India and the USSR) the value of a tacit alliance with the United States.

Kissinger sneered at people who "bleed" for "the dying Bengalis" and ignored the first telegram from the United States consul general in East Pakistan, Archer K. Blood, and 20 members of his staff, which informed the US that their allies West Pakistan were undertaking, in Blood's words, "a selective genocide". In the second, more famous, Blood Telegram the word genocide was again used to describe the events, and further that with its continuing support for West Pakistan the US government had "evidenced [...] moral bankruptcy".
As a direct response to the dissent against US policy Kissinger and Nixon ended Archer Blood's tenure as United States consul general in East Pakistan and put him to work in the State Department's Personnel Office.

Henry Kissinger had also come under fire for private comments he made to Nixon during the Bangladesh–Pakistan War in which he described Indian Prime Minister Indira Gandhi as a "bitch" and a "witch". He also said "The Indians are bastards", shortly before the war. Kissinger has since expressed his regret over the comments.

According to notes taken by H.R. Haldeman, Nixon "ordered his aides to exclude all Jewish-Americans from policy-making on Israel", including Kissinger. One note quotes Nixon as saying "get K. [Kissinger] out of the play—Haig handle it".

In 1973, Kissinger did not feel that pressing the Soviet Union concerning the plight of Jews being persecuted there was in the interest of U.S. foreign policy. In conversation with Nixon shortly after a meeting with Israeli Prime Minister Golda Meir on March 1, 1973, Kissinger stated, "The emigration of Jews from the Soviet Union is not an objective of American foreign policy, and if they put Jews into gas chambers in the Soviet Union, it is not an American concern. Maybe a humanitarian concern." Kissinger argued, however:

Documents show that Kissinger delayed telling President Richard Nixon about the start of the Yom Kippur War in 1973 to keep him from interfering. On October 6, 1973, the Israelis informed Kissinger about the attack at 6 am; Kissinger waited nearly 3 and a half hours before he informed Nixon.
According to Kissinger, in an interview in November 2013, he was notified at 6:30 a.m. (12:30 pm. Israel time) that war was imminent, and his urgent calls to the Soviets and Egyptians were ineffective. He says Golda Meir's decision not to preempt was wise and reasonable, balancing the risk of Israel looking like the aggressor and Israel's actual ability to strike within such a brief span of time.

The war began on October 6, 1973, when Egypt and Syria attacked Israel. Kissinger published lengthy telephone transcripts from this period in the 2002 book "Crisis". On October 12, under Nixon's direction, and against Kissinger's initial advice, while Kissinger was on his way to Moscow to discuss conditions for a cease-fire, Nixon sent a message to Brezhnev giving Kissinger full negotiating authority.

Israel regained the territory it lost in the early fighting and gained new territories from Syria and Egypt, including land in Syria east of the previously captured Golan Heights, and additionally on the western bank of the Suez Canal, although they did lose some territory on the eastern side of the Suez Canal that had been in Israeli hands since the end of the Six-Day War. Kissinger pressured the Israelis to cede some of the newly captured land back to its Arab neighbors, contributing to the first phases of Israeli–Egyptian non-aggression. The move saw a warming in U.S.–Egyptian relations, bitter since the 1950s, as the country moved away from its former independent stance and into a close partnership with the United States. The peace was finalized in 1978 when U.S. President Jimmy Carter mediated the Camp David Accords, during which Israel returned the Sinai Peninsula in exchange for an Egyptian peace agreement that included the recognition of the state of Israel.

In the midst of the war, in what journalist Elizabeth Drew called “Strangelove Day,” Kissinger put U.S. military forces on DEFCON III late in the evening of October 24, in what a historian argues is "best understood as an emotional response to a misunderstanding" with Soviet ambassador to the United States Anatoly Dobrynin.

Following a period of steady relations between the U.S. Government and the Greek military regime after 1967, Secretary of State Kissinger was faced with the coup by the Greek junta and the Turkish invasion of Cyprus in July and August 1974. In an August 1974 edition of "The New York Times", it was revealed that Kissinger and State Department were informed in advance οf the impending coup by the Greek junta in Cyprus. Indeed, according to the journalist, the official version of events as told by the State Department was that it felt it had to warn the Greek military regime not to carry out the coup. The warning had been delivered by July 9, according to repeated assurances from its Athens services, that is, the U.S. embassy and the American ambassador Henry J. Tasca himself.

Ioannis Zigdis, then a Greek MP for Centre Union and former minister, claimed that "the Cyprus crisis will become Kissinger's Watergate". Zigdis also stressed: "Not only did Kissinger know about the coup for the overthrow of Archbishop Makarios before July 15th, he also encouraged it, if he did not instigate it." It is unclear what evidence Zigdis had to support this allegation.

Kissinger was a target of anti-American sentiment which was a significant feature of Greek public opinion at the time—particularly among young people—viewing the U.S. role in Cyprus as negative. In a demonstration by students in Heraklion, Crete, soon after the second phase of the Turkish invasion in August 1974, slogans such as "Kissinger, murderer", "Americans get out", "No to Partition" and "Cyprus is no Vietnam" were heard.

Some years later, Kissinger expressed the opinion that the Cyprus issue was resolved in 1974.

The United States continued to recognize and maintain relationships with non-left-wing governments, democratic and authoritarian alike. John F. Kennedy's Alliance for Progress was ended in 1973. In 1974, negotiations over a new settlement for the Panama Canal began, and they eventually led to the Torrijos-Carter Treaties and the handing over of the Canal to Panamanian control.

Kissinger initially supported the normalization of United States-Cuba relations, broken since 1961 (all U.S.–Cuban trade was blocked in February 1962, a few weeks after the exclusion of Cuba from the Organization of American States because of U.S. pressure). However, he quickly changed his mind and followed Kennedy's policy. After the involvement of the Cuban Revolutionary Armed Forces in the independence struggles in Angola and Mozambique, Kissinger said that unless Cuba withdrew its forces relations would not be normalized. Cuba refused.

Chilean Socialist Party presidential candidate Salvador Allende was elected by a plurality of 36.2 percent in 1970, causing serious concern in Washington, D.C. due to his openly socialist and pro-Cuban politics. The Nixon administration, with Kissinger's input, authorized the Central Intelligence Agency (CIA) to encourage a military coup that would prevent Allende's inauguration, but the plan was not successful.

United States-Chile relations remained frosty during Salvador Allende's tenure, following the complete nationalization of the partially U.S.-owned copper mines and the Chilean subsidiary of the U.S.-based ITT Corporation, as well as other Chilean businesses. The U.S. claimed that the Chilean government had greatly undervalued fair compensation for the nationalization by subtracting what it deemed "excess profits". Therefore, the U.S. implemented economic sanctions against Chile. The CIA also provided funding for the mass anti-government strikes in 1972 and 1973, and extensive black propaganda in the newspaper "El Mercurio".

The most expeditious way to prevent Allende from assuming office was somehow to convince the Chilean congress to confirm Jorge Alessandri as the winner of the election. Once elected by the congress, Alessandri—a party to the plot through intermediaries—was prepared to resign his presidency within a matter of days so that new elections could be held. This first, nonmilitary, approach to stopping Allende was called the Track I approach. The CIA's second approach, the Track II approach, was designed to encourage a military overthrow.

On September 11, 1973, Allende died during a military coup launched by Army Commander-in-Chief Augusto Pinochet, who became President.
A document released by the CIA in 2000 titled "CIA Activities in Chile" revealed that the United States, acting through the CIA, actively supported the military junta after the overthrow of Allende, and that it made many of Pinochet's officers into paid contacts of the CIA or U.S. military.

In September 1976, Orlando Letelier, a Chilean opponent of the Pinochet regime, was assassinated in Washington, D.C. with a car bomb. Previously, Kissinger had helped secure his release from prison, and had chosen to cancel a letter to Chile warning them against carrying out any political assassinations. The U.S. ambassador to Chile, David H. Popper, said that Pinochet might take as an insult any inference that he was connected with assassination plots. It has been confirmed that Pinochet directly ordered the assassination. This murder was part of Operation "Condor", a covert program of political repression and assassination carried out by Southern Cone nations that Kissinger has been accused of being involved in.

On September 10, 2001, the family of Chilean general René Schneider filed a suit against Kissinger, accusing him of collaborating in arranging Schneider's kidnapping which resulted in his death. According to phone records, Kissinger claimed to have "turned off" the operation. However, the CIA claimed that no such "stand-down" order was ever received, and he and Nixon later joked that an "incompetent" CIA had struggled to kill Schneider. A subsequent Congressional investigation found that the CIA was not directly involved in Schneider's death. The case was later dismissed by a U.S. District Court, citing separation of powers: "The decision to support a coup of the Chilean government to prevent Dr. Allende from coming to power, and the means by which the United States Government sought to effect that goal, implicate policy makers in the murky realm of foreign affairs and national security best left to the political branches." Decades later the CIA admitted its involvement in the kidnapping of General Schneider, but not his murder, and subsequently paid the group responsible for his death $35,000 "to keep the prior contact secret, maintain the goodwill of the group, and for humanitarian reasons."

Kissinger took a similar line as he had toward Chile when the Argentine military, led by Jorge Videla, toppled the elected government of Isabel Perón in 1976 with a process called the National Reorganization Process by the military, with which they consolidated power, launching brutal reprisals and "disappearances" against political opponents. An October 1987 investigative report in The Nation broke the story of how, in a June 1976 meeting in the Hotel Carrera in Santiago, Kissinger gave the bloody military junta in neighboring Argentina the "green light" for their own clandestine repression against leftwing guerrillas and other dissidents, thousands of whom were kept in more than 400 secret concentration camps before they were executed. During a meeting with Argentine foreign minister César Augusto Guzzetti, Kissinger assured him that the United States was an ally, but urged him to "get back to normal procedures" quickly before the U.S. Congress reconvened and had a chance to consider sanctions.

As the article published in "The Nation" noted, as the state-sponsored terror mounted, conservative Republican U.S. Ambassador to Buenos Aires Robert C. Hill "'was shaken, he became very disturbed, by the case of the son of a thirty-year embassy employee, a student who was arrested, never to be seen again,' recalled former "New York Times" reporter Juan de Onis. 'Hill took a personal interest.' He went to the Interior Minister, a general with whom he had worked on drug cases, saying, 'Hey, what about this? We're interested in this case.' He questioned (Foreign Minister Cesar) Guzzetti and, finally, President Jorge R. Videla himself. 'All he got was stonewalling; he got nowhere.' de Onis said. 'His last year was marked by increasing disillusionment and dismay, and he backed his staff on human rights right to the hilt."

In a letter to "The Nation" editor Victor Navasky, protesting publication of the article, Kissinger claimed that: "At any rate, the notion of Hill as a passionate human rights advocate is news to all his former associates." Yet Kissinger aide Harry W. Shlaudeman later disagreed with Kissinger, telling the oral historian William E. Knight of the Association for Diplomatic Studies and Training Foreign Affairs Oral History Project: "It really came to a head when I was Assistant Secretary, or it began to come to a head, in the case of Argentina where the dirty war was in full flower. Bob Hill, who was Ambassador then in Buenos Aires, a very conservative Republican politician -- by no means liberal or anything of the kind, began to report quite effectively about what was going on, this slaughter of innocent civilians, supposedly innocent civilians -- this vicious war that they were conducting, underground war. He, at one time in fact, sent me a back-channel telegram saying that the Foreign Minister, who had just come for a visit to Washington and had returned to Buenos Aires, had gloated to him that Kissinger had said nothing to him about human rights. I don't know -- I wasn't present at the interview."

Navasky later wrote in his book about being confronted by Kissinger, "'Tell me, Mr. Navasky,' [Kissinger] said in his famous guttural tones, 'how is it that a short article in a obscure journal such as yours about a conversation that was supposed to have taken place years ago about something that did or didn't happen in Argentina resulted in sixty people holding placards denouncing me a few months ago at the airport when I got off the plane in Copenhagen?'"

According to declassified state department files, Kissinger also attempted to thwart the Carter Administration's efforts to halt the mass killings by the 1976–83 military dictatorship.

In September 1976 Kissinger was actively involved in negotiations regarding the Rhodesian Bush War. Kissinger, along with South Africa's Prime Minister John Vorster, pressured Rhodesian Prime Minister Ian Smith to hasten the transition to black majority rule in Rhodesia. With FRELIMO in control of Mozambique and even South Africa withdrawing its support, Rhodesia's isolation was nearly complete. According to Smith's autobiography, Kissinger told Smith of Mrs. Kissinger's admiration for him, but Smith stated that he thought Kissinger was asking him to sign Rhodesia's "death certificate". Kissinger, bringing the weight of the United States, and corralling other relevant parties to put pressure on Rhodesia, hastened the end of minority-rule.

The Portuguese decolonization process brought U.S. attention to the former Portuguese colony of East Timor, which lies within the Indonesian archipelago and declared its independence in 1975. Indonesian president Suharto was a strong U.S. ally in Southeast Asia and began to mobilize the Indonesian army, preparing to annex the nascent state, which had become increasingly dominated by the popular leftist Fretilin party. In December 1975, Suharto discussed the invasion plans during a meeting with Kissinger and President Ford in the Indonesian capital of Jakarta. Both Ford and Kissinger made clear that U.S. relations with Indonesia would remain strong and that it would not object to the proposed annexation. They only wanted it done "fast" and proposed that it be delayed until after they had returned to Washington. Accordingly, Suharto delayed the operation for one day. Finally on December 7 Indonesian forces invaded the former Portuguese colony. U.S. arms sales to Indonesia continued, and Suharto went ahead with the annexation plan. According to Ben Kiernan, the invasion and occupation resulted in the deaths of nearly a quarter of the Timorese population from 1975 to 1981.

In February 1976 Kissinger considered launching air strikes against ports and military installations in Cuba, as well as deploying Marine battalions based at the US Navy base at Guantanamo Bay, in retaliation for Cuban President Fidel Castro's decision in late 1975 to send troops to Angola to help the newly independent nation fend off attacks from South Africa and right-wing guerrillas.

Kissinger left office when Democrat Jimmy Carter defeated Republican Gerald Ford in the 1976 presidential elections. Kissinger continued to participate in policy groups, such as the Trilateral Commission, and to maintain political consulting, speaking, and writing engagements.

After Kissinger left office in 1977, he was offered an endowed chair at Columbia University. There was student opposition to the appointment, which became a subject of media commentary. Columbia canceled the appointment as a result.

Kissinger was then appointed to Georgetown University's Center for Strategic and International Studies. He taught at Georgetown's Edmund Walsh School of Foreign Service for several years in the late 1970s. In 1982, with the help of a loan from the international banking firm of E.M. Warburg, Pincus and Company, Kissinger founded a consulting firm, Kissinger Associates, and is a partner in affiliate Kissinger McLarty Associates with Mack McLarty, former chief of staff to President Bill Clinton. He also serves on the board of directors of Hollinger International, a Chicago-based newspaper group, and as of March 1999, was a director of Gulfstream Aerospace.

From 1995 to 2001, Kissinger served on the board of directors for Freeport-McMoRan, a multinational copper and gold producer with significant mining and milling operations in Papua, Indonesia. In February 2000, then-president of Indonesia Abdurrahman Wahid appointed Kissinger as a political advisor. He also serves as an honorary advisor to the United States-Azerbaijan Chamber of Commerce.

From 2000–2006, Kissinger served as chairman of the board of trustees of Eisenhower Fellowships. In 2006, upon his departure from Eisenhower Fellowships, he received the Dwight D. Eisenhower Medal for Leadership and Service.

In November 2002, he was appointed by President George W. Bush to chair the newly established National Commission on Terrorist Attacks Upon the United States to investigate the September 11 attacks. Kissinger stepped down as chairman on December 13, 2002, rather than reveal his business client list, when queried about potential conflicts of interest.

In the Rio Tinto espionage case of 2009–2010, Kissinger was paid $5 million to advise the multinational mining company how to distance itself from an employee who had been arrested in China for bribery.

Kissinger—along with William Perry, Sam Nunn, and George Shultz—has called upon governments to embrace the vision of a world free of nuclear weapons, and in three "Wall Street Journal" op-eds proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. In 2010, the four were featured in a documentary film entitled "Nuclear Tipping Point". The film is a visual and historical depiction of the ideas laid forth in the "Wall Street Journal" op-eds and reinforces their commitment to a world without nuclear weapons and the steps that can be taken to reach that goal.

In December 2008, Kissinger was given the American Patriot Award by the National Defense University Foundation "in recognition for his distinguished career in public service." Earlier that year, a NDU professor had blown the whistle on the fact that a Chilean colleague at the William J. Perry Center for Hemispheric Defense Studies of U.S. Southern Command headquartered at NDU had not only been a member of Pinochet's DINA death squad operation (the same organization responsible for the 1976 car bomb murder of former Chilean Foreign Minister Orlando Letelier and American aide Ronni Karpen Moffitt less than a mile from the White House), but was in addition accused of participating in the torture and murder of seven detainees in Chile. The whistleblower, Martin Edwin Andersen, was not only a senior staff member who earlier—as a senior advisor for policy planning at the Criminal Division of the U.S. Department of Justice—was the first national security whistleblower to receive the U.S. Office of Special Counsel's "Public Servant Award," but was also the same person who broke the story in The Nation on Kissinger's "green light" for Argentina's dirty "war."

On November 17, 2016, Kissinger met with then President-elect Donald Trump during which they discussed global affairs. Kissinger also met with President Trump at the White House in May 2017.

In an interview with Charlie Rose on August 17, 2017, Kissinger said about President Trump: "I'm hoping for an Augustinian moment, for St. Augustine ... who in his early life followed a pattern that was quite incompatible with later on when he had a vision, and rose to sainthood. One does not expect the president to become that, but it's conceivable ..." Kissinger also argued that Russian President Vladimir Putin wanted to weaken Hillary Clinton, not elect Donald Trump. Kissinger said that Putin "thought—wrongly incidentally—that she would be extremely confrontational ... I think he tried to weaken the incoming president [Clinton]".

In several articles of his and interviews that he gave during the Yugoslav wars, he criticized the United States' policies in Southeast Europe, among other things for the recognition of Bosnia and Herzegovina as a sovereign state, which he described as a foolish act. Most importantly he dismissed the notion of Serbs and Croats being aggressors or separatist, saying that "they can't be separating from something that has never existed". In addition, he repeatedly warned the West against inserting itself into a conflict that has its roots at least hundreds of years back in time, and said that the West would do better if it allowed the Serbs and Croats to join their respective countries. Kissinger shared similarly critical views on Western involvement in Kosovo. In particular, he held a disparaging view of the Rambouillet Agreement:

However, as the Serbs did not accept the Rambouillet text and NATO bombings started, he opted for a continuation of the bombing as NATO's credibility was now at stake, but dismissed the use of ground forces, claiming that it was not worth it.

In 2006, it was reported in the book "" by Bob Woodward that Kissinger met regularly with President George W. Bush and Vice President Dick Cheney to offer advice on the Iraq War. Kissinger confirmed in recorded interviews with Woodward that the advice was the same as he had given in a column in "The Washington Post" on August 12, 2005: "Victory over the insurgency is the only meaningful exit strategy."

In an interview on the BBC's "Sunday AM" on November 19, 2006, Kissinger was asked whether there is any hope left for a clear military victory in Iraq and responded, "If you mean by 'military victory' an Iraqi government that can be established and whose writ runs across the whole country, that gets the civil war under control and sectarian violence under control in a time period that the political processes of the democracies will support, I don't believe that is possible. ... I think we have to redefine the course. But I don't believe that the alternative is between military victory as it had been defined previously, or total withdrawal."

In an interview with Peter Robinson of the Hoover Institution on April 3, 2008, Kissinger reiterated that even though he supported the 2003 invasion of Iraq, he thought that the George W. Bush administration rested too much of its case for war on Saddam's supposed weapons of mass destruction. Robinson noted that Kissinger had criticized the administration for invading with too few troops, for disbanding the Iraqi Army, and for mishandling relations with certain allies.

Kissinger said in April 2008 that "India has parallel objectives to the United States," and he called it an ally of the U.S.

Kissinger was present at the opening ceremony of the 2008 Beijing Summer Olympics.

In 2011, Kissinger published "On China", chronicling the evolution of Sino-American relations and laying out the challenges to a partnership of 'genuine strategic trust' between the U.S. and China.

In his 2011 book On China, his 2014 book World Order and in a 2018 interview with Financial Times, Kissinger stated that he believes China wants to restore its historic role as the Middle Kingdom and be "the principal adviser to all humanity".

Kissinger's position on this issue of U.S.–Iran talks was reported by the "Tehran Times" to be that "Any direct talks between the U.S. and Iran on issues such as the nuclear dispute would be most likely to succeed if they first involved only diplomatic staff and progressed to the level of secretary of state before the heads of state meet." In 2016, Kissinger said that the biggest challenge facing the Middle East is the "potential domination of the region by an Iran that is both imperial and jihadist." He further wrote in August 2017 that if the Islamic Revolutionary Guard Corps of Iran and its Shiite allies were allowed to fill the territorial vacuum left by a militarily defeated Islamic State of Iraq and the Levant, the region would be left with a land corridor extending from Iran to the Levant "which could mark the emergence of an Iranian radical empire." Commenting on the Joint Comprehensive Plan of Action, Kissinger said that he wouldn't have agreed to it, but that Trump's plan to end the agreement after it was signed would "enable the Iranians to do more than us."

On March 5, 2014, "The Washington Post" published an op-ed piece by Kissinger, 11 days before the Crimean referendum on whether Autonomous Republic of Crimea should officially rejoin Ukraine or join neighboring Russia. In it, he attempted to balance the Ukrainian, Russian and Western desires for a functional state. He made four main points:

Kissinger also wrote: "The west speaks Ukrainian; the east speaks mostly Russian. Any attempt by one wing of Ukraine to dominate the other—as has been the pattern—would lead eventually to civil war or break up."

Following the publication of his book titled "World Order", Kissinger participated in an interview with Charlie Rose and updated his position on Ukraine, which he sees as a possible geographical mediator between Russia and the West. In a question he posed to himself for illustration regarding re-conceiving policy regarding Ukraine, Kissinger stated: "If Ukraine is considered an outpost, then the situation is that its eastern border is the NATO strategic line, and NATO will be within of Volgograd. That will never be accepted by Russia. On the other hand, if the Russian western line is at the border of Poland, Europe will be permanently disquieted. The Strategic objective should have been to see whether one can build Ukraine as a bridge between East and West, and whether one can do it as a kind of a joint effort."

In December 2016, Kissinger advised then President-elect Donald Trump to accept "Crimea as a part of Russia" in an attempt to secure a rapprochement between the United States and Russia, whose relations soured as a result of the Crimean crisis.

When asked if he explicitly considered Russia's sovereignty over Crimea legitimate, Kissinger answered in the affirmative, reversing the position he took in his "Washington Post" op-ed.

At the height of Kissinger's prominence, many commented on his wit. In February 1972, at the Washington Press Club annual congressional dinner, "Kissinger mocked his reputation as a secret swinger." The insight, "Power is the ultimate aphrodisiac", is widely attributed to him, although Kissinger was paraphrasing Napoleon Bonaparte. Some scholars have ranked Kissinger as the most effective U.S. Secretary of State in the 50 years to 2015. A number of activists and human rights lawyers, however, have sought his prosecution for alleged war crimes. According to historian and Kissinger biographer Niall Ferguson, however, accusing Kissinger alone of war crimes "requires a double standard" because "nearly all the secretaries of state ... and nearly all the presidents" have taken similar actions.
Kissinger was interviewed in "", a documentary examining the underpinnings of the 1979 peace treaty between Israel and Egypt. In the film, Kissinger revealed how close he felt the world came to nuclear war during the 1973 Yom Kippur War launched by Egypt and Syria against Israel.
Attempts were made to blame Kissinger for injustices in American foreign policy during his tenure in government. In September 2001, relatives and survivors of General Rene Schneider (former head of the Chilean general staff) filed civil proceedings in Federal Court in Washington, DC, and, in April 2002, a petition for Kissinger's arrest was filed in the High Court in London by human rights campaigner Peter Tatchell, citing the destruction of civilian populations and the environment in Indochina during the years 1969–75. Both suits were determined to lack legal foundation and were dismissed. British-American journalist and author Christopher Hitchens authored "The Trial of Henry Kissinger", in which Hitchens calls for the prosecution of Kissinger "for war crimes, for crimes against humanity, and for offenses against common or customary or international law, including conspiracy to commit murder, kidnap, and torture". Critics on the right, such as Ray Takeyh, have faulted Kissinger for his role in the Nixon administration's opening to China and secret negotiations with North Vietnam. Takeyh writes that while rapprochement with China was a worthy goal, the Nixon administration failed to achieve any meaningful concessions from Chinese officials in return, as China continued to support North Vietnam and various "revolutionary forces throughout the Third World," "nor does there appear to be even a remote, indirect connection between Nixon and Kissinger's diplomacy and the communist leadership's decision, after Mao's bloody rule, to move away from a communist economy towards state capitalism."

On Vietnam, Takeyh claims that Kissinger's negotiations with Le Duc Tho were intended only "to secure a 'decent interval' between America's withdrawal and South Vietnam's collapse." Johannes Kadura offers a more positive assessment of Nixon and Kissinger's strategy, arguing that the two men "simultaneously maintained a Plan A of further supporting Saigon and a Plan B of shielding Washington should their maneuvers prove futile." According to Kadura, the "decent interval" concept has been "largely misrepresented," in that Nixon and Kissinger "sought to gain time, make the North turn inward, and create a perpetual equilibrium" rather than acquiescing in the collapse of South Vietnam, but the strength of the anti-war movement and the sheer unpredictability of events in Indochina compelled them to prepare for the possibility that South Vietnam might collapse despite their best efforts. Kadura concludes: "Without Nixon, Kissinger, and Ford's clever use of triangular diplomacy ... The Soviets and the Chinese could have been tempted into a far more aggressive stance" following the "U.S. defeat in Indochina" than actually occurred. In 2011, Chimerica Media released an interview-based documentary, titled "Kissinger", in which Kissinger "reflects on some of his most important and controversial decisions" during his tenure as Secretary of State.

Kissinger's record was brought up during the 2016 Democratic Party presidential primaries. Hillary Clinton had cultivated a close relationship with Kissinger, describing him as a "friend" and a source of "counsel." During the Democratic Primary Debates, Clinton touted Kissinger's praise for her record as Secretary of State. In response, candidate Bernie Sanders issued a critique of Kissinger's foreign policy, declaring: "I am proud to say that Henry Kissinger is not my friend. I will not take advice from Henry Kissinger."

Kissinger married Ann Fleischer on February 6, 1949. They had two children, Elizabeth and David, and divorced in 1964. On March 30, 1974, he married Nancy Maginnes. They now live in Kent, Connecticut, and in New York City. Kissinger's son David Kissinger served as an executive with NBCUniversal before becoming head of Conaco, Conan O'Brien's production company. In February 1982, Kissinger underwent coronary bypass surgery at the age of 58.

Kissinger described "Diplomacy" as his favorite game in a 1973 interview.

Daryl Grove characterised Kissinger as one of the most influential people in the growth of soccer in the United States. Kissinger was named chairman of the North American Soccer League board of directors in 1978.

Since his childhood, Kissinger has been a fan of his hometown's soccer club, SpVgg Greuther Fürth. Even during his time in office German Embassy informed him about the team's results every Monday morning. He is an honorary member with lifetime season-tickets. In September 2012 Kissinger attended a home game in which SpVgg Greuther Fürth lost, 0–2, against Schalke after promising years ago he would attend a Greuther Fürth home game if they were promoted to the Bundesliga, the top football league in Germany, from the 2. Bundesliga. Kissinger is an honorary member of the German soccer club FC Bayern München.











</doc>
<doc id="13767" url="https://en.wikipedia.org/wiki?curid=13767" title="Hydra (genus)">
Hydra (genus)

Hydra is a genus of small, fresh-water organisms of the phylum Cnidaria and class Hydrozoa. They are native to the temperate and tropical regions. Biologists are especially interested in "Hydra" because of their regenerative ability — they do not appear to die of old age, or indeed to age at all.

"Hydra" has a tubular, radially symmetric body up to long when extended, secured by a simple adhesive foot called the basal disc. Gland cells in the basal disc secrete a sticky fluid that accounts for its adhesive properties.

At the free end of the body is a mouth opening surrounded by one to twelve thin, mobile tentacles. Each tentacle, or cnida (plural: cnidae), is clothed with highly specialised stinging cells called cnidocytes. Cnidocytes contain specialized structures called nematocysts, which look like miniature light bulbs with a coiled thread inside. At the narrow outer edge of the cnidocyte is a short trigger hair called a cnidocil. Upon contact with prey, the contents of the nematocyst are explosively discharged, firing a dart-like thread containing neurotoxins into whatever triggered the release. This can paralyze the prey, especially if many hundreds of nematocysts are fired.

"Hydra" has two main body layers, which makes it "diploblastic". The layers are separated by mesoglea, a gel-like substance. The outer layer is the epidermis, and the inner layer is called the gastrodermis, because it lines the stomach. The cells making up these two body layers are relatively simple. Hydramacin is a bactericide recently discovered in "Hydra"; it protects the outer layer against infection. A single "Hydra" is composed of 50,000 to 100,000 cells which consist of three specific stem cell populations that will create many different cell types. These stem cells will continually renew themselves in the body column"." "Hydras" have two significant structures on their body: the "head" and the "foot". When a "Hydra" is cut in half, each half will regenerate and form into a small "Hydra"; the "head" will regenerate a "foot" and the "foot" will regenerate a "head". If the "Hydra" is sliced into many segments then the middle slices will form both a "head" and a "foot".

Respiration and excretion occur by diffusion throughout the surface of the epidermis, while larger excreta are discharged through the mouth.

The nervous system of "Hydra" is a nerve net, which is structurally simple compared to more derived animal nervous systems. "Hydra" does not have a recognizable brain or true muscles. Nerve nets connect sensory photoreceptors and touch-sensitive nerve cells located in the body wall and tentacles.

The structure of the nerve net has two levels: 

Some has only two sheets of neurons.

If "Hydra" are alarmed or attacked, the tentacles can be retracted to small buds, and the body column itself can be retracted to a small gelatinous sphere. "Hydra" generally react in the same way regardless of the direction of the stimulus, and this may be due to the simplicity of the nerve nets.

"Hydra" are generally or sessile, but do occasionally move quite readily, especially when hunting. They have two distinct methods for moving – 'looping' and 'somersaulting'. They do this by bending over and attaching themselves to the with the mouth and tentacles and then relocate the foot, which provides the usual attachment, this process is called looping. In somersaulting, the body then bends over and makes a new place of attachment with the foot. By this process of "looping" or "somersaulting", a "Hydra" can move several inches (c. 100 mm) in a day. "Hydra" may also move by amoeboid motion of their bases or by detaching from the substrate and floating away in the current.

When food is plentiful, many "Hydra" reproduce asexually by producing buds in the body wall, which grow to be miniature adults and break away when they are mature.

When a hydra is well fed, a new bud can form every two days. When conditions are harsh, often before winter or in poor feeding conditions, sexual reproduction occurs in some "Hydra". Swellings in the body wall develop into either ovaries or testes. The testes release free-swimming gametes into the water, and these can fertilize the egg in the ovary of another individual. The fertilized eggs secrete a tough outer coating, and, as the adult dies (due to starvation and/or cold), these resting eggs fall to the bottom of the lake or pond to await better conditions, whereupon they hatch into nymph "Hydra". Some "Hydra" species, like "Hydra circumcincta" and "Hydra viridissima", are hermaphrodites and may produce both testes and ovaries at the same time.

Many members of the Hydrozoa go through a body change from a polyp to an adult form called a medusa, which is usually the life stage where sexual reproduction occurs, but "Hydra" do not progress beyond the polyp phase.

"Hydra" mainly feed on aquatic invertebrates such as "Daphnia" and "Cyclops".

While feeding, "Hydra" extend their body to maximum length and then slowly extend their tentacles. Despite their simple construction, the tentacles of "Hydra" are extraordinarily extensible and can be four to five times the length of the body. Once fully extended, the tentacles are slowly manoeuvred around waiting for contact with a suitable prey animal. Upon contact, nematocysts on the tentacle fire into the prey, and the tentacle itself coils around the prey. Within 30 seconds, most of the remaining tentacles will have already joined in the attack to subdue the struggling prey. Within two minutes, the tentacles will have surrounded the prey and moved it into the opened mouth aperture. Within ten minutes, the prey will have been engulfed within the body cavity, and digestion will have started. "Hydra" are able to stretch their body wall considerably in order to digest prey more than twice their size. After two or three days, the indigestible remains of the prey will be discharged through the mouth aperture via contractions.

The feeding behaviour of "Hydra" demonstrates the sophistication of what appears to be a simple nervous system.

Some species of "Hydra" exist in a mutual relationship with various types of unicellular algae. The algae are protected from predators by "Hydra" and, in return, photosynthetic products from the algae are beneficial as a food source to "Hydra".

The feeding response in "Hydra" is induced by reduced glutathione released from injured prey. There are several methods conventionally used for quantification of the feeding response. In some, the duration for which the mouth remains open is measured. Other methods rely on counting the number of "Hydra" among a small population showing the feeding response after addition of glutathione. Recently, an assay for measuring the feeding response in hydra has been developed. In this method, the linear two-dimensional distance between the tip of the tentacle and the mouth of hydra was shown to be a direct measure of the extent of the feeding response. This method has been validated using a starvation model, as starvation is known to cause enhancement of the "Hydra" feeding response. 

"Hydra" undergoes morphallaxis (tissue regeneration) when injured or severed. Typically, "Hydras" will reproduce by just budding off a whole new individual, the bud will occur around two-thirds of the way down the body axis. When a "Hydra" is cut in half, each half will regenerate and form into a small "Hydra"; the "head" will regenerate a "foot" and the "foot" will regenerate a "head". This regeneration occurs without cell division. If the "Hydra" is sliced into many segments then the middle slices will form both a "head" and a "foot". The polarity of the regeneration is explained by two pairs of positional value gradients. There is both a head and foot activation and inhibition gradient. The head activation and inhibition works in an opposite direction of the pair of foot gradients. The evidence for these gradients was shown in the early 1900s with grafting experiments. The inhibitors for both gradients have shown to be important to block the bud formation. The location that the bud will form is where the gradients are low for both the head and foot. "Hydras" are capable of regenerating from pieces of tissue from the body and additionally after tissue dissociation from reaggregates. 

Daniel Martinez claimed in a 1998 article in "Experimental Gerontology" that "Hydra" are biologically immortal. This publication has been widely cited as evidence that "Hydra" do not senesce (do not age), and that they are proof of the existence of non-senescing organisms generally. In 2010, Preston Estep published (also in "Experimental Gerontology") a letter to the editor arguing that the Martinez data "refute" the hypothesis that "Hydra" do not senesce.

The controversial unlimited life span of "Hydra" has attracted much attention from scientists. Research today appears to confirm Martinez' study. "Hydra" stem cells have a capacity for indefinite self-renewal. The transcription factor "forkhead box O" (FoxO) has been identified as a critical driver of the continuous self-renewal of "Hydra". In experiments, a drastically reduced population growth resulted from FoxO down-regulation. 

In bilaterally symmetrical organisms (Bilateria), the transcription factor FoxO impacts stress response, lifespan, and increase in stem cells. If this transcription factor is knocked down in bilaterian model organisms, such as fruit flies and nematodes, their lifespan is significantly decreased. In experiments on "H. vulgaris" (a radially symmetrical member of phylum Cnidaria), when FoxO levels were decreased, there was a negative impact of many key features of the "Hydra", but no death was observed, thus it is believed other factors may contribute to the apparent lack of aging in these creatures.

While "Hydra" immortality is well-supported today, the implications for human aging are still controversial. There is much optimism; however, it appears that researchers still have a long way to go before they are able to understand how the results of their work might apply to the reduction or elimination of human senescence.

An orthologome analysis done within the last decade demonstrated that "Hydra" share a minimum of 6071 genes with humans. "Hydra" is becoming an increasingly better model system as more genetic approaches become available. A draft of the genome of "Hydra magnipapillata" was reported in 2010.



</doc>
<doc id="13768" url="https://en.wikipedia.org/wiki?curid=13768" title="Hydrus">
Hydrus

Hydrus is a small constellation in the deep southern sky. It was one of twelve constellations created by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman and it first appeared on a 35-cm (14 in) diameter celestial globe published in late 1597 (or early 1598) in Amsterdam by Plancius and Jodocus Hondius. The first depiction of this constellation in a celestial atlas was in Johann Bayer's Uranometria of 1603. The French explorer and astronomer Nicolas Louis de Lacaille charted the brighter stars and gave their Bayer designations in 1756. Its name means "male water snake", as opposed to Hydra, a much larger constellation that represents a female water snake. It remains below the horizon for most Northern Hemisphere observers.

The brightest star is the 2.8-magnitude Beta Hydri, also the closest reasonably bright star to the south celestial pole. Pulsating between magnitude 3.26 and 3.33, Gamma Hydri is a variable red giant 60 times the diameter of our Sun. Lying near it is VW Hydri, one of the brightest dwarf novae in the heavens. Four star systems in Hydrus have been found to have exoplanets to date, including HD 10180, which could bear up to nine planetary companions.

Hydrus was one of the twelve constellations established by the Dutch astronomer Petrus Plancius from the observations of the southern sky by the Dutch explorers Pieter Dirkszoon Keyser and Frederick de Houtman, who had sailed on the first Dutch trading expedition, known as the "Eerste Schipvaart", to the East Indies. It first appeared on a 35-cm (14 in) diameter celestial globe published in 1598 in Amsterdam by Plancius with Jodocus Hondius. The first depiction of this constellation in a celestial atlas was in the German cartographer Johann Bayer's "Uranometria" of 1603. De Houtman included it in his southern star catalogue the same year under the Dutch name "De Waterslang", "The Water Snake", it representing a type of snake encountered on the expedition rather than a mythical creature. The French explorer and astronomer Nicolas Louis de Lacaille called it "l’Hydre Mâle" on the 1756 version of his planisphere of the southern skies, distinguishing it from the feminine Hydra. The French name was retained by Jean Fortin in 1776 for his "Atlas Céleste", while Lacaille Latinised the name to Hydrus for his revised "Coelum Australe Stelliferum" in 1763.

Irregular in shape, Hydrus is bordered by Mensa to the southeast, Eridanus to the east, Horologium and Reticulum to the northeast, Phoenix to the north, Tucana to the northwest and west, and Octans to the south; Lacaille had shortened Hydrus' tail to make space for this last constellation he had drawn up. Covering 243 square degrees and 0.589% of the night sky, it ranks 61st of the 88 constellations in size. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Hyi'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 12 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −57.85° and −82.06°. As one of the deep southern constellations, it remains below the horizon at latitudes north of the 30th parallel in the Northern Hemisphere, and is circumpolar at latitudes south of the 50th parallel in the Southern Hemisphere. Indeed, Herman Melville mentions it and Argo Navis in Moby Dick "beneath effulgent Antarctic Skies", highlighting his knowledge of the southern constellations from whaling voyages. A line drawn between the long axis of the Southern Cross to Beta Hydri and then extended 4.5 times will mark a point due south. Hydrus culminates at midnight around 26 October.

Keyzer and de Houtman assigned fifteen stars to the constellation in their Malay and Madagascan vocabulary, with a star that would be later designated as Alpha Hydri marking the head, Gamma the chest and a number of stars that were later allocated to Tucana, Reticulum, Mensa and Horologium marking the body and tail. Lacaille charted and designated 20 stars with the Bayer designations Alpha through to Tau in 1756. Of these, he used the designations Eta, Pi and Tau twice each, for three sets of two stars close together, and omitted Omicron and Xi. He assigned Rho to a star that subsequent astronomers were unable to find.

Beta Hydri, the brightest star in Hydrus, is a yellow star of apparent magnitude 2.8, lying 24 light-years from Earth. It has about 104% of the mass of the Sun and 181% of the Sun's radius, with more than three times the Sun's luminosity. The spectrum of this star matches a stellar classification of G2 IV, with the luminosity class of 'IV' indicating this is a subgiant star. As such, it is a slightly more evolved star than the Sun, with the supply of hydrogen fuel at its core becoming exhausted. It is the nearest subgiant star to the Sun and one of the oldest stars in the solar neighbourhood. Thought to be between 6.4 and 7.1 billion years old, this star bears some resemblance to what the Sun may look like in the far distant future, making it an object of interest to astronomers. It is also the closest bright star to the south celestial pole.

Located at the northern edge of the constellation and just southwest of Achernar is Alpha Hydri, a white sub-giant star of magnitude 2.9, situated 72 light-years from Earth. Of spectral type F0IV, it is beginning to cool and enlarge as it uses up its supply of hydrogen. It is twice as massive and 3.3 times as wide as our sun and 26 times more luminous. A line drawn between Alpha Hydri and Beta Centauri is bisected by the south celestial pole.

In the southeastern corner of the constellation is Gamma Hydri, a red giant of spectral type M2III located 214 light-years from Earth. It is a semi-regular variable star, pulsating between magnitudes 3.26 and 3.33. Observations over five years were not able to establish its periodicity. It is around 1.5 to 2 times as massive as our Sun, and has expanded to about 60 times the Sun's diameter. It shines with about 655 times the luminosity of the Sun. Located 3° northeast of Gamma is the VW Hydri, a dwarf nova of the SU Ursae Majoris type. It is a close binary system that consists of a white dwarf and another star, the former drawing off matter from the latter into a bright accretion disk. These systems are characterised by frequent eruptions and less frequent supereruptions. The former are smooth, while the latter exhibit short "superhumps" of heightened activity. One of the brightest dwarf novae in the sky, it has a baseline magnitude of 14.4 and can brighten to magnitude 8.4 during peak activity. BL Hydri is another close binary system composed of a low mass star and a strongly magnetic white dwarf. Known as a polar or AM Herculis variable, these produce polarized optical and infrared emissions and intense soft and hard X-ray emissions to the frequency of the white dwarf's rotation period—in this case 113.6 minutes.
There are two notable optical double stars in Hydrus. Pi Hydri, composed of Pi Hydri and Pi Hydri, is divisible in binoculars. Around 476 light-years distant, Pi is a red giant of spectral type M1III that varies between magnitudes 5.52 and 5.58. Pi is an orange giant of spectral type K2III and shining with a magnitude of 5.7, around 488 light-years from Earth.

Eta Hydri is the other optical double, composed of Eta and Eta. Eta is a blue-white main sequence star of spectral type B9V that was suspected of being variable, and is located just over 700 light-years away. Eta has a magnitude of 4.7 and is a yellow giant star of spectral type G8.5III around 218 light-years distant, which has evolved off the main sequence and is expanding and cooling on its way to becoming a red giant. Calculations of its mass indicate it was most likely a white A-type main sequence star for most of its existence, around twice the mass of our Sun. A planet, Eta2 Hydri b, greater than 6.5 times the mass of Jupiter was discovered in 2005, orbiting around Eta every 711 days at a distance of 1.93 astronomical units (AU).

Three other systems have been found to have planets, most notably the Sun-like star HD 10180, which has seven planets, plus possibly an additional two for a total of nine—as of 2012 more than any other system to date, including the Solar System. Lying around from the Earth, it has an apparent magnitude of 7.33.

GJ 3021 is a solar twin—a star very like our own Sun—around 57 light-years distant with a spectral type G8V and magnitude of 6.7. It has a Jovian planet companion (GJ 3021 b). Orbiting about 0.5 AU from its sun, it has a minimum mass 3.37 times that of Jupiter and a period of around 133 days. The system is a complex one as the faint star GJ 3021B orbits at a distance of 68 AU; it is a red dwarf of spectral type M4V.

HD 20003 is a star of magnitude 8.37. It is a yellow main sequence star of spectral type G8V a little cooler and smaller than our Sun around 143 light-years away. It has two planets that are around 12 and 13.5 times as massive as the Earth with periods of just under 12 and 34 days respectively.

Hydrus contains only faint deep-sky objects. IC 1717 was a deep-sky object discovered by the Danish astronomer John Louis Emil Dreyer in the late 19th century. The object at the coordinate Dreyer observed is no longer there, and is now a mystery. It was very likely to have been a faint comet. PGC 6240, known as the White Rose Galaxy, is a giant spiral galaxy surrounded by shells resembling rose petals, located around 345 million light years from the Solar System. Unusually, it has cohorts of globular clusters of three distinct ages suggesting bouts of post-starburst formation following a merger with another galaxy. The constellation also contains a spiral galaxy, NGC 1511, which lies edge on to observers on Earth and is readily viewed in amateur telescopes.

Located mostly in Dorado, the Large Magellanic Cloud extends into Hydrus. The globular cluster NGC 1466 is an outlying component of the galaxy, and contains many RR Lyrae-type variable stars. It has a magnitude of 11.59 and is thought to be over 12 billion years old. Two stars, HD 24188 of magnitude 6.3 and HD 24115 of magnitude 9.0, lie nearby in its foreground. NGC 602 is composed of an emission nebula and a young, bright open cluster of stars that is an outlying component on the eastern edge of the Small Magellanic Cloud, a satellite galaxy to the Milky Way. Most of the cloud is located in the neighbouring constellation Tucana.



</doc>
<doc id="13770" url="https://en.wikipedia.org/wiki?curid=13770" title="Hercules">
Hercules

Hercules () is a Roman hero and god. He was the equivalent of the Greek divine hero Heracles, who was the son of Zeus (Roman equivalent Jupiter) and the mortal Alcmene. In classical mythology, Hercules is famous for his strength and for his numerous far-ranging adventures.

The Romans adapted the Greek hero's iconography and myths for their literature and art under the name "Hercules". In later Western art and literature and in popular culture, "Hercules" is more commonly used than "Heracles" as the name of the hero. Hercules was a multifaceted figure with contradictory characteristics, which enabled later artists and writers to pick and choose how to represent him. This article provides an introduction to representations of Hercules in the later tradition.

Hercules is known for his many adventures, which took him to the far reaches of the Greco-Roman world. One cycle of these adventures became canonical as the "Twelve Labours", but the list has variations. One traditional order of the labours is found in the "Bibliotheca" as follows:

Hercules had a greater number of "deeds on the side" "(parerga)" that have been popular subjects for art, including:

The Latin name "Hercules" was borrowed through Etruscan, where it is represented variously as Heracle, Hercle, and other forms. Hercules was a favorite subject for Etruscan art, and appears often on bronze mirrors. The Etruscan form "Herceler" derives from the Greek "Heracles" via syncope. A mild oath invoking Hercules ("Hercule!" or "Mehercle!") was a common interjection in Classical Latin.

Hercules had a number of myths that were distinctly Roman. One of these is Hercules' defeat of Cacus, who was terrorizing the countryside of Rome. The hero was associated with the Aventine Hill through his son Aventinus. Mark Antony considered him a personal patron god, as did the emperor Commodus. Hercules received various forms of religious veneration, including as a deity concerned with children and childbirth, in part because of myths about his precocious infancy, and in part because he fathered countless children. Roman brides wore a special belt tied with the "knot of Hercules", which was supposed to be hard to untie. The comic playwright Plautus presents the myth of Hercules' conception as a sex comedy in his play "Amphitryon"; Seneca wrote the tragedy "Hercules Furens" about his bout with madness. During the Roman Imperial era, Hercules was worshipped locally from Hispania through Gaul.

Tacitus records a special affinity of the Germanic peoples for Hercules. In chapter 3 of his "Germania", Tacitus states:

Some have taken this as Tacitus equating the Germanic "Þunraz" with Hercules by way of "interpretatio romana".

In the Roman era Hercules' Club amulets appear from the 2nd to 3rd century, distributed over the empire (including Roman Britain, c.f. Cool 1986), mostly made of gold, shaped like wooden clubs. A specimen found in Köln-Nippes bears the inscription [culi]", confirming the association with Hercules.

In the 5th to 7th centuries, during the Migration Period, the amulet is theorized to have rapidly spread from the Elbe Germanic area across Europe. These Germanic "Donar's Clubs" were made from deer antler, bone or wood, more rarely also from bronze or precious metals. They are found exclusively in female graves, apparently worn either as a belt pendant, or as an ear pendant. The amulet type is replaced by the Viking Age Thor's hammer pendants in the course of the Christianization of Scandinavia from the 8th to 9th century.

After the Roman Empire became Christianized, mythological narratives were often reinterpreted as allegory, influenced by the philosophy of late antiquity. In the 4th century, Servius had described Hercules' return from the underworld as representing his ability to overcome earthly desires and vices, or the earth itself as a consumer of bodies. In medieval mythography, Hercules was one of the heroes seen as a strong role model who demonstrated both valor and wisdom, while the monsters he battles were regarded as moral obstacles. One glossator noted that when Hercules became a constellation, he showed that strength was necessary to gain entrance to Heaven.

Medieval mythography was written almost entirely in Latin, and original Greek texts were little used as sources for Hercules' myths.

In 1600, the citizens of Avignon bestowed on Henry of Navarre (the future King Henry IV of France) the title of the "Hercule Gaulois" ("Gallic Hercules"), justifying the extravagant flattery with a genealogy that traced the origin of the House of Navarre to a nephew of Hercules' son Hispalus.

The Renaissance and the invention of the printing press brought a renewed interest in and publication of Greek literature. Renaissance mythography drew more extensively on the Greek tradition of Heracles, typically under the Romanized name Hercules, or the alternate name Alcides. In a chapter of his book "Mythologiae" (1567), the influential mythographer Natale Conti collected and summarized an extensive range of myths concerning the birth, adventures, and death of the hero under his Roman name Hercules. Conti begins his lengthy chapter on Hercules with an overview description that continues the moralizing impulse of the Middle Ages:

Hercules, who subdued and destroyed monsters, bandits, and criminals, was justly famous and renowned for his great courage. His great and glorious reputation was worldwide, and so firmly entrenched that he'll always be remembered. In fact the ancients honored him with his own temples, altars, ceremonies, and priests. But it was his wisdom and great soul that earned those honors; noble blood, physical strength, and political power just aren't good enough.
In Roman works of art and in Renaissance and post-Renaissance art, Hercules can be identified by his attributes, the lion skin and the gnarled club (his favorite weapon); in mosaic he is shown tanned bronze, a virile aspect.

Hercules was among the earliest figures on ancient Roman coinage, and has been the main motif of many collector coins and medals since. One example is the 20 euro Baroque Silver coin issued on September 11, 2002. The obverse side of the coin shows the Grand Staircase in the town palace of Prince Eugene of Savoy in Vienna, currently the Austrian Ministry of Finance. Gods and demi-gods hold its flights, while Hercules stands at the turn of the stairs.

Six successive ships of the British Royal Navy, from the 18th to the 20th century, bore the name "HMS Hercules".

In the French Navy, there were no less that nineteen ships called "Hercule", plus three more named "Alcide" which is another name of the same hero.

Hercules' name was also used for five ships of the US Navy, four ships of the Spanish Navy, four of the Argentine Navy and two of the Swedish Navy, as well as for numerous civilian sailing and steam ships - see links at Hercules (ship).

In modern aviation a military transport aircraft produced by Lockheed Martin carries the title Lockheed C-130 Hercules.

A series of nineteen Italian Hercules movies were made in the late 1950s and early 1960s. The actors who played Hercules in these films were Steve Reeves, Gordon Scott, Kirk Morris, Mickey Hargitay, Mark Forest, Alan Steel, Dan Vadis, Brad Harris, Reg Park, Peter Lupus (billed as Rock Stevens) and Michael Lane. A number of English-dubbed Italian films that featured the name of Hercules in their title were not intended to be movies about Hercules.





</doc>
<doc id="13772" url="https://en.wikipedia.org/wiki?curid=13772" title="History of Poland">
History of Poland

The history of Poland has its roots in the migrations of Slavs, who established permanent settlements in the Polish lands during the Early Middle Ages. The first ruling dynasty, the Piasts, emerged by the 10th century AD. Duke Mieszko I (d. 992) is considered the "de facto" creator of the Polish state and is widely recognized for the adoption of Western Christianity that followed his baptism in 966. Mieszko's duchy of Poland was formally reconstituted as a medieval kingdom in 1025 by his son Bolesław I the Brave, known for military expansion under his rule. Perhaps the most successful of the Piast kings was the last one, Casimir III the Great, who presided over a brilliant period of economic prosperity and territorial aggrandizement before his death in 1370 without male heirs. The period of the Jagiellonian dynasty in the 14th–16th centuries brought close ties with the Grand Duchy of Lithuania, a cultural Renaissance in Poland and continued territorial expansion that culminated in the establishment of the Polish–Lithuanian Commonwealth in 1569. 

In its early phases, the Commonwealth was able to sustain the levels of prosperity achieved during the Jagiellonian period, while its political system matured as a unique noble democracy. From the mid-17th century, however, the huge state entered a period of decline caused by devastating wars and the deterioration of its political system. Significant internal reforms were introduced during the later part of the 18th century, especially in the Constitution of 3 May 1791, but neighboring powers did not allow the reform process to advance. The independent existence of the Commonwealth ended in 1795 after a series of invasions and partitions of Polish territory carried out by the Russian Empire, the Kingdom of Prussia, and the Austrian Habsburg Monarchy.

From 1795 until 1918, no truly independent Polish state existed, although strong Polish resistance movements operated. After the failure of the last military uprising against the Russian Empire, the January Uprising of 1863, the nation preserved its identity through educational initiatives and a program of "organic work" intended to modernize the economy and society. The opportunity to regain independence only materialized after World War I, when the three partitioning imperial powers were fatally weakened in the wake of war and revolution.

The Second Polish Republic, established in 1918, existed as an independent state until 1939, when Nazi Germany and the Soviet Union destroyed it in their invasion of Poland at the beginning of World War II. Millions of Polish citizens perished in the course of the Nazi occupation of Poland between 1939 and 1945 as Germany classified ethnic Poles and other Slavs, Jews and Romani (Gypsies) as subhuman. Nazi authorities targeted the last two groups for extermination in the short term, deferring the extermination and/or enslavement of the Slavs as part of the "Generalplan Ost" ('General Plan for the East') conceived by the Nazi régime. A Polish government-in-exile nonetheless functioned throughout the war and the Poles contributed to the Allied victory through participation in military campaigns on both the eastern and western fronts. The westward advances of the Soviet Red Army in 1944 and 1945 compelled Nazi Germany's forces to retreat from Poland, which led to the establishment of a communist satellite state of the Soviet Union, known from 1952 as the Polish People's Republic.

As a result of territorial adjustments mandated by the victorious Allies at the end of World War II in 1945, Poland's geographic centre of gravity shifted towards the west and the re-defined Polish lands largely lost their historic multi-ethnic character through the extermination, expulsion and migration of various ethnic groups during and after the war. By the late 1980s, the Polish reform movement Solidarity became crucial in bringing about a peaceful transition from a communist state to a capitalist economic system and a liberal parliamentary democracy. This process resulted in the creation of the modern Polish state: the Third Polish Republic, founded in 1989.

In prehistoric and protohistoric times, over a period of at least 500,000 years, the area of present-day Poland was intermittently inhabited by members of the "Homo" genus. It went through the Stone Age, Bronze Age and Iron Age stages of development, along with the nearby regions. The Neolithic period ushered in the Linear Pottery culture, whose founders migrated from the Danube River area beginning about 5,500 BC. This culture was distinguished by the establishment of the first settled agricultural communities in modern Polish territory. Later, between about 4,400 and 2,000 BC, the native post-Mesolithic populations would also adopt and further develop the agricultural way of life.
Poland's Early Bronze Age began around 2300–2400 BC, whereas its Iron Age commenced c. 700–750 BC. One of the many cultures that have been uncovered, the Lusatian culture, spanned the Bronze and Iron Ages and left notable settlement sites. Around 400 BC, Poland was settled by Celts of the La Tène culture. They were soon followed by emerging cultures with a strong Germanic component, influenced first by the Celts and then by the Roman Empire. The Germanic peoples migrated out of the area by about 500 AD during the great Migration Period of the European Dark Ages. Wooded regions to the north and east were settled by Balts.

According to mainstream archaeological research, Slavs have resided in modern Polish territories for over 1500 years. Recent genetic studies, however, determined that people who live in the current territory of Poland include the descendants of people who inhabited the area for thousands of years, beginning in the early Neolithic period.

Slavs on the territory of Poland were organized into tribal units, of which the larger ones were later known as the Polish tribes; the names of many tribes are found on the list compiled by the anonymous Bavarian Geographer in the 9th century. In the 9th and 10th centuries, these tribes gave rise to developed regions along the upper Vistula, the coast of the Baltic Sea and in Greater Poland. The latest tribal undertaking, in Greater Poland, resulted in the formation of a lasting political structure in the 10th century that became the state of Poland, one of the West Slavic nations.

Poland was established as a state under the Piast dynasty, which ruled the country between the 10th and 14th centuries. Historical records referring to the Polish state begin with the rule of Duke Mieszko I. Mieszko, whose reign commenced sometime before 963 and who continued as the Polish monarch until his death in 992, chose to be baptized in the Western Latin Rite, probably on 14 April 966, following his marriage to Princess Doubravka of Bohemia, a fervent Christian. This event has become known as the baptism of Poland, and its date is often used to mark a symbolic beginning of Polish statehood. Mieszko completed a unification of the West Slavic tribal lands that was fundamental to the new country's existence. Following its emergence, Poland was led by a series of rulers who converted the population to Christianity, created a strong kingdom and fostered a distinctive Polish culture that was integrated into the broader European culture.

Mieszko's son, Duke Bolesław I the Brave (r. 992–1025), established a Polish Church structure, pursued territorial conquests and was officially crowned the first king of Poland in 1025, near the end of his life. Bolesław also sought to spread Christianity to parts of eastern Europe that remained pagan, but suffered a setback when his greatest missionary, Adalbert of Prague, was killed in Prussia in 997. During the Congress of Gniezno in the year 1000, Holy Roman Emperor Otto III recognized the Archbishopric of Gniezno, an institution crucial for the continuing existence of the sovereign Polish state. During the reign of Otto's successor, Holy Roman Emperor Henry II, Bolesław fought prolonged wars with the Kingdom of Germany between 1002 and 1018.

Bolesław I's expansive rule overstretched the resources of the early Polish state, and it was followed by a collapse of the monarchy. Recovery took place under Casimir I the Restorer (r. 1039–58). Casimir's son Bolesław II the Generous (r. 1058–79) became involved in a conflict with Bishop Stanislaus of Szczepanów that ultimately caused his downfall. Bolesław had the bishop murdered in 1079 after being excommunicated by the Polish church on charges of adultery. This act sparked a revolt of Polish nobles that led to Bolesław's deposition and expulsion from the country. Around 1116, Gallus Anonymus wrote a seminal chronicle, the "Gesta principum Polonorum", intended as a glorification of his patron Bolesław III Wrymouth (r. 1107–38), a ruler who revived the tradition of military prowess of Bolesław I's time. Gallus' work remains a paramount written source for the early history of Poland.

After Bolesław III divided Poland among his sons in his Testament of 1138, internal fragmentation eroded the Piast monarchical structures in the 12th and 13th centuries. In 1180, Casimir II the Just, who sought papal confirmation of his status as a senior duke, granted immunities and additional privileges to the Polish Church at the Congress of Łęczyca. Around 1220, Wincenty Kadłubek wrote his "Chronica seu originale regum et principum Poloniae", another major source for early Polish history. In 1226, one of the regional Piast dukes, Konrad I of Masovia, invited the Teutonic Knights to help him fight the Baltic Prussian pagans. The Teutonic Order destroyed the Prussians but kept their lands, which resulted in centuries of warfare between Poland and the Teutonic Knights, and later between Poland and the German Prussian state. The first Mongol invasion of Poland began in 1240; it culminated in the defeat of Polish and allied Christian forces and the death of the Silesian Piast Duke Henry II the Pious at the Battle of Legnica in 1241. In 1242, Wrocław became the first Polish municipality to be incorporated, as the period of fragmentation brought economic development and growth of towns. In 1264, Bolesław the Pious granted Jewish liberties in the Statute of Kalisz.

Attempts to reunite the Polish lands gained momentum in the 13th century, and in 1295, Duke Przemysł II of Greater Poland managed to become the first ruler since Bolesław II to be crowned king of Poland. He ruled over a limited territory and was soon killed. In 1300–05 King Wenceslaus II of Bohemia also reigned as king of Poland. The Piast Kingdom was effectively restored under Władysław I the Elbow-high (r. 1306–33), who became king in 1320. In 1308, the Teutonic Knights seized Gdańsk and the surrounding region of Pomerelia.

King Casimir III the Great (r. 1333–70), Władysław's son and the last of the Piast rulers, strengthened and expanded the restored Kingdom of Poland, but the western provinces of Silesia (formally ceded by Casimir in 1339) and most of Polish Pomerania were lost to the Polish state for centuries to come. Progress was made in the recovery of the separately governed central province of Mazovia, however, and in 1340, the conquest of Red Ruthenia began, marking Poland's expansion to the east. The Congress of Kraków, a vast convocation of central, eastern, and northern European rulers probably assembled to plan an anti-Turkish crusade, took place in 1364, the same year that the future Jagiellonian University, one of the oldest European universities, was founded. On 9 October 1334, Casimir III confirmed the privileges granted to Jews in 1264 by Bolesław the Pious and allowed them to settle in Poland in great numbers.

After the Polish royal line and Piast junior branch died out in 1370, Poland came under the rule of Louis I of Hungary of the Capetian House of Anjou, who presided over a union of Hungary and Poland that lasted until 1382. In 1374, Louis granted the Polish nobility the Privilege of Koszyce to assure the succession of one of his daughters in Poland. His youngest daughter Jadwiga (d. 1399) assumed the Polish throne in 1384.

In 1386, Grand Duke Jogaila of Lithuania converted to Catholicism and married Queen Jadwiga of Poland. This act enabled him to become a king of Poland himself, and he ruled as Władysław II Jagiełło until his death in 1434. The marriage established a personal Polish–Lithuanian union ruled by the Jagiellonian dynasty. The first in a series of formal "unions" was the Union of Krewo of 1385, whereby arrangements were made for the marriage of Jogaila and Jadwiga. The Polish–Lithuanian partnership brought vast areas of Ruthenia controlled by the Grand Duchy of Lithuania into Poland's sphere of influence and proved beneficial for the nationals of both countries, who coexisted and cooperated in one of the largest political entities in Europe for the next four centuries. When Queen Jadwiga died in 1399, the Kingdom of Poland fell to her husband's sole possession.
In the Baltic Sea region, Poland's struggle with the Teutonic Knights continued and culminated in the Battle of Grunwald (1410), a great victory that the Poles and Lithuanians were unable to follow up with a decisive strike against the main seat of the Teutonic Order at Malbork Castle. The Union of Horodło of 1413 further defined the evolving relationship between the Kingdom of Poland and the Grand Duchy of Lithuania.

The privileges of the "szlachta" (nobility) kept expanding and in 1425 the rule of "Neminem captivabimus", which protected the noblemen from arbitrary royal arrests, was formulated.

The reign of the young Władysław III (1434–44), who succeeded his father Władysław II Jagiełło and ruled as king of Poland and Hungary, was cut short by his death at the Battle of Varna against the forces of the Ottoman Empire. This disaster led to an interregnum of three years that ended with the accession of Władysław's brother Casimir IV Jagiellon in 1447.

Critical developments of the Jagiellonian period were concentrated during Casimir IV's long reign, which lasted until 1492. In 1454, Royal Prussia was incorporated by Poland and the Thirteen Years' War of 1454–66 with the Teutonic state ensued. In 1466, the milestone Peace of Thorn was concluded. This treaty divided Prussia to create East Prussia, the future Duchy of Prussia, a separate entity that functioned as a fief of Poland under the administration of the Teutonic Knights. Poland also confronted the Ottoman Empire and the Crimean Tatars in the south, and in the east helped Lithuania fight the Grand Duchy of Moscow. The country was developing as a feudal state, with a predominantly agricultural economy and an increasingly dominant landed nobility. Kraków, the royal capital, was turning into a major academic and cultural center, and in 1473 the first printing press began operating there. With the growing importance of "szlachta" (middle and lower nobility), the king's council evolved to become by 1493 a bicameral General Sejm (parliament) that no longer represented exclusively top dignitaries of the realm.

The "Nihil novi" act, adopted in 1505 by the Sejm, transferred most of the legislative power from the monarch to the Sejm. This event marked the beginning of the period known as "Golden Liberty", when the state was ruled in principle by the "free and equal" Polish nobility. In the 16th century, the massive development of folwark agribusinesses operated by the nobility led to increasingly abusive conditions for the peasant serfs who worked them. The political monopoly of the nobles also stifled the development of cities, some of which were thriving during the late Jagiellonian era, and limited the rights of townspeople, effectively holding back the emergence of the middle class.

In the 16th century, Protestant Reformation movements made deep inroads into Polish Christianity and the resulting Reformation in Poland involved a number of different denominations. The policies of religious tolerance that developed in Poland were nearly unique in Europe at that time and many who fled regions torn by religious strife found refuge in Poland. The reigns of King Sigismund I the Old (1506–1548) and King Sigismund II Augustus (1548–1572) witnessed an intense cultivation of culture and science (a Golden Age of the Renaissance in Poland), of which the astronomer Nicolaus Copernicus (1473–1543) is the best known representative. Jan Kochanowski (1530–1584) was a poet and the premier artistic personality of the period. In 1525, during the reign of Sigismund I, the Teutonic Order was secularized and Duke Albert performed an act of homage before the Polish king (the Prussian Homage) for his fief, the Duchy of Prussia. Mazovia was finally fully incorporated into the Polish Crown in 1529.

The reign of Sigismund II ended the Jagiellonian period, but gave rise to the Union of Lublin (1569), an ultimate fulfillment of the union with Lithuania. This agreement transferred Ukraine from the Grand Duchy of Lithuania to Poland and transformed the Polish–Lithuanian polity into a real union, preserving it beyond the death of the childless Sigismund II, whose active involvement made the completion of this process possible.

Livonia in the far northeast was incorporated by Poland in 1561 and Poland entered the Livonian War against Russia. The executionist movement, which attempted to check the progressing domination of the state by the magnate families of Poland and Lithuania, peaked at the Sejm in Piotrków in 1562–63. On the religious front, the Polish Brethren split from the Calvinists, and the Protestant Brest Bible was published in 1563. The Jesuits, who arrived in 1564, were destined to make a major impact on Poland's history.

The Union of Lublin of 1569 established the Polish–Lithuanian Commonwealth, a federal state more closely unified than the earlier political arrangement between Poland and Lithuania. The union was run largely by the nobility through the system of central parliament and local assemblies, but was headed by elected kings. The formal rule of the nobility, who were proportionally more numerous than in other European countries, constituted an early democratic system ("a sophisticated noble democracy"), in contrast to the absolute monarchies prevalent at that time in the rest of Europe. The beginning of the Commonwealth coincided with a period in Polish history when great political power was attained and advancements in civilization and prosperity took place. The Polish–Lithuanian Union became an influential participant in European affairs and a vital cultural entity that spread Western culture (with Polish characteristics) eastward. In the second half of the 16th century and the first half of the 17th century, the Commonwealth was one of the largest and most populous states in contemporary Europe, with an area approaching and a population of about ten million. Its economy was dominated by export-focused agriculture. Nationwide religious toleration was guaranteed at the Warsaw Confederation in 1573.

After the rule of the Jagiellonian dynasty ended in 1572, Henry of Valois (later King Henry III of France) was the winner of the first "free election" by the Polish nobility, held in 1573. He had to agree to the restrictive "pacta conventa" obligations and fled Poland in 1574 when news arrived of the vacancy of the French throne, to which he was the heir presumptive. From the start, the royal elections increased foreign influence in the Commonwealth as foreign powers sought to manipulate the Polish nobility to place candidates amicable to their interests. The reign of Stephen Báthory of Hungary followed (r. 1576–1586). He was militarily and domestically assertive and is revered in Polish historical tradition as a rare case of successful elective king. The establishment of the legal Crown Tribunal in 1578 meant a transfer of many appellate cases from the royal to noble jurisdiction.

A period of rule under the Swedish House of Vasa began in the Commonwealth in the year 1587. The first two kings from this dynasty, Sigismund III (r. 1587–1632) and Władysław IV (r. 1632–1648), repeatedly attempted to intrigue for accession to the throne of Sweden, which was a constant source of distraction for the affairs of the Commonwealth. At that time, the Catholic Church embarked on an ideological counter-offensive and the Counter-Reformation claimed many converts from Polish and Lithuanian Protestant circles. In 1596, the Union of Brest split the Eastern Christians of the Commonwealth to create the Uniate Church of the Eastern Rite, but subject to the authority of the pope. The Zebrzydowski rebellion against Sigismund III unfolded in 1606–1608.

Seeking supremacy in Eastern Europe, the Commonwealth fought wars with Russia between 1605 and 1618 in the wake of Russia's Time of Troubles; the series of conflicts is referred to as the Polish–Muscovite War or the "Dymitriads". The efforts resulted in expansion of the eastern territories of the Polish–Lithuanian Commonwealth, but the goal of taking over the Russian throne for the Polish ruling dynasty was not achieved. Sweden sought supremacy in the Baltic during the Polish–Swedish wars of 1617–1629, and the Ottoman Empire pressed from the south in the Battles at Cecora in 1620 and Khotyn in 1621. The agricultural expansion and serfdom policies in Polish Ukraine resulted in a series of Cossack uprisings. Allied with the Habsburg Monarchy, the Commonwealth did not directly participate in the Thirty Years' War. Władysław's IV reign was mostly peaceful, with a Russian invasion in the form of the Smolensk War of 1632–1634 successfully repelled. The Orthodox Church hierarchy, banned in Poland after the Union of Brest, was re-established in 1635.

During the reign of John II Casimir Vasa (r. 1648–1668), the third and last king of his dynasty, the nobles' democracy fell into decline as a result of foreign invasions and domestic disorder. These calamities multiplied rather suddenly and marked the end of the Polish Golden Age. Their effect was to render the once powerful Commonwealth increasingly vulnerable to foreign intervention.

The Cossack Khmelnytsky Uprising of 1648–1657 engulfed the south-eastern regions of the Polish crown; its long-term effects were disastrous for the Commonwealth. The first "liberum veto" (a parliamentary device that allowed any member of the Sejm to dissolve a current session immediately) was exercised by a deputy in 1652. This practice would eventually weaken Poland's central government critically. In the Treaty of Pereyaslav (1654), the Ukrainian rebels declared themselves subjects of the Tsar of Russia. The Second Northern War raged through the core Polish lands in 1655–1660; it included a brutal and devastating invasion of Poland referred to as the Swedish Deluge. The war ended in 1660 with the Treaty of Oliva, which resulted in the loss of some of Poland's northern possessions. In 1657 the Treaty of Bromberg established the independence of the Duchy of Prussia. The Commonwealth forces did well in the Russo-Polish War (1654–1667), but the end result was the permanent division of Ukraine between Poland and Russia, as agreed to in the Truce of Andrusovo (1667). Towards the end of the war, the Lubomirski's rebellion, a major magnate revolt against the king, destabilized and weakened the country. The large-scale slave raids of the Crimean Tatars also had highly deleterious effects on the Polish economy. "Merkuriusz Polski", the first Polish newspaper, was published in 1661.

In 1668, grief-stricken at the recent death of his wife and frustrated by the disastrous political setbacks of his reign, John II Casimir abdicated the throne and fled to France.

King Michał Korybut Wiśniowiecki, a native Pole, was elected to replace John II Casimir in 1669. The Polish–Ottoman War (1672–76) broke out during his reign, which lasted until 1673, and continued under his successor, John III Sobieski (r. 1674–1696). Sobieski intended to pursue Baltic area expansion (and to this end he signed the secret Treaty of Jaworów with France in 1675), but was forced instead to fight protracted wars with the Ottoman Empire. By doing so, Sobieski briefly revived the Commonwealth's military might. He defeated the expanding Muslims at the Battle of Khotyn in 1673 and decisively helped deliver Vienna from a Turkish onslaught at the Battle of Vienna in 1683. Sobieski's reign marked the last high point in the history of the Commonwealth: in the first half of the 18th century, Poland ceased to be an active player in international politics. The Treaty of Perpetual Peace (1686) with Russia was the final border settlement between the two countries before the First Partition of Poland in 1772.

The Commonwealth, subjected to almost constant warfare until 1720, suffered enormous population losses and massive damage to its economy and social structure. The government became ineffective in the wake of large-scale internal conflicts, corrupted legislative processes and manipulation by foreign interests. The nobility fell under the control of a handful of feuding magnate families with established territorial domains. The urban population and infrastructure fell into ruin, together with most peasant farms, whose inhabitants were subjected to increasingly extreme forms of serfdom. The development of science, culture and education came to a halt or regressed.

The royal election of 1697 brought a ruler of the Saxon House of Wettin to the Polish throne: Augustus II the Strong (r. 1697–1733), who was able to assume the throne only by agreeing to convert to Roman Catholicism. He was succeeded by his son Augustus III (r. 1734–1763). The reigns of the Saxon kings (who were both simultaneously prince-electors of Saxony) were disrupted by competing candidates for the throne and witnessed further disintegration of the Commonwealth. The Great Northern War of 1700–1721, a period seen by the contemporaries as a temporary eclipse, may have been the fatal blow that brought down the Polish political system. Stanisław Leszczyński was installed as king in 1704 under Swedish protection, but lasted only a few years. The Silent Sejm of 1717 marked the beginning of the Commonwealth's existence as a Russian protectorate: the Tsardom would guarantee the reform-impeding Golden Liberty of the nobility from that time on in order to cement the Commonwealth's weak central authority and a state of perpetual political impotence. In a resounding break with traditions of religious tolerance, Protestants were executed during the Tumult of Thorn in 1724. In 1732, Russia, Austria and Prussia, Poland's three increasingly powerful and scheming neighbors, entered into the secret Treaty of the Three Black Eagles with the intention of controlling the future royal succession in the Commonwealth. The War of the Polish Succession was fought in 1733–1735 to assist Leszczyński in assuming the throne of Poland for a second time. Amidst considerable foreign involvement, his efforts were unsuccessful. The Kingdom of Prussia became a strong regional power and succeeded in wresting the historically Polish province of Silesia from the Habsburg Monarchy in the Silesian Wars; it thus constituted an ever-greater threat to Poland's security. The personal union between the Commonwealth and the Electorate of Saxony did give rise to the emergence of a reform movement in the Commonwealth and the beginnings of the Polish Enlightenment culture, the major positive developments of this era. The first Polish public library was the Załuski Library in Warsaw, opened to the public in 1747.

During the later part of the 18th century, fundamental internal reforms were attempted in the Polish–Lithuanian Commonwealth as it slid into extinction. The reform activity, initially promoted by the magnate Czartoryski family faction known as the "Familia", provoked a hostile reaction and military response from neighboring powers, but it did create conditions that fostered economic improvement. The most populous urban center, the capital city of Warsaw, replaced Danzig (Gdańsk) as the leading trade center, and the importance of the more prosperous urban social classes increased. The last decades of the independent Commonwealth's existence were characterized by aggressive reform movements and far-reaching progress in the areas of education, intellectual life, art and the evolution of the social and political system.

The royal election of 1764 resulted in the elevation of Stanisław August Poniatowski, a refined and worldly aristocrat connected to the Czartoryski family, but hand-picked and imposed by Empress Catherine the Great of Russia, who expected him to be her obedient follower. Stanisław August ruled the Polish–Lithuanian state until its dissolution in 1795. The king spent his reign torn between his desire to implement reforms necessary to save the failing state and the perceived necessity of remaining in a subordinate relationship to his Russian sponsors.

The Bar Confederation (1768–1772) was a rebellion of nobles directed against Russia's influence in general and Stanisław August, who was seen as its representative, in particular. It was fought to preserve Poland's independence and the nobility's traditional interests. After several years, it was brought under control by forces loyal to the king and those of the Russian Empire.

Following the suppression of the Bar Confederation, parts of the Commonwealth were divided up among Prussia, Austria and Russia in 1772 at the instigation of Frederick the Great of Prussia, an action that became known as the First Partition of Poland: the outer provinces of the Commonwealth were seized by agreement among the country's three powerful neighbors and only a rump state remained. In 1773, the "Partition Sejm" ratified the partition under duress as a "fait accompli". However, it also established the Commission of National Education, a pioneering in Europe education authority often called the world's first ministry of education.

The long-lasting session of parliament convened by King Stanisław August is known as the Great Sejm or Four-Year Sejm; it first met in 1788. Its landmark achievement was the passing of the Constitution of 3 May 1791, the first singular pronouncement of a supreme law of the state in modern Europe. A moderately reformist document condemned by detractors as sympathetic to the ideals of the French Revolution, it soon generated strong opposition from the conservative circles of the Commonwealth's upper nobility and from Empress Catherine of Russia, who was determined to prevent the rebirth of a strong Commonwealth. The nobility's Targowica Confederation, formed in Russian imperial capital of Saint Petersburg, appealed to Catherine for help, and in May 1792, the Russian army entered the territory of the Commonwealth. The Polish–Russian War of 1792, a defensive war fought by the forces of the Commonwealth against Russian invaders, ended when the Polish king, convinced of the futility of resistance, capitulated by joining the Targowica Confederation. The Russian-allied confederation took over the government, but Russia and Prussia in 1793 arranged for the Second Partition of Poland anyway. The partition left the country with a critically reduced territory that rendered it essentially incapable of an independent existence. The Commonwealth's Grodno Sejm of 1793, the last Sejm of the state's existence, was compelled to confirm the new partition.

Radicalized by recent events, Polish reformers (whether in exile or still resident in the reduced area remaining to the Commonwealth) were soon working on preparations for a national insurrection. Tadeusz Kościuszko, a popular general and a veteran of the American Revolution, was chosen as its leader. He returned from abroad and issued Kościuszko's proclamation in Kraków on March 24, 1794. It called for a national uprising under his supreme command. Kościuszko emancipated many peasants in order to enroll them as "kosynierzy" in his army, but the hard-fought insurrection, despite widespread national support, proved incapable of generating the foreign assistance necessary for its success. In the end, it was suppressed by the combined forces of Russia and Prussia, with Warsaw captured in November 1794 in the aftermath of the Battle of Praga.

In 1795, a Third Partition of Poland was undertaken by Russia, Prussia and Austria as a final division of territory that resulted in the effective dissolution of the Polish–Lithuanian Commonwealth. King Stanisław August Poniatowski was escorted to Grodno, forced to abdicate, and retired to Saint Petersburg. Tadeusz Kościuszko, initially imprisoned, was allowed to emigrate to the United States in 1796.

The response of the Polish leadership to the last partition is a matter of historical debate. Literary scholars found that the dominant emotion of the first decade was despair that produced a moral desert ruled by violence and treason. On the other hand, historians have looked for signs of resistance to foreign rule. Apart from those who went into exile, the nobility took oaths of loyalty to their new rulers and served as officers in their armies.

Although no sovereign Polish state existed between 1795 and 1918, the idea of Polish independence was kept alive throughout the 19th century. There were a number of uprisings and other armed undertakings waged against the partitioning powers. Military efforts after the partitions were first based on the alliances of Polish émigrés with post-revolutionary France. Jan Henryk Dąbrowski's Polish Legions fought in French campaigns outside of Poland between 1797 and 1802 in hopes that their involvement and contribution would be rewarded with the liberation of their Polish homeland. The Polish national anthem, "Poland Is Not Yet Lost", or "Dąbrowski's Mazurka", was written in praise of his actions by Józef Wybicki in 1797.

The Duchy of Warsaw, a small, semi-independent Polish state, was created in 1807 by Napoleon in the wake of his defeat of Prussia and the signing of the Treaties of Tilsit with Emperor Alexander I of Russia. The Army of the Duchy of Warsaw, led by Józef Poniatowski, participated in numerous campaigns in alliance with France, including the successful Austro-Polish War of 1809, which, combined with the outcomes of other theaters of the War of the Fifth Coalition, resulted in an enlargement of the duchy's territory. The French invasion of Russia in 1812 and the German Campaign of 1813 saw the duchy's last military engagements. The Constitution of the Duchy of Warsaw abolished serfdom as a reflection of the ideals of the French Revolution, but it did not promote land reform.

After Napoleon's defeat, a new European order was established at the Congress of Vienna, which met in the years 1814 and 1815. Adam Jerzy Czartoryski, a former close associate of Emperor Alexander I, became the leading advocate for the Polish national cause. The Congress implemented a new partition scheme, which took into account some of the gains realized by the Poles during the Napoleonic period. The Duchy of Warsaw was replaced in 1815 with a new Kingdom of Poland, unofficially known as Congress Poland. The residual Polish kingdom was joined to the Russian Empire in a personal union under the Russian tsar and it was allowed its own constitution and military. East of the kingdom, large areas of the former Polish–Lithuanian Commonwealth remained directly incorporated into the Russian Empire as the Western Krai. These territories, along with Congress Poland, are generally considered to form the Russian Partition. The Russian, Prussian, and Austrian "partitions" are informal names for the lands of the former Commonwealth, not actual units of administrative division of Polish–Lithuanian territories after partitions. The Prussian Partition included a portion separated as the Grand Duchy of Posen. Peasants under the Prussian administration were gradually enfranchised under the reforms of 1811 and 1823. The limited legal reforms in the Austrian Partition were overshadowed by its rural poverty. The Free City of Cracow was a tiny republic created by the Congress of Vienna under the joint supervision of the three partitioning powers. Despite the bleak from the standpoint of Polish patriots political situation, economic progress was made in the lands taken over by foreign powers because the period after the Congress of Vienna witnessed a significant development in the building of early industry.

The increasingly repressive policies of the partitioning powers led to resistance movements in partitioned Poland, and in 1830 Polish patriots staged the November Uprising. This revolt developed into a full-scale war with Russia, but the leadership was taken over by Polish conservatives who were reluctant to challenge the empire and hostile to broadening the independence movement's social base through measures such as land reform. Despite the significant resources mobilized, a series of errors by several successive chief commanders appointed by the insurgent Polish National Government led to the defeat of its forces by the Russian army in 1831. Congress Poland lost its constitution and military, but formally remained a separate administrative unit within the Russian Empire.

After the defeat of the November Uprising, thousands of former Polish combatants and other activists emigrated to Western Europe. This phenomenon, known as the Great Emigration, soon dominated Polish political and intellectual life. Together with the leaders of the independence movement, the Polish community abroad included the greatest Polish literary and artistic minds, including the Romantic poets Adam Mickiewicz, Juliusz Słowacki, Cyprian Norwid, and the composer Frédéric Chopin. In occupied and repressed Poland, some sought progress through nonviolent activism focused on education and economy, known as organic work; others, in cooperation with the emigrant circles, organized conspiracies and prepared for the next armed insurrection.

The planned national uprising failed to materialize because the authorities in the partitions found out about secret preparations. The Greater Poland uprising ended in a fiasco in early 1846. In the Kraków uprising of February 1846, patriotic action was combined with revolutionary demands, but the result was the incorporation of the Free City of Cracow into the Austrian Partition. The Austrian officials took advantage of peasant discontent and incited villagers against the noble-dominated insurgent units. This resulted in the Galician slaughter of 1846, a large-scale rebellion of serfs seeking relief from their post-feudal condition of mandatory labor as practiced in "folwarks". The uprising freed many from bondage and hastened decisions that led to the abolition of Polish serfdom in the Austrian Empire in 1848. A new wave of Polish involvement in revolutionary movements soon took place in the partitions and in other parts of Europe in the context of the Spring of Nations revolutions of 1848 (e.g. Józef Bem's participation in the revolutions in Austria and Hungary). The 1848 German revolutions precipitated the Greater Poland uprising of 1848, in which peasants in the Prussian Partition, who were by then largely enfranchised, played a prominent role.

As a matter of continuous policy, the Russian autocracy kept assailing Polish national core values of language, religion and culture. In consequence, despite the limited liberalization measures allowed in Congress Poland under the rule of Tsar Alexander II of Russia, a renewal of popular liberation activities took place in 1860–1861. During large-scale demonstrations in Warsaw, Russian forces inflicted numerous casualties on the civilian participants. The "Red", or left-wing faction of Polish activists, which promoted peasant enfranchisement and cooperated with Russian revolutionaries, became involved in immediate preparations for a national uprising. The "White", or right-wing faction, was inclined to cooperate with the Russian authorities and countered with partial reform proposals. In order to cripple the manpower potential of the Reds, Aleksander Wielopolski, the conservative leader of the government of Congress Poland, arranged for a partial selective conscription of young Poles for the Russian army in the years 1862 and 1863. This action hastened the outbreak of hostilities. The January Uprising, joined and led after the initial period by the Whites, was fought by partisan units against an overwhelmingly advantaged enemy. The uprising lasted from January 1863 to the spring of 1864, when Romuald Traugutt, the last supreme commander of the insurgency, was captured by the tsarist police.

On 2 March 1864, the Russian authority, compelled by the uprising to compete for the loyalty of Polish peasants, officially published an enfranchisement decree in Congress Poland along the lines of an earlier land reform proclamation of the insurgents. The act created the conditions necessary for the development of the capitalist system on central Polish lands. At the time when most Poles realized the futility of armed resistance without external support, the various sections of Polish society were undergoing deep and far-reaching evolution in the areas of social, economic and cultural development.

The failure of the January Uprising in Poland caused a major psychological trauma and became a historic watershed; indeed, it sparked the development of modern Polish nationalism. The Poles, subjected within the territories under the Russian and Prussian administrations to still stricter controls and increased persecution, sought to preserve their identity in non-violent ways. After the uprising, Congress Poland was downgraded in official usage from the "Kingdom of Poland" to the "Vistula Land" and was more fully integrated into Russia proper, but not entirely obliterated. The Russian and German languages were imposed in all public communication, and the Catholic Church was not spared from severe repression. Public education was increasingly subjected to Russification and Germanisation measures. Illiteracy was reduced, most effectively in the Prussian partition, but education in the Polish language was preserved mostly through unofficial efforts. The Prussian government pursued German colonization, including the purchase of Polish-owned land. On the other hand, the region of Galicia (western Ukraine and southern Poland) experienced a gradual relaxation of authoritarian policies and even a Polish cultural revival. Economically and socially backward, it was under the milder rule of the Austro-Hungarian Monarchy and from 1867 was increasingly allowed limited autonomy. "Stańczycy", a conservative Polish pro-Austrian faction led by great land owners, dominated the Galician government. The Polish Academy of Learning (an academy of sciences) was founded in Kraków in 1872. 

Social activities termed "organic work" consisted of self-help organizations that promoted economic advancement and work on improving the competitiveness of Polish-owned businesses, industrial, agricultural or other. New commercial methods of generating higher productivity were discussed and implemented through trade associations and special interest groups, while Polish banking and cooperative financial institutions made the necessary business loans available. The other major area of effort in organic work was educational and intellectual development of the common people. Many libraries and reading rooms were established in small towns and villages, and numerous printed periodicals manifested the growing interest in popular education. Scientific and educational societies were active in a number of cities. Such activities were most pronounced in the Prussian Partition.

Positivism in Poland replaced Romanticism as the leading intellectual, social and literary trend. It reflected the ideals and values of the emerging urban bourgeoisie. Around 1890, the urban classes gradually abandoned the positivist ideas and came under the influence of modern pan-European nationalism.

Under the partitioning powers, economic diversification and progress, including large-scale industrialisation, were introduced in the traditionally agrarian Polish lands, but this development turned out to be very uneven. Advanced agriculture was practiced in the Prussian Partition, except for Upper Silesia, where the coal-mining industry created a large labor force. The densest network of railroads was built in German-ruled western Poland. In Russian Congress Poland, a striking growth of industry, railways and towns took place, all against the background of an extensive, but less productive agriculture. The industrial initiative, capital and know-how were provided largely by entrepreneurs who were not ethnic Poles. Warsaw (a metallurgical center) and Łódź (a textiles center) grew rapidly, as did the total proportion of urban population, making the region the most economically advanced in the Russian Empire (industrial production exceeded agricultural production there by 1909). The coming of the railways spurred some industrial growth even in the vast Russian Partition territories outside of Congress Poland. The Austrian Partition was rural and poor, except for the industrialized Cieszyn Silesia area. Galician economic expansion after 1890 included oil extraction and resulted in the growth of Lemberg (Lwów, Lviv) and Kraków.

Economic and social changes involving land reform and industrialization, combined with the effects of foreign domination, altered the centuries-old social structure of Polish society. Among the newly emergent strata were wealthy industrialists and financiers, distinct from the traditional, but still critically important landed aristocracy. The intelligentsia, an educated, professional or business middle class, often originated from lower gentry, landless or alienated from their rural possessions, and from urban people. Many smaller agricultural enterprises based on serfdom did not survive the land reforms. The industrial proletariat, a new underprivileged class, was composed mainly of poor peasants or townspeople forced by deteriorating conditions to migrate and search for work in urban centers in their countries of origin or abroad. Millions of residents of the former Commonwealth of various ethnic groups worked or settled in Europe and in North and South America.

Social and economic changes were partial and gradual. The degree of industrialisation, relatively fast-paced in some areas, lagged behind the advanced regions of Western Europe. The three partitions developed different economies and were more economically integrated with their mother states than with each other. In the Prussian Partition, for example, agricultural production depended heavily on the German market, whereas the industrial sector of Congress Poland relied more on the Russian market.

In the 1870s–1890s, large-scale socialist, nationalist, agrarian and other political movements of great ideological fervor became established in partitioned Poland and Lithuania, along with corresponding political parties to promote them. Of the major parties, the socialist First Proletariat was founded in 1882, the Polish League (precursor of National Democracy) in 1887, the Polish Social Democratic Party of Galicia and Silesia in 1890, the Polish Socialist Party in 1892, the Marxist Social Democracy of the Kingdom of Poland and Lithuania in 1893, the agrarian People's Party of Galicia in 1895 and the Jewish socialist Bund in 1897. Christian democracy regional associations allied with the Catholic Church were also active; they united into the Polish Christian Democratic Party in 1919. The main minority ethnic groups of the former Commonwealth, including Ukrainians, Lithuanians, Belarusians and Jews, were getting involved in their own national movements and plans, which met with disapproval on the part of those Polish independence activists who counted on an eventual rebirth of the Commonwealth or the rise of a Commonwealth-inspired federal structure (a political movement referred to as Prometheism).

Around the start of the 20th century, the Young Poland cultural movement, centered in Austrian Galicia, took advantage of a milieu conducive to liberal expression in that region and was the source of Poland's finest artistic and literary productions. In this same era, Marie Skłodowska Curie, a pioneer radiation scientist, performed her groundbreaking research in Paris.

The Revolution of 1905–1907 in Russian Poland, the result of many years of pent-up political frustrations and stifled national ambitions, was marked by political maneuvering, strikes and rebellion. The revolt was part of much broader disturbances throughout the Russian Empire associated with the general Revolution of 1905. In Poland, the principal revolutionary figures were Roman Dmowski and Józef Piłsudski. Dmowski was associated with the right-wing nationalist movement National Democracy, whereas Piłsudski was associated with the Polish Socialist Party. As the authorities re-established control within the Russian Empire, the revolt in Congress Poland, placed under martial law, withered as well, partially as a result of tsarist concessions in the areas of national and workers' rights, including Polish representation in the newly created Russian Duma. The collapse of the revolt in the Russian Partition, coupled with intensified Germanization in the Prussian Partition, left Austrian Galicia as the territory where Polish patriotic action was most likely to flourish..

In the Austrian Partition, Polish culture was openly cultivated, and in the Prussian Partition, there were high levels of education and living standards, but the Russian Partition remained of primary importance for the Polish nation and its aspirations. About 15.5 million Polish-speakers lived in the territories most densely populated by Poles: the western part of the Russian Partition, the Prussian Partition and the western Austrian Partition. Ethnically Polish settlement spread over a large area further to the east, including its greatest concentration in the Vilnius Region, amounted to only over 20% of that number.
Polish paramilitary organizations oriented toward independence, such as the Union of Active Struggle, were formed in 1908–1914, mainly in Galicia. The Poles were divided and their political parties fragmented on the eve of World War I, with Dmowski's National Democracy (pro-Entente) and Piłsudski's faction assuming opposing positions.

The outbreak of World War I in the Polish lands offered Poles unexpected hopes for achieving independence as a result of the turbulence that engulfed the empires of the partitioning powers. All three of the monarchies that had benefited from the partition of Polish territories (Germany, Austria and Russia) were dissolved by the end of the war, and many of their territories were dispersed into new political units. At the start of the war, the Poles found themselves conscripted into the armies of the partitioning powers in a war that was not theirs. Furthermore, they were frequently forced to fight each other, since the armies of Germany and Austria were allied against Russia. Piłsudski's paramilitary units stationed in Galicia were turned into the Polish Legions in 1914 and as a part of the Austro-Hungarian Army fought on the Russian front until 1917, when the formation was disbanded. Piłsudski, who refused demands that his men fight under German command, was arrested and imprisoned by the Germans and became a heroic symbol of Polish nationalism.

Due to a series of German victories on the Eastern Front, the area of Congress Poland became occupied by the Central Powers of Germany and Austria; Warsaw was captured by the Germans on 5 August 1915. In the Act of 5th November 1916, a fresh incarnation of the Kingdom of Poland ("Królestwo Regencyjne") was proclaimed by Germany and Austria on formerly Russian-controlled territories, within the German "Mitteleuropa" scheme. The sponsor states were never able to agree on a candidate to assume the throne, however; rather, it was governed in turn by German and Austrian governor-generals, a Provisional Council of State, and a Regency Council. This increasingly autonomous puppet state existed until November 1918, when it was replaced by the newly established Republic of Poland. The existence of this "kingdom" and its planned Polish army had a positive effect on the Polish national efforts on the Allied side, but in the Treaty of Brest-Litovsk of March 1918 the victorious in the east Germany imposed harsh conditions on defeated Russia and ignored Polish interests. Toward the end of the war, the German authorities engaged in massive, purposeful devastation of industrial and other economic potential of Polish lands in order to impoverish the country, a likely future competitor of Germany.

The independence of Poland had been campaigned for in Russia and in the West by Dmowski and in the West by Ignacy Jan Paderewski. Tsar Nicholas II of Russia, and then the leaders of the February Revolution and the October Revolution of 1917, installed governments who declared in turn their support for Polish independence. In 1917, France formed the Blue Army (placed under Józef Haller) that comprised about 70,000 Poles by the end of the war, including men captured from German and Austrian units and 20,000 volunteers from the United States. There was also a 30,000-men strong Polish anti-German army in Russia. Dmowski, operating from Paris as head of the Polish National Committee (KNP), became the spokesman for Polish nationalism in the Allied camp. On the initiative of Woodrow Wilson's Fourteen Points, Polish independence was officially endorsed by the Allies in June 1918.

In all, about two million Poles served in the war, counting both sides, and about 400–450,000 died. Much of the fighting on the Eastern Front took place in Poland, and civilian casualties and devastation were high.

The final push for independence of Poland took place on the ground in October–November 1918. Near the end of the war, Austro-Hungarian and German units were being disarmed, and the Austrian army's collapse freed Cieszyn and Kraków at the end of October. Lviv was then contested in the Polish–Ukrainian War of 1918–1919. Ignacy Daszyński headed the first short-lived independent Polish government in Lublin from 7 November, the leftist Provisional People's Government of the Republic of Poland, proclaimed as a democracy. Germany, now defeated, was forced by the Allies to stand down its large military forces in Poland. Overtaken by the German Revolution of 1918–1919 at home, the Germans released Piłsudski from prison. He arrived in Warsaw on 10 November and was granted extensive authority by the Regency Council; Piłsudski's authority was also recognized by the Lublin government. On 22 November, he became the temporary head of state. Piłsudski was held by many in high regard, but was resented by the right-wing National Democrats. The emerging Polish state was internally divided, heavily war-damaged and economically dysfunctional.

After more than a century of foreign rule, Poland regained its independence at the end of World War I as one of the outcomes of the negotiations that took place at the Paris Peace Conference of 1919. The Treaty of Versailles that emerged from the conference set up an independent Polish nation with an outlet to the sea, but left some of its boundaries to be decided by plebiscites. The largely German-inhabited Free City of Danzig was granted a separate status that guaranteed its use as a port by Poland. In the end, the settlement of the German-Polish border turned out to be a prolonged and convoluted process. The dispute helped engender the Greater Poland Uprising of 1918–1919, the three Silesian uprisings of 1919–1921, the East Prussian plebiscite of 1920, the Upper Silesia plebiscite of 1921 and the 1922 Silesian Convention in Geneva.

Other boundaries were settled by war and subsequent treaties. A total of six border wars were fought in 1918–1921, including the Polish–Czechoslovak border conflicts over Cieszyn Silesia in January 1919.

As distressing as these border conflicts were, the Polish–Soviet War of 1919–1921 was the most important series of military actions of the era. Piłsudski had entertained far-reaching anti-Russian cooperative designs in Eastern Europe, and in 1919 the Polish forces pushed eastward into Lithuania, Belarus and Ukraine by taking advantage of the Russian preoccupation with a civil war, but they were soon confronted with the Soviet westward offensive of 1918–1919. Western Ukraine was already a theater of the Polish–Ukrainian War, which eliminated the proclaimed West Ukrainian People's Republic in July 1919. In the autumn of 1919, Piłsudski rejected urgent pleas from the former Entente powers to support Anton Denikin's White movement in its advance on Moscow. The Polish–Soviet War proper began with the Polish Kiev Offensive in April 1920. Allied with the Directorate of Ukraine of the Ukrainian People's Republic, the Polish armies had advanced past Vilnius, Minsk and Kiev by June. At that time, a massive Soviet counter-offensive pushed the Poles out of most of Ukraine. On the northern front, the Soviet army reached the outskirts of Warsaw in early August. A Soviet triumph and the quick end of Poland seemed inevitable. However, the Poles scored a stunning victory at the Battle of Warsaw (1920). Afterwards, more Polish military successes followed, and the Soviets had to pull back. They left swathes of territory populated largely by Belarusians or Ukrainians to Polish rule. The new eastern boundary was finalized by the Peace of Riga in March 1921.

The defeat of the Russian armies forced Vladimir Lenin and the Soviet leadership to postpone their strategic objective of linking up with the German and other European revolutionary leftist collaborators to spread communist revolution. Lenin also hoped for generating support for the Red Army in Poland, which failed to materialize.

Piłsudski's seizure of Vilnius in October 1920 (known as Żeligowski's Mutiny) was a nail in the coffin of the already poor Lithuania–Poland relations that had been strained by the Polish–Lithuanian War of 1919–1920; both states would remain hostile to one another for the remainder of the interwar period. Piłsudski's concept of Intermarium (an East European federation of states inspired by the tradition of the multiethnic Polish–Lithuanian Commonwealth that would include a hypothetical multinational successor state to the Grand Duchy of Lithuania) had the fatal flaw of being incompatible with his assumption of Polish domination, which would amount to an encroachment on the neighboring peoples' lands and aspirations. At the time of rising national movements, the plan thus ceased being a feature of Poland's politics. A larger federated structure was also opposed by Dmowski's National Democrats. Their representative at the Peace of Riga talks, Stanisław Grabski, opted for leaving Minsk, Berdychiv, Kamianets-Podilskyi and the surrounding areas on the Soviet side of the border. The National Democrats did not want to assume the lands they considered politically undesirable, as such territorial enlargement would result in a reduced proportion of citizens who were ethnically Polish.

The Peace of Riga settled the eastern border by preserving for Poland a substantial portion of the old Commonwealth's eastern territories at the cost of partitioning the lands of the former Grand Duchy of Lithuania (Lithuania and Belarus) and Ukraine. The Ukrainians ended up with no state of their own and felt betrayed by the Riga arrangements; their resentment gave rise to extreme nationalism and anti-Polish hostility. The Kresy (or borderland) territories in the east won by 1921 would form the basis for a swap arranged and carried out by the Soviets in 1943–1945, who at that time compensated the re-emerging Polish state for the eastern lands lost to the Soviet Union with conquered areas of eastern Germany.

The successful outcome of the Polish–Soviet War gave Poland a false sense of its prowess as a self-sufficient military power and encouraged the government to try to resolve international problems through imposed unilateral solutions. The territorial and ethnic policies of the interwar period contributed to bad relations with most of Poland's neighbors and uneasy cooperation with more distant centers of power, especially France and Great Britain.

Among the chief difficulties faced by the government of the new Polish republic was the lack of an integrated infrastructure among the formerly separate partitions, a deficiency that disrupted industry, transportation, trade, and other areas.

The first Polish legislative election for the re-established Sejm (national parliament) took place in January 1919. A temporary Small Constitution was passed by the body the following month.

The rapidly growing population of Poland within its new boundaries was ¾ agricultural and ¼ urban; Polish was the primary language of only ⅔ of the inhabitants of the new country. The minorities had very little voice in the government. The permanent March Constitution of Poland was adopted in March 1921. At the insistence of the National Democrats, who were concerned about how aggressively Józef Piłsudski might exercise presidential powers if he were elected to office, the constitution mandated limited prerogatives for the presidency.

The proclamation of the March Constitution was followed by a short and turbulent period of constitutional order and parliamentary democracy that lasted until 1926. The legislature remained fragmented, without stable majorities, and governments changed frequently. The open-minded Gabriel Narutowicz was elected president constitutionally (without a popular vote) by the National Assembly in 1922. However, members of the nationalist right-wing faction did not regard his elevation as legitimate. They viewed Narutowicz rather as a traitor whose election was pushed through by the votes of alien minorities. Narutowicz and his supporters were subjected to an intense harassment campaign, and the president was assassinated on 16 December 1922, after serving only five days in office.

Corruption was held to be commonplace in the political culture of the early Polish Republic. However, the investigations conducted by the new regime after the 1926 May Coup failed to uncover any major affair or corruption scheme within the state apparatus of its predecessors.

Land reform measures were passed in 1919 and 1925 under pressure from an impoverished peasantry. They were partially implemented, but resulted in the parcellation of only 20% of the great agricultural estates. Poland endured numerous economic calamities and disruptions in the early 1920s, including waves of workers' strikes such as the 1923 Kraków riot. The German–Polish customs war, initiated by Germany in 1925, was one of the most damaging external factors that put a strain on Poland's economy. On the other hand, there were also signs of progress and stabilization, for example a critical reform of finances carried out by the competent government of Władysław Grabski, which lasted almost two years. Certain other achievements of the democratic period having to do with the management of governmental and civic institutions necessary to the functioning of the reunited state and nation were too easily overlooked. Lurking on the sidelines was a disgusted army officer corps unwilling to subject itself to civilian control, but ready to follow the retired Piłsudski, who was highly popular with Poles and just as dissatisfied with the Polish system of government as his former colleagues in the military.

On 12 May 1926, Piłsudski staged the May Coup, a military overthrow of the civilian government mounted against President Stanisław Wojciechowski and the troops loyal to the legitimate government. Hundreds died in fratricidal fighting. Piłsudski was supported by several leftist factions who ensured the success of his coup by blocking the railway transportation of government forces. He also had the support of the conservative great landowners, a move that left the right-wing National Democrats as the only major social force opposed to the takeover.

Following the coup, the new regime initially respected many parliamentary formalities, but gradually tightened its control and abandoned pretenses. The Centrolew, a coalition of center-left parties, was formed in 1929, and in 1930 called for the "abolition of dictatorship". In 1930, the Sejm was dissolved and a number of opposition deputies were imprisoned at the Brest Fortress. Five thousand political opponents were arrested ahead of the Polish legislative election of 1930, which was rigged to award a majority of seats to the pro-regime Nonpartisan Bloc for Cooperation with the Government (BBWR).

The authoritarian Sanation regime ("sanation" meant to denote "healing") that Piłsudski led until his death in 1935 (and would remain in place until 1939) reflected the dictator's evolution from his center-left past to conservative alliances. Political institutions and parties were allowed to function, but the electoral process was manipulated and those not willing to cooperate submissively were subjected to repression. From 1930, persistent opponents of the regime, many of the leftist persuasion, were imprisoned and subjected to staged legal processes with harsh sentences, such as the Brest trials, or else detained in the Bereza Kartuska prison and similar camps for political prisoners. About three thousand were detained without trial at different times at the Bereza internment camp between 1934 and 1939. In 1936 for example, 369 activists were taken there, including 342 Polish communists. Rebellious peasants staged riots in 1932, 1933 and the 1937 peasant strike in Poland. Other civil disturbances were caused by striking industrial workers (e.g. events of the "Bloody Spring" of 1936), nationalist Ukrainians and the activists of the incipient Belarusian movement. All became targets of ruthless police-military pacification. Besides sponsoring political repression, the regime fostered Józef Piłsudski's cult of personality that had already existed long before he assumed dictatorial powers.

Piłsudski signed the Soviet–Polish Non-Aggression Pact in 1932 and the German–Polish Non-Aggression Pact in 1934, but in 1933 he insisted that there was no threat from the East or West and said that Poland's politics were focused on becoming fully independent without serving foreign interests. He initiated the policy of maintaining an equal distance and an adjustable middle course regarding the two great neighbors, later continued by Józef Beck. Piłsudski kept personal control of the army, but it was poorly equipped, poorly trained and had poor preparations in place for possible future conflicts. His only war plan was a defensive war against a Soviet invasion. The slow modernization after Piłsudski's death fell far behind the progress made by Poland's neighbors and measures to protect the western border, discontinued by Piłsudski from 1926, were not undertaken until March 1939.

Sanation deputies in the Sejm used a parliamentary maneuver to abolish the democratic March Constitution and push through a more authoritarian April Constitution in 1935; it reduced the powers of the Sejm, which Piłsudski despised. The process and the resulting document were seen as illegitimate by the anti-Sanation opposition, but during World War II, the Polish government-in-exile recognized the April Constitution in order to uphold the legal continuity of the Polish state.

When Marshal Piłsudski died in 1935, he retained the support of dominant sections of Polish society even though he never risked testing his popularity in an honest election. His regime was dictatorial, but at that time only Czechoslovakia remained democratic in all of the regions neighboring Poland. Historians have taken widely divergent views of the meaning and consequences of the coup Piłsudski perpetrated and his personal rule that followed.

Independence stimulated the development of Polish culture in the Interbellum and intellectual achievement was high. Warsaw, whose population almost doubled between World War I and World War II, was a restless, burgeoning metropolis. It outpaced Kraków, Lwów and Wilno, the other major population centers of the country.

Mainstream Polish society was not affected by the repressions of the Sanation authorities overall; many Poles enjoyed relative stability, and the economy improved markedly between 1926 and 1929, only to become caught up in the global Great Depression. After 1929, the country's industrial production and gross national income slumped by about 50%.

The Great Depression brought low prices for farmers and unemployment for workers. Social tensions increased, including rising antisemitism. A major economic transformation and multi-year state plan to achieve national industrial development, as embodied in the Central Industrial Region initiative launched in 1936, was led by Minister Eugeniusz Kwiatkowski. Motivated primarily by the need for a native arms industry, the initiative was in progress at the time of the outbreak of World War II. Kwiatkowski was also the main architect of the earlier Gdynia seaport project.

The prevalent in political circles nationalism was fueled by the large size of Poland's minority populations and their separate agendas. According to the language criterion of the Polish census of 1931, the Poles constituted 69% of the population, Ukrainians 15%, Jews (defined as speakers of the Yiddish language) 8.5%, Belarusians 4.7%, Germans 2.2%, Lithuanians 0.25%, Russians 0.25% and Czechs 0.09%, with some geographical areas dominated by a particular minority. In time, the ethnic conflicts intensified, and the Polish state grew less tolerant of the interests of its national minorities. In interwar Poland, compulsory free general education substantially reduced illiteracy rates, but discrimination was practiced in a way that resulted in a dramatic decrease in the number of Ukrainian language schools and official restrictions on Jewish attendance at selected schools in the late 1930s.

The population grew steadily, reaching 35 million in 1939. However, the overall economic situation in the interwar period was one of stagnation. There was little money for investment inside Poland, and few foreigners were interested in investing there. Total industrial production barely increased between 1913 and 1939 (within the area delimited by the 1939 borders), but because of population growth (from 26.3 millions in 1919 to 34.8 millions in 1939), the "per capita" output actually decreased by 18%.

Conditions in the predominant agricultural sector kept deteriorating between 1929 and 1939, which resulted in rural unrest and a progressive radicalization of the Polish peasant movement that became increasingly inclined toward militant anti-state activities. It was firmly repressed by the authorities. According to Norman Davies, the failures of the Sanation regime (combined with the objective economic realities) caused a radicalization of the Polish masses by the end of the 1930s, but he warns against drawing parallels with the incomparably more repressive regimes of Nazi Germany or the Stalinist Soviet Union.

After Piłsudski's death in 1935, Poland was governed until (and initially during) the German invasion of 1939 by old allies and subordinates known as "Piłsudski's colonels". They had neither the vision nor the resources to cope with the perilous situation facing Poland in the late 1930s. The colonels had gradually assumed greater powers during Piłsudski's life by manipulating the ailing marshal behind the scenes. Eventually they achieved an overt politicization of the army that did nothing to help prepare the country for war.

Foreign policy was the responsibility of Józef Beck, under whom Polish diplomacy attempted balanced approaches toward Germany and the Soviet Union, unfortunately without success, on the basis of a flawed understanding of the European geopolitics of his day. Beck had numerous foreign policy schemes and harbored illusions of Poland's status as a great power. He alienated most of Poland's neighbors, but is not blamed by historians for the ultimate failure of relations with Germany. The principal events of his tenure were concentrated in its last two years. In the case of the 1938 Polish ultimatum to Lithuania, the Polish action nearly resulted in a German takeover of southwest Lithuania, the Klaipėda Region (Memel Territory), which had a largely German population. Also in 1938, the Polish government opportunistically undertook a hostile action against the Czechoslovak state as weakened by the Munich Agreement and annexed a small piece of territory on its borders. In this case, Beck's understanding of the consequences of the Polish military move turned out to be completely mistaken, because in the end the German occupation of Czechoslovakia markedly weakened Poland's own position. Furthermore, Beck erroneously believed that Nazi-Soviet ideological contradictions would preclude their cooperation.

At home, increasingly alienated minorities threatened unrest and violence and were suppressed. Extreme nationalist circles such as the National Radical Camp grew more outspoken. One of the groups, the Camp of National Unity, combined many nationalists with Sanation supporters and was connected to the new strongman, Marshal Edward Rydz-Śmigły, whose faction of the Sanation ruling movement was increasingly nationalistic.

In the late 1930s, the exile bloc Front Morges united several major Polish anti-Sanation figures, including Ignacy Paderewski, Władysław Sikorski, Wincenty Witos, Wojciech Korfanty and Józef Haller. It gained little influence inside Poland, but its spirit soon reappeared during World War II, within the Polish government-in-exile.

In October 1938, Joachim von Ribbentrop first proposed German-Polish territorial adjustments and Poland's participation in the Anti-Comintern Pact against the Soviet Union. The status of the Free City of Danzig was one of the key bones of contention. Approached by Ribbentrop again in March 1939, the Polish government expressed willingness to address issues causing German concern, but effectively rejected Germany's stated demands and thus refused to allow Poland to be turned by Adolf Hitler into a German puppet state. Hitler, incensed by the British and French declarations of support for Poland, abrogated the German–Polish Non-Aggression Pact in late April 1939.

To protect itself from an increasingly aggressive Nazi Germany, already responsible for the annexations of Austria (in the Anschluss of 1938), Czechoslovakia (in 1939) and a part of Lithuania after the 1939 German ultimatum to Lithuania, Poland entered into a military alliance with Britain and France (the 1939 Anglo-Polish military alliance and the Franco-Polish alliance (1921), as updated in 1939). However, the two Western powers were defense-oriented and not in a strong position, either geographically or in terms of resources, to assist Poland. Attempts were therefore made by them to induce Soviet-Polish cooperation, which they viewed as the only militarily viable arrangement.

Diplomatic manoeuvers continued in the spring and summer of 1939, but in their final attempts, the Franco-British talks with the Soviets in Moscow on forming an anti-Nazi defensive military alliance failed. Warsaw's refusal to allow the Red Army to operate on Polish territory doomed the Western efforts. The final contentious Allied-Soviet exchanges took place on 21 and 23 August 1939. Stalin's regime was the target of an intense German counter-initiative and was concurrently involved in increasingly effective negotiations with Hitler's agents. On 23 August, an outcome contrary to the exertions of the Allies became a reality: in Moscow, Germany and the Soviet Union hurriedly signed the Molotov–Ribbentrop Pact, which secretly provided for the dismemberment of Poland into Nazi- and Soviet-controlled zones.

On 1 September 1939, Hitler ordered an invasion of Poland, the opening event of World War II. Poland had signed an Anglo-Polish military alliance as recently as 25 August, and had long been in alliance with France. The two Western powers soon declared war on Germany, but they remained largely inactive (the period early in the conflict became known as the Phoney War) and extended no aid to the attacked country. The technically and numerically superior "Wehrmacht" formations rapidly advanced eastwards and engaged massively in the murder of Polish civilians over the entire occupied territory. On 17 September, a Soviet invasion of Poland began. The Soviet Union quickly occupied most of the areas of eastern Poland that contained large populations of Ukrainians and Belarusians. The two invading powers divided up the country as they had agreed in the secret provisions of the Molotov–Ribbentrop Pact. Poland's top government officials and military high command fled the war zone and arrived at the Romanian Bridgehead in mid-September. After the Soviet entry they sought refuge in Romania.

Among the military operations in which Poles held out the longest (until late September or early October) were the Siege of Warsaw, the Battle of Hel and the resistance of the Independent Operational Group Polesie. Warsaw fell on 27 September after a heavy German bombardment that killed tens of thousands civilians and soldiers. Poland was ultimately partitioned between Germany and the Soviet Union according to the terms of the German–Soviet Frontier Treaty signed by the two powers in Moscow on 29 September.

Gerhard Weinberg has argued that the most significant Polish contribution to World War II was sharing its code-breaking results. This allowed the British to perform the cryptanalysis of the Enigma and decipher the main German military code, which gave the Allies a major advantage in the conflict. As regards actual military campaigns, some Polish historians have argued that simply resisting the initial invasion of Poland was the country's greatest contribution to the victory over Nazi Germany, despite its defeat. The Polish Army of nearly one million men significantly delayed the start of the Battle of France, planned by the Germans for 1939. When the Nazi offensive in the West did happen, the delay caused it to be less effective, a possibly crucial factor in the victory of the Battle of Britain.

After Germany invaded the Soviet Union as part of its Operation Barbarossa in June 1941, the whole of pre-war Poland was overrun and occupied by German troops.

German-occupied Poland was divided from 1939 into two regions: Polish areas annexed by Nazi Germany directly into the German "Reich" and areas ruled under a so-called General Government of occupation. The Poles formed an underground resistance movement and a Polish government-in-exile that operated first in Paris, then, from July 1940, in London. Polish-Soviet diplomatic relations, broken since September 1939, were resumed in July 1941 under the Sikorski–Mayski agreement, which facilitated the formation of a Polish army (the Anders' Army) in the Soviet Union. In November 1941, Prime Minister Sikorski flew to the Soviet Union to negotiate with Stalin on its role on the Soviet-German front, but the British wanted the Polish soldiers in the Middle East. Stalin agreed, and the army was evacuated there.

The organizations forming the Polish Underground State that functioned in Poland throughout the war were loyal to and formally under the Polish government-in-exile, acting through its Government Delegation for Poland. During World War II, hundreds of thousands of Poles joined the underground Polish Home Army ("Armia Krajowa"), a part of the Polish Armed Forces of the government-in-exile. About 200,000 Poles fought on the Western Front in the Polish Armed Forces in the West loyal to the government-in-exile, and about 300,000 in the under the Soviet command on the Eastern Front. The pro-Soviet resistance movement in Poland, led by the Polish Workers' Party, was active from 1941. It was opposed by the gradually forming extreme nationalistic National Armed Forces.

Beginning in late 1939, hundreds of thousands of Poles from the Soviet-occupied areas were deported and taken east. Of the upper-ranking military personnel and others deemed uncooperative or potentially harmful by the Soviets, about 22,000 were secretly executed by them at the Katyn massacre. In April 1943, the Soviet Union broke off deteriorating relations with the Polish government-in-exile after the German military announced the discovery of mass graves containing murdered Polish army officers. The Soviets claimed that the Poles committed a hostile act by requesting that the Red Cross investigate these reports.

From 1941, the implementation of the Nazi Final Solution began, and the Holocaust in Poland proceeded with force. Warsaw was the scene of the Warsaw Ghetto Uprising in April–May 1943, triggered by the liquidation of the Warsaw Ghetto by German SS units. The elimination of Jewish ghettos in German-occupied Poland took place in many cities. As the Jewish people were being removed to be exterminated, uprisings were waged against impossible odds by the Jewish Combat Organization and other desperate Jewish insurgents.

At a time of increasing cooperation between the Western Allies and the Soviet Union in the wake of the Nazi invasion of 1941, the influence of the Polish government-in-exile was seriously diminished by the death of Prime Minister Władysław Sikorski, its most capable leader, in a plane crash on 4 July 1943. His successors lacked the ability or willingness to negotiate effectively with the Soviets and proved equally ineffective in pressing for the interests of the Polish people with the Western Allies.

In July 1944, the Soviet Red Army and Soviet-controlled Polish People's Army entered the territory of future postwar Poland. In protracted fighting in 1944 and 1945, the Soviets and their Polish allies defeated and expelled the German army from Poland at a cost of over 600,000 Soviet soldiers lost.

The greatest single undertaking of the Polish resistance movement in World War II and a major political event was the Warsaw Uprising that began on 1 August 1944. The uprising, in which most of the city's population participated, was instigated by the underground Home Army and approved by the Polish government-in-exile in an attempt to establish a non-communist Polish administration ahead of the arrival of the Red Army. The uprising was originally planned as a short-lived armed demonstration in expectation that the Soviet forces approaching Warsaw would assist in any battle to take the city. The Soviets had never agreed to an intervention, however, and they halted their advance at the Vistula River. The Germans used the opportunity to carry out a brutal suppression of the forces of the pro-Western Polish underground.

The bitterly fought uprising lasted for two months and resulted in the death or expulsion from the city of hundreds of thousands of civilians. After the defeated Poles surrendered on 2 October, the Germans carried out a planned destruction of Warsaw on Hitler's orders that obliterated the remaining infrastructure of the city. The Polish First Army, fighting alongside the Soviet Red Army, entered a devastated Warsaw on 17 January 1945.

From the time of the Tehran Conference in late 1943, there was broad agreement among the three Great Powers (the United States, the United Kingdom, and the Soviet Union) that the locations of the borders between Germany and Poland and between Poland and the Soviet Union would be fundamentally changed after the conclusion of World War II. Stalin's proposal that Poland should be moved far to the west was readily accepted by Polish communists, including the Polish Workers' Party and the Union of Polish Patriots, who were at that time in the early stages of forming a post-war government: the State National Council, a quasi-parliamentary body, was created. In July 1944, a communist-controlled Polish Committee of National Liberation was established in Lublin nominally to govern the areas liberated from German control, a move that prompted protests from Prime Minister Stanisław Mikołajczyk and his Polish government-in-exile.

By the time of the Yalta Conference in February 1945, the communists had already established a Provisional Government of the Republic of Poland. The Soviet position at the conference was strong because of their decisive contribution to the war effort and as a result of their occupation of immense amounts of land in central and eastern Europe. The Great Powers gave assurances that the communist provisional government would be converted into an entity that would include democratic forces from within the country and active abroad, but the London-based government-in-exile was not mentioned. A Provisional Government of National Unity and subsequent democratic elections were the agreed stated goals. The disappointing results of these plans and the failure of the Western powers to ensure a strong participation of non-communists in the immediate post-war Polish government were seen by many Poles as a manifestation of Western betrayal.

A lack of accurate data makes it difficult to document numerically the extent of the human losses suffered by Polish citizens during World War II. Additionally, many assertions made in the past must be considered suspect due to flawed methodology and a desire to promote certain political agendas. The last available enumeration of ethnic Poles and the large ethnic minorities is the Polish census of 1931. Exact population figures for 1939 are therefore not known.

According to the United States Holocaust Memorial Museum, at least 3 million Polish Jews and at least 1.9 million non-Jewish Polish civilians were killed. According to the historians Brzoza and Sowa, about 2 million ethnic Poles were killed, but it is not known, even approximately, how many Polish citizens of other ethnicities perished, including Ukrainians, Belarusians, and Germans. Millions of Polish citizens were deported to Germany for forced labor or to German extermination camps such as Treblinka, Auschwitz and Sobibór. Nazi Germany intended to exterminate the Jews completely, in actions that have come to be described collectively as the Holocaust. The Poles were to be expelled from areas controlled by Nazi Germany through a process of resettlement that started in 1939. Such Nazi operations matured into a plan known as the "Generalplan Ost" that amounted to displacement, enslavement and partial extermination of the Slavic people and was expected to be completed within 15 years. 

In an attempt to incapacitate Polish society, the Nazis and the Soviets executed tens of thousands of members of the intelligentsia and community leadership during events such as the German AB-Aktion in Poland, Operation Tannenberg and the Katyn massacre. Over 95% of the Jewish losses and 90% of the ethnic Polish losses were caused directly by Nazi Germany, whereas 5% of the ethnic Polish losses were caused by the Soviets and 5% by Ukrainian nationalists. The large-scale Jewish presence in Poland that had endured for centuries was rather quickly put to an end by the policies of extermination implemented by the Nazis during the war. Waves of displacement and emigration that took place both during and after the war removed from Poland a majority of the Jews who survived. Further significant Jewish emigration followed events such as the Polish October political thaw of 1956 and the 1968 Polish political crisis.

In 1940–1941, some 325,000 Polish citizens were deported by the Soviet regime. The number of Polish citizens who died at the hands of the Soviets is estimated at less than 100,000.

In 1943–1944, Ukrainian nationalists associated with the Organization of Ukrainian Nationalists (OUN) and the Ukrainian Insurgent Army perpetrated the Massacres of Poles in Volhynia and Eastern Galicia. Estimates of the number of Polish civilian victims vary greatly, from tens to hundreds of thousands.

Approximately 90% of Poland's war casualties were the victims of prisons, death camps, raids, executions, the annihilation of ghettos, epidemics, starvation, excessive work and ill treatment. The war left one million children orphaned and 590,000 persons disabled. The country lost 38% of its national assets (whereas Britain lost only 0.8%, and France only 1.5%). Nearly half of pre-war Poland was expropriated by the Soviet Union, including the two great cultural centers of Lwów and Wilno.

By the terms of the 1945 Potsdam Agreement signed by the three victorious Great Powers, the Soviet Union retained most of the territories captured as a result of the Molotov–Ribbentrop Pact of 1939, including western Ukraine and western Belarus, and gained others. Lithuania and the Königsberg area of East Prussia were officially incorporated into the Soviet Union, in the case of the former without the recognition of the Western powers. Poland was compensated with the bulk of Silesia, including Breslau (Wrocław) and Grünberg (Zielona Góra), the bulk of Pomerania, including Stettin (Szczecin), and the greater southern portion of the former East Prussia, along with Danzig (Gdańsk). Collectively referred to by the Polish authorities as the "Recovered Territories", they were included in the reconstituted Polish state. With Germany's defeat Poland was thus shifted west in relation to its prewar location, to the area between the Oder–Neisse and Curzon lines, which resulted in a country more compact and with much broader access to the sea. The Poles lost 70% of their pre-war oil capacity to the Soviets, but gained from the Germans a highly developed industrial base and infrastructure that made a diversified industrial economy possible for the first time in Polish history.
The flight and expulsion of Germans from what was eastern Germany prior to the war began before and during the Soviet conquest of those regions from the Nazis, and the process continued in the years immediately after the war. 
8,030,000 Germans were evacuated, expelled, or migrated by 1950. Early expulsions in Poland were undertaken by the Polish communist authorities even before the Potsdam Conference (the "wild expulsions" from June to mid July 1945, when the Polish military and militia expelled nearly all people from the districts immediately east of the Oder–Neisse line), to ensure the establishment of ethnically homogeneous Poland. About 1% (100,000) of the German civilian population east of the Oder–Neisse line perished in the fighting prior to the surrender in May 1945, and afterwards some 200,000 Germans in Poland were employed as forced labor prior to being expelled. Of those Germans who remained, many later chose to emigrate to post-war Germany. On the other hand, 1.5–2 million ethnic Poles moved or were expelled from Polish areas annexed by the Soviet Union. The vast majority were resettled in the former German territories. At least one million Poles remained in what had become the Soviet Union, and at least half a million ended up in the West or elsewhere outside of Poland.

Many exiled Poles could not return to the country for which they had fought because they belonged to political groups incompatible with the new communist regimes, or because they originated from areas of pre-war eastern Poland that were incorporated into the Soviet Union (see Polish population transfers (1944–1946)). Some were deterred from returning simply on the strength of warnings that anyone who had served in military units in the West would be endangered. Many Poles were pursued, arrested, tortured and imprisoned by the Soviet authorities for belonging to the Home Army or other formations (see Anti-communist resistance in Poland (1944–1946)), or were persecuted because they had fought on the Western front.

Territories on both sides of the new Polish-Ukrainian border were also "ethnically cleansed". Of the Ukrainians and Lemkos living in Poland within the new borders (about 700,000), close to 95% were forcibly moved to the Soviet Ukraine, or (in 1947) to the new territories in northern and western Poland under Operation Vistula. In Volhynia, 98% of the Polish pre-war population was either killed or expelled; in Eastern Galicia, the Polish population was reduced by 92%. According to Timothy D. Snyder, about 70,000 Poles and about 20,000 Ukrainians were killed in the ethnic violence that occurred in the 1940s, both during and after the war.

According to an estimate by historian Jan Grabowski, about 50,000 of the 250,000 Polish Jews who escaped the Nazis during the liquidation of ghettos survived without leaving Poland (the remainder perished). More were repatriated from the Soviet Union and elsewhere, and the February 1946 population census showed about 300,000 Jews within Poland's new borders. Of the surviving Jews, many chose to emigrate or felt compelled to because of the anti-Jewish violence in Poland.

Because of changing borders and the mass movements of people of various nationalities, the emerging communist Poland ended up with a mainly homogeneous, ethnically Polish population (97.6% according to the December 1950 census). The remaining members of ethnic minorities were not encouraged, by the authorities or by their neighbors, to emphasize their ethnic identities.

In response to the February 1945 Yalta Conference directives, a Polish Provisional Government of National Unity was formed in June 1945 under Soviet auspices; it was soon recognized by the United States and many other countries. The Soviet domination was apparent from the beginning, as prominent leaders of the Polish Underground State were brought to trial in Moscow (the "Trial of the Sixteen" of June 1945). In the immediate post-war years, the emerging communist rule was challenged by opposition groups, including militarily by the so-called "cursed soldiers", of whom thousands perished in armed confrontations or were pursued by the Ministry of Public Security and executed. Such guerillas often pinned their hopes on expectations of an imminent outbreak of World War III and defeat of the Soviet Union. The Polish right-wing insurgency faded after the amnesty of February 1947.

The Polish people's referendum of June 1946 was arranged by the communist Polish Workers' Party to legitimize its dominance in Polish politics and claim widespread support for the party's policies. Although the Yalta agreement called for free elections, the Polish legislative election of January 1947 was controlled by the communists. Some democratic and pro-Western elements, led by Stanisław Mikołajczyk, former prime minister-in-exile, participated in the Provisional Government and the 1947 elections, but were ultimately eliminated through electoral fraud, intimidation and violence. In times of severe political confrontation and radical economic change, members of Mikołajczyk's agrarian movement (the Polish People's Party) attempted to preserve the existing aspects of mixed economy and protect property and other rights. However, after the 1947 elections, the Government of National Unity ceased to exist and the communists moved towards abolishing the post-war partially pluralistic "people's democracy" and replacing it with a state socialist system. The communist-dominated front Democratic Bloc of the 1947 elections, turned into the Front of National Unity in 1952, became officially the source of governmental authority. The Polish government-in-exile, lacking international recognition, remained in continuous existence until 1990.

The Polish People's Republic ("Polska Rzeczpospolita Ludowa") was established under the rule of the communist Polish United Workers' Party (PZPR). The name change from the Polish Republic was not officially adopted, however, until the proclamation of the Constitution of the Polish People's Republic in 1952.

The ruling PZPR was formed by the forced amalgamation in December 1948 of the communist Polish Workers' Party (PPR) and the historically non-communist Polish Socialist Party (PPS). The PPR chief had been its wartime leader Władysław Gomułka, who in 1947 declared a "Polish road to socialism" as intended to curb, rather than eradicate, capitalist elements. In 1948 he was overruled, removed and imprisoned by Stalinist authorities.The PPS, re-established in 1944 by its left wing, had since been allied with the communists. The ruling communists, who in post-war Poland preferred to use the term "socialism" instead of "communism" to identify their ideological basis, needed to include the socialist junior partner to broaden their appeal, claim greater legitimacy and eliminate competition on the political Left. The socialists, who were losing their organization, were subjected to political pressure, ideological cleansing and purges in order to become suitable for unification on the terms of the PPR. The leading pro-communist leaders of the socialists were the prime ministers Edward Osóbka-Morawski and Józef Cyrankiewicz.

During the most oppressive phase of the Stalinist period (1948–1953), terror was justified in Poland as necessary to eliminate reactionary subversion. Many thousands of perceived opponents of the regime were arbitrarily tried and large numbers were executed. The People's Republic was led by discredited Soviet operatives such as Bolesław Bierut, Jakub Berman and Konstantin Rokossovsky. The independent Catholic Church in Poland was subjected to property confiscations and other curtailments from 1949, and in 1950 was pressured into signing an accord with the government. In 1953 and later, despite a partial thaw after the death of Joseph Stalin that year, the persecution of the Church intensified and its head, Cardinal Stefan Wyszyński, was detained. A key event in the persecution of the Polish Church was the Stalinist show trial of the Kraków Curia in January 1953.

In the Warsaw Pact, formed in 1955, the army of the Polish People's Republic was the second largest, after the Soviet Army.

In 1944, large agricultural holdings and former German property in Poland started to be redistributed through land reform, and industry started to be nationalized. Communist restructuring and the imposition of work-space rules encountered active worker opposition already in the years 1945–1947. The moderate Three-Year Plan of 1947–1949 continued with the rebuilding, socialization and socialist restructuring of the economy. It was followed by the Six-Year Plan of 1950–1955 for heavy industry. The rejection of the Marshall Plan in 1947 made aspirations for catching up with West European standards of living unrealistic.

The government's highest economic priority was the development of heavy industry useful to the military. State-run or controlled institutions common in all the socialist countries of eastern Europe were imposed on Poland, including collective farms and worker cooperatives. The latter were dismantled in the late 1940s as not socialist enough, although they were later re-established; even small-scale private enterprises were eradicated. Stalinism introduced heavy political and ideological propaganda and indoctrination in social life, culture and education.

Great strides were made, however, in the areas of employment (which became nearly full), universal public education (which nearly eradicated adult illiteracy), health care and recreational amenities. Many historic sites, including the central districts of Warsaw and Gdańsk, both devastated during the war, were rebuilt at great cost.

The communist industrialization program led to increased urbanization and educational and career opportunities for the intended beneficiaries of the social transformation, along the lines of the peasants-workers-working intelligentsia paradigm. The most significant improvement was accomplished in the lives of Polish peasants, many of whom were able to leave their impoverished and overcrowded village communities for better conditions in urban centers. Those who stayed behind took advantage of the implementation of the 1944 land reform decree of the Polish Committee of National Liberation, which terminated the antiquated but widespread parafeudal socioeconomic relations in Poland. The Stalinist attempts at establishing collective farms generally failed. Due to urbanization, the national percentage of the rural population decreased in communist Poland by about 50%. A majority of Poland's residents of cities and towns still live in apartment blocks built during the communist era, in part to accommodate migrants from rural areas.

In March 1956, after the 20th Congress of the Communist Party of the Soviet Union in Moscow ushered in de-Stalinization, Edward Ochab was chosen to replace the deceased Bolesław Bierut as first secretary of the Polish United Workers' Party. As a result, Poland was rapidly overtaken by social restlessness and reformist undertakings; thousands of political prisoners were released and many people previously persecuted were officially rehabilitated. Worker riots in Poznań in June 1956 were violently suppressed, but they gave rise to the formation of a reformist current within the communist party.

Amidst the continuing social and national upheaval, a further shakeup took place in the party leadership as part of what is known as the Polish October of 1956. While retaining most traditional communist economic and social aims, the regime led by Władysław Gomułka, the new first secretary of the PZPR, liberalized internal life in Poland. The dependence on the Soviet Union was somewhat mollified, and the state's relationships with the Church and Catholic lay activists were put on a new footing. A repatriation agreement with the Soviet Union allowed the repatriation of hundreds of thousands of Poles who were still in Soviet hands, including many former political prisoners. Collectivization efforts were abandoned—agricultural land, unlike in other Comecon countries, remained for the most part in the private ownership of farming families. State-mandated provisions of agricultural products at fixed, artificially low prices were reduced, and from 1972 eliminated.

The legislative election of 1957 was followed by several years of political stability that was accompanied by economic stagnation and curtailment of reforms and reformists. One of the last initiatives of the brief reform era was a nuclear weapons–free zone in Central Europe proposed in 1957 by Adam Rapacki, Poland's foreign minister.

Culture in the Polish People's Republic, to varying degrees linked to the intelligentsia's opposition to the authoritarian system, developed to a sophisticated level under Gomułka and his successors. The creative process was often compromised by state censorship, but significant works were created in fields such as literature, theater, cinema and music, among others. Journalism of veiled understanding and varieties of native and Western popular culture were well represented. Uncensored information and works generated by émigré circles were conveyed through a variety of channels. The Paris-based "Kultura" magazine developed a conceptual framework for dealing with the issues of borders and the neighbors of a future free Poland, but for ordinary Poles Radio Free Europe was of foremost importance.

One of the confirmations of the end of an era of greater tolerance was the expulsion from the communist party of several prominent "Marxist revisionists" in the 1960s.

In 1965, the Conference of Polish Bishops issued the Letter of Reconciliation of the Polish Bishops to the German Bishops, a gesture intended to heal bad mutual feelings left over from World War II. In 1966, the celebrations of the 1,000th anniversary of the Christianization of Poland led by Cardinal Stefan Wyszyński and other bishops turned into a huge demonstration of the power and popularity of the Catholic Church in Poland.

The post-1956 liberalizing trend, in decline for a number of years, was reversed in March 1968, when student demonstrations were suppressed during the 1968 Polish political crisis. Motivated in part by the Prague Spring movement, the Polish opposition leaders, intellectuals, academics and students used a historical-patriotic "Dziady" theater spectacle series in Warsaw (and its termination forced by the authorities) as a springboard for protests, which soon spread to other centers of higher education and turned nationwide. The authorities responded with a major crackdown on opposition activity, including the firing of faculty and the dismissal of students at universities and other institutions of learning. At the center of the controversy was also the small number of Catholic deputies in the Sejm (the Znak Association members) who attempted to defend the students.

In an official speech, Gomułka drew attention to the role of Jewish activists in the events taking place. This provided ammunition to a nationalistic and antisemitic communist party faction headed by Mieczysław Moczar that was opposed to Gomułka's leadership. Using the context of the military victory of Israel in the Six-Day War of 1967, some in the Polish communist leadership waged an antisemitic campaign against the remnants of the Jewish community in Poland. The targets of this campaign were accused of disloyalty and active sympathy with Israeli aggression. Branded "Zionists", they were scapegoated and blamed for the unrest in March 1968, which eventually led to the emigration of much of Poland's remaining Jewish population (about 15,000 Polish citizens left the country).

With the active support of the Gomułka regime, the Polish People's Army took part in the infamous Warsaw Pact invasion of Czechoslovakia in August 1968, after the Brezhnev Doctrine was informally announced.

In the final major achievement of Gomułka diplomacy, the governments of Poland and West Germany signed in December 1970 the Treaty of Warsaw, which normalized their relations and made possible meaningful cooperation in a number of areas of bilateral interest. In particular, West Germany recognized the post-World War II "de facto" border between Poland and East Germany.

Price increases for essential consumer goods triggered the Polish protests of 1970. In December, there were disturbances and strikes in the Baltic Sea port cities of Gdańsk, Gdynia, and Szczecin that reflected deep dissatisfaction with living and working conditions in the country. The activity was centered in the industrial shipyard areas of the three coastal cities. Dozens of protesting workers and bystanders were killed in police and military actions, generally under the authority of Gomułka and Minister of Defense Wojciech Jaruzelski. In the aftermath, Edward Gierek replaced Gomułka as first secretary of the communist party. The new regime was seen as more modern, friendly and pragmatic, and at first it enjoyed a degree of popular and foreign support.

To revitalize the economy, from 1971 the Gierek regime introduced wide-ranging reforms that involved large-scale foreign borrowing. These actions initially caused improved conditions for consumers, but in a few years the strategy backfired and the economy deteriorated. Another attempt to raise food prices resulted in the June 1976 protests. The Workers' Defence Committee (KOR), established in response to the crackdown that followed, consisted of dissident intellectuals determined to support industrial workers, farmers and students persecuted by the authorities. The opposition circles active in the late 1970s were emboldened by the Helsinki Conference processes.

In October 1978, the Archbishop of Kraków, Cardinal Karol Józef Wojtyła, became Pope John Paul II, head of the Catholic Church. Catholics and others rejoiced at the elevation of a Pole to the papacy and greeted his June 1979 visit to Poland with an outpouring of emotion.

Fueled by large infusions of Western credit, Poland's economic growth rate was one of the world's highest during the first half of the 1970s, but much of the borrowed capital was misspent, and the centrally planned economy was unable to use the new resources effectively. The 1973 oil crisis caused recession and high interest rates in the West, to which the Polish government had to respond with sharp domestic consumer price increases. The growing debt burden became insupportable in the late 1970s, and negative economic growth set in by 1979.

Around 1 July 1980, with the Polish foreign debt standing at more than $20 billion, the government made yet another attempt to increase meat prices. Workers responded with escalating work stoppages that culminated in the 1980 general strikes in Lublin. In mid-August, labor protests at the Gdańsk Shipyard gave rise to a chain reaction of strikes that virtually paralyzed the Baltic coast by the end of the month and, for the first time, closed most coal mines in Silesia. The Inter-Enterprise Strike Committee coordinated the strike action across hundreds of workplaces and formulated the 21 demands as the basis for negotiations with the authorities. The Strike Committee was sovereign in its decision-making, but was aided by a team of "expert" advisers that included the well-known dissidents Jacek Kuroń, Karol Modzelewski, Bronisław Geremek and Tadeusz Mazowiecki.

On 31 August 1980, representatives of workers at the Gdańsk Shipyard, led by an electrician and activist Lech Wałęsa, signed the Gdańsk Agreement with the government that ended their strike. Similar agreements were concluded in Szczecin (the Szczecin Agreement) and in Silesia. The key provision of these agreements was the guarantee of the workers' right to form independent trade unions and the right to strike. Following the successful resolution of the largest labor confrontation in communist Poland's history, nationwide union organizing movements swept the country.

Edward Gierek was blamed by the Soviets for not following their "fraternal" advice, not shoring up the communist party and the official trade unions and allowing "anti-socialist" forces to emerge. On 5 September 1980, Gierek was replaced by Stanisław Kania as first secretary of the PZPR.

Delegates of the emergent worker committees from all over Poland gathered in Gdańsk on 17 September and decided to form a single national union organization named "Solidarity".

While party–controlled courts took up the contentious issues of Solidarity's legal registration as a trade union (finalized by November 10), planning had already begun for the imposition of martial law. A parallel farmers' union was organized and strongly opposed by the regime, but Rural Solidarity was eventually registered (12 May 1981). In the meantime, a rapid deterioration of the authority of the communist party, disintegration of state power and escalation of demands and threats by the various Solidarity–affiliated groups were occurring. According to Kuroń, a "tremendous social democratization movement in all spheres" was taking place and could not be contained. Wałęsa had meetings with Kania, which brought no resolution to the impasse.

Following the Warsaw Pact summit in Moscow, the Soviet Union proceeded with a massive military build-up along Poland's border in December 1980, but during the summit Kania forcefully argued with Leonid Brezhnev and other allied communists leaders against the feasibility of an external military intervention, and no action was taken. The United States, under presidents Jimmy Carter and Ronald Reagan, repeatedly warned the Soviets about the consequences of a direct intervention, while discouraging an open insurrection in Poland and signaling to the Polish opposition that there would be no rescue by the NATO forces.

In February 1981, Defense Minister General Wojciech Jaruzelski assumed the position of prime minister. The Solidarity social revolt had thus far been free of any major use of force, but in March 1981 in Bydgoszcz three activists were beaten up by the secret police. In a nationwide "warning strike" the 9.5-million-strong Solidarity union was supported by the population at large, but a general strike was called off by Wałęsa after the 30 March settlement with the government. Both Solidarity and the communist party were badly split and the Soviets were losing patience. Kania was re-elected at the Party Congress in July, but the collapse of the economy continued and so did the general disorder.

At the first Solidarity National Congress in September–October 1981 in Gdańsk, Lech Wałęsa was elected national chairman of the union with 55% of the vote. An appeal was issued to the workers of the other East European countries, urging them to follow in the footsteps of Solidarity. To the Soviets, the gathering was an "anti-socialist and anti-Soviet orgy" and the Polish communist leaders, increasingly led by Jaruzelski and General Czesław Kiszczak, were ready to apply force.

In October 1981, Jaruzelski was named first secretary of the PZPR. The Plenum's vote was 180 to 4, and he kept his government posts. Jaruzelski asked parliament to ban strikes and allow him to exercise extraordinary powers, but when neither request was granted, he decided to proceed with his plans anyway.

On 12–13 December 1981, the regime declared martial law in Poland, under which the army and ZOMO riot police were used to crush Solidarity. The Soviet leaders insisted that Jaruzelski pacify the opposition with the forces at his disposal, without direct Soviet involvement. Virtually all Solidarity leaders and many affiliated intellectuals were arrested or detained. Nine workers were killed in the Pacification of Wujek. The United States and other Western countries responded by imposing economic sanctions against Poland and the Soviet Union. Unrest in the country was subdued, but continued.

During martial law, Poland was ruled by the so-called Military Council of National Salvation. The open or semi-open opposition communications, as recently practiced, were replaced by underground publishing (known in the eastern bloc as Samizdat), and Solidarity was reduced to a few thousand underground activists.

Having achieved some semblance of stability, the Polish regime relaxed and then rescinded martial law over several stages. By December 1982 martial law was suspended and a small number of political prisoners, including Wałęsa, were released. Although martial law formally ended in July 1983 and a partial amnesty was enacted, several hundred political prisoners remained in jail. Jerzy Popiełuszko, a popular pro-Solidarity priest, was abducted and murdered by security functionaries in October 1984.

Further developments in Poland occurred concurrently with and were influenced by the reformist leadership of Mikhail Gorbachev in the Soviet Union (processes known as Glasnost and Perestroika). In September 1986, a general amnesty was declared and the government released nearly all political prisoners. However, the country lacked basic stability, as the regime's efforts to organize society from the top down had failed, while the opposition's attempts at creating an "alternate society" were also unsuccessful. With the economic crisis unresolved and societal institutions dysfunctional, both the ruling establishment and the opposition began looking for ways out of the stalemate. Facilitated by the indispensable mediation of the Catholic Church, exploratory contacts were established.

Student protests resumed in February 1988. Continuing economic decline led to strikes across the country in April, May and August. The Soviet Union, increasingly destabilized, was unwilling to apply military or other pressure to prop up allied regimes in trouble. The Polish government felt compelled to negotiate with the opposition and in September 1988 preliminary talks with Solidarity leaders ensued in Magdalenka. Numerous meetings that took place involved Wałęsa and General Kiszczak, among others. In November, the regime made a major public relations mistake by allowing a televised debate between Wałęsa and Alfred Miodowicz, chief of the All-Poland Alliance of Trade Unions, the official trade union organization. The fitful bargaining and intra-party squabbling led to the official Round Table Negotiations in 1989, followed by the Polish legislative election in June of that year, a watershed event marking the fall of communism in Poland.

The Polish Round Table Agreement of April 1989 called for local self-government, policies of job guarantees, legalization of independent trade unions and many wide-ranging reforms. The current Sejm promptly implemented the deal and agreed to National Assembly elections that were set for 4 June and 18 June. Only 35% of the seats in the Sejm (national legislature's lower house) and all of the Senate seats were freely contested; the remaining Sejm seats (65%) were guaranteed for the communists and their allies.

The failure of the communists at the polls (almost all of the contested seats were won by the opposition) resulted in a political crisis. The new April Novelization to the constitution called for re-establishment of the Polish presidency and on 19 July the National Assembly elected the communist leader, General Wojciech Jaruzelski, to that office. His election, seen at the time as politically necessary, was barely accomplished with tacit support from some Solidarity deputies, and the new president's position was not strong. Moreover, the unexpected definitiveness of the parliamentary election results created new political dynamics and attempts by the communists to form a government failed.

On 19 August, President Jaruzelski asked journalist and Solidarity activist Tadeusz Mazowiecki to form a government; on 12 September, the Sejm voted approval of Prime Minister Mazowiecki and his cabinet. Mazowiecki decided to leave the economic reform entirely in the hands of economic liberals led by the new Deputy Prime Minister Leszek Balcerowicz, who proceeded with the design and implementation of his "shock therapy" policy. For the first time in post-war history, Poland had a government led by non-communists, setting a precedent soon to be followed by other Eastern Bloc nations in a phenomenon known as the Revolutions of 1989. Mazowiecki's acceptance of the "thick line" formula meant that there would be no "witch-hunt", i.e., an absence of revenge seeking or exclusion from politics in regard to former communist officials.

In part because of the attempted indexation of wages, inflation reached 900% by the end of 1989, but was soon dealt with by means of radical methods. In December 1989, the Sejm approved the Balcerowicz Plan to transform the Polish economy rapidly from a centrally planned one to a free market economy. The Constitution of the Polish People's Republic was amended to eliminate references to the "leading role" of the communist party and the country was renamed the "Republic of Poland". The communist Polish United Workers' Party dissolved itself in January 1990. In its place, a new party, Social Democracy of the Republic of Poland, was created. "Territorial self-government", abolished in 1950, was legislated back in March 1990, to be led by locally elected officials; its fundamental unit was the administratively independent gmina.

In October 1990, the constitution was amended to curtail the term of President Jaruzelski. In November 1990, the German–Polish Border Treaty, with unified Germany, was signed.

In November 1990, Lech Wałęsa was elected president for a five-year term; in December, he became the first popularly elected president of Poland. Poland's first free parliamentary election was held in October 1991. 18 parties entered the new Sejm, but the largest representation received only 12% of the total vote.

There were several post-Solidarity governments between the 1989 election and the 1993 election, after which the "post-communist" left-wing parties took over. In 1993, the formerly Soviet Northern Group of Forces, a vestige of past domination, left Poland.

In 1995, Aleksander Kwaśniewski of the social democratic party was elected president and remained in that capacity for the next ten years (two terms).

In 1997, the new Constitution of Poland was finalized and approved in a referendum; it replaced the Small Constitution of 1992, an amended version of the communist constitution.

Poland joined NATO in 1999. Elements of the Polish Armed Forces have since participated in the Iraq War and the Afghanistan War. Poland joined the European Union as part of its enlargement in 2004. The two memberships were indicative of the Third Polish Republic's integration with the West. Poland has not adopted the euro currency, however.

"a."Piłsudski's family roots in the Polonized gentry of the Grand Duchy of Lithuania and the resulting perspective of seeing himself and people like him as legitimate Lithuanians put him in conflict with modern Lithuanian nationalists (who in Piłsudski's lifetime redefined the scope and meaning of the "Lithuanian" identity), and, by extension, with other nationalists including the Polish modern nationalist movement.

"b."In 1938, Poland and Romania refused to agree to a Franco-British proposal that in the event of war with Nazi Germany, Soviet forces would be allowed to cross their territories to aid Czechoslovakia. The Polish ruling elites considered the Soviets in some ways more threatening than the Nazis.

The Soviet Union repeatedly declared intention to fulfill its obligations under the 1935 treaty with Czechoslovakia and defend Czechoslovakia militarily. A transfer of land and air forces through Poland and/or Romania was required and the Soviets approached about it the French, who also had a treaty with Czechoslovakia (and with Poland and with the Soviet Union). Edward Rydz-Śmigły rebuked the French suggestion on that matter in 1936, and in 1938 Józef Beck pressured Romania not to allow even Soviet warplanes to fly over its territory. Like Hungary, Poland was looking into using the German-Czechoslovak conflict to settle its own territorial grievances, namely disputes over parts of Zaolzie, Spiš and Orava.

"c." In October 1939, the British Foreign Office notified the Soviets that the United Kingdom would be satisfied with a postwar creation of small ethnic Poland, patterned after the Duchy of Warsaw. An establishment of Poland restricted to "minimal size", according to ethnographic boundaries (such as the lands common to both the prewar Poland and postwar Poland), was planned by the Soviet People's Commissariat for Foreign Affairs in 1943–1944. Such territorial reduction was recommended by Ivan Maisky to Vyacheslav Molotov in early 1944, because of what Maisky saw as Poland's historically unfriendly disposition toward Russia and the Soviet Union, likely in some way to continue. Joseph Stalin opted for a larger version, allowing a "swap" (territorial compensation for Poland), which involved the eastern lands gained by Poland at the Peace of Riga of 1921 and now lost, and eastern Germany conquered from the Nazis in 1944–1945. In regard to the several major disputed areas: Lower Silesia west of the Oder and the Eastern Neisse rivers (the British wanted it to remain a part of the future German state), Stettin (in 1945 the German communists already established their administration there), "Zakerzonia" (western Red Ruthenia demanded by the Ukrainians), and the Białystok region (Białystok was claimed by the communists of the Byelorussian SSR), the Soviet leader made decisions that favored Poland.

Other territorial and ethnic scenarios were also possible, generally with possible outcomes less advantageous to Poland than the form the country assumed.

"d."Timothy D. Snyder spoke of about 100,000 Jews killed by Poles during the Nazi occupation, the majority probably by members of the collaborationist Blue Police. This number would have likely been many times higher had Poland entered into an alliance with Germany in 1939, as advocated by some Polish historians and others.

"e."Some may have falsely claimed the Jewish identity hoping for permission to emigrate. The communist authorities, pursuing the concept of Poland of single ethnicity (in accordance with the recent border changes and expulsions), were allowing the Jews to leave the country. For a discussion of early communist Poland's ethnic politics, see Timothy D. Snyder, "The Reconstruction of Nations", chapters on modern "Ukrainian Borderland".

"f."A Communist Party of Poland had existed in the past, but was eliminated in Stalin's purges in 1938.

"g."The Soviet leadership, which had previously ordered the crushing of the Uprising of 1953 in East Germany, the Hungarian Revolution of 1956 and the Prague Spring in 1968, in late 1970 became worried about potential demoralizing effects that deployment against Polish workers would have on the Polish army, a crucial Warsaw Pact component. The Soviets withdrew their support for Gomułka, who insisted on the use of force; he and his close associates were subsequently ousted from the Polish Politburo by the Polish Central Committee.

"h."East of the Molotov-Ribbentrop line, the population was 43% Polish, 33% Ukrainian, 8% Belarusian and 8% Jewish. The Soviet Union did not want to appear as an aggressor, and moved its troops to eastern Poland under the pretext of offering protection to "the kindred Ukrainian and Belorussian people".

"i."Joseph Stalin at the 1943 Tehran Conference discussed with Winston Churchill and Franklin D. Roosevelt new post-war borders in central-eastern Europe, including the shape of a future Poland. He endorsed the Piast Concept, which justified a massive shift of Poland's frontiers to the west. Stalin resolved to secure and stabilize the western reaches of the Soviet Union and disable the future military potential of Germany by constructing a compact and ethnically defined Poland (along with the Soviet ethnic Ukraine, Belarus and Lithuania) and by radically altering the region's system of national borders. After 1945, the Polish communist regime wholeheartedly adopted and promoted the Piast Concept, making it the centerpiece of their claim to be the true inheritors of Polish nationalism. After all the killings and population transfers during and after the war, the country was 99% "Polish".

"j.""All the currently available documents of Nazi administration show that, together with the Jews, the stratum of the Polish intelligentsia was marked for total extermination. In fact, Nazi Germany achieved this goal almost by half, since Poland lost 50 percent of her citizens with university diplomas and 35 percent of those with a gimnazium diploma." According to Brzoza and Sowa, 450,000 of Polish citizens had completed higher, secondary, or trade school education by the outbreak of the war. 37.5% of people with higher education perished, 30% of those with general secondary education, and 53.3% of trade school graduates.

"k."Decisive political events took place in Poland shortly before the Hungarian Revolution of 1956. Władysław Gomułka, a reformist party leader, was reinstated to the Politburo of the PZPR and the Eighth Plenum of its Central Committee was announced to convene on 19 October 1956, all without seeking a Soviet approval. The Soviet Union responded with military moves and intimidation and its "military-political delegation", led by Nikita Khrushchev, quickly arrived in Warsaw. Gomułka tried to convince them of his loyalty but insisted on the reforms that he considered essential, including a replacement of Poland's Soviet-trusted minister of defense, Konstantin Rokossovsky. The disconcerted Soviets returned to Moscow, the PZPR Plenum elected Gomułka first secretary and removed Rokossovsky from the Politburo. On 21 October, the Soviet Presidium followed Khrushchev's lead and decided unanimously to "refrain from military intervention" in Poland, a decision likely influenced also by the ongoing preparations for the invasion of Hungary. The Soviet gamble paid off, because Gomułka in the coming years turned out to be a very dependable Soviet ally and an orthodox communist.

However, unlike the other Warsaw Pact countries, Poland did not endorse the Soviet armed intervention in Hungary. The Hungarian Revolution was intensely supported by the Polish public.

"l."The delayed reinforcements were coming and the government military commanders General Tadeusz Rozwadowski and Władysław Anders wanted to keep on fighting the coup perpetrators, but President Stanisław Wojciechowski and the government decided to surrender to prevent the imminent spread of civil war. The coup brought to power the "Sanation" regime under Józef Piłsudski (Edward Rydz-Śmigły after Piłsudski's death). The Sanation regime persecuted the opposition within the military and in general. Rozwadowski died after abusive imprisonment, according to some accounts murdered. Another major opponent of Piłsudski, General Włodzimierz Zagórski, disappeared in 1927. According to Aleksandra Piłsudska, the marshal's wife, following the coup and for the rest of his life Piłsudski lost his composure and appeared over-burdened.

At the time of Rydz-Śmigły's command, the Sanation camp embraced the ideology of Roman Dmowski, Piłsudski's nemesis. Rydz-Śmigły did not allow General Władysław Sikorski, an enemy of the Sanation movement, to participate as a soldier in the country's defense against the Invasion of Poland in September 1939. During World War II in France and then in Britain, the Polish government-in-exile became dominated by anti-Sanation politicians. The perceived Sanation followers were in turn persecuted (in exile) under prime ministers Sikorski and Stanisław Mikołajczyk.

"m."General Zygmunt Berling of the Soviet-allied First Polish Army attempted in mid-September a crossing of the Vistula and landing at Czerniaków to aid the insurgents, but the operation was defeated by the Germans and the Poles suffered heavy losses.

"n."The decision to launch the Warsaw Uprising resulted in the destruction of the city, its population and its elites and has been a source of lasting controversy. According to the historians Czesław Brzoza and Andrzej Leon Sowa, orders of further military offensives, issued at the end of August 1944 as a continuation of Operation Tempest, show a loss of the sense of responsibility for the country's fate on the part of the underground Polish leadership.

"o."One of the party leaders Mieczysław Rakowski, who abandoned his mentor Gomułka following the 1970 crisis, saw the demands of the demonstrating workers as "exclusively socialist" in character, because of the way they were phrased. Most people in communist Poland, including opposition activists, did not question the supremacy of socialism or the socialist idea; misconduct by party officials, such as not following the provisions of the constitution, was blamed. From the time of Gierek, this assumed standard of political correctness was increasingly challenged: pluralism, and then free market, became frequently used concepts.

"p."The Polish Sanation authorities were provoked by the independence-seeking Organization of Ukrainian Nationalists (OUN). OUN engaged in political assassinations, terror and sabotage, to which the Polish state responded with a repressive campaign in the 1930s, as Józef Piłsudski and his successors imposed collective responsibility on the villagers in the affected areas. After the disturbances of 1933 and 1934, the Bereza Kartuska prison camp was established; it became notorious for its brutal regime. The government brought Polish settlers and administrators to parts of Volhynia with a centuries-old tradition of Ukrainian peasant rising against Polish land owners (and to Eastern Galicia). In the late 1930s, after Piłsudski's death, military persecution intensified and a policy of "national assimilation" was aggressively pursued. Military raids, public beatings, property confiscations and the closing and destruction of Orthodox churches aroused lasting enmity in Galicia and antagonized Ukrainian society in Volhynia at the worst possible moment, according to Timothy D. Snyder. However, he also notes that "Ukrainian terrorism and Polish reprisals touched only part of the population, leaving vast regions unaffected" and "the OUN's nationalist prescription, a Ukrainian state for ethnic Ukrainians alone was far from popular". Halik Kochanski wrote of the legacy of bitterness between the Ukrainians and Poles that soon exploded in the context of World War II. See also: History of the Ukrainian minority in Poland.

"q."In Poland, officials of central government (the provincial office of "wojewoda") can overrule elected territorial and municipal local governments. However, in such cases "wojewoda" decisions have sometimes been invalidated by courts.

"r."Foreign policy was one of the few governmental areas in which Piłsudski took an active interest. He saw Poland's role and opportunity as lying in Eastern Europe and advocated passive relations with the West. He felt that a German attack should not be feared, because even if this unlikely event were to take place, the Western powers would be bound to restrain Germany and come to Poland's rescue.

"s."According to the researcher Jan Sowa, the Commonwealth failed as a state because it was not able to conform to the emerging new European order established at the Peace of Westphalia of 1648. Poland's elective kings, restricted by the self-serving and short-sighted nobility, could not impose a strong and efficient central government with its characteristic post-Westphalian internal and external sovereignty. The inability of Polish kings to levy and collect taxes (and therefore sustain a standing army) and conduct independent foreign policy were among the chief obstacles to Poland competing effectively on the changed European scene, where absolutist power was a prerequisite for survival and became the foundation for the abolition of serfdom and gradual formation of parliamentarism.

"t."Besides the Home Army there were other major underground fighting formations: Bataliony Chłopskie, National Armed Forces (NSZ) and Gwardia Ludowa (later Armia Ludowa). From 1943, the leaders of the nationalistic NSZ collaborated with Nazi Germany in a case unique in occupied Poland. The NSZ conducted an anti-communist civil war. Before the arrival of the Soviets, the NSZ's Holy Cross Mountains Brigade left Poland under the protection of the German army. According to the historians Czesław Brzoza and Andrzej Leon Sowa, participation figures given for the underground resistance are often inflated. In the spring of 1944, the time of the most extensive involvement of the underground organizations, there were most likely considerably fewer than 500,000 military and civilian personnel participating, over the entire spectrum, from the right wing to the communists.

"u."According to Jerzy Eisler, about 1.1 million people may have been imprisoned or detained in 1944–1956 and about 50,000 may have died because of the struggle and persecution, including about 7,000 soldiers of the right-wing underground killed in the 1940s. According to Adam Leszczyński, up to 30,000 people were killed by the communist regime during the first several years after the war.

"v."According to Andrzej Stelmachowski, one of the key participants of the Polish systemic transformation, Minister Leszek Balcerowicz pursued extremely liberal economic policies, often extraordinarily painful for society. The December 1989 Sejm statute of credit relations reform introduced an "incredible" system of privileges for banks, which were allowed to unilaterally alter interest rates on already existing contracts. The exceedingly high rates they instantly introduced ruined many previously profitable enterprises and caused a complete breakdown of the apartment block construction industry, which had long-term deleterious effects on the state budget as well. Balcerowicz's policies also caused permanent damage to Polish agriculture, an area in which he lacked expertise, and to the often successful and useful Polish cooperative movement.

According to Karol Modzelewski, a dissident and critic of the economic transformation, in 1989 Solidarity no longer existed, having been in reality eliminated during the martial law period. What the "post-Solidarity elites" did in 1989 amounted to a betrayal of the old Solidarity base, and the retribution was only a matter of time.

"w."Led by Władysław Anders, the Polish II Corps fought in 1944–1945 in the Allied Italian Campaign, where the corps' main engagement was the Battle of Monte Cassino.

"x."The Piast Concept, of which the chief proponent was Jan Ludwik Popławski (late 19th century), was based on the claim that the Piast homeland was inhabited by so-called "native" aboriginal Slavs and Slavonic Poles since time immemorial and only later was "infiltrated" by "alien" Celts, Germanic peoples, and others. After 1945, the so-called "autochthonous" or "aboriginal" school of Polish prehistory received official backing and a considerable degree of popular support in Poland. According to this view, the Lusatian Culture, which flourished between the Oder and the Vistula in the early Iron Age, was said to be Slavonic; all non-Slavonic tribes and peoples recorded in the area at various points in ancient times were dismissed as "migrants" and "visitors". In contrast, the critics of this theory, such as Marija Gimbutas, regarded it as an unproved hypotheses and for them the date and origin of the westward migration of the Slavs were largely uncharted; the Slavonic connections of the Lusatian Culture were entirely imaginary; and the presence of an ethnically mixed and constantly changing collection of peoples on the North European Plain was taken for granted.

"y."According to the count presented by Prime Minister and Internal Affairs Minister Felicjan Sławoj Składkowski before the Sejm committee in January 1938, 818 people were killed in police suppression of labor protests (industrial and agricultural) during the 1932–1937 period.

"z."John II Casimir Vasa is known for his remarkable and accurate prediction of the Partitions of Poland, made over a century before the event's occurrence.

"a1."According to war historian Ben Macintyre, "The Polish contribution to allied victory in the Second World War was extraordinary, perhaps even decisive, but for many years it was disgracefully played down, obscured by the politics of the Cold War."

"b1."Piłsudski left the Polish Socialist Party in 1914 and severed his connections with the socialist movement, but many activists from the Left and of other political orientations presumed his continuing involvement there.

"c1."Woodrow Wilson's Fourteen Points program was subsequently weakened by internal developments in the US, Britain, France, and Germany. In the last case, Poland was denied the city of Danzig on the Baltic coast.

"d1."The government of Soviet Russia issued in August 1918 a decree strongly supportive of the independence of Poland, but at that time no Polish lands were under Russian control.

More recent general history of Poland books in English

Published in Poland





</doc>
<doc id="13773" url="https://en.wikipedia.org/wiki?curid=13773" title="Hradčany">
Hradčany

Hradčany (; ), the Castle District, is the district of the city of Prague, Czech Republic surrounding Prague Castle.

The castle is one of the biggest castle in the world at about in length and an average of about wide. Its history stretches back to the 9th century. St Vitus Cathedral is located in the castle area.

Most of the district consists of noble historical . There are many other attractions for visitors: romantic nooks, peaceful places and beautiful lookouts.

Hradčany was an independent borough until 1784, when the four independent boroughs that had formerly constituted Prague were proclaimed a single city. The other three were Malá Strana (, Lesser Quarter), Staré Město (, Old Town) and Nové Město (, New Town).



</doc>
