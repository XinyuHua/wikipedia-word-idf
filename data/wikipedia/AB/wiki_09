<doc id="13774" url="https://en.wikipedia.org/wiki?curid=13774" title="Houston">
Houston

Houston ( ) is the most populous city in the U.S. state of Texas and the fourth most populous city in the United States, with a census-estimated population of 2.312 million in 2017. It is the most populous city in the Southern United States and on the Gulf Coast of the United States. Located in Southeast Texas near Galveston Bay and the Gulf of Mexico, it is the seat of Harris County and the principal city of the Greater Houston metropolitan area, which is the fifth most populous metropolitan statistical area (MSA) in the United States and the second most populous in Texas after the Dallas-Fort Worth MSA. With a total area of , Houston is the eighth most expansive city in the United States (including consolidated city-counties). It is the largest city in the United States by total area, whose government is similarly not consolidated with that of a county or borough. Though primarily in Harris County, small portions of the city extend into Fort Bend and Montgomery counties.

Houston was founded by land speculators on August 30, 1836, at the confluence of Buffalo Bayou and White Oak Bayou (a point now known as Allen's Landing) and incorporated as a city on June 5, 1837. The city is named after former General Sam Houston, who was president of the Republic of Texas and had won Texas' independence from Mexico at the Battle of San Jacinto east of Allen's Landing. After briefly serving as the capital of the Texas Republic in the late 1830s, Houston grew steadily into a regional trading center for the remainder of the 19th century.

The arrival of the 20th century saw a convergence of economic factors which fueled rapid growth in Houston, including a burgeoning port and railroad industry, the decline of Galveston as Texas' primary port following a devastating 1900 hurricane, the subsequent construction of the Houston Ship Channel, and the Texas oil boom. In the mid-20th century, Houston's economy diversified as it became home to the Texas Medical Center—the world's largest concentration of healthcare and research institutions—and NASA's Johnson Space Center, where the Mission Control Center is located.

Houston's economy has a broad industrial base in energy, manufacturing, aeronautics, and transportation. Leading in healthcare sectors and building oilfield equipment, Houston has the second most Fortune 500 headquarters of any U.S. municipality within its city limits (after New York City). The Port of Houston ranks first in the United States in international waterborne tonnage handled and second in total cargo tonnage handled. Nicknamed the "Space City", Houston is a global city, with strengths in culture, medicine, and research. The city has a population from various ethnic and religious backgrounds and a large and growing international community. Houston is the most diverse metropolitan area in Texas and has been described as the most racially and ethnically diverse major metropolis in the U.S. It is home to many cultural institutions and exhibits, which attract more than 7 million visitors a year to the Museum District. Houston has an active visual and performing arts scene in the Theater District and offers year-round resident companies in all major performing arts.

The Allen brothers—Augustus Chapman and John Kirby—explored town sites on Buffalo Bayou and Galveston Bay. According to historian David McComb, "[T]he brothers, on August 26, 1836, bought from Elizabeth E. Parrott, wife of T.F.L. Parrott and widow of John Austin, the south half of the lower league [ tract] granted to her by her late husband. They paid $5,000 total, but only $1,000 of this in cash; notes made up the remainder."

The Allen brothers ran their first advertisement for Houston just four days later in the "Telegraph and Texas Register", naming the notional town in honor of President Sam Houston. They successfully lobbied the Republic of Texas Congress to designate Houston as the temporary capital, agreeing to provide the new government with a capital building. About a dozen persons resided in the town at the beginning of 1837, but that number grew to about 1,500 by the time the Texas Congress convened in Houston for the first time that May. Houston was granted incorporation on June 5, 1837, with James S. Holman becoming its first mayor. In the same year, Houston became the county seat of Harrisburg County (now Harris County).

In 1839, the Republic of Texas relocated its capital to Austin. The town suffered another setback that year when a yellow fever epidemic claimed about one life out of every eight residents. Yet it persisted as a commercial center, forming a symbiosis with its Gulf Coast port, Galveston. Landlocked farmers brought their produce to Houston, using Buffalo Bayou to gain access to Galveston and the Gulf of Mexico. Houston merchants profited from selling staples to farmers and shipping the farmers' produce to Galveston.

The great majority of slaves in Texas came with their owners from the older slave states. Sizable numbers, however, came through the domestic slave trade. New Orleans was the center of this trade in the Deep South, but slave dealers were in Houston. Thousands of enslaved blacks lived near the city before the American Civil War. Many of them near the city worked on sugar and cotton plantations, while most of those in the city limits had domestic and artisan jobs.

In 1840, the community established a chamber of commerce in part to promote shipping and navigation at the newly created port on Buffalo Bayou.

By 1860, Houston had emerged as a commercial and railroad hub for the export of cotton. Railroad spurs from the Texas inland converged in Houston, where they met rail lines to the ports of Galveston and Beaumont. During the American Civil War, Houston served as a headquarters for General John Magruder, who used the city as an organization point for the Battle of Galveston. After the Civil War, Houston businessmen initiated efforts to widen the city's extensive system of bayous so the city could accept more commerce between Downtown and the nearby port of Galveston. By 1890, Houston was the railroad center of Texas.

In 1900, after Galveston was struck by a devastating hurricane, efforts to make Houston into a viable deep-water port were accelerated. The following year, the discovery of oil at the Spindletop oil field near Beaumont prompted the development of the Texas petroleum industry. In 1902, President Theodore Roosevelt approved a $1 million improvement project for the Houston Ship Channel. By 1910, the city's population had reached 78,800, almost doubling from a decade before. African Americans formed a large part of the city's population, numbering 23,929 people, which was nearly one-third of Houston's residents.

President Woodrow Wilson opened the deep-water Port of Houston in 1914, seven years after digging began. By 1930, Houston had become Texas' most populous city and Harris County the most populous county. In 1940, the U.S. Census Bureau reported Houston's population as 77.5% white and 22.4% black.

When World War II started, tonnage levels at the port decreased and shipping activities were suspended; however, the war did provide economic benefits for the city. Petrochemical refineries and manufacturing plants were constructed along the ship channel because of the demand for petroleum and synthetic rubber products by the defense industry during the war. Ellington Field, initially built during World War I, was revitalized as an advanced training center for bombardiers and navigators. The Brown Shipbuilding Company was founded in 1942 to build ships for the U.S. Navy during World War II. Due to the boom in defense jobs, thousands of new workers migrated to the city, both blacks and whites competing for the higher-paying jobs. President Roosevelt had established a policy of nondiscrimination for defense contractors, and blacks gained some opportunities, especially in shipbuilding, although not without resistance from whites and increasing social tensions that erupted into occasional violence. Economic gains of blacks who entered defense industries continued in the postwar years.

In 1945, the M.D. Anderson Foundation formed the Texas Medical Center. After the war, Houston's economy reverted to being primarily port-driven. In 1948, the city annexed several unincorporated areas, more than doubling its size. Houston proper began to spread across the region.

In 1950, the availability of air conditioning provided impetus for many companies to relocate to Houston, where wages were lower than those in the North; this resulted in an economic boom and produced a key shift in the city's economy toward the energy sector.

The increased production of the expanded shipbuilding industry during World War II spurred Houston's growth, as did the establishment in 1961 of NASA's "Manned Spacecraft Center" (renamed the Lyndon B. Johnson Space Center in 1973). This was the stimulus for the development of the city's aerospace industry. The Astrodome, nicknamed the "Eighth Wonder of the World", opened in 1965 as the world's first indoor domed sports stadium.

During the late 1970s, Houston had a population boom as people from the Rust Belt states moved to Texas in large numbers. The new residents came for numerous employment opportunities in the petroleum industry, created as a result of the Arab oil embargo. With the increase in professional jobs, Houston has become a destination for many college-educated persons, most recently including African Americans in a reverse Great Migration from northern areas.

In 1997, Houstonians elected Lee P. Brown as the city's first African American mayor.

In June 2001, Tropical Storm Allison dumped up to of rain on parts of Houston, causing what was then the worst flooding in the city's history. The storm cost billions of dollars in damage and killed 20 people in Texas. By December of the same year, Houston-based energy company Enron collapsed into the largest U.S. bankruptcy (at that time), a result of being investigated for off-the-books partnerships which were allegedly used to hide debt and inflate profits. The company lost no less than $70 billion.

In August 2005, Houston became a shelter to more than 150,000 people from New Orleans, who evacuated from Hurricane Katrina. One month later, about 2.5 million Houston-area residents evacuated when Hurricane Rita approached the Gulf Coast, leaving little damage to the Houston area. This was the largest urban evacuation in the history of the United States. In September 2008, Houston was hit by Hurricane Ike. As many as 40% of residents refused to leave Galveston Island because they feared the type of traffic problems that had happened after Hurricane Rita.

During its recent history, Houston has flooded several times from heavy rainfall, which has been becoming increasingly common. This has been exacerbated by a lack of zoning laws, which allowed unregulated building of residential homes and other structures in flood-prone areas. During the floods in 2015 and 2016, each of which dropped at least a foot of rain, parts of the city were covered in several inches of water. Even worse flooding happened in late August 2017, when Hurricane Harvey stalled over southeastern Texas, much like Tropical Storm Allison did sixteen years earlier, causing severe flooding in the Houston area, with some areas receiving over of rain. The rainfall exceeded 50 inches in several areas locally, breaking the national record for rainfall. The damage for the Houston area is estimated at up to $125 billion U.S. dollars, and it is considered to be one of the worst natural disasters in the history of the United States, with the death toll exceeding 70 people. On January 31, 2018, the Houston City Council agreed to forgive large water bills thousands of households faced in the aftermath of Hurricane Harvey, as Houston Public Works found 6,362 homeowners' water utility bills had at least doubled.

Houston is located east of Austin, west of the Louisiana border, and south of Dallas. The city has a total area of ; this comprises of land and covered by water. The Piney Woods are north of Houston. Most of Houston is located on the gulf coastal plain, and its vegetation is classified as temperate grassland and forest. Much of the city was built on forested land, marshes, swamp, or prairie and are all still visible in surrounding areas. Flat terrain and extensive greenfield development have combined to worsen flooding. Downtown stands about above sea level, and the highest point in far northwest Houston is about in elevation. The city once relied on groundwater for its needs, but land subsidence forced the city to turn to ground-level water sources such as Lake Houston, Lake Conroe, and Lake Livingston. The city owns surface water rights for 1.20 billion gallons of water a day in addition to 150 million gallons a day of groundwater.

Houston has four major bayous passing through the city that accept water from the extensive drainage system. Buffalo Bayou runs through Downtown and the Houston Ship Channel, and has three tributaries: White Oak Bayou, which runs through the Houston Heights community northwest of Downtown and then towards Downtown; Brays Bayou, which runs along the Texas Medical Center; and Sims Bayou, which runs through the south of Houston and Downtown Houston. The ship channel continues past Galveston and then into the Gulf of Mexico.

Houston is a flat marshy area where an extensive drainage system has been built. The adjoining prairie land drains into the city which is prone to flooding. Underpinning Houston's land surface are unconsolidated clays, clay shales, and poorly cemented sands up to several miles deep. The region's geology developed from river deposits formed from the erosion of the Rocky Mountains. These sediments consist of a series of sands and clays deposited on decaying organic marine matter, that over time, transformed into oil and natural gas. Beneath the layers of sediment is a water-deposited layer of halite, a rock salt. The porous layers were compressed over time and forced upward. As it pushed upward, the salt dragged surrounding sediments into salt dome formations, often trapping oil and gas that seeped from the surrounding porous sands. The thick, rich, sometimes black, surface soil is suitable for rice farming in suburban outskirts where the city continues to grow.

The Houston area has over 150 active faults (estimated to be 300 active faults) with an aggregate length of up to , including the Long Point–Eureka Heights fault system which runs through the center of the city. No significant historically recorded earthquakes have occurred in Houston, but researchers do not discount the possibility of such quakes having occurred in the deeper past, nor occurring in the future. Land in some areas southeast of Houston is sinking because water has been pumped out of the ground for many years. It may be associated with slip along the faults; however, the slippage is slow and not considered an earthquake, where stationary faults must slip suddenly enough to create seismic waves. These faults also tend to move at a smooth rate in what is termed "fault creep", which further reduces the risk of an earthquake.

Houston's climate is classified as humid subtropical ("Cfa" in the Köppen climate classification system), typical of the Southern United States. The city experiences hot, long, and humid summers, and mild winters. While not located in Tornado Alley, like much of northern Texas, spring supercell thunderstorms sometimes bring tornadoes to the area.

Prevailing winds are from the south and southeast during most of the year, which bring heat and moisture from the nearby Gulf of Mexico and Galveston Bay.

During the summer, temperatures in Houston commonly reach over . The city reaches or surpasses this temperature on an average of 106.5 days per year, including a majority of days from June to September; additionally, an average of 4.6 days per year reach or exceed . Houston's characteristic subtropical humidity often results in a higher apparent temperature, and summer mornings average over 90% relative humidity. Air conditioning is ubiquitous in Houston; in 1981, annual spending on electricity for interior cooling exceeded $600 million (equivalent to $ billion in ), and by the late 1990s, approximately 90% of Houston homes featured air conditioning systems. The record highest temperature recorded in Houston is at Bush Intercontinental Airport, during September 4, 2000, and again on August 28, 2011.

Houston has mild winters. In January, the normal mean temperature at George Bush Intercontinental Airport is , with an average of 13 days per year with a low at or below , occurring on average between December 3 and February 20, allowing for a growing season of 286 days. Twenty-first century snow events in Houston include a storm on December 24, 2004, which saw of snow accumulate in parts of the metro area, and an event on December 7, 2017, which precipitated of snowfall. Snowfalls of at least on both December 10, 2008, and December 4, 2009, marked the first time measurable snowfall had occurred in two consecutive years in the city's recorded history. Overall, Houston has seen measurable snowfall 38 times between 1895 and 2018. On February 14 and 15, 1895, Houston received of snow, its largest snowfall from one storm on record. The coldest temperature officially recorded in Houston was on January 18, 1930.

Houston generally receives ample rainfall, averaging about annually based on records between 1981 and 2010. Many parts of the city have a high risk of localized flooding due to flat topography, ubiquitous low-permeability clay-silt prairie soils, and inadequate infrastructure. During the mid-2010s, Greater Houston experienced consecutive major flood events in 2015 ("Memorial Day"), 2016 ("Tax Day"), and 2017 (Hurricane Harvey). Overall, there have been more casualties and property loss from floods in Houston than in any other locality in the United States.

Houston has excessive ozone levels and is routinely ranked among the most ozone-polluted cities in the United States. Ground-level ozone, or smog, is Houston's predominant air pollution problem, with the American Lung Association rating the metropolitan area's ozone level twelfth on the "Most Polluted Cities by Ozone" in 2017, after major cities such as Los Angeles, Phoenix, New York City and Denver. The industries located along the ship channel are a major cause of the city's air pollution. The rankings are in terms of peak-based standards, focusing strictly on the worst days of the year; the average ozone levels in Houston are lower than what is seen in most other areas of the country, as dominant winds ensure clean, marine air from the Gulf.

Houston was incorporated in 1837 and adopted a ward system of representation shortly afterward in 1840. The six original wards of Houston are the progenitors of the 11 modern-day geographically-oriented Houston City Council districts, though the city abandoned the ward system in 1905 in favor of a commission government, and, later, the existing mayor–council government.

Locations in Houston are generally classified as either being inside or outside the Interstate 610 loop. The "Inner Loop" encompasses a area which includes Downtown, pre–World War II residential neighborhoods and streetcar suburbs, and newer high-density apartment and townhouse developments. Outside the loop, the city's typology is more suburban, though many major business districts—such as Uptown, Westchase, and the Energy Corridor—lie well outside the urban core. In addition to Interstate 610, two additional loop highways encircle the city: Beltway 8, with a radius of approximately from Downtown, and State Highway 99 (the Grand Parkway), with a radius of . Approximately 470,000 people live within the Interstate 610 loop, while 1.65 million live between Interstate 610 and Beltway 8 and 2.25 million live within Harris County outside Beltway 8.

Though Houston is the largest city in the United States without formal zoning regulations, it has developed similarly to other Sun Belt cities because the city's land use regulations and legal covenants have played a similar role. Regulations include mandatory lot size for single-family houses and requirements that parking be available to tenants and customers. Such restrictions have had mixed results. Though some have blamed the city's low density, urban sprawl, and lack of pedestrian-friendliness on these policies, the city's land use has also been credited with having significant affordable housing, sparing Houston the worst effects of the 2008 real estate crisis. The city issued 42,697 building permits in 2008 and was ranked first in the list of healthiest housing markets for 2009.

Voters rejected efforts to have separate residential and commercial land-use districts in 1948, 1962, and 1993. Consequently, rather than a single central business district as the center of the city's employment, multiple districts have grown throughout the city in addition to Downtown, which include Uptown, the Texas Medical Center, Midtown, Greenway Plaza, Memorial City, the Energy Corridor, Westchase, and Greenspoint.

Houston has the fifth-tallest skyline in North America (after New York City, Chicago, Toronto and Miami) and 36th-tallest in the world . A seven-mile (11 km) system of tunnels and skywalks links Downtown buildings containing shops and restaurants, enabling pedestrians to avoid summer heat and rain while walking between buildings.

In the 1960s, Downtown Houston consisted of a collection of midrise office structures. Downtown was on the threshold of an energy industryled boom in 1970. A succession of skyscrapers was built throughout the 1970s—many by real estate developer Gerald D. Hines—culminating with Houston's tallest skyscraper, the 75-floor, -tall JPMorgan Chase Tower (formerly the Texas Commerce Tower), completed in 1982. It is the tallest structure in Texas, 15th tallest building in the United States, and the 85th-tallest skyscraper in the world, based on highest architectural feature. In 1983, the 71-floor, -tall Wells Fargo Plaza (formerly Allied Bank Plaza) was completed, becoming the second-tallest building in Houston and Texas. Based on highest architectural feature, it is the 17th-tallest in the United States and the 95th-tallest in the world. In 2007, Downtown had over 43 million square feet (4,000,000 m²) of office space.

Centered on Post Oak Boulevard and Westheimer Road, the Uptown District boomed during the 1970s and early 1980s when a collection of midrise office buildings, hotels, and retail developments appeared along Interstate 610 West. Uptown became one of the most prominent instances of an edge city. The tallest building in Uptown is the 64-floor, -tall, Philip Johnson and John Burgee designed landmark Williams Tower (known as the Transco Tower until 1999). At the time of construction, it was believed to be the world's tallest skyscraper outside a central business district. The new 20-story Skanska building and BBVA Compass Plaza are the newest office buildings built in Uptown after 30 years. The Uptown District is also home to buildings designed by noted architects I. M. Pei, César Pelli, and Philip Johnson. In the late 1990s and early 2000s, a mini-boom of midrise and highrise residential tower construction occurred, with several over 30 stories tall. Since 2000 over 30 skyscrapers grown up in Houston; all told, 72 high-rises tower over the city, which adds up to about 8,300 units. In 2002, Uptown had more than 23 million square feet (2,100,000 m²) of office space with 16 million square feet (1,500,000 m²) of class A office space.

The Rice University Kinder Institute for Urban Research, a think tank, has described Greater Houston as "one of the most ethnically and culturally diverse metropolitan areas in the country". A 2012 Kinder Institute report found that, based on the evenness of population distribution between the four major racial groups in the United States (non-Hispanic white, non-Hispanic black, Hispanic, and Asian), Greater Houston was the most ethnically diverse metropolitan area in the United States, ahead of New York City. In 2017, non-Hispanic whites made up 38% of the population of the Houston metropolitan area, Hispanics 36%, African-Americans 25%, and Asians 9%.

Houston's multiculturalism, fueled by large waves of immigrants, has been attributed to its relatively low cost of living, strong job market, proximity to Latin America, and role as a hub for refugee resettlement. At least 145 languages are spoken by city residents. Greater Houston is one of the youngest metropolitan areas in the nation, with an estimated average age of 33.5 in 2014, compared with the national average of 37.4; the city's youthfulness has been attributed to an influx of Hispanic and Asian immigrants into Texas. , an estimated 600,000 undocumented immigrants reside in the Houston area, comprising nearly 9% of the metropolitan population.

Compared with its metropolitan area, the city of Houston's population has a higher proportion of minorities. According to the 2010 Census, whites made up 51% of the city of Houston's population; 26% of the total population was non-Hispanic whites. Blacks or African Americans made up 25% of Houston's population, American Indians made up 0.7% of the population, Asians made up 6% (1.7% Vietnamese, 1.3% Chinese, 1.3% Indian, 0.9% Pakistani, 0.4% Filipino, 0.3% Korean, 0.1% Japanese) and Pacific Islanders made up 0.1%. Individuals from some other race made up 15.2% of the city's population, of which 0.2% were non-Hispanic. Individuals from two or more races made up 3.3% of the city.

At the 2000 Census, 1,953,631 people inhabited the city, and the population density was 3,371.7 people per square mile (1,301.8/km²). The racial makeup of the city in 2000 was 49.3% White, 25.3% African American, 6.3% Asian, 0.7% American Indian, 0.1% Pacific Islander, 16.5% from some other race, and 3.1% from two or more races. In addition, Hispanics made up 37.4% of Houston's population in 2000, while non-Hispanic whites made up 30.8%. The proportion of non-Hispanic whites in Houston has decreased significantly since 1970, when it was 62.4%.

The median income for a household in the city was $37,000, and for a family was $40,000. Males had a median income of $32,000 versus $27,000 for females. The per capita income was $20,000. About 19% of the population and 16% of families were below the poverty line. Of the total population, 26% of those under the age of 18 and 14% of those 65 and older were living below the poverty line.

Historically, Houston has been a center of Protestant Christianity, being part of the Bible Belt. Other Christian groups including Eastern and Oriental Orthodox Christianity, and non-Christian religions did not grow for much of the city's history because immigration was predominantly from Western Europe (which at the time was dominated by Western Christianity and favored by the quotas in federal immigration law). The Immigration and Nationality Act of 1965 removed the quotas, allowing for the growth of other religions.

According to a 2014 study by the Pew Research Center, 73% of the population of the Houston area identified themselves as Christians, about 50% of whom claimed Protestant affiliations and about 19% claimed Roman Catholic affiliations. Nationwide, about 71% of respondents identified as Christians. About 20% of Houston-area residents claimed no religious affiliation, compared to about 23% nationwide. The same study says that area residents identifying with other religions (including Judaism, Buddhism, Islam, and Hinduism) collectively make up about 7% of the area population.

Lakewood Church in Houston, led by Pastor Joel Osteen, is the largest church in the United States. A megachurch, it had 44,800 weekly attendees in 2010, up from 11,000 weekly in 2000. Since 2005 it has occupied the former Compaq Center sports stadium. In September 2010, "Outreach Magazine" published a list of the 100 largest Christian churches in the United States, and inside the list were the following Houston-area churches: Lakewood, Second Baptist Church Houston, Woodlands Church, Church Without Walls and First Baptist Church. According to the list, Houston and Dallas were tied as the second most popular city for megachurches.

The Roman Catholic Archdiocese of Galveston-Houston, the largest Catholic jurisdiction in Texas and fifth-largest in the United States, was established in 1847. The Archdiocese of Galveston-Houston claims approximately 1.7 million Catholics within its boundaries.

A variety of Eastern and Oriental Orthodox churches can be found in Houston. Immigrants from Eastern Europe, the Middle East, Ethiopia, India and other areas have added to Houston's Eastern and Oriental Orthodox population. As of 2011 in the entire State of Texas there were 32,000 people who actively attend Orthodox churches. In 2013 Father John Whiteford, the pastor of St. Jonah Orthodox Church near Spring, stated that there were about 6,000-9,000 Eastern Orthodox Christians in Houston.

Houston's Jewish community, estimated at 47,000 in 2001, has been present in the city from the 1800s. Houstonian Jews have origins from throughout the United States, Israel, Mexico, Russia, and other places. As of 2016 there were over 40 synagogues in Greater Houston. The largest synagogues in Houston are Congregation Beth Yeshurun, a Conservative Jewish temple, and the Reform Jewish congregations Beth Israel and Emanu-El.

Houston is home to the largest Muslim population in Texas and the Southern United States as of 2012. It is estimated that Muslims make up 1.2% of Houston's population. As of 2016 Muslims in the Houston area included South Asians, Middle Easterners, Africans, Turks, and Indonesians. In 2000 there were over 41 mosques and storefront religious centers, with the largest being the "Al-Noor" Mosque (Mosque of Light) of the Islamic Society of Greater Houston.

Houston is recognized worldwide for its energy industry—particularly for oil and natural gas—as well as for biomedical research and aeronautics. Renewable energy sources—wind and solar—are also growing economic bases in the city.
The Houston Ship Channel is also a large part of Houston's economic base. Because of these strengths, Houston is designated as a global city by the Globalization and World Cities Study Group and Network and global management consulting firm A.T. Kearney. The Houston area is the top U.S. market for exports, surpassing New York City in 2013, according to data released by the U.S. Department of Commerce's International Trade Administration. In 2012, the Houston–The Woodlands–Sugar Land area recorded $110.3 billion in merchandise exports. Petroleum products, chemicals, and oil and gas extraction equipment accounted for roughly two-thirds of the metropolitan area's exports last year. The top three destinations for exports were Mexico, Canada, and Brazil.

The Houston area is a leading center for building oilfield equipment. Much of its success as a petrochemical complex is due to its busy ship channel, the Port of Houston. In the United States, the port ranks first in international commerce and 10th among the largest ports in the world. Unlike most places, high oil and gasoline prices are beneficial for Houston's economy, as many of its residents are employed in the energy industry. Houston is the beginning or end point of numerous oil, gas, and products pipelines.

The Houston–The Woodlands–Sugar Land MSA's gross domestic product (GDP) in 2016 was $478 billion, making it the sixth-largest of any metropolitan area in the United States and larger than Iran's, Colombia's, or the United Arab Emirates' GDP. Only 27 countries other than the United States have a gross domestic product exceeding Houston's regional gross area product (GAP). In 2010, mining (which consists almost entirely of exploration and production of oil and gas in Houston) accounted for 26.3% of Houston's GAP up sharply in response to high energy prices and a decreased worldwide surplus of oil production capacity, followed by engineering services, health services, and manufacturing.

The University of Houston System's annual impact on the Houston area's economy equates to that of a major corporation: $1.1 billion in new funds attracted annually to the Houston area, $3.13 billion in total economic benefit, and 24,000 local jobs generated. This is in addition to the 12,500 new graduates the U.H. System produces every year who enter the workforce in Houston and throughout the state of Texas. These degree-holders tend to stay in Houston. After five years, 80.5% of graduates are still living and working in the region.

In 2006, the Houston metropolitan area ranked first in Texas and third in the U.S. within the category of "Best Places for Business and Careers" by "Forbes" magazine. Ninety-one foreign governments have established consular offices in Houston's metropolitan area, the third-highest in the nation. Forty foreign governments maintain trade and commercial offices here with 23 active foreign chambers of commerce and trade associations. Twenty-five foreign banks representing 13 nations operate in Houston, providing financial assistance to the international community.

In 2008, Houston received top ranking on "Kiplinger's Personal Finance" "Best Cities of 2008" list, which ranks cities on their local economy, employment opportunities, reasonable living costs, and quality of life. The city ranked fourth for highest increase in the local technological innovation over the preceding 15 years, according to "Forbes" magazine. In the same year, the city ranked second on the annual "Fortune" 500 list of company headquarters, first for "Forbes" magazine's "Best Cities for College Graduates", and first on their list of "Best Cities to Buy a Home". In 2010, the city was rated the best city for shopping, according to "Forbes".

In 2012, the city was ranked number one for paycheck worth by "Forbes" and in late May 2013, Houston was identified as America's top city for employment creation.

In 2013, Houston was identified as the number one U.S. city for job creation by the U.S. Bureau of Statistics after it was not only the first major city to regain all the jobs lost in the preceding economic downturn, but also after the crash, more than two jobs were added for every one lost. Economist and vice president of research at the Greater Houston Partnership Patrick Jankowski attributed Houston's success to the ability of the region's real estate and energy industries to learn from historical mistakes. Furthermore, Jankowski stated that "more than 100 foreign-owned companies relocated, expanded or started new businesses in Houston" between 2008 and 2010, and this openness to external business boosted job creation during a period when domestic demand was problematically low. Also in 2013, Houston again appeared on "Forbes"' list of "Best Places for Business and Careers".

Located in the American South, Houston is a diverse city with a large and growing international community. The Houston metropolitan area is home to an estimated 1.1 million (21.4 percent) residents who were born outside the United States, with nearly two-thirds of the area's foreign-born population from south of the United States–Mexico border. Additionally, more than one in five foreign-born residents are from Asia. The city is home to the nation's third-largest concentration of consular offices, representing 92 countries.

Many annual events celebrate the diverse cultures of Houston. The largest and longest-running is the annual Houston Livestock Show and Rodeo, held over 20 days from early to late March, and is the largest annual livestock show and rodeo in the world. Another large celebration is the annual night-time Houston Gay Pride Parade, held at the end of June. Other notable annual events include the Houston Greek Festival, Art Car Parade, the Houston Auto Show, the Houston International Festival, and the Bayou City Art Festival, which is considered to be one of the top five art festivals in the United States.

Houston received the official nickname of "Space City" in 1967 because it is the location of NASA's Lyndon B. Johnson Space Center. Other nicknames often used by locals include "Bayou City", "Clutch City", "Crush City", "Magnolia City", and "H-Town".
The Houston Theater District, located in Downtown, is home to nine major performing arts organizations and six performance halls. It is the second-largest concentration of theater seats in a downtown area in the United States.

Houston is one of few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Houston Grand Opera), ballet (Houston Ballet), music (Houston Symphony Orchestra), and theater (The Alley Theatre, Theatre Under the Stars). Houston is also home to folk artists, art groups and various small progressive arts organizations.

Houston attracts many touring Broadway acts, concerts, shows, and exhibitions for a variety of interests. Facilities in the Theater District include the Jones Hall—home of the Houston Symphony Orchestra and Society for the Performing Arts—and the Hobby Center for the Performing Arts.

The Museum District's cultural institutions and exhibits attract more than 7 million visitors a year. Notable facilities include The Museum of Fine Arts, Houston Museum of Natural Science, the Contemporary Arts Museum Houston, the Station Museum of Contemporary Art, Holocaust Museum Houston, and the Houston Zoo.

Located near the Museum District are The Menil Collection, Rothko Chapel, and the Byzantine Fresco Chapel Museum.

Bayou Bend is a facility of the Museum of Fine Arts that houses one of America's most prominent collections of decorative art, paintings, and furniture. Bayou Bend is the former home of Houston philanthropist Ima Hogg.

The National Museum of Funeral History is located in Houston near the George Bush Intercontinental Airport. The museum houses the original Popemobile used by Pope John Paul II in the 1980s along with numerous hearses, embalming displays, and information on famous funerals.

Venues across Houston regularly host local and touring rock, blues, country, dubstep, and Tejano musical acts. While Houston has never been widely known for its music scene, Houston hip-hop has become a significant, independent music scene that is influential nationwide.

The Theater District is a 17-block area in the center of Downtown Houston that is home to the Bayou Place entertainment complex, restaurants, movies, plazas, and parks. Bayou Place is a large multilevel building containing full-service restaurants, bars, live music, billiards, and Sundance Cinema. The Bayou Music Center stages live concerts, stage plays, and stand-up comedy.
Space Center Houston is the official visitors' center of NASA's Lyndon B. Johnson Space Center. The Space Center has many interactive exhibits including moon rocks, a shuttle simulator, and presentations about the history of NASA's manned space flight program. Other tourist attractions include the Galleria (Texas' largest shopping mall, located in the Uptown District), Old Market Square, the Downtown Aquarium, and Sam Houston Race Park.
Houston's current Chinatown and the Mahatma Gandhi District are two major ethnic enclaves, reflecting Houston's multicultural makeup. Restaurants, bakeries, traditional-clothing boutiques, and specialty shops can be found in both areas.

Houston is home to 337 parks, including Hermann Park, Terry Hershey Park, Lake Houston Park, Memorial Park, Tranquility Park, Sesquicentennial Park, Discovery Green, Buffalo Bayou Park and Sam Houston Park. Within Hermann Park are the Houston Zoo and the Houston Museum of Natural Science. Sam Houston Park contains restored and reconstructed homes which were originally built between 1823 and 1905. A proposal has been made to open the city's first botanic garden at Herman Brown Park.

Of the 10 most populous U.S. cities, Houston has the most total area of parks and green space, . The city also has over 200 additional green spaces—totaling over that are managed by the city—including the Houston Arboretum and Nature Center. The Lee and Joe Jamail Skatepark is a public skatepark owned and operated by the city of Houston, and is one of the largest skateparks in Texas consisting of a 30,000-ft (2,800 m)in-ground facility.

The Gerald D. Hines Waterwall Park—located in the Uptown District of the city—serves as a popular tourist attraction and for weddings and various celebrations. A 2011 study by Walk Score ranked Houston the 23rd most walkable of the 50 largest cities in the United States. The Bayport Cruise Terminal on the Houston Ship Channel is port of call for both Princess Cruises and Norwegian Cruise Line.

Houston has sports teams for every major professional league except the National Hockey League. The Houston Astros are a Major League Baseball expansion team formed in 1962 (known as the "Colt .45s" until 1965) that won the World Series in 2017 and previously appeared in 2005. It is the only MLB team to have won pennants in both modern leagues. The Houston Rockets are a National Basketball Association franchise based in the city since 1971. They have won two NBA Championships: in 1994 and 1995 under star players Hakeem Olajuwon, Otis Thorpe, Clyde Drexler, Vernon Maxwell, and Kenny Smith. The Houston Texans are a National Football League expansion team formed in 2002. The Houston Dynamo is a Major League Soccer franchise that has been based in Houston since 2006, winning two MLS Cup titles in 2006 and 2007. The Houston Dash team plays in the National Women's Soccer League. The Houston SaberCats are a Rugby team that plays in the Major League Rugby. Minute Maid Park (home of the Astros) and Toyota Center (home of the Rockets), are located in Downtown Houston. Houston has the NFL's first retractable-roof stadium with natural grass, NRG Stadium (home of the Texans). Minute Maid Park is also a retractable-roof stadium. Toyota Center also has the largest screen for an indoor arena in the United States built to coincide with the arena's hosting of the 2013 NBA All-Star Game. BBVA Compass Stadium is a soccer-specific stadium for the Houston Dynamo, the Texas Southern Tigers football team, and Houston Dash, located in East Downtown. Aveva Stadium (home of the SaberCats) is located in south Houston. In addition, NRG Astrodome was the first indoor stadium in the world, built in 1965. Other sports facilities include Hofheinz Pavilion (Houston Cougars basketball), Rice Stadium (Rice Owls football), and NRG Arena. TDECU Stadium is where the University of Houston Houston Cougars football team plays.

Houston has hosted several major sports events: the 1968, 1986 and 2004 Major League Baseball All-Star Games; the 1989, 2006 and 2013 NBA All-Star Games; Super Bowl VIII, Super Bowl XXXVIII, and Super Bowl LI, as well as hosting the 1981, 1986, 1994 and 1995 NBA Finals, winning the latter two, and co-hosting the 2005 World Series and 2017 World Series, winning the latter. NRG Stadium hosted Super Bowl LI on February 5, 2017.

The city has hosted several major professional and college sporting events, including the annual Houston Open golf tournament. Houston hosts the annual Houston College Classic baseball tournament every February, the Texas Kickoff and Bowl in September and December, respectively.

The Grand Prix of Houston, an annual auto race on the IndyCar Series circuit is held on a 1.7-mile temporary street circuit in NRG Park. The October 2013 event was held using a tweaked version of the 2006–2007 course. The event has a 5-year race contract through 2017 with IndyCar. In motorcycling, the Astrodome hosted an AMA Supercross Championship round from 1974 to 2003 and the NRG Stadium since 2003.

The city of Houston has a strong mayoral form of municipal government. Houston is a home rule city and all municipal elections in the state of Texas are nonpartisan. The city's elected officials are the mayor, city controller and 16 members of the Houston City Council. The current mayor of Houston is Sylvester Turner, a Democrat elected on a nonpartisan ballot. Houston's mayor serves as the city's chief administrator, executive officer, and official representative, and is responsible for the general management of the city and for seeing that all laws and ordinances are enforced.

The original city council line-up of 14 members (nine district-based and five at-large positions) was based on a U.S. Justice Department mandate which took effect in 1979. At-large council members represent the entire city.Under the city charter, once the population in the city limits exceeded 2.1 million residents, two additional districts were to be added. The city of Houston's official 2010 census count was 600 shy of the required number; however, as the city was expected to grow beyond 2.1 million shortly thereafter, the two additional districts were added for, and the positions filled during, the August 2011 elections.

The city controller is elected independently of the mayor and council. The controller's duties are to certify available funds prior to committing such funds and processing disbursements. The city's fiscal year begins on July 1 and ends on June 30. Chris Brown is the city controller, serving his first term .

As the result of a 2015 referendum in Houston, a mayor is elected for a four-year term, and can be elected to as many as two consecutive terms. The term limits were spearheaded in 1991 by conservative political activist Clymer Wright. During 1991–2015, the city controller and city council members were subjected to a two-year, three-term limitation – the 2015 referendum amended term limits to two four-year terms. some councilmembers who served two terms and won a final term will have served eight years in office, whereas a freshman councilmember who won a position in 2013 can serve up to two additional terms under the previous term limit law – a select few will have at least 10 years of incumbency once their term expires.

Houston is considered to be a politically divided city whose balance of power often sways between Republicans and Democrats. Much of the city's wealthier areas vote Republican while the city's working class and minority areas vote Democratic. According to the 2005 Houston Area Survey, 68 percent of non-Hispanic whites in Harris County are declared or favor Republicans while 89 percent of non-Hispanic blacks in the area are declared or favor Democrats. About 62 percent of Hispanics (of any race) in the area are declared or favor Democrats. The city has often been known to be the most politically diverse city in Texas, a state known for being generally conservative. As a result, the city is often a contested area in statewide elections. In 2009, Houston became the first US city with a population over 1 million citizens to elect a gay mayor, by electing Annise Parker.

Houston had 303 homicides in 2015 and 302 homicides in 2016. Officials predicted there would be 323 homicides in 2016. Instead, there was no increase in Houston's homicide rate between 2015 and 2016.

Houston's murder rate ranked 46th of U.S. cities with a population over 250,000 in 2005 (per capita rate of 16.3 murders per 100,000 population). In 2010, the city's murder rate (per capita rate of 11.8 murders per 100,000 population) was ranked sixth among U.S. cities with a population of over 750,000 (behind New York City, Chicago, Detroit, Dallas, and Philadelphia) according to the Federal Bureau of Investigation (FBI).

Murders fell by 37 percent from January to June 2011, compared with the same period in 2010. Houston's total crime rate including violent and nonviolent crimes decreased by 11 percent. The FBI's Uniform Crime Report (UCR) indicates a downward trend of violent crime in Houston over the ten- and twenty-year periods ending in 2016, which is consistent with national trends. This trend toward lower rates of violent crime in Houston includes the murder rate, though it had seen a four-year uptick that lasted through 2015. Houston's violent crime rate was 8.6% percent higher in 2016 from the previous year. However, from 2006 to 2016, violent crime was still down 12 percent in Houston.

Houston is a significant hub for trafficking of cocaine, cannabis, heroin, MDMA, and methamphetamine due to its size and proximity to major illegal drug exporting nations. Houston is one of the country's largest hubs for human trafficking.

In the early 1970s, Houston, Pasadena and several coastal towns were the site of the Houston mass murders, which at the time were the deadliest case of serial killing in American history.

Seventeen school districts exist within the city of Houston. The "Houston Independent School District" (HISD) is the seventh-largest school district in the United States and the largest in Texas. HISD has 112 campuses that serve as magnet or vanguard schools—specializing in such disciplines as health professions, visual and performing arts, and the sciences. There are also many charter schools that are run separately from school districts. In addition, some public school districts also have their own charter schools.

The Houston area encompasses more than 300 private schools, many of which are accredited by Texas Private School Accreditation Commission recognized agencies. The Houston Area independent schools offer education from a variety of different religious as well as secular viewpoints. The Houston area Catholic schools are operated by the Archdiocese of Galveston-Houston.

Four distinct state universities are located in Houston. The University of Houston (UH) is a nationally recognized and is the flagship institution of the University of Houston System. The university in Texas, the University of Houston has nearly 44,000 students on its campus in the Third Ward. The University of Houston–Clear Lake and the University of Houston–Downtown are universities within the University of Houston System; they are not branch campuses of the University of Houston. Slightly west of the University of Houston is Texas Southern University (TSU), one of the largest and most comprehensive historically black universities in the United States with approximately 10,000 students. Texas Southern University was the first state university in Houston, founded in 1927.

Several private institutions of higher learning are located within the city. Rice University, the most selective university in Texas and one of the most selective in the United States, is a private, secular institution with a high level of research activity. Founded in 1912, Rice's historic, heavily wooded campus, located adjacent to Hermann Park and the Texas Medical Center, hosts approximately 4,000 undergraduate and 3,000 post-graduate students. To the north in Neartown, the University of St. Thomas, founded in 1947, is Houston's only Catholic university. St. Thomas provides a liberal arts curriculum for roughly 3,000 students at its historic 19-block campus along Montrose Boulevard. In southwest Houston, Houston Baptist University (HBU), founded in 1960, offers bachelor's and graduate degrees at its Sharpstown campus. The school is affiliated with the Baptist General Convention of Texas and has a student population of approximately 3,000.

Three community college districts have campuses in and around Houston. The Houston Community College System (HCC) serves most of Houston proper; its main campus and headquarters are located in Midtown. Suburban northern and western parts of the metropolitan area are served by various campuses of the Lone Star College System, while the southeastern portion of Houston is served by San Jacinto College, and a northeastern portion is served by Lee College. The Houston Community College and Lone Star College systems are among the 10 largest institutions of higher learning in the United States.

Houston also hosts a number of graduate schools in law and healthcare. The University of Houston Law Center and Thurgood Marshall School of Law at Texas Southern University are public, ABA-accredited law schools, while the South Texas College of Law, located in Downtown, serves as a private, independent alternative. The Texas Medical Center is home to a high density of health professions schools, including two medical schools: McGovern Medical School, part of The University of Texas Health Science Center at Houston, and Baylor College of Medicine, a highly selective private institution. Prairie View A&M University's nursing school is located in the Texas Medical Center. Additionally, both Texas Southern University and the University of Houston have pharmacy schools, and the University of Houston hosts a college of optometry.

The primary network-affiliated television stations are KPRC-TV (NBC), KHOU (CBS), KTRK-TV (ABC), KRIV (Fox), KIAH (The CW), and KTXH (MyNetworkTV). KTRK-TV, KRIV and KTXH operate as owned-and-operated stations of their networks.

The Houston–The Woodlands–Sugar Land metropolitan area is served by one public television station and one public radio station. KUHT ("Houston Public Media") is a PBS member station and is the first public television station in the United States. Houston Public Radio is listener-funded and comprises one NPR member station, KUHF ("News 88.7"). The University of Houston System owns and holds broadcasting licenses to KUHT and KUHF. The stations broadcast from the Melcher Center for Public Broadcasting, located on the campus of the University of Houston.

Houston is served by the "Houston Chronicle", its only major daily newspaper with wide distribution. The Hearst Corporation, which owns and operates the "Houston Chronicle", bought the assets of the "Houston Post"—its long-time rival and main competition—when "Houston Post" ceased operations in 1995. The "Houston Post" was owned by the family of former Lieutenant Governor Bill Hobby of Houston. The only other major publication to serve the city is the "Houston Press"—which was a free alternative weekly newspaper before the destruction caused by Hurricane Harvey resulted in the publication switching to an online-only format on November 2, 2017.

Houston is the seat of the Texas Medical Center, which describes itself as containing the world's largest concentration of research and healthcare institutions. All 49 member institutions of the Texas Medical Center are non-profit organizations. They provide patient and preventive care, research, education, and local, national, and international community well-being. Employing more than 73,600 people, institutions at the medical center include 13 hospitals and two specialty institutions, two medical schools, four nursing schools, and schools of dentistry, public health, pharmacy, and virtually all health-related careers. It is where one of the first—and still the largest—air emergency service, Life Flight, was created, and an inter-institutional transplant program was developed. Around 2007, more heart surgeries were performed at the Texas Medical Center than anywhere else in the world.

Some of the academic and research health institutions at the center include MD Anderson Cancer Center, Baylor College of Medicine, UT Health Science Center, Memorial Hermann Hospital, Houston Methodist Hospital, Texas Children's Hospital, and University of Houston College of Pharmacy.

In the 2000s, the Baylor College of Medicine was annually considered within the top ten medical schools in the nation; likewise, the MD Anderson Cancer Center had been consistently ranked as one of the top two U.S. hospitals specializing in cancer care by "U.S. News & World Report" since 1990. The Menninger Clinic, a psychiatric treatment center, is affiliated with Baylor College of Medicine and the Houston Methodist Hospital System. With hospital locations nationwide and headquarters in Houston, the Triumph Healthcare hospital system was the third largest long term acute care provider nationally in 2005.

Houston is considered an automobile-dependent city, with an estimated 77.2% of commuters driving alone to work in 2016, up from 71.7% in 1990 and 75.6% in 2009. In 2016, another 11.4% of Houstonians carpooled to work, while 3.6% used public transit, 2.1% walked, and 0.5% bicycled. A commuting study estimated that the median length of commute in the region was in 2012. According to the 2013 American Community Survey, the average work commute in Houston (city) takes 26.3 minutes. A 1999 Murdoch University study found that Houston had both the lengthiest commute and lowest urban density of 13 large American cities surveyed, and a 2017 Arcadis study ranked Houston 22nd out of 23 American cities in transportation sustainability. Harris County is one of the largest consumers of gasoline in the United States, ranking second (behind Los Angeles County) in 2013.

Despite the region's high rate of automobile usage, attitudes towards transportation among Houstonians indicate a growing preference for walkability. A 2017 study by the Rice University Kinder Institute for Urban Research found that 56% of Harris County residents have a preference for dense housing in a mixed-use, walkable setting as opposed to single-family housing in a low-density area. A plurality of survey respondents also indicated that traffic congestion was the most significant problem facing the metropolitan area. In addition, many households in the City of Houston have no car. In 2015, 8.3 percent of Houston households lacked a car, which was virtually unchanged in 2016 (8.1 percent). The national average was 8.7 percent in 2016. Houston averaged 1.59 cars per household in 2016, compared to a national average of 1.8.

The eight-county Greater Houston metropolitan area contains over of roadway, of which 10%, or approximately , is limited-access highway. The Houston region's extensive freeway system handles over 40% of the regional daily vehicle miles traveled (VMT). Arterial roads handle an additional 40% of daily VMT, while toll roads, of which Greater Houston has , handle nearly 10%.

Greater Houston possesses a hub-and-spoke limited-access highway system, in which a number of freeways radiate outward from Downtown, with ring roads providing connections between these radial highways at intermediate distances from the city center. The city is crossed by three Interstate highways, Interstate 10, Interstate 45, and Interstate 69 (commonly known as U.S. Route 59), as well as a number of other United States routes and state highways. Major freeways in Greater Houston are often referred to by either the cardinal direction or geographic location they travel towards. Highways that follow the cardinal convention include U.S. Route 290 ("Northwest Freeway"), Interstate 45 north of Downtown ("North Freeway"), Interstate 10 east of Downtown "(East Freeway"), Texas State Highway 288 ("South" "Freeway"), and Interstate 69 south of Downtown ("Southwest Freeway"). Highways that follow the location convention include Interstate 10 west of Downtown ("Katy Freeway"), Interstate 69 north of Downtown ("Eastex Freeway"), Interstate 45 south of Downtown ("Gulf Freeway"), and Texas State Highway 225 ("La Porte" or "Pasadena Freeway").

Three loop freeways provide north-south and east-west connectivity between Greater Houston's radial highways. The innermost loop is Interstate 610, commonly known as the "Inner Loop", which encircles Downtown, the Texas Medical Center, Greenway Plaza, the cities of West University Place and Southside Place, and many core neighborhoods. The State Highway Beltway 8, often referred to as "the Beltway", forms the middle loop at a radius of roughly . A third, loop with a radius of approximately , State Highway 99 (the "Grand Parkway"), is currently under construction, with six of eleven segments completed . Completed segments D through G provide a continuous limited-access tollway connection between Sugar Land, Katy, Cypress, Spring, and Porter.

A system of toll roads, operated by the Harris County Toll Road Authority (HCTRA) and Fort Bend County Toll Road Authority (FBCTRA), provides additional options for regional commuters. The Sam Houston Tollway, which encompasses the mainlanes of Beltway 8 (as opposed to the frontage roads, which are untolled), is the longest tollway in the system, covering the entirety of the Beltway with the exception of a free section between Interstate 45 and Interstate 69 near George Bush Intercontinental Airport. The region is serviced by four spoke tollways: a set of managed lanes on the Katy Freeway; the Hardy Toll Road, which parallels Interstate 45 north of Downtown up to Spring; the Westpark Tollway, which services Houston's western suburbs out to Fulshear; and Fort Bend Parkway, which connects to Sienna Plantation. Westpark Tollway and Fort Bend Parkway are operated conjunctly with the Fort Bend County Toll Road Authority.

Greater Houston's freeway system is monitored by Houston TranStar, a partnership of four government agencies which is responsible for providing transportation and emergency management services to the region.

Greater Houston's arterial road network is established at the municipal level, with the City of Houston exercising planning control over both its incorporated area and extraterritorial jurisdiction (ETJ). Therefore, Houston exercises transportation planning authority over a area over five counties, many times larger than its corporate area. The "Major Thoroughfare and Freeway Plan", updated annually, establishes the city's street hierarchy, identifies roadways in need of widening, and proposes new roadways in unserved areas. Arterial roads are organized into four categories, in decreasing order of intensity: "major thoroughfares", "transit corridor streets", "collector streets", and "local streets". Roadway classification affects anticipated traffic volumes, roadway design, and right of way breadth. Ultimately, the system is designed to ferry traffic from neighborhood streets to major thoroughfares, which connect into the limited-access highway system. Notable arterial roads in the region include Westheimer Road, Memorial Drive, Texas State Highway 6, Farm to Market Road 1960, Bellaire Boulevard, and Telephone Road.

The Metropolitan Transit Authority of Harris County (METRO) provides public transportation in the form of buses, light rail, high-occupancy vehicle (HOV) lanes, and paratransit to fifteen municipalities throughout the Greater Houston area and parts of unincorporated Harris County. METRO's service area covers containing a population of 3.6 million.

METRO's local bus network services approximately 275,000 riders daily with a fleet of over 1,200 buses. The agency's 75 local routes contain nearly 8,900 stops and saw nearly 67 million boardings during the 2016 fiscal year. A park and ride system provides commuter bus service from 34 transit centers scattered throughout the region's suburban areas; these express buses operate independently of the local bus network and utilize the region's extensive system of HOV lanes. Downtown and the Texas Medical Center have the highest rates of transit use in the region, largely due to the park and ride system, with nearly 60% of commuters in each district utilizing public transit to get to work.

METRO began light rail service in 2004 with the opening of the north-south Red Line connecting Downtown, Midtown, the Museum District, the Texas Medical Center, and NRG Park. In the early 2010s, two additional lines—the Green Line, servicing the East End, and the Purple Line, servicing the Third Ward—opened, and the Red Line was extended northward to Northline, bringing the total length of the system to . Two light rail lines outlined in a five-line system approved by voters in a 2003 referendum have yet to be constructed. The Uptown Line, which would run along Post Oak Boulevard in Uptown, is currently under construction as a bus rapid transit line—the city's first—while the University Line has been postponed indefinitely. The light rail system saw approximately 16.8 million boardings in fiscal year 2016.

Amtrak, the national passenger rail system, provides service three times a week to Houston via the (Los Angeles–New Orleans), which stops at a station northwest of Downtown. The station saw 14,891 boardings and alightings in fiscal year 2008. In 2012, there was a 25 percent increase in ridership to 20,327 passengers embarking from the Houston Amtrak Station.

Houston City Council approved the Houston Bike Plan in March 2017, at that time entering the plan into the Houston Code of Ordinances.

Houston has the largest number of bike commuters in Texas with over 160 miles of dedicated bikeways. The city is currently in the process of expanding its on and off street bikeway network. In 2015, Downtown Houston added a cycle track on Lamar Street, running from Sam Houston Park to Discovery Green. In August 2017, Houston City Council approved spending for construction of 13 additional miles of bike trails.

Houston's bicycle sharing system started service with nineteen stations in May 2012. Houston Bcycle (also known as B-Cycle), a local non-profit, runs the subscription program, supplying bicycles and docking stations, while partnering with other companies to maintain the system. The network expanded to 29 stations and 225 bicycles in 2014, registering over 43,000 checkouts of equipment during the first half of the same year. In 2017, Bcycle logged over 142,000 check outs while expanding to 56 docking stations.

The Houston Airport System, a branch of the municipal government, oversees the operation of three major public airports in the city. Two of these airports, George Bush Intercontinental Airport and William P. Hobby Airport, offer commercial aviation service to a variety of domestic and international destinations and served 55 million passengers in 2016. The third, Ellington Airport, is home to the Ellington Field Joint Reserve Base. The Federal Aviation Administration and the state of Texas selected the Houston Airport System as "Airport of the Year" in 2005, largely due to the implementation of a $3.1 billion airport improvement program for both major airports in Houston.

George Bush Intercontinental Airport (IAH), located north of Downtown Houston between Interstates 45 and 69, is the eighth busiest commercial airport in the United States (by total passengers and aircraft movements) and forty-third busiest globally. The five-terminal, five-runway, airport served 40 million passengers in 2016, including 10 million international travelers. In 2006, the United States Department of Transportation named IAH the fastest-growing of the top ten airports in the United States. The Houston Air Route Traffic Control Center is located at Bush Intercontinental.

Houston was the headquarters of Continental Airlines until its 2010 merger with United Airlines with headquarters in Chicago; regulatory approval for the merger was granted in October of that year. Bush Intercontinental is currently United Airlines' second largest hub, behind O'Hare International Airport. United Airlines' share of the Houston Airport System's commercial aviation market was nearly 60% in 2017 with 16 million enplaned passengers. In early 2007, Bush Intercontinental Airport was named a model "port of entry" for international travelers by U.S. Customs and Border Protection.

William P. Hobby Airport (HOU), known as Houston International Airport until 1967, operates primarily short- to medium-haul domestic and international flights to 60 destinations. The four-runway, facility is located approximately southeast of Downtown Houston. In 2015, Southwest Airlines launched service from a new international terminal at Hobby to several destinations in Mexico, Central America, and the Caribbean. These were the first international flights flown from Hobby since the opening of Bush Intercontinental in 1969. Houston's aviation history is showcased in the 1940 Air Terminal Museum, located in the old terminal building on the west side of the airport. In 2009, Hobby Airport was recognized with two awards for being one of the top five performing airports globally and for customer service by Airports Council International.

Houston's third municipal airport is Ellington Airport, used by the military, government (including NASA) and general aviation sectors.

The Houston Office of Protocol and International Affairs is the city's liaison to Houston's sister cities and to the national governing organization, Sister Cities International. Through their official city-to-city relationships, these volunteer associations promote people-to-people diplomacy and encourage citizens to develop mutual trust and understanding through commercial, cultural, educational, and humanitarian exchanges.



</doc>
<doc id="13776" url="https://en.wikipedia.org/wiki?curid=13776" title="Head (disambiguation)">
Head (disambiguation)

The head (Human head) is the part of an animal or human that usually includes the brain, eyes, ears, nose, and mouth.

Head may also refer to:















</doc>
<doc id="13777" url="https://en.wikipedia.org/wiki?curid=13777" title="Hard disk drive">
Hard disk drive

A hard disk drive (HDD), hard disk, hard drive, or fixed disk, is an electromechanical data storage device that uses magnetic storage to store and retrieve digital information using one or more rigid rapidly rotating disks (platters) coated with magnetic material. The platters are paired with magnetic heads, usually arranged on a moving actuator arm, which read and write data to the platter surfaces. Data is accessed in a random-access manner, meaning that individual blocks of data can be stored or retrieved in any order and not only sequentially. HDDs are a type of non-volatile storage, retaining stored data even when powered off.

Introduced by IBM in 1956, HDDs became the dominant secondary storage device for general-purpose computers by the early 1960s. Continuously improved, HDDs have maintained this position into the modern era of servers and personal computers. More than 200 companies have produced HDDs historically, though after extensive industry consolidation most units are manufactured by Seagate, Toshiba, and Western Digital. HDDs dominate the volume of storage produced (exabytes per year) for servers. Though production is growing slowly, sales revenues and unit shipments are declining because solid-state drives (SSDs) have higher data-transfer rates, higher areal storage density, better reliability, and much lower latency and access times. 

The revenues for SSDs, most of which use NAND, slightly exceed those for HDDs. Though SSDs have nearly 10 times higher cost per bit, they are replacing HDDs in applications where speed, power consumption, small size, and durability are important.

The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of : a 1-terabyte (TB) drive has a capacity of gigabytes (GB; where 1 gigabyte = bytes). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Also there is confusion regarding storage capacity, since capacities are stated in decimal Gigabytes (powers of 10) by HDD manufacturers, whereas some operating systems report capacities in binary Gibibytes, which results in a smaller number than advertised. Performance is specified by the time required to move the heads to a track or cylinder (average access time) adding the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).

The two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as PATA (Parallel ATA), SATA (Serial ATA), USB or SAS (Serial Attached SCSI) cables.

The first production IBM hard disk drive, the 350 disk storage, shipped in 1957 as a component of the IBM 305 RAMAC system. It was approximately the size of two medium-sized refrigerators and stored five million six-bit characters (3.75 megabytes) on a stack of 50 disks.

In 1962, the IBM 350 was superseded by the IBM 1301 disk storage unit, which consisted of 50 platters, each about -inch thick and 24 inches in diameter. While the IBM 350 used only two read/write heads, the 1301 used an array of heads, one per platter, moving as a single unit. Cylinder-mode read/write operations were supported, and the heads flew about 250 micro-inches (about 6 µm) above the platter surface. Motion of the head array depended upon a binary adder system of hydraulic actuators which assured repeatable positioning. The 1301 cabinet was about the size of three home refrigerators placed side by side, storing the equivalent of about 21 million eight-bit bytes. Access time was about a quarter of a second.

Also in 1962, IBM introduced the model 1311 disk drive, which was about the size of a washing machine and stored two million characters on a removable disk pack. Users could buy additional packs and interchange them as needed, much like reels of magnetic tape. Later models of removable pack drives, from IBM and others, became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s. Non-removable HDDs were called "fixed disk" drives.

Some high-performance HDDs were manufactured with one head per track (e.g. IBM 2305 in 1970) so that no time was lost physically moving the heads to a track. Known as fixed-head or head-per-track disk drives they were very expensive and are no longer in production.

In 1973, IBM introduced a new type of HDD code-named "Winchester". Its primary distinguishing feature was that the disk heads were not withdrawn completely from the stack of disk platters when the drive was powered down. Instead, the heads were allowed to "land" on a special area of the disk surface upon spin-down, "taking off" again when the disk was later powered on. This greatly reduced the cost of the head actuator mechanism, but precluded removing just the disks from the drive as was done with the disk packs of the day. Instead, the first models of "Winchester technology" drives featured a removable disk module, which included both the disk pack and the head assembly, leaving the actuator motor in the drive upon removal. Later "Winchester" drives abandoned the removable media concept and returned to non-removable platters.

Like the first removable pack drive, the first "Winchester" drives used platters in diameter. A few years later, designers were exploring the possibility that physically smaller platters might offer advantages. Drives with non-removable eight-inch platters appeared, and then drives that used a form factor (a mounting width equivalent to that used by contemporary floppy disk drives). The latter were primarily intended for the then-fledgling personal computer (PC) market.

As the 1980s began, HDDs were a rare and very expensive additional feature in PCs, but by the late 1980s their cost had been reduced to the point where they were standard on all but the cheapest computers.

Most HDDs in the early 1980s were sold to PC end users as an external, add-on subsystem. The subsystem was not sold under the drive manufacturer's name but under the subsystem manufacturer's name such as Corvus Systems and Tallgrass Technologies, or under the PC system manufacturer's name such as the Apple ProFile. The IBM PC/XT in 1983 included an internal 10 MB HDD, and soon thereafter internal HDDs proliferated on personal computers.

External HDDs remained popular for much longer on the Apple Macintosh. Many Macintosh computers made between 1986 and 1998 featured a SCSI port on the back, making external expansion simple. Older compact Macintosh computers did not have user-accessible hard drive bays (indeed, the Macintosh 128K, Macintosh 512K, and Macintosh Plus did not feature a hard drive bay at all), so on those models external SCSI disks were the only reasonable option for expanding upon any internal storage.

Driven by ever increasing areal density, HDDs have continuously improved; a few highlights are listed in the table above. Market applications expanded through the 2000s, from the mainframe computers of the late 1950s to most mass storage applications including computers and consumer applications such as storage of entertainment content. 

NAND performance is improving faster than HDDs, and applications for HDDs are eroding. In 2018, the largest hard drive had a capacity of 15TB while the largest capacity SSD had a capacity of 30.72TB or 100TB and HDDs are not expected to reach 100TB capacities until somewhere around 2025. Smaller form factors, 1.8-inches and below, were discontinued around 2010. The price of solid-state storage (NAND), represented by Moore's law, is improving faster than HDDs. NAND has a higher price elasticity of demand than HDDs, and this drives market growth. During the late 2000s and 2010s, the product life cycle of HDDs entered a mature phase, and slowing sales may indicate the onset of the declining phase.
Relatively new technologies like HDMR, HAMR and MAMR, Bit patterned media and dual independent actuator arms increase the speed and capacity of HDDs and are expected to make HDDs more competitive with SSDs.

The 2011 Thailand floods damaged the manufacturing plants and impacted hard disk drive cost adversely between 2011 and 2013.

A modern HDD records data by magnetizing a thin film of ferromagnetic material on a disk. Sequential changes in the direction of magnetization represent binary data bits. The data is read from the disk by detecting the transitions in magnetization. User data is encoded using an encoding scheme, such as run-length limited encoding, which determines how the data is represented by the magnetic transitions.

A typical HDD design consists of a "" that holds flat circular disks, also called platters, which hold the recorded data. The platters are made from a non-magnetic material, usually aluminum alloy, glass, or ceramic. They are coated with a shallow layer of magnetic material typically 10–20 nm in depth, with an outer layer of carbon for protection. For reference, a standard piece of copy paper is thick. 

The platters in contemporary HDDs are spun at speeds varying from 4,200 rpm in energy-efficient portable devices, to 15,000 rpm for high-performance servers. The first HDDs spun at 1,200 rpm and, for many years, 3,600 rpm was the norm. As of December 2013, the platters in most consumer-grade HDDs spin at either 5,400 rpm or 7,200 rpm.

Information is written to and read from a platter as it rotates past devices called read-and-write heads that are positioned to operate very close to the magnetic surface, with their flying height often in the range of tens of nanometers. The read-and-write head is used to detect and modify the magnetization of the material passing immediately under it.

In modern drives, there is one head for each magnetic platter surface on the spindle, mounted on a common arm. An actuator arm (or access arm) moves the heads on an arc (roughly radially) across the platters as they spin, allowing each head to access almost the entire surface of the platter as it spins. The arm is moved using a voice coil actuator or in some older designs a stepper motor. Early hard disk drives wrote data at some constant bits per second, resulting in all tracks having the same amount of data per track but modern drives (since the 1990s) use zone bit recording – increasing the write speed from inner to outer zone and thereby storing more data per track in the outer zones.

In modern drives, the small size of the magnetic regions creates the danger that their magnetic state might be lost because of thermal effects⁠ ⁠— thermally induced magnetic instability which is commonly known as the "superparamagnetic limit". To counter this, the platters are coated with two parallel magnetic layers, separated by a three-atom layer of the non-magnetic element ruthenium, and the two layers are magnetized in opposite orientation, thus reinforcing each other. Another technology used to overcome thermal effects to allow greater recording densities is perpendicular recording, first shipped in 2005, and as of 2007 used in certain HDDs.

In 2004, a new concept was introduced to allow further increase of the data density in magnetic recording: the use of recording media consisting of coupled soft and hard magnetic layers. So-called "exchange spring media" magnetic storage technology, also known as "exchange coupled composite media", allows good writability due to the write-assist nature of the soft layer. However, the thermal stability is determined only by the hardest layer and not influenced by the soft layer.

A typical HDD has two electric motors; a spindle motor that spins the disks and an actuator (motor) that positions the read/write head assembly across the spinning disks. The disk motor has an external rotor attached to the disks; the stator windings are fixed in place. Opposite the actuator at the end of the head support arm is the read-write head; thin printed-circuit cables connect the read-write heads to amplifier electronics mounted at the pivot of the actuator. The head support arm is very light, but also stiff; in modern drives, acceleration at the head reaches 550 "g".

The "" is a permanent magnet and moving coil motor that swings the heads to the desired position. A metal plate supports a squat neodymium-iron-boron (NIB) high-flux magnet. Beneath this plate is the moving coil, often referred to as the "voice coil" by analogy to the coil in loudspeakers, which is attached to the actuator hub, and beneath that is a second NIB magnet, mounted on the bottom plate of the motor (some drives have only one magnet).

The voice coil itself is shaped rather like an arrowhead, and made of doubly coated copper magnet wire. The inner layer is insulation, and the outer is thermoplastic, which bonds the coil together after it is wound on a form, making it self-supporting. The portions of the coil along the two sides of the arrowhead (which point to the actuator bearing center) then interact with the magnetic field of the fixed magnet. Current flowing radially outward along one side of the arrowhead and radially inward on the other produces the tangential force. If the magnetic field were uniform, each side would generate opposing forces that would cancel each other out. Therefore, the surface of the magnet is half north pole and half south pole, with the radial dividing line in the middle, causing the two sides of the coil to see opposite magnetic fields and produce forces that add instead of canceling. Currents along the top and bottom of the coil produce radial forces that do not rotate the head.

The HDD's electronics control the movement of the actuator and the rotation of the disk, and perform reads and writes on demand from the disk controller. Feedback of the drive electronics is accomplished by means of special segments of the disk dedicated to servo feedback. These are either complete concentric circles (in the case of dedicated servo technology), or segments interspersed with real data (in the case of embedded servo technology). The servo feedback optimizes the signal to noise ratio of the GMR sensors by adjusting the voice-coil of the actuated arm. The spinning of the disk also uses a servo motor. Modern disk firmware is capable of scheduling reads and writes efficiently on the platter surfaces and remapping sectors of the media which have failed.

Modern drives make extensive use of error correction codes (ECCs), particularly Reed–Solomon error correction. These techniques store extra bits, determined by mathematical formulas, for each block of data; the extra bits allow many errors to be corrected invisibly. The extra bits themselves take up space on the HDD, but allow higher recording densities to be employed without causing uncorrectable errors, resulting in much larger storage capacity. For example, a typical 1 TB hard disk with 512-byte sectors provides additional capacity of about 93 GB for the ECC data.

In the newest drives, as of 2009, low-density parity-check codes (LDPC) were supplanting Reed–Solomon; LDPC codes enable performance close to the Shannon Limit and thus provide the highest storage density available.

Typical hard disk drives attempt to "remap" the data in a physical sector that is failing to a spare physical sector provided by the drive's "spare sector pool" (also called "reserve pool"), while relying on the ECC to recover stored data while the number of errors in a bad sector is still low enough. The S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology) feature counts the total number of errors in the entire HDD fixed by ECC (although not on all hard drives as the related S.M.A.R.T attributes "Hardware ECC Recovered" and "Soft ECC Correction" are not consistently supported), and the total number of performed sector remappings, as the occurrence of many such errors may predict an HDD failure.

The "No-ID Format", developed by IBM in the mid-1990s, contains information about which sectors are bad and where remapped sectors have been located.

Only a tiny fraction of the detected errors end up as not correctable. Examples of specified uncorrected bit read error rates include:
Within a given manufacturers model the uncorrected bit error rate is typically the same regardless of capacity of the drive.

The worst type of errors are silent data corruptions which are errors undetected by the disk firmware or the host operating system; some of these errors may be caused by hard disk drive malfunctions while others originate elsewhere in the connection between the drive and the host.

The rate of areal density advancement was similar to Moore's law (doubling every two years) through 2010: 60% per year during 1988–1996, 100% during 1996–2003 and 30% during 2003–2010. Speaking in 1997, Gordon Moore called the increase "flabbergasting", while observing later that growth cannot continue forever. Price improvement decelerated to −12% per year during 2010–2017, as the growth of areal density slowed. The rate of advancement for areal density slowed to 10% per year during 2010–2016, and there was difficulty in migrating from perpendicular recording to newer technologies.

As bit cell size decreases, more data can be put onto a single drive platter. In 2013, a production desktop 3 TB HDD (with four platters) would have had an areal density of about 500 Gbit/in which would have amounted to a bit cell comprising about 18 magnetic grains (11 by 1.6 grains). Since the mid-2000s areal density progress has increasingly been challenged by a superparamagnetic trilemma involving grain size, grain magnetic strength and ability of the head to write. In order to maintain acceptable signal to noise smaller grains are required; smaller grains may self-reverse (electrothermal instability) unless their magnetic strength is increased, but known write head materials are unable to generate a strong enough magnetic field sufficient to write the medium in the increasingly-smaller space taken by grains. 

Several new magnetic storage technologies are being developed to overcome or at least abate this trilemma and thereby maintain the competitiveness of HDDs with respect to products such as flash memory–based solid-state drives (SSDs). In 2013, Seagate introduced shingled magnetic recording (SMR), intended as something of a "stopgap" technology between PMR and Seagate's intended successor heat-assisted magnetic recording (HAMR), SMR utilises overlapping tracks for increased data density, at the cost of design complexity and lower data access speeds (particularly write speeds and random access 4k speeds). By contrast, competitor Western Digital focused on developing ways to seal helium-filled drives, the aim being to reduce turbulence and friction effects, and fit more platters of a traditional design into the same enclosure space, by filling the drives with helium (which is a notoriously difficult gas to prevent escaping) instead of the usual filtered air.

Other new recording technologies that remain under development , include Seagate's heat-assisted magnetic recording (HAMR) drives, scheduled for commercial launch in the first half of 2019, HAMR's planned successor, bit-patterned recording (BPR), Western Digital's microwave-assisted magnetic recording (MAMR), two-dimensional magnetic recording (TDMR), and "current perpendicular to plane" giant magnetoresistance (CPP/GMR) heads.

The rate of areal density growth has dropped below the historical Moore's law rate of 40% per year, and the deceleration is expected to persist through at least 2020. Depending upon assumptions on feasibility and timing of these technologies, the median forecast by industry observers and analysts for 2020 and beyond for areal density growth is 20% per year with a range of 10–30%. The achievable limit for the HAMR technology in combination with BPR and SMR may be 10 Tbit/in, which would be 20 times higher than the 500 Gbit/in represented by 2013 production desktop HDDs. Seagate began sampling HAMR HDDs in 2018. They require a different architecture, with redesigned media and read/write heads, new lasers, and new near-field optical transducers.

The capacity of a hard disk drive, as reported by an operating system to the end user, is smaller than the amount stated by the manufacturer for several reasons: the operating system using some space, use of some space for data redundancy, and space use for file system structures. Also the difference in capacity reported in SI decimal prefixed units vs. binary prefixes can lead to a false impression of missing capacity.

Modern hard disk drives appear to their host controller as a contiguous set of logical blocks, and the gross drive capacity is calculated by multiplying the number of blocks by the block size. This information is available from the manufacturer's product specification, and from the drive itself through use of operating system functions that invoke low-level drive commands.

The gross capacity of older HDDs is calculated as the product of the number of cylinders per recording zone, the number of bytes per sector (most commonly 512), and the count of zones of the drive. Some modern SATA drives also report cylinder-head-sector (CHS) capacities, but these are not physical parameters because the reported values are constrained by historic operating system interfaces. The C/H/S scheme has been replaced by logical block addressing (LBA), a simple linear addressing scheme that locates blocks by an integer index, which starts at LBA 0 for the first block and increments thereafter. When using the C/H/S method to describe modern large drives, the number of heads is often set to 64, although a typical hard disk drive, , has between one and four platters.

In modern HDDs, spare capacity for defect management is not included in the published capacity; however, in many early HDDs a certain number of sectors were reserved as spares, thereby reducing the capacity available to the operating system.

For RAID subsystems, data integrity and fault-tolerance requirements also reduce the realized capacity. For example, a RAID 1 array has about half the total capacity as a result of data mirroring, while a RAID 5 array with drives loses of capacity (which equals to the capacity of a single drive) due to storing parity information. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provide fault tolerance. Most RAID vendors use checksums to improve data integrity at the block level. Some vendors design systems using HDDs with sectors of 520 bytes to contain 512 bytes of user data and eight checksum bytes, or by using separate 512-byte sectors for the checksum data.

Some systems may use hidden partitions for system recovery, reducing the capacity available to the end user.

Data is stored on a hard drive in a series of logical blocks. Each block is delimited by markers identifying its start and end, error detecting and correcting information, and space between blocks to allow for minor timing variations. These blocks often contained 512 bytes of usable data, but other sizes have been used. As drive density increased, an initiative known as Advanced Format extended the block size to 4096 bytes of usable data, with a resulting significant reduction in the amount of disk space used for block headers, error checking data, and spacing.

The process of initializing these logical blocks on the physical disk platters is called "low-level formatting", which is usually performed at the factory and is not normally changed in the field. "High-level formatting" writes data structures used by the operating system to organize data files on the disk. This includes writing partition and file system structures into selected logical blocks. For example, some of the disk space will be used to hold a directory of disk file names and a list of logical blocks associated with a particular file.

Examples of partition mapping scheme include Master boot record (MBR) and GUID Partition Table (GPT). Examples of data structures stored on disk to retrieve files include the File Allocation Table (FAT) in the DOS file system and inodes in many UNIX file systems, as well as other operating system data structures (also known as metadata). As a consequence, not all the space on an HDD is available for user files, but this system overhead is usually small compared with user data.

The total capacity of HDDs is given by manufacturers using SI decimal prefixes such as gigabytes (1 GB = 1,000,000,000 bytes) and terabytes (1 TB = 1,000,000,000,000 bytes). This practice dates back to the early days of computing; by the 1970s, "million", "mega" and "M" were consistently used in the decimal sense for drive capacity. However, capacities of memory are quoted using a binary interpretation of the prefixes, i.e. using powers of 1024 instead of 1000.

Software reports hard disk drive or memory capacity in different forms using either decimal or binary prefixes. The Microsoft Windows family of operating systems uses the binary convention when reporting storage capacity, so an HDD offered by its manufacturer as a 1 TB drive is reported by these operating systems as a 931 GB HDD. Mac OS X 10.6 ("Snow Leopard") uses decimal convention when reporting HDD capacity. The default behavior of the command-line utility on Linux is to report the HDD capacity as a number of 1024-byte units.

The difference between the decimal and binary prefix interpretation caused some consumer confusion and led to class action suits against HDD manufacturers. The plaintiffs argued that the use of decimal prefixes effectively misled consumers while the defendants denied any wrongdoing or liability, asserting that their marketing and advertising complied in all respects with the law and that no class member sustained any damages or injuries.

HDD price per byte improved at the rate of −40% per year during 1988–1996, −51% per year during 1996–2003, and −34% per year during 2003–2010. The price improvement decelerated to −13% per year during 2011–2014, as areal density increase slowed and the 2011 Thailand floods damaged manufacturing facilities.

IBM's first hard disk drive, the IBM 350, used a stack of fifty 24-inch platters and was of a size comparable to two large refrigerators. In 1962, IBM introduced its model 1311 disk, which used six 14-inch (nominal size) platters in a removable pack and was roughly the size of a washing machine. This became a standard platter size for many years, used also by other manufacturers. The IBM 2314 used platters of the same size in an eleven-high pack and introduced the "drive in a drawer" layout. sometimes called the"pizza oven", although the "drawer" was not the complete drive. Into the 1970s HDDs were offered in standalone cabinets of varying dimensions containing from one to four HDDs.

Beginning in the late 1960s drives were offered that fit entirely into a chassis that would mount in a 19-inch rack. Digital's RK05 and RL01 were early examples using single 14-inch platters in removable packs, the entire drive fitting in a 10.5-inch-high rack space (six rack units). In the mid-to-late 1980s the similarly sized Fujitsu Eagle, which used (coincidentally) 10.5-inch platters, was a popular product.

With increasing sales of microcomputers having built in floppy-disk drives (FDDs), HDDs that would fit to the FDD mountings became desirable. Starting with the Shugart Associates SA1000 HDD "Form factors", initially followed those of 8-inch, 5½-inch, and 3½-inch floppy disk drives. Although referred to by these nominal sizes, the actual sizes for those three drives respectively are 9.5″, 5.75″ and 4″ wide. Because there were no smaller floppy disk drives, smaller HDD form factors developed from product offerings or industry standards. 2½-inch drives are actually 2.75″ wide.

, 2½-inch and 3½-inch hard disks are the most popular sizes. By 2009, all manufacturers had discontinued the development of new products for the 1.3-inch, 1-inch and 0.85-inch form factors due to falling prices of flash memory, which has no moving parts. While nominal sizes are in inches, actual dimensions are specified in millimeters.

The factors that limit the time to access the data on an HDD are mostly related to the mechanical nature of the rotating disks and moving heads.

Seek time is a measure of how long it takes the head assembly to travel to the track of the disk that contains data. The first HDD had an average seek time of about 600 ms. Some early PC drives used a stepper motor to move the heads, and as a result had seek times as slow as 80–120 ms, but this was quickly improved by voice coil type actuation in the 1980s, reducing seek times to around 20 ms. Seek time has continued to improve slowly over time. The fastest server drives today have a seek time around 4 ms. The average seek time is strictly the time to do all possible seeks divided by the number of all possible seeks, but in practice is determined by statistical methods or simply approximated as the time of a seek over one-third of the number of tracks.

Rotational latency is incurred because the desired disk sector may not be directly under the head when data transfer is requested. Average rotational latency is shown in the table, based on the statistical relation that the average latency is one-half the rotational period.

The bit rate or data transfer rate (once the head is in the right position) creates delay which is a function of the number of blocks transferred; typically relatively small, but can be quite long with the transfer of large contiguous files.

Delay may also occur if the drive disks are stopped to save energy.

Defragmentation is a procedure used to minimize delay in retrieving data by moving related items to physically proximate areas on the disk. Some computer operating systems perform defragmentation automatically. Although automatic defragmentation is intended to reduce access delays, performance will be temporarily reduced while the procedure is in progress.

Time to access data can be improved by increasing rotational speed (thus reducing latency) or by reducing the time spent seeking. Increasing areal density increases throughput by increasing data rate and by increasing the amount of data under a set of heads, thereby potentially reducing seek activity for a given amount of data. The time to access data has not kept up with throughput increases, which themselves have not kept up with growth in bit density and storage capacity.

, a typical 7,200-rpm desktop HDD has a sustained "disk-to-buffer" data transfer rate up to 1,030 Mbit/s. This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the "buffer-to-computer" interface is 3.0 Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by file system fragmentation and the layout of the files.

HDD data transfer rate depends upon the rotational speed of the platters and the data recording density. Because heat and vibration limit rotational speed, advancing density becomes the main method to improve sequential transfer rates. Higher speeds require a more powerful spindle motor, which creates more heat. While areal density advances by increasing both the number of tracks across the disk and the number of sectors per track, only the latter increases the data transfer rate for a given rpm. Since data transfer rate performance tracks only one of the two components of areal density, its performance improves at a lower rate.

Other performance considerations include quality-adjusted price, power consumption, audible noise, and both operating and non-operating shock resistance.

The Federal Reserve Board has a quality-adjusted price index for large-scale enterprise storage systems including three or more enterprise HDDs and associated controllers, racks and cables. Prices for these large-scale storage systems improved at the rate of ‒30% per year during 2004–2009 and ‒22% per year during 2009–2014.

Current hard drives connect to a computer over one of several bus types, including parallel ATA, Serial ATA , SCSI, Serial Attached SCSI (SAS), and Fibre Channel. Some drives, especially external portable drives, use IEEE 1394, or USB. All of these interfaces are digital; electronics on the drive process the analog signals from the read/write heads. Current drives present a consistent interface to the rest of the computer, independent of the data encoding scheme used internally, and independent of the physical number of disks and heads within the drive. 

Typically a DSP in the electronics inside the drive takes the raw analog voltages from the read head and uses PRML and Reed–Solomon error correction to decode the data, then sends that data out the standard interface. That DSP also watches the error rate detected by error detection and correction, and performs bad sector remapping, data collection for Self-Monitoring, Analysis, and Reporting Technology, and other internal tasks.

Modern interfaces connect the drive to the host interface with a single data/control cable. Each drive also has an additional power cable, usually direct to the power supply unit. Older interfaces had separate cables for data signals and for drive control signals.

Due to the extremely close spacing between the heads and the disk surface, HDDs are vulnerable to being damaged by a head crash – a failure of the disk in which the head scrapes across the platter surface, often grinding away the thin magnetic film and causing data loss. Head crashes can be caused by electronic failure, a sudden power failure, physical shock, contamination of the drive's internal enclosure, wear and tear, corrosion, or poorly manufactured platters and heads.

The HDD's spindle system relies on air density inside the disk enclosure to support the heads at their proper flying height while the disk rotates. HDDs require a certain range of air densities to operate properly. The connection to the external environment and density occurs through a small hole in the enclosure (about 0.5 mm in breadth), usually with a filter on the inside (the "breather filter"). If the air density is too low, then there is not enough lift for the flying head, so the head gets too close to the disk, and there is a risk of head crashes and data loss. Specially manufactured sealed and pressurized disks are needed for reliable high-altitude operation, above about . Modern disks include temperature sensors and adjust their operation to the operating environment. Breather holes can be seen on all disk drives – they usually have a sticker next to them, warning the user not to cover the holes. The air inside the operating drive is constantly moving too, being swept in motion by friction with the spinning platters. This air passes through an internal recirculation (or "recirc") filter to remove any leftover contaminants from manufacture, any particles or chemicals that may have somehow entered the enclosure, and any particles or outgassing generated internally in normal operation. Very high humidity present for extended periods of time can corrode the heads and platters.

For giant magnetoresistive (GMR) heads in particular, a minor head crash from contamination (that does not remove the magnetic surface of the disk) still results in the head temporarily overheating, due to friction with the disk surface, and can render the data unreadable for a short period until the head temperature stabilizes (so called "thermal asperity", a problem which can partially be dealt with by proper electronic filtering of the read signal).

When the logic board of a hard disk fails, the drive can often be restored to functioning order and the data recovered by replacing the circuit board with one of an identical hard disk. In the case of read-write head faults, they can be replaced using specialized tools in a dust-free environment. If the disk platters are undamaged, they can be transferred into an identical enclosure and the data can be copied or cloned onto a new drive. In the event of disk-platter failures, disassembly and imaging of the disk platters may be required. For logical damage to file systems, a variety of tools, including fsck on UNIX-like systems and CHKDSK on Windows, can be used for data recovery. Recovery from logical damage can require file carving.

A common expectation is that hard disk drives designed and marketed for server use will fail less frequently than consumer-grade drives usually used in desktop computers. However, two independent studies by Carnegie Mellon University and Google found that the "grade" of a drive does not relate to the drive's failure rate.

A 2011 summary of research, into SSD and magnetic disk failure patterns by Tom's Hardware summarized research findings as follows:





More than 200 companies have manufactured HDDs over time, but consolidations have concentrated production to just three manufacturers today: Western Digital, Seagate, and Toshiba. Production is mainly in the Pacific rim.

Worldwide revenue for disk storage declined 4% per year, from a peak of $38 billion in 2012 to $27 billion in 2016. Production of HDDs grew 16% per year, from 335 exabytes in 2011 to 693 exabytes in 2016. Shipments declined 7% per year during this time period, from 620 million units to 425 million. Seagate and Western Digital each have 40–45% of unit shipments, while Toshiba has 13–17%. The average sales price for the two largest manufacturers was $60 per unit in 2015.

HDDs are being superseded by SSDs in markets where their higher speed (up to 3500 megabytes per second for m.2 SSDs or 2500 megabytes per second for PCIe drives), ruggedness and lower power are more important than price since SSDs are sill twice as expensive than an HDD of the same capacity. They also have a relatively short lifespan (Enterprise HDDs can be rated for up to 550TB of read and/or write endurance per year of warranty where as SSDs are usually not) and their performance drops over time, because NAND flash memory, on which most SSDs are based, degrades with every write operation. The lifespan or write endurance of SSDs is measured by either Terabytes Written (TBW) or Drive Writes Per Day (DWPD). In the case of the latter, if a 1 TB drive is rated for 1 DWPD, and the drive has a 1-year warranty, the Terabytes Written will be equal to 365. Some SSDs offer larger capacities (up to 100 TB) than the largest HDD and/or higher storage densities (100 TB and 30 TB SSDs are housed in 2.5 inch HDD cases but with the same height as a 3.5-inch HDD), although their cost remains prohibitive.

The maximum areal storage density for flash memory used in solid state drives (SSDs) is 2.8 Tbit/in in laboratory demonstrations as of 2016, and the maximum for HDDs is 1.5 Tbit/in. The areal density of flash memory is doubling every two years, similar to Moore's law (40% per year) and faster than the 10–20% per year for HDDs. As of 2018, the maximum possible capacity was 16 terabytes for an HDD, and 100 terabytes for an SSD. HDDs were used in 70% of the desktop and notebook computers produced in 2016, and SSDs were used in 30%. The usage share of HDDs is declining and could drop below 50% in 2018–2019 according to one forecast, because SSDs are replacing smaller-capacity (less than one-terabyte) HDDs in desktop and notebook computers and MP3 players.

The market for silicon-based flash memory (NAND) chips, used in SSDs and other applications, is growing rapidly. Worldwide revenue grew 12% per year during 2011–2016. It rose from $22 billion in 2011 to $39 billion in 2016, while production grew 46% per year from 19 exabytes to 120 exabytes.

External hard disk drives typically connect via USB; variants using USB 2.0 interface generally have slower data transfer rates when compared to internally mounted hard drives connected through SATA. Plug and play drive functionality offers system compatibility and features large storage options and portable design. , available capacities for external hard disk drives ranged from 500 GB to 10 TB.

External hard disk drives are usually available as pre-assembled integrated products, but may be also assembled by combining an external enclosure (with USB or other interface) with a separately purchased drive. They are available in 2.5-inch and 3.5-inch sizes; 2.5-inch variants are typically called "portable external drives", while 3.5-inch variants are referred to as "desktop external drives". "Portable" drives are packaged in smaller and lighter enclosures than the "desktop" drives; additionally, "portable" drives use power provided by the USB connection, while "desktop" drives require external power bricks.

Features such as biometric security or multiple interfaces (for example, Firewire) are available at a higher cost. There are pre-assembled external hard disk drives that, when taken out from their enclosures, cannot be used internally in a laptop or desktop computer due to embedded USB interface on their printed circuit boards, and lack of SATA (or Parallel ATA) interfaces.




</doc>
<doc id="13782" url="https://en.wikipedia.org/wiki?curid=13782" title="Hebrew calendar">
Hebrew calendar

The Hebrew or Jewish calendar (, ) is a lunisolar calendar used today predominantly for Jewish religious observances. It determines the dates for Jewish holidays and the appropriate public reading of Torah portions, "yahrzeits" (dates to commemorate the death of a relative), and daily Psalm readings, among many ceremonial uses. In Israel, it is used for religious purposes, provides a time frame for agriculture and is an official calendar for civil purposes, although the latter usage has been steadily declining in favor of the Gregorian calendar.

The present Hebrew calendar is the product of evolution, including a Babylonian influence. Until the Tannaitic period (approximately 10–220 CE), the calendar employed a new crescent moon, with an additional month normally added every two or three years to correct for the difference between twelve lunar months and the solar year. The year in which it was added was based on observation of natural agriculture-related events in ancient Israel. Through the Amoraic period (200–500 CE) and into the Geonic period, this system was gradually displaced by the mathematical rules used today. The principles and rules were fully codified by Maimonides in the in the 12th century. Maimonides' work also replaced counting "years since the destruction of the Temple" with the modern creation-era .

The Hebrew lunar year is about eleven days shorter than the solar year and uses the 19-year Metonic cycle to bring it into line with the solar year, with the addition of an intercalary month every two or three years, for a total of seven times per 19 years. Even with this intercalation, the average Hebrew calendar year is longer by about 6 minutes and 40 seconds than the current mean tropical year, so that every 217 years the Hebrew calendar will fall a day behind the current mean tropical year; and about every 238 years it will fall a day behind the mean Gregorian calendar year.

The era used since the Middle Ages is the epoch (Latin for "in the year of the world"; , "from the creation of the world"). As with (A.D. or AD), the words or abbreviation for (A.M. or AM) for the era should properly "precede" the date rather than follow it. 

AM began at sunset on and will end at sunset on .

The Jewish day is of no fixed length. The Jewish day is modeled on the reference to "...there was evening and there was morning..." in the creation account in the first chapter of Genesis. Based on the classic rabbinic interpretation of this text, a day in the rabbinic Hebrew calendar runs from sunset (start of "the evening") to the next sunset. Halachically, a day ends and a new one starts when three stars are visible in the sky. The time between true sunset and the time when the three stars are visible (known as 'tzait ha'kochavim') is known as 'bein hashmashot', and there are differences of opinion as to which day it falls into for some uses. This may be relevant, for example, in determining the date of birth of a child born during that gap.

There is no clock in the Jewish scheme, so that the local civil clock is used. Though the civil clock, including the one in use in Israel, incorporates local adoptions of various conventions such as time zones, standard times and daylight saving, these have no place in the Jewish scheme. The civil clock is used only as a reference point – in expressions such as: "Shabbat starts at ...". The steady progression of sunset around the world and seasonal changes results in gradual civil time changes from one day to the next based on observable astronomical phenomena (the sunset) and not on man-made laws and conventions.

In Judaism, an hour is defined as 1/12 of the time from sunrise to sunset, so, during the winter, an hour can be much less than 60 minutes, and during the summer, it can be much more than 60 minutes. This proportional hour is known as a "sha'ah z'manit" (lit. a timely hour). A Jewish hour is divided into 1080 "halakim" (singular: "helek") or parts. A part is 3⅓ seconds or / minute. The ultimate ancestor of the helek was a small Babylonian time period called a "barleycorn", itself equal to / of a Babylonian "time degree" (1° of celestial rotation). These measures are not generally used for everyday purposes.

Instead of the international date line convention, there are varying opinions as to where the day changes. One opinion uses the antimeridian of Jerusalem. (Jerusalem is 35°13' east of the prime meridian, so the antimeridian is at 144°47' W, passing through eastern Alaska.) Other opinions exist as well. (See International date line in Judaism.)

The weekdays start with Sunday (day 1, or "Yom Rishon") and proceed to Saturday (day 7), Shabbat. Since some calculations use division, a remainder of 0 signifies Saturday.

While calculations of days, months and years are based on fixed hours equal to / of a day, the beginning of each "halachic" day is based on the local time of sunset. The end of the Shabbat and other Jewish holidays is based on nightfall ("Tzeth haKochabim") which occurs some amount of time, typically 42 to 72 minutes, after sunset. According to Maimonides, nightfall occurs when three medium-sized stars become visible after sunset. By the 17th century, this had become three-second-magnitude stars. The modern definition is when the center of the sun is 7° below the geometric (airless) horizon, somewhat later than civil twilight at 6°. The beginning of the daytime portion of each day is determined both by dawn and sunrise. Most "halachic" times are based on some combination of these four times and vary from day to day throughout the year and also vary significantly depending on location. The daytime hours are often divided into "Sha'oth Zemaniyoth" or "Halachic hours" by taking the time between sunrise and sunset or between dawn and nightfall and dividing it into 12 equal hours. The nighttime hours are similarly divided into 12 equal portions, albeit a different amount of time than the "hours" of the daytime. The earliest and latest times for Jewish services, the latest time to eat chametz on the day before Passover and many other rules are based on "Sha'oth Zemaniyoth". For convenience, the modern day using "Sha'oth Zemaniyoth" is often discussed as if sunset were at 6:00 pm, sunrise at 6:00 am and each hour were equal to a fixed hour. For example, "halachic" noon may be after 1:00 pm in some areas during daylight saving time. Within the Mishnah, however, the numbering of the hours starts with the "first" hour after the start of the day.

Shavua [שבוע] is a weekly cycle of seven days, mirroring the seven-day period of the Book of Genesis in which the world is created. The names for the days of the week, like those in the creation account, are simply the day number within the week, with Shabbat being the seventh day. Each day of the week runs from sunset to the following sunset and is figured locally.

The Hebrew calendar follows a seven-day weekly cycle, which runs concurrently with but independently of the monthly and annual cycles. The names for the days of the week are simply the day number within the week. In Hebrew, these names may be abbreviated using the numerical value of the Hebrew letters, for example ("Day 1", or Yom Rishon ()):

Yom Shabbat () is also known as Yom Shabbat Kodesh – meaning "holy rest day."

The names of the days of the week are modeled on the seven days mentioned in the creation story. For example, "... And there was evening and there was morning, one day". "One day" () in Genesis 1:5 is translated in JPS as "first day", and in some other contexts (including KJV) as "day one". In subsequent verses, the Hebrew refers to the days using ordinal numbers, e.g., 'second day', 'third day', and so forth, but with the sixth and seventh days the Hebrew includes the definite article ("the").

The rest day, Shabbat, has a special role in the Jewish weekly cycle as being a special and set apart day, where no work is done. There are many special rules that relate to Shabbat, discussed more fully in the Talmudic tractate Shabbat.

In (Talmudic) Hebrew, the word "Shabbat" () can also mean "week", so that in ritual liturgy a phrase like "Yom Reviʻi bəShabbat" means "the fourth day in the week".

The period from 1 Adar (or Adar II, in leap years) to 29 Marcheshvan contains all of the festivals specified in the Bible – Pesach (15 Nisan), Shavuot (6 Sivan), Rosh Hashanah (1 Tishrei), Yom Kippur (10 Tishrei), Sukkot (15 Tishrei), and Shemini Atzeret (22 Tishrei). This period is fixed, during which no adjustments are made.
There are additional rules in the Hebrew calendar to prevent certain holidays from falling on certain days of the week. (See Rosh Hashanah postponement rules, below.) These rules are implemented by adding an extra day to Marcheshvan (making it 30 days long) or by removing one day from Kislev (making it 29 days long). Accordingly, a common Hebrew calendar year can have a length of 353, 354 or 355 days, while a leap Hebrew calendar year can have a length of 383, 384 or 385 days.

The Hebrew calendar is a lunisolar calendar, meaning that months are based on lunar months, but years are based on solar years. The calendar year features twelve lunar months of twenty-nine or thirty days, with an intercalary lunar month added periodically to synchronize the twelve lunar cycles with the longer solar year. (These extra months are added seven times every nineteen years. See Leap months, below.) The beginning of each Jewish lunar month is based on the appearance of the new moon. Although originally the new lunar crescent had to be observed and certified by witnesses, the moment of the true new moon is now approximated arithmetically as the molad, which is the mean new moon to a precision of one part.

The mean period of the lunar month (precisely, the synodic month) is very close to 29.5 days. Accordingly, the basic Hebrew calendar year is one of twelve lunar months alternating between 29 and 30 days:

In leap years (such as 5779) an additional month, Adar I (30 days) is added after Shevat, while the regular Adar is referred to as "Adar II."

The insertion of the leap month mentioned above is based on the requirement that Passover—the festival celebrating the Exodus from Egypt, which took place in the spring—always occurs in the [northern hemisphere's] spring season. Since the adoption of a fixed calendar, intercalations in the Hebrew calendar have been assigned to fixed points in a 19-year cycle. Prior to this, the intercalation was determined empirically:

The year may be intercalated on three grounds: 'aviv [i.e.the ripeness of barley], fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone.

From very early times, the Mesopotamian lunisolar calendar was in wide use by the countries of the western Asia region. The structure, which was also used by the Israelites, was based on lunar months with the intercalation of an additional month to bring the cycle closer to the solar cycle, although there is no evidence of a thirteenth month mentioned anywhere in the Hebrew Bible.

According to the "Mishnah" and Tosefta, in the Maccabean, Herodian, and Mishnaic periods, new months were determined by the sighting of a new crescent, with two eyewitnesses required to testify to the Sanhedrin to having seen the new lunar crescent at sunset. The practice in the time of Gamaliel II (c. 100 CE) was for witnesses to select the appearance of the moon from a collection of drawings that depicted the crescent in a variety of orientations, only a few of which could be valid in any given month. These observations were compared against calculations.

At first the beginning of each Jewish month was signaled to the communities of Israel and beyond by fires lit on mountaintops, but after the Samaritans began to light false fires, messengers were sent. The inability of the messengers to reach communities outside Israel before mid-month High Holy Days (Succot and Passover) led outlying communities to celebrate scriptural festivals for two days rather than one, observing the second feast-day of the Jewish diaspora because of uncertainty of whether the previous month ended after 29 or 30 days.

In his work "Mishneh Torah" (1178), Maimonides included a chapter "Sanctification of the New Moon", in which he discusses the calendrical rules and their scriptural basis. He notes, "By how much does the solar year exceed the lunar year? By approximately 11 days. Therefore, whenever this excess accumulates to about 30 days, or a little more or less, one month is added and the particular year is made to consist of 13 months, and this is the so-called embolismic (intercalated) year. For the year could not consist of twelve months plus so-and-so many days, since it is said: throughout the months of the year (), which implies that we should count the year by months and not by days."

Both the Syrian calendar, currently used in the Arabic-speaking countries of the Fertile crescent, and the modern Assyrian calendar share many of the names for months with the Hebrew calendar, such as Nisan, Iyyar, Tammuz, Ab, Elul, Tishri and Adar, indicating a common origin. The origin is thought to be the Babylonian calendar. The modern Turkish calendar includes the names Şubat (February), Nisan (April), Temmuz (July) and Eylul (September). The former name for October was Tesrin.

Babylonian exile

The Jewish people left Babylon and returned to live in Judea around 586 BCE. At this time they adopted the Babylonian names for the months. The Babylonian calendar descended directly from the Sumerian calendar.

Biblical references to the pre-exilic calendar include ten months identified by number rather than by name. In parts of the Torah portion "Noach" ("Noah") (specifically, , , ) it is implied that the months are thirty days long. There is also an indication that there were twelve months in the annual cycle (, ). Prior to the Babylonian exile, the names of only four months are referred to in the Tanakh:

All of these are believed to be Canaanite names. These names are only mentioned in connection with the building of the First Temple. Håkan Ulfgard suggests that the use of what are rarely used Canaanite (or in the case of Ethanim perhaps Northwest-semitic) names indicates that "the author is consciously utilizing an archaizing terminology, thus giving the impression of an ancient story...".

In a regular ("kesidran") year, Marcheshvan has 29 days and Kislev has 30 days. However, because of the Rosh Hashanah postponement rules (see below) Kislev may lose a day to have 29 days, and the year is called a short ("chaser") year, or Marcheshvan may acquire an additional day to have 30 days, and the year is called a full ("maleh") year. The calendar rules have been designed to ensure that Rosh Hashanah does not fall on a Sunday, Wednesday or Friday. This is to ensure that Yom Kippur does not directly precede or follow Shabbat, which would create practical difficulties, and that Hoshana Rabbah is not on a Shabbat, in which case certain ceremonies would be lost for a year.

The solar year is about eleven days longer than twelve lunar months. The Bible does not directly mention the addition of "embolismic" or intercalary months. However, without the insertion of embolismic months, Jewish festivals would gradually shift outside of the seasons required by the Torah. This has been ruled as implying a requirement for the insertion of embolismic months to reconcile the lunar cycles to the seasons, which are integral to solar yearly cycles.

When the observational form of the calendar was in use, whether or not an embolismic month was announced after the "last month" (Adar) depended on 'aviv [i.e., the ripeness of barley], fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone. It may be noted that in the Bible the name of the first month, "Aviv", literally means "spring". Thus, if Adar was over and spring had not yet arrived, an additional month was observed.

Traditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle forms the basis of the Christian ecclesiastical calendar and the Hebrew calendar and is used for the computation of the date of Easter each year

During leap years Adar I (or Adar Aleph – "first Adar") is added before the regular Adar. Adar I is actually considered to be the extra month, and has 30 days. Adar II (or Adar Bet – "second Adar") is the "real" Adar, and has the usual 29 days. For this reason, holidays such as Purim are observed in Adar II, not Adar I.

Chronology was a chief consideration in the study of astronomy among the Jews; sacred time was based upon the cycles of the Sun and the Moon. The Talmud identified the twelve constellations of the zodiac with the twelve months of the Hebrew calendar. The correspondence of the constellations with their names in Hebrew and the months is as follows:


Some scholars identified the 12 signs of the zodiac with the 12 sons of Jacob/twelve tribes of Israel.
It should be noted that the 12 lunar months of the Hebrew calendar are the normal months from new moon to new moon: the year normally contains twelve months averaging 29.52 days each. The discrepancy compared to the mean synodic month of 29.53 days is due to Adar I in a leap year always having thirty days. This means that the calendar year normally contains 354 days.

The Hebrew calendar year conventionally begins on Rosh Hashanah. However, other dates serve as the beginning of the year for different religious purposes.

There are three qualities that distinguish one year from another: whether it is a leap year or a common year, on which of four permissible days of the week the year begins, and whether it is a deficient, regular, or complete year. Mathematically, there are 24 (2×4×3) possible combinations, but only 14 of them are valid. Each of these patterns is called a "keviyah" (Hebrew קביעה for "a setting" or "an established thing"), and is encoded as a series of two or three Hebrew letters. See Four gates.

In Hebrew there are two common ways of writing the year number: with the thousands, called ("major era"), and without the thousands, called ("minor era"). Thus, the current year is written as ' ‎() using the "major era" and ' ‎(%1000) using the "minor era".

In 1178 CE, Maimonides wrote in the "Mishneh Torah", "Sanctification of the Moon" (11.16), that he had chosen the epoch from which calculations of all dates should be as "the third day of Nisan in this present year ... which is the year 4938 of the creation of the world" (22 March 1178). He included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, and beginning formal usage of the "anno mundi" era. From the eleventh century, "anno mundi" dating became dominant throughout most of the world's Jewish communities. Today, the rules detailed in Maimonides' calendrical code are those generally used by Jewish communities throughout the world.

Since the codification by Maimonides in 1178, the Jewish calendar has used the Anno Mundi epoch (Latin for "in the year of the world," abbreviated "AM" or "A.M.", Hebrew ), sometimes referred to as the "Hebrew era", to distinguish it from other systems based on some computation of creation, such as the Byzantine calendar.

There is also reference in the Talmud to years since the creation based on the calculation in the "Seder Olam Rabbah" of Rabbi Jose ben Halafta in about 160 CE. By his calculation, based on the Masoretic Text, Adam was created in 3760 BCE, later confirmed by the Muslim chronologist al-Biruni as 3448 years before the Seleucid era. An example is the c. 8th century Baraita of Samuel.

According to rabbinic reckoning, the beginning of "year 1" is "not" Creation, but about one year before Creation, with the new moon of its first month (Tishrei) to be called "molad tohu" (the mean new moon of chaos or nothing). The Jewish calendar's epoch (reference date), 1 Tishrei AM 1, is equivalent to Monday, 7 October 3761 BCE in the proleptic Julian calendar, the equivalent tabular date (same daylight period) and is about one year "before" the traditional Jewish date of Creation on 25 Elul AM 1, based upon the "Seder Olam Rabbah". Thus, adding 3760 before Rosh Hashanah or 3761 after to a Julian year number starting from 1 CE will yield the Hebrew year. For earlier years there may be a discrepancy [see: Missing years (Jewish calendar)].

The "Seder Olam Rabbah" also recognized the importance of the Jubilee and Sabbatical cycles as a long-term calendrical system, and attempted at various places to fit the Sabbatical and Jubilee years into its chronological scheme.

Occasionally, "Anno Mundi" is styled as "Anno Hebraico (AH)", though this is subject to confusion with notation for the Islamic Hijri year.

Before the adoption of the current AM year numbering system, other systems were in use. In early times, the years were counted from some significant historic event. (e.g., ) During the period of the monarchy, it was the widespread practice in western Asia to use era year numbers according to the accession year of the monarch of the country involved. This practice was also followed by the united kingdom of Israel (e.g., ), kingdom of Judah (e.g., ), kingdom of Israel (e.g., ), Persia (e.g., ) and others. Besides, the author of Kings coordinated dates in the two kingdoms by giving the accession year of a monarch in terms of the year of the monarch of the other kingdom, (e.g., ) though some commentators note that these dates do not always synchronise. Other era dating systems have been used at other times. For example, Jewish communities in the Babylonian diaspora counted the years from the first deportation from Israel, that of Jehoiachin in 597 BCE, (e.g., ). The era year was then called "year of the captivity of Jehoiachin". (e.g., )

During the Hellenistic Maccabean period, Seleucid era counting was used, at least in the Greek-influenced area of Israel. The Books of the Maccabees used Seleucid era dating exclusively (e.g., , , , , ). Josephus writing in the Roman period also used Seleucid era dating exclusively. During the Talmudic era, from the 1st to the 10th century, the center of world Judaism was in the Middle East, primarily in the Talmudic Academies of Iraq and Palestine. Jews in these regions used Seleucid era dating (also known as the "Era of Contracts"). The Avodah Zarah states:

Rav Aha b. Jacob then put this question: How do we know that our Era [of Documents] is connected with the Kingdom of Greece at all? Why not say that it is reckoned from the Exodus from Egypt, omitting the first thousand years and giving the years of the next thousand? In that case, the document is really post-dated!<br> Said Rav Nahman: In the Diaspora the Greek Era alone is used. He [the questioner] thought that Rav Nahman wanted to dispose of him anyhow, but when he went and studied it thoroughly he found that it is indeed taught [in a Baraita]: In the Diaspora the Greek Era alone is used.

The use of the era of documents (i.e., Seleucid era) continued till the 16th century in the East, and was employed even in the 19th century among the Jews of Yemen.

Occasionally in Talmudic writings, reference was made to other starting points for eras, such as destruction era dating, being the number of years since the 70 CE destruction of the Second Temple. In the 8th and 9th centuries, as the center of Jewish life moved from Babylonia to Europe, counting using the Seleucid era "became meaningless". There is indication that Jews of the Rhineland in the early Middle Ages used the "years after the destruction of the Temple" (e.g., ).

 and set Aviv (now Nisan) as "the first of months":

Nisan 1 is referred to as the "ecclesiastical new year".

In ancient Israel, the start of the ecclesiastical new year for the counting of months and festivals (i.e., Nisan) was determined by reference to Passover. Passover is on 15 Nisan, () which corresponds to the full moon of Nisan. As Passover is a spring festival, it should fall on a full moon day around, and normally just after, the vernal (northward) equinox. If the twelfth full moon after the previous Passover is too early compared to the equinox, a leap month is inserted near the end of the previous year before the new year is set to begin. According to normative Judaism, the verses in require that the months be determined by a proper court with the necessary authority to sanctify the months. Hence the court, not the astronomy, has the final decision.

According to some Christian and Karaite sources, the tradition in ancient Israel was that 1 Nisan would not start until the barley is ripe, being the test for the onset of spring. If the barley was not ripe, an intercalary month would be added before Nisan.

The day most commonly referred to as the "New Year" is 1 Tishrei, which actually begins in the seventh month of the ecclesiastical year. On that day the formal New Year for the counting of years (such as Shmita and Yovel), Rosh Hashanah ("head of the year") is observed. (see , which uses the phrase "beginning of the year".) This is the civil new year, and the date on which the year number advances. Certain agricultural practices are also marked from this date.

In the 1st century, Josephus stated that while –

Moses...appointed Nisan...as the first month for the festivals...the commencement of the year for everything relating to divine worship, but for selling and buying and other ordinary affairs he preserved the ancient order [i. e. the year beginning with Tishrei]."

Edwin Thiele has concluded that the ancient northern Kingdom of Israel counted years using the ecclesiastical new year starting on 1 Aviv (Nisan), while the southern Kingdom of Judah counted years using the civil new year starting on 1 Tishrei. The practice of the Kingdom of Israel was also that of Babylon, as well as other countries of the region. The practice of Judah is still followed.

In fact the Jewish calendar has a multiplicity of new years for different purposes. The use of these dates has been in use for a long time. The use of multiple starting dates for a year is comparable to different starting dates for civil "calendar years", "tax or fiscal years", "academic years", "religious cycles", etc. By the time of the redaction of the "Mishnah", (c. 200 CE), jurists had identified four new-year dates:

The 1st of Nisan is the new year for kings and feasts; the 1st of Elul is the new year for the tithe of cattle... the 1st of Tishri is the new year for years, of the years of release and jubilee years, for the planting and for vegetables; and the 1st of Shevat is the new year for trees—so the school of Shammai; and the school of Hillel say: On the 15th thereof.

The month of Elul is the new year for counting animal tithes ("ma'aser behemah"). "Tu Bishvat" ("the 15th of Shevat") marks the new year for trees (and agricultural tithes).

For the dates of the Jewish New Year see Jewish and Israeli holidays 2000–2050 or calculate using the section "Conversion between Jewish and civil calendars".

The Jewish calendar is based on the Metonic cycle of 19 years, of which 12 are common (non-leap) years of 12 months and 7 are leap years of 13 months. To determine whether a Jewish year is a leap year, one must find its position in the 19-year Metonic cycle. This position is calculated by dividing the Jewish year number by 19 and finding the remainder. (Since there is no year 0, a remainder of 0 indicates that the year is year 19 of the cycle.) For example, the Jewish year divided by 19 results in a remainder of % 19, indicating that it is year of the Metonic cycle. 

Years 3, 6, 8, 11, 14, 17, and 19 of the Metonic cycle are leap years. To assist in remembering this sequence, some people use the mnemonic Hebrew word <nowiki>GUCHADZaT</nowiki> , where the Hebrew letters "gimel-vav-het aleph-dalet-zayin-tet" are used as Hebrew numerals equivalent to 3, 6, 8, 1, 4, 7, 9. The "keviyah" records whether the year is leap or common: פ for "peshuta" (פשוטה), meaning simple and indicating a common year, and מ indicating a leap year (me'uberet, מעוברת).

Another memory aid notes that intervals of the major scale follow the same pattern as do Jewish leap years, with "do" corresponding to year 19 (or 0): a whole step in the scale corresponds to two common years between consecutive leap years, and a half step to one common year between two leap years. This connection with the major scale is more plain in the context of 19 equal temperament: counting the tonic as 0, the notes of the major scale in 19 equal temperament are numbers 0 (or 19), 3, 6, 8, 11, 14, 17, the same numbers as the leap years in the Hebrew calendar.

A simple rule for determining whether a year is a leap year has been given above. However, there is another rule which not only tells whether the year is leap but also gives the fraction of a month by which the calendar is behind the seasons, useful for agricultural purposes. To determine whether year "n" of the calendar is a leap year, find the remainder on dividing [(7 × "n") + 1] by 19. If the remainder is 6 or less it is a leap year; if it is 7 or more it is not. For example, the The This works because as there are seven leap years in nineteen years the difference between the solar and lunar years increases by 7/19-month per year. When the difference goes above 18/19-month this signifies a leap year, and the difference is reduced by one month.

To calculate the day on which Rosh Hashanah of a given year will fall, it is necessary first to calculate the expected molad (moment of lunar conjunction or new moon) of Tishrei in that year, and then to apply a set of rules to determine whether the first day of the year must be postponed. The molad can be calculated by multiplying the number of months that will have elapsed since some (preceding) molad whose weekday is known by the mean length of a (synodic) lunar month, which is 29 days, 12 hours, and 793 parts (there are 1080 "parts" in an hour, so that one part is equal to 3 seconds). The very first molad, the molad tohu, fell on Sunday evening at 11.11, or in Jewish terms Day 2, 5 hours, and 204 parts.

In calculating the number of months that will have passed since the known molad that one uses as the starting point, one must remember to include any leap months that falls within the elapsed interval, according to the cycle of leap years. A 19-year cycle of 235 synodic months has 991 weeks 2 days 16 hours 595 parts, a common year of 12 synodic months has 50 weeks 4 days 8 hours 876 parts, while a leap year of 13 synodic months has 54 weeks 5 days 21 hours 589 parts.

The two months whose numbers of days may be adjusted, Marcheshvan and Kislev, are the eighth and ninth months of the Hebrew year, whereas Tishrei is the seventh month (in the traditional counting of the months, even though it is the first month of a new calendar year). Any adjustments needed to postpone Rosh Hashanah must be made to the adjustable months in the year that precedes the year of which the Rosh Hashanah will be the first day.

Just four potential conditions are considered to determine whether the date of Rosh Hashanah must be postponed. These are called the Rosh Hashanah postponement rules, or "deḥiyyot":


The first of these rules ("deḥiyyat molad zaken") is referred to in the Talmud. Nowadays, molad zaken is used as a device to prevent the molad falling on the second day of the month. The second rule, ("deḥiyyat lo ADU"), is applied for religious reasons.

Another two rules are applied much less frequently and serve to prevent impermissible year lengths. Their names are Hebrew acronyms that refer to the ways they are calculated:


At the innovation of the sages, the calendar was arranged to ensure that Yom Kippur would not fall on a Friday or Sunday, and Hoshana Rabbah would not fall on Shabbat. These rules have been instituted because Shabbat restrictions also apply to Yom Kippur, so that if Yom Kippur were to fall on Friday, it would not be possible to make necessary preparations for Shabbat (such as candle lighting). Similarly, if Yom Kippur fell on a Sunday, it would not be possible to make preparations for Yom Kippur because the preceding day is Shabbat. Additionally, the laws of Shabbat override those of Hoshana Rabbah, so that if Hoshana Rabbah were to fall on Shabbat certain rituals that are a part of the Hoshana Rabbah service (such as carrying willows, which is a form of work) could not be performed.

To prevent Yom Kippur (10 Tishrei) from falling on a Friday or Sunday, Rosh Hashanah (1 Tishrei) cannot fall on Wednesday or Friday. Likewise, to prevent Hoshana Rabbah (21 Tishrei) from falling on a Saturday, Rosh Hashanah cannot fall on a Sunday. This leaves only four days on which Rosh Hashanah can fall: Monday, Tuesday, Thursday, and Saturday, which are referred to as the "four gates". Each day is associated with a number (its order in the week, beginning with Sunday as day 1). Numbers in Hebrew have been traditionally denominated by Hebrew letters. Thus the "keviyah" uses the letters ה ,ג ,ב and ז (representing 2, 3, 5, and 7, for Monday, Tuesday, Thursday, and Saturday) to denote the starting day of the year.

The postponement of the year is compensated for by adding a day to the second month or removing one from the third month. A Jewish common year can only have 353, 354, or 355 days. A leap year is always 30 days longer, and so can have 383, 384, or 385 days.


Whether a year is deficient, regular, or complete is determined by the time between two adjacent Rosh Hashanah observances and the leap year. While the "keviyah" is sufficient to describe a year, a variant specifies the day of the week for the first day of Pesach (Passover) in lieu of the year length.

A Metonic cycle equates to 235 lunar months in each 19-year cycle. This gives an average of 6939 days, 16 hours, and 595 parts for each cycle. But due to the Rosh Hashanah postponement rules (preceding section) a cycle of 19 Jewish years can be either 6939, 6940, 6941, or 6942 days in duration. Since none of these values is evenly divisible by seven, the Jewish calendar repeats exactly only following 36,288 Metonic cycles, or 689,472 Jewish years. There is a near-repetition every 247 years, except for an excess of about 50 minutes (905 parts).

The annual calendar of a numbered Hebrew year, displayed as 12 or 13 months partitioned into weeks, can be determined by consulting the table of Four gates, whose inputs are the year's position in the 19-year cycle and its molad Tishrei. The resulting type ("keviyah") of the desired year in the body of the table is a triple consisting of two numbers and a letter (written left-to-right in English). The left number of each triple is the day of the week of , Rosh Hashanah ; the letter indicates whether that year is deficient (D), regular (R), or complete (C), the number of days in Chesvan and Kislev; while the right number of each triple is the day of the week of , the first day of Passover or Pesach , within the same Hebrew year (next Julian/Gregorian year). The "keviyah" in Hebrew letters are written right-to-left, so their days of the week are reversed, the right number for and the left for . The year within the 19-year cycle alone determines whether that year has one or two Adars.

This table numbers the days of the week and hours for the limits of molad Tishrei in the Hebrew manner for calendrical calculations, that is, both begin at , thus is noon Saturday. The years of a 19-year cycle are organized into four groups: common years after a leap year but before a common year ; common years between two leap years ; common years after a common year but before a leap year ; and leap years , all between common years. The oldest surviving table of Four gates was written by Saadia Gaon (892–942). It is so named because it identifies the four allowable days of the week on which can occur.

Comparing the days of the week of molad Tishrei with those in the "keviyah" shows that during 39% of years is not postponed beyond the day of the week of its molad Tishrei, 47% are postponed one day, and 14% are postponed two days. This table also identifies the seven types of common years and seven types of leap years. Most are represented in any 19-year cycle, except one or two may be in neighboring cycles. The most likely type of year is 5R7 in 18.1% of years, whereas the least likely is 5C1 in 3.3% of years. The day of the week of is later than that of by one, two or three days for common years and three, four or five days for leap years in deficient, regular or complete years, respectively.

See Jewish and Israeli holidays 2000–2050

The Tanakh contains several commandments related to the keeping of the calendar and the lunar cycle, and records changes that have taken place to the Hebrew calendar.

It has been noted that the procedures described in the Mishnah and Tosefta are all plausible procedures for regulating an empirical lunar calendar. Fire-signals, for example, or smoke-signals, are known from the pre-exilic Lachish ostraca. Furthermore, the Mishnah contains laws that reflect the uncertainties of an empirical calendar. Mishnah Sanhedrin, for example, holds that when one witness holds that an event took place on a certain day of the month, and another that the same event took place on the following day, their testimony can be held to agree, since the length of the preceding month was uncertain. Another Mishnah takes it for granted that it cannot be known in advance whether a year's lease is for twelve or thirteen months. Hence it is a reasonable conclusion that the Mishnaic calendar was actually used in the Mishnaic period.

The accuracy of the Mishnah's claim that the Mishnaic calendar was also used in the late Second Temple period is less certain. One scholar has noted that there are no laws from Second Temple period sources that indicate any doubts about the length of a month or of a year. This led him to propose that the priests must have had some form of computed calendar or calendrical rules that allowed them to know in advance whether a month would have 30 or 29 days, and whether a year would have 12 or 13 months.

Between 70 and 1178 CE, the observation-based calendar was gradually replaced by a mathematically calculated one.

The Talmuds indicate at least the beginnings of a transition from a purely empirical to a computed calendar. Samuel of Nehardea (c.165-254) stated that he could determine the dates of the holidays by calculation rather than observation. According to a statement attributed to Yose (late 3rd century), Purim could not fall on a Sabbath nor a Monday, lest Yom Kippur fall on a Friday or a Sunday. This indicates that, by the time of the redaction of the Jerusalem Talmud (c. 400 CE), there were a fixed number of days in all months from Adar to Elul, also implying that the extra month was already a second Adar added before the regular Adar. Elsewhere, Rabbi Simon is reported to have counseled "those who make the computations" not to set Rosh Hashana or Hoshana Rabbah on Shabbat. This indicates that there was a group who "made computations" and controlled, to some extent, the day of the week on which Rosh Hashana would fall.

There is a tradition, first mentioned by Hai Gaon (died 1038 CE), that Hillel b. R. Yehuda "in the year 670 of the Seleucid era" (i.e., 358–359 CE) was responsible for the new calculated calendar with a fixed intercalation cycle. Later writers, such as Nachmanides, explained Hai Gaon's words to mean that the entire computed calendar was due to Hillel b. Yehuda in response to persecution of Jews. Maimonides (12th century) stated that the Mishnaic calendar was used "until the days of Abaye and Rava" (c. 320–350 CE), and that the change came when "the land of Israel was destroyed, and no permanent court was left." Taken together, these two traditions suggest that Hillel b. Yehuda (whom they identify with the mid-4th-century Jewish patriarch Ioulos, attested in a letter of the Emperor Julian, and the Jewish patriarch Ellel, mentioned by Epiphanius) instituted the computed Hebrew calendar because of persecution. H. Graetz linked the introduction of the computed calendar to a sharp repression following a failed Jewish insurrection that occurred during the rule of the Christian emperor Constantius and Gallus. A later writer, S. Lieberman, argued instead that the introduction of the fixed calendar was due to measures taken by Christian Roman authorities to prevent the Jewish patriarch from sending calendrical messengers.

Both the tradition that Hillel b. Yehuda instituted the complete computed calendar, and the theory that the computed calendar was introduced due to repression or persecution, have been questioned. Furthermore, two Jewish dates during post-Talmudic times (specifically in 506 and 776) are impossible under the rules of the modern calendar, indicating that its arithmetic rules were developed in Babylonia during the times of the Geonim (7th to 8th centuries). The Babylonian rules required the delay of the first day of Tishrei when the new moon occurred after noon.

Except for the epoch year number (the fixed reference point at the beginning of year 1, which at that time was one year later than the epoch of the modern calendar), the calendar rules reached their current form by the beginning of the 9th century, as described by the Persian Muslim astronomer al-Khwarizmi in 823. Al-Khwarizmi's study of the Jewish calendar describes the 19-year intercalation cycle, the rules for determining on what day of the week the first day of the month Tishrī shall fall, the interval between the Jewish era (creation of Adam) and the Seleucid era, and the rules for determining the mean longitude of the sun and the moon using the Jewish calendar. Not all the rules were in place by 835.

In 921, Aaron ben Meïr proposed changes to the calendar. Though the proposals were rejected, they indicate that all of the rules of the modern calendar (except for the epoch) were in place before that date. In 1000, the Muslim chronologist al-Biruni described all of the modern rules of the Hebrew calendar, except that he specified three different epochs used by various Jewish communities being one, two, or three years later than the modern epoch.

While imprisoned in Auschwitz, Jews made every effort to observe Jewish tradition in the camps, despite the monumental dangers in doing so. The Hebrew calendar, which is a tradition with great importance to Jewish practice and rituals was particularly dangerous since no tools of telling of time, such as watches and calendars were permitted in the camps. The keeping of a Hebrew calendar was a rarity amongst prisoners and there are only two known surviving calendars that were made in Auschwitz, both of which were made by women. Before this, the tradition of making a Hebrew calendar was greatly assumed to be the job of a man in Jewish society.

Early Zionist pioneers were impressed by the fact that the calendar preserved by Jews over many centuries in far-flung diasporas, as a matter of religious ritual, was geared to the climate of their original country: the Jewish New Year marks the transition from the dry season to the rainy one, and major Jewish holidays such as Sukkot, Passover, and Shavuot correspond to major points of the country's agricultural year such as planting and harvest.

Accordingly, in the early 20th century the Hebrew calendar was re-interpreted as an agricultural rather than religious calendar.

After the creation of the State of Israel, the Hebrew calendar became one of the official calendars of Israel, along with the Gregorian calendar. Holidays and commemorations not derived from previous Jewish tradition were to be fixed according to the Hebrew calendar date. For example, the Israeli Independence Day falls on 5 Iyar, Jerusalem Reunification Day on 28 Iyar, Yom HaAliyah on 10 Nisan, and the Holocaust Commemoration Day on 27 Nisan.

Nevertheless, since the 1950s usage of the Hebrew calendar has steadily declined, in favor of the Gregorian calendar. At present, Israelis—except for the religiously observant—conduct their private and public life according to the Gregorian calendar, although the Hebrew calendar is still widely acknowledged, appearing in public venues such as banks (where it is legal for use on cheques and other documents, though only rarely do people make use of this option) and on the mastheads of newspapers.

The Jewish New Year (Rosh Hashanah) is a two-day public holiday in Israel. However, since the 1980s an increasing number of secular Israelis celebrate the Gregorian New Year (usually known as "Silvester Night"—"ליל סילבסטר") on the night between 31 December and 1 January. Prominent rabbis have on several occasions sharply denounced this practice, but with no noticeable effect on the secularist celebrants.

Wall calendars commonly used in Israel are hybrids. Most are organised according to Gregorian rather than Jewish months, but begin in September, when the Jewish New Year usually falls, and provide the Jewish date in small characters.

Outside of Rabbinic Judaism, evidence shows a diversity of practice.

Karaites use the lunar month and the solar year, but the Karaite calendar differs from the current Rabbinic calendar in a number of ways. The Karaite calendar is identical to the Rabbinic calendar used before the Sanhedrin changed the Rabbinic calendar from the lunar, observation based, calendar to the current, mathematically based, calendar used in Rabbinic Judaism today.

In the lunar Karaite calendar, the beginning of each month, the Rosh Chodesh, can be calculated, but is confirmed by the observation in Israel of the first sightings of the new moon. This may result in an occasional variation of a maximum of one day, depending on the inability to observe the new moon. The day is usually "picked up" in the next month.

The addition of the leap month (Adar II) is determined by observing in Israel the ripening of barley at a specific stage (defined by Karaite tradition) (called aviv), rather than using the calculated and fixed calendar of rabbinic Judaism. Occasionally this results in Karaites being one month ahead of other Jews using the calculated rabbinic calendar. The "lost" month would be "picked up" in the next cycle when Karaites would observe a leap month while other Jews would not.

Furthermore, the seasonal drift of the rabbinic calendar is avoided, resulting in the years affected by the drift starting one month earlier in the Karaite calendar.

Also, the four rules of postponement of the rabbinic calendar are not applied, since they are not mentioned in the Tanakh. This can affect the dates observed for all the Jewish holidays in a particular year by one or two days.

In the Middle Ages many Karaite Jews outside Israel followed the calculated rabbinic calendar, because it was not possible to retrieve accurate aviv barley data from the land of Israel. However, since the establishment of the State of Israel, and especially since the Six-Day War, the Karaite Jews that have made "aliyah" can now again use the observational calendar.

The Samaritan community's calendar also relies on lunar months and solar years. Calculation of the Samaritan calendar has historically been a secret reserved to the priestly family alone, and was based on observations of the new crescent moon. More recently, a 20th-century Samaritan High Priest transferred the calculation to a computer algorithm. The current High Priest confirms the results twice a year, and then distributes calendars to the community.

The epoch of the Samaritan calendar is year of the entry of the Children of Israel into the Land of Israel with Joshua. The month of Passover is the first month in the Samaritan calendar, but the year number increments in the sixth month. Like in the Rabbinic calendar, there are seven leap years within each 19-year cycle. However, the Rabbinic and Samaritan calendars' cycles are not synchronized, so Samaritan festivals—notionally the same as the Rabbinic festivals of Torah origin—are frequently one month off from the date according to the Rabbinic calendar. Additionally, as in the Karaite calendar, the Samaritan calendar does not apply the four rules of postponement, since they are not mentioned in the Tanakh. This can affect the dates observed for all the Jewish holidays in a particular year by one or two days.

Many of the Dead Sea (Qumran) Scrolls have references to a unique calendar, used by the people there, who are often assumed to be Essenes.

The year of this calendar used the ideal Mesopotamian calendar of twelve 30-day months, to which were added 4 days at the equinoxes and solstices (cardinal points), making a total of 364 days.

There was some ambiguity as to whether the cardinal days were at the beginning of the months or at the end, but the clearest calendar attestations give a year of four seasons, each having three months of 30, 30, and 31 days with the cardinal day the extra day at the end, for a total of 91 days, or exactly 13 weeks. Each season started on the 4th day of the week (Wednesday), every year. (Ben-Dov, "Head of All Years", pp. 16–17)

With only 364 days, it is clear that the calendar would after a few years be very noticeably different from the actual seasons, but there is nothing to indicate what was done about this problem. Various suggestions have been made by scholars. One is that nothing was done and the calendar was allowed to change with respect to the seasons. Another suggestion is that changes were made irregularly, only when the seasonal anomaly was too great to be ignored any longer. (Ben-Dov, "Head of All Years", pp. 19–20)

The writings often discuss the moon, but the calendar was not based on the movement of the moon any more than indications of the phases of the moon on a modern western calendar indicate that that is a lunar calendar. Recent analysis of one of the last scrolls remaining to be deciphered has revealed it relates to this calendar and that the sect used the word "tekufah" to identify each of the four special days marking the transitions between the seasons.

Calendrical evidence for the postexilic Persian period is found in papyri from the Jewish colony at Elephantine, in Egypt. These documents show that the Jewish community of Elephantine used the Egyptian and Babylonian calendars.

The Sardica paschal table shows that the Jewish community of some eastern city, possibly Antioch, used a calendrical scheme that kept Nisan 14 within the limits of the Julian month of March. Some of the dates in the document are clearly corrupt, but they can be emended to make the sixteen years in the table consistent with a regular intercalation scheme. Peter, the bishop of Alexandria (early 4th century CE), mentions that the Jews of his city "hold their Passover according to the course of the moon in the month of Phamenoth, or according to the intercalary month every third year in the month of Pharmuthi", suggesting a fairly consistent intercalation scheme that kept Nisan 14 approximately between Phamenoth 10 (March 6 in the 4th century CE) and Pharmuthi 10 (April 5). Jewish funerary inscriptions from Zoar, south of the Dead Sea, dated from the 3rd to the 5th century, indicate that when years were intercalated, the intercalary month was at least sometimes a repeated month of Adar. The inscriptions, however, reveal no clear pattern of regular intercalations, nor do they indicate any consistent rule for determining the start of the lunar month.

In 1178, Maimonides included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, "Mishneh Torah". Today, the rules detailed in Maimonides' code are those generally used by Jewish communities throughout the world.

A "new moon" (astronomically called a lunar conjunction and, in Hebrew, a molad) is the moment at which the sun and moon are aligned horizontally with respect to a north-south line (technically, they have the same ecliptical longitude). The period between two new moons is a synodic month. The actual length of a synodic month varies from about 29 days 6 hours and 30 minutes (29.27 days) to about 29 days and 20 hours (29.83 days), a variation range of about 13 hours and 30 minutes. Accordingly, for convenience, a long-term average length, identical to the mean synodic month of ancient times (also called the molad interval) is used. The molad interval is formula_1 days, or 29 days, 12 hours, and 793 "parts" (1 "part" = / minute; 3 "parts" = 10 seconds) (i.e., 29.530594 days), and is the same value determined by the Babylonians in their System B about 300 BCE and was adopted by the Greek astronomer Hipparchus in the 2nd century BCE and by the Alexandrian astronomer Ptolemy in the "Almagest" four centuries later (who cited Hipparchus as his source). Its remarkable accuracy (less than one second from the true value) is thought to have been achieved using records of lunar eclipses from the 8th to 5th centuries BCE.

This value is as close to the correct value of 29.530589 days as it is possible for a value to come that is rounded off to whole "parts". The discrepancy makes the molad interval about 0.6 seconds too long. Put another way, if the molad is taken as the time of mean conjunction at some reference meridian, then this reference meridian is drifting slowly eastward. If this drift of the reference meridian is traced back to the mid-4th century, the traditional date of the introduction of the fixed calendar, then it is found to correspond to a longitude midway between the Nile and the end of the Euphrates. The modern molad moments match the mean solar times of the lunar conjunction moments near the meridian of Kandahar, Afghanistan, more than 30° east of Jerusalem.

Furthermore, the discrepancy between the molad interval and the mean synodic month is accumulating at an accelerating rate, since the mean synodic month is progressively shortening due to gravitational tidal effects. Measured on a strictly uniform time scale, such as that provided by an atomic clock, the mean synodic month is becoming gradually longer, but since the tides slow Earth's rotation rate even more, the mean synodic month is becoming gradually shorter in terms of mean solar time.

The mean year of the current mathematically based Hebrew calendar is 365 days 5 hours 55 minutes and 25+/ seconds (365.2468 days) – computed as the molad/monthly interval of 29.530594 days × 235 months in a 19-year metonic cycle ÷ 19 years per cycle. In relation to the Gregorian calendar, the mean Gregorian calendar year is 365 days 5 hours 49 minutes and 12 seconds (365.2425 days), and the drift of the Hebrew calendar in relation to it is about a day every 231 years.

Although the molad of Tishrei is the only molad moment that is not ritually announced, it is actually the only one that is relevant to the Hebrew calendar, for it determines the provisional date of Rosh Hashanah, subject to the Rosh Hashanah postponement rules. The other monthly molad moments are announced for mystical reasons. With the moladot on average almost 100 minutes late, this means that the molad of Tishrei lands one day later than it ought to in (100 minutes) ÷ (1440 minutes per day) = 5 of 72 years or nearly 7% of years.

Therefore, the seemingly small drift of the moladot is already significant enough to affect the date of Rosh Hashanah, which then cascades to many other dates in the calendar year and sometimes, due to the Rosh Hashanah postponement rules, also interacts with the dates of the prior or next year. The molad drift could be corrected by using a progressively shorter molad interval that corresponds to the actual mean lunar conjunction interval at the original molad reference meridian. Furthermore, the molad interval determines the calendar mean year, so using a progressively shorter molad interval would help correct the excessive length of the Hebrew calendar mean year, as well as helping it to "hold onto" the northward equinox for the maximum duration.

When the 19-year intercalary cycle was finalised in the 4th century, the earliest Passover (in year 16 of the cycle) coincided with the northward equinox, which means that Passover fell near the "first" full moon after the northward equinox, or that the northward equinox landed within one lunation before 16 days after the "molad" of "Nisan". This is still the case in about 80% of years; but, in about 20% of years, Passover is a month late by these criteria (as it was in AM 5765, 5768 and 5776, the 8th, 11th and 19th years of the 19-year cycle = Gregorian 2005, 2008 and 2016 CE). Presently, this occurs after the "premature" insertion of a leap month in years 8, 11, and 19 of each 19-year cycle, which causes the northward equinox to land on exceptionally early Hebrew dates in such years. This problem will get worse over time, and so beginning in AM 5817 (2057 CE), year 3 of each 19-year cycle will also be a month late. If the calendar is not amended, then Passover will start to land on or after the summer solstice around AM 16652 (12892 CE). In theory, the exact year when this will begin to occur depends on uncertainties in the future tidal slowing of the Earth rotation rate, and on the accuracy of predictions of precession and Earth axial tilt.
The seriousness of the spring equinox drift is widely discounted on the grounds that Passover will remain in the spring season for many millennia, and the text of the Torah is generally not interpreted as having specified tight calendrical limits. The Hebrew calendar also drifts with respect to the autumn equinox, and at least part of the harvest festival of Sukkot is already more than a month after the equinox in years 1, 9, and 12 of each 19-year cycle; beginning in AM 5818 (2057 CE), this will also be the case in year 4. (These are the same year numbers as were mentioned for the spring season in the previous paragraph, except that they get incremented at Rosh Hashanah.) This progressively increases the probability that Sukkot will be cold and wet, making it uncomfortable or impractical to dwell in the traditional "succah" during Sukkot. The first winter seasonal prayer for rain is not recited until "Shemini Atzeret", after the end of Sukkot, yet it is becoming increasingly likely that the rainy season in Israel will start before the end of Sukkot.

No equinox or solstice will ever be more than a day or so away from its mean date according to the solar calendar, while nineteen Jewish years average 6939d 16h 33m 03s compared to the 6939d 14h 26m 15s of nineteen mean tropical years. This discrepancy has mounted up to six days, which is why the earliest Passover currently falls on 26 March (as in AM 5773 / 2013 CE).

Given the length of the year, the length of each month is fixed as described above, so the real problem in determining the calendar for a year is determining the number of days in the year. In the modern calendar, this is determined in the following manner.

The day of Rosh Hashanah and the length of the year are determined by the time and the day of the week of the Tishrei "molad", that is, the moment of the average conjunction. Given the Tishrei "molad" of a certain year, the length of the year is determined as follows:

First, one must determine whether each year is an ordinary or leap year by its position in the 19-year Metonic cycle. Years 3, 6, 8, 11, 14, 17, and 19 are leap years.

Secondly, one must determine the number of days between the starting Tishrei "molad" (TM1) and the Tishrei "molad" of the next year (TM2). For calendar descriptions in general the day begins at 6 p.m., but for the purpose of determining Rosh Hashanah, a "molad" occurring on or after noon is treated as belonging to the next day (the first "deḥiyyah"). All months are calculated as 29d, 12h, 44m, 3s long (MonLen). Therefore, in an ordinary year TM2 occurs 12 × MonLen days after TM1. This is usually 354 calendar days after TM1, but if TM1 is on or after 3:11:20 a.m. and before noon, it will be 355 days. Similarly, in a leap year, TM2 occurs 13 × MonLen days after TM1. This is usually 384 days after TM1, but if TM1 is on or after noon and before 2:27:16 p.m., TM2 will be only 383 days after TM1. In the same way, from TM2 one calculates TM3. Thus the four natural year lengths are 354, 355, 383, and 384 days.

However, because of the holiday rules, Rosh Hashanah cannot fall on a Sunday, Wednesday, or Friday, so if TM2 is one of those days, Rosh Hashanah in year 2 is postponed by adding one day to year 1 (the second "deḥiyyah"). To compensate, one day is subtracted from year 2. It is to allow for these adjustments that the system allows 385-day years (long leap) and 353-day years (short ordinary) besides the four natural year lengths.

But how can year 1 be lengthened if it is already a long ordinary year of 355 days or year 2 be shortened if it is a short leap year of 383 days? That is why the third and fourth "deḥiyyah"s are needed.

If year 1 is already a long ordinary year of 355 days, there will be a problem if TM1 is on a Tuesday, as that means TM2 falls on a Sunday and will have to be postponed, creating a 356-day year. In this case, Rosh Hashanah in year 1 is postponed from Tuesday (the third "deḥiyyah"). As it cannot be postponed to Wednesday, it is postponed to Thursday, and year 1 ends up with 354 days.

On the other hand, if year 2 is already a short year of 383 days, there will be a problem if TM2 is on a Wednesday. because Rosh Hashanah in year 2 will have to be postponed from Wednesday to Thursday and this will cause year 2 to be only 382 days long. In this case, year 2 is extended by one day by postponing Rosh Hashanah in year 3 from Monday to Tuesday (the fourth "deḥiyyah"), and year 2 will have 383 days.

The attribution of the fixed arithmetic Hebrew calendar solely to Hillel II has, however, been questioned by a few authors, such as Sasha Stern, who claim that the calendar rules developed gradually over several centuries.

Given the importance in Jewish ritual of establishing the accurate timing of monthly and annual times, some futurist writers and researchers have considered whether a "corrected" system of establishing the Hebrew date is required. The mean year of the current mathematically based Hebrew calendar has "drifted" an average of 7–8 days late relative to the equinox relationship that it originally had. It is not possible, however, for any individual Hebrew date to be a week or more "late", because Hebrew months always begin within a day or two of the "molad" moment. What happens instead is that the traditional Hebrew calendar "prematurely" inserts a leap month one year before it "should have been" inserted, where "prematurely" means that the insertion causes the spring equinox to land more than 30 days before the latest acceptable moment, thus causing the calendar to run "one month late" until the time when the leap month "should have been" inserted prior to the following spring. This presently happens in 4 years out of every 19-year cycle (years 3, 8, 11, and 19), implying that the Hebrew calendar currently runs "one month late" more than 21% of the time.

Dr. Irv Bromberg has proposed a 353-year cycle of 4366 months, which would include 130 leap months, along with use of a progressively shorter "molad" interval, which would keep an amended fixed arithmetic Hebrew calendar from drifting for more than seven millennia. It takes about 3 centuries for the spring equinox to drift an average of th of a "molad" interval earlier in the Hebrew calendar. That is a very important time unit, because it can be cancelled by simply truncating a 19-year cycle to 11 years, omitting 8 years including three leap years from the sequence. That is the essential feature of the 353-year leap cycle ().

Religious questions abound about how such a system might be implemented and administered throughout the diverse aspects of the world Jewish community.

The list below gives a time which can be used to determine the day the Jewish ecclesiastical (spring) year starts over a period of nineteen years:

Every nineteen years this time is 2 days, 16 hours, 33 1/18 minutes later in the week. That is either the same or the previous day in the civil calendar, depending on whether the difference in the day of the week is three or two days. If 29 February is included fewer than five times in the nineteen – year period the date will be later by the number of days which corresponds to the difference between the actual number of insertions and five. If the year is due to start on Sunday, it actually begins on the following Tuesday if the following year is due to start on Friday morning. If due to start on Monday, Wednesday or Friday it actually begins on the following day. If due to start on Saturday, it actually begins on the following day if the previous year was due to begin on Monday morning.

The table below lists, for a Jewish year commencing on 23 March, the civil date of the first day of each month. If the year does not begin on 23 March, each month's first day will differ from the date shown by the number of days that the start of the year differs from 23 March. The correct column is the one which shows the correct starting date for the following year in the last row. If 29 February falls within a Jewish month the first day of later months will be a day earlier than shown.

For long period calculations, dates should be reduced to the Julian calendar and converted back to the civil calendar at the end of the calculation. The civil calendar used here (Exigian) is correct to one day in 44,000 years and omits the leap day in centennial years which do not give remainder 200 or 700 when divided by 900. It is identical to the Gregorian calendar between 15 October 1582 CE and 28 February 2400 CE (both dates inclusive).

To find how many days the civil calendar is ahead of the Julian in any year from 301 BCE (the calendar is proleptic [assumed] up to 1582 CE) add 300 to the year, multiply the hundreds by 7, divide by 9 and subtract 4. Ignore any fraction of a day. When the difference between the calendars changes the calculated value applies on and from March 1 (civil date) for conversions to Julian. For earlier dates reduce the calculated value by one. For conversions to the civil date the calculated value applies on and from February 29 (Julian date). Again, for earlier dates reduce the calculated value by one. The difference is applied to the calendar one is converting into. A negative value indicates that the Julian date is ahead of the civil date. In this case it is important to remember that when calculating the civil equivalent of February 29 (Julian), February 29 is discounted. Thus if the calculated value is −4 the civil equivalent of this date is February 24. Before 1 CE use astronomical years rather than years BCE. The astronomical year is (year BCE) – 1.

Up to the 4th century CE, these tables give the day of the Jewish month to within a day or so and the number of the month to within a month or so. From the 4th century, the number of the month is given exactly and from the 9th century the day of the month is given exactly as well.

In the Julian calendar, every 76 years the Jewish year is due to start 5h 47 14/18m earlier, and 3d 18h 12 4/18m later in the week.


On what civil date does the eighth month begin in CE 20874-5?

20874=2026+(248x76). In (248x76) Julian years the Jewish year is due to start (248x3d 18h 12 4/18m) later in the week, which is 932d 2h 31 2/18m or 1d 2h 31 2/18m later after removing complete weeks. Allowing for the current difference of thirteen days between the civil and Julian calendars, the Julian date is 13+(248x0d 5h 47 4/18m) earlier, which is 72d 21h 28 16/18m earlier. Convert back to the civil calendar by applying the formula.

So, in 20874 CE, the Jewish year is due to begin 87d 2h 31 2/18m later than in 2026 CE and 1d 2h 31 2/18m later in the week. In 20874 CE, therefore, the Jewish year is due to begin at 11.30 3/18 A.M. on Friday, 14 June. Because of the displacements, it actually begins on Saturday, 15 June. Odd months have 30 days and even months 29, so the starting dates are 2, 15 July; 3, 13 August; 4, 12 September; 5, 11 October; 6, 10 November; 7, 9 December, and 8, 8 January.

The rules are based on the theory that Maimonides explains in his book "Rabbinical Astronomy" – no allowance is made for the secular (centennial) decrease of ½ second in the length of the mean tropical year and the increase of about four yards in the distance between the earth and the moon resulting from tidal friction because astronomy was not sufficiently developed in the 12th century (when Maimonides wrote his book) to detect this.


723–730.




</doc>
<doc id="13786" url="https://en.wikipedia.org/wiki?curid=13786" title="The Holocaust Industry">
The Holocaust Industry

The Holocaust Industry: Reflections on the Exploitation of Jewish Suffering is a 2000 book by Norman G. Finkelstein, in which the author argues that the American Jewish establishment exploits the memory of the Nazi Holocaust for political and financial gain, as well as to further the interests of Israel. According to Finkelstein, this "Holocaust industry" has corrupted Jewish culture and the authentic memory of the Holocaust.

Finkelstein states that his consciousness of "the Nazi holocaust" is rooted in his parents' experiences in the Warsaw Ghetto; with the exception of his parents themselves, "every family member on both sides was exterminated by the Nazis". Nonetheless, during his childhood, no one ever asked any questions about what his mother and father had suffered. He suggests, "This was not a respectful silence. It was indifference." It was only after the establishment of "the Holocaust industry", he suggests, that outpourings of anguish over the plight of the Jews in World War II began. This ideology in turn served to endow Israel with a status as "'victim' state" despite its "horrendous" human rights record.

According to Finkelstein, his book is "an anatomy and an indictment of the Holocaust industry". He argues that "'The Holocaust' is an ideological representation of the Nazi holocaust".

In the foreword to the first paperback edition, Finkelstein notes that the first hardback edition had been a considerable hit in several European countries and many languages, but had been largely ignored in the United States. He sees "The New York Times" as the main promotional vehicle of the "Holocaust industry", and notes that the 1999 Index listed 273 entries for the Holocaust and just 32 entries for the entire continent of Africa.

The second (2003) edition contained 100 pages of new material, primarily in chapter 3 on the World Jewish Congress lawsuit against Swiss banks. Finkelstein set out to provide a guide to the relevant sections of the case. He feels that the presiding judge elected not to docket crucial documents, and that the Claims Resolution Tribunal could no longer be trusted. Finkelstein claims the CRT was on course to vindicate the Swiss banks before it changed tack in order to "protect the blackmailers' reputation".

The critical response has been varied. In addition to prominent supporters, such as Noam Chomsky and Alexander Cockburn, the Holocaust historian Raul Hilberg is on record as praising Finkelstein's book: 

On the other hand, many have argued that "The Holocaust Industry" is an unscholarly work that promotes antisemitic stereotypes. For example, according to Israeli journalist Yair Sheleg, in August 2000, German historian Hans Mommsen called it "a most trivial book, which appeals to easily aroused anti-Semitic prejudices." Wolfgang Benz stated to "Le Monde": "It is impossible to learn anything from Finkelstein's book. At best, it is interesting for a psychotherapist." The reviewer of this daily added that Norman Finkelstein "hardly cares about nuance" and Rony Brauman wrote in the preface to the French edition ("L'Industrie de l'Holocauste", Paris, La Fabrique, 2001) that some assertions of N. Finkelstein (especially on the impact of the Six-days war) are wrong, others being pieces of "propaganda".

University of Chicago Professor Peter Novick, whose work Finkelstein described as providing the "initial stimulus" for "The Holocaust Industry", asserted in the July 28, 2000 issue of "The Jewish Chronicle" (London) that the book is replete with "false accusations", "egregious misrepresentations", "absurd claims" and "repeated mis-statements" ("A charge into darkness that sheds no light"). Finkelstein replied to the allegations by Novick on his homepage.

Hasia Diner has accused Peter Novick and Finkelstein of being "harsh critics of American Jewry from the left," and challenges the notion reflected in their books that American Jews did not begin to commemorate the Holocaust until after 1967.

Andrew Ross, reviewing the book for "Salon", wrote:

Finkelstein responded to his critics in the foreword to the second edition:

Finkelstein describes two known frauds, that of "The Painted Bird" by Polish writer Jerzy Kosinski and "Fragments" by Binjamin Wilkomirski, and how they were defended by people even after they had been exposed. He identifies some of these people as members of the "Holocaust Industry", and notes that they also support each other. Elie Wiesel supported Kosinski; Israel Gutman and Daniel Goldhagen (see below) supported Wilkomirski; Wiesel and Gutman support Goldhagen.

Finkelstein scathingly compared the media treatment of the Holocaust and the media treatment of other genocides such as the Holodomor and the Armenian Genocide, particularly by members of what he calls "The Holocaust Industry". 1 to 1.5 million Armenians died in the years between 1915 and 1917/1923 - denial includes the claim that they were the result of a civil war within World War I, or refusal to accept there were deaths. In 2001, Israeli Foreign Minister Shimon Peres went so far as to dismiss it as "allegations". However, by this time historical consensus was changing, and he was "angrily compared ... to a holocaust denier" by Israel Charny, executive director of the Institute on the Holocaust and Genocide in Jerusalem.

According to Finkelstein, Elie Wiesel characterized any suggestion that he has profited from the "Holocaust Industry", or even any criticism at all, as Holocaust denial. Questioning a survivor's testimony, denouncing the role of Jewish collaborators, suggesting that Germans suffered during the bombing of Dresden or that any state except Germany committed crimes in World War II are all evidence of Holocaust denial – according to Deborah Lipstadt – and the most "insidious" forms of Holocaust denial are "immoral equivalencies", denying the uniqueness of The Holocaust. Finkelstein examines the implications of applying this standard to another member of the "Holocaust Industry", Daniel Goldhagen, who argued that Serbian actions in Kosovo "are, in their essence, different from those of Nazi Germany only in scale".

According to Finkelstein, Deborah Lipstadt claims there is widespread Holocaust denial - yet in "Denying the Holocaust" (1993) her prime example is Arthur Butz, author of "The Hoax of the Twentieth Century". The chapter on him is entitled "Entering the Mainstream" - but Finkelstein considers that, were it not for the likes of Lipstadt, no one would ever have heard of Arthur Butz. Holocaust deniers have as much influence in the US as the Flat Earth Society (p. 69). Finkelstein believes there to be only one "truly mainstream" holocaust denier—Bernard Lewis, who was convicted in France of denying the Armenian genocide. Since Lewis is pro-Israel, "this instance ... raises no hackles in the United States."

Publishing history of "The Holocaust Industry":




</doc>
<doc id="13787" url="https://en.wikipedia.org/wiki?curid=13787" title="Hermetic Order of the Golden Dawn">
Hermetic Order of the Golden Dawn

The Hermetic Order of the Golden Dawn (; or, more commonly, the Golden Dawn ("Aurora Aurea")) was an organization devoted to the study and practice of the occult, metaphysics, and paranormal activities during the late 19th and early 20th centuries. Known as a magical order, the Hermetic Order of the Golden Dawn was active in Great Britain and focused its practices on theurgy and spiritual development. Many present-day concepts of ritual and magic that are at the centre of contemporary traditions, such as Wicca and Thelema, were inspired by the Golden Dawn, which became one of the largest single influences on 20th-century Western occultism.

The three founders, William Robert Woodman, William Wynn Westcott and Samuel Liddell Mathers, were Freemasons. Westcott appears to have been the initial driving force behind the establishment of the Golden Dawn.

The Golden Dawn system was based on hierarchy and initiation like the Masonic lodges; however women were admitted on an equal basis with men. The "Golden Dawn" was the first of three Orders, although all three are often collectively referred to as the "Golden Dawn". The First Order taught esoteric philosophy based on the Hermetic Qabalah and personal development through study and awareness of the four Classical Elements as well as the basics of astrology, tarot divination, and geomancy. The Second or "Inner" Order, the "Rosae Rubeae et Aureae Crucis" (the Ruby Rose and Cross of Gold), taught magic, including scrying, astral travel, and alchemy. The Third Order was that of the "Secret Chiefs", who were said to be highly skilled; they supposedly directed the activities of the lower two orders by spirit communication with the Chiefs of the Second Order.

The foundational documents of the original Order of the Golden Dawn, known as the Cipher Manuscripts, are written in English using the Trithemius cipher. The manuscripts give the specific outlines of the Grade Rituals of the Order and prescribe a curriculum of graduated teachings that encompass the Hermetic Qabalah, astrology, occult tarot, geomancy, and alchemy.

According to the records of the Order, the manuscripts passed from Kenneth R. H. Mackenzie, a Masonic scholar, to the Rev. A. F. A. Woodford, whom British occult writer Francis King describes as the fourth founder (although Woodford died shortly after the Order was founded). The documents did not excite Woodford, and in February 1886 he passed them on to Freemason William Wynn Westcott, who managed to decode them in 1887. Westcott, pleased with his discovery, called on fellow Freemason Samuel Liddell MacGregor Mathers for a second opinion. Westcott asked for Mathers' help to turn the manuscripts into a coherent system for lodge work. Mathers in turn asked fellow Freemason William Robert Woodman to assist the two, and he accepted. Mathers and Westcott have been credited with developing the ritual outlines in the Cipher Manuscripts into a workable format. Mathers, however, is generally credited with the design of the curriculum and rituals of the Second Order, which he called the "Rosae Rubae et Aureae Crucis" ("Ruby Rose and Golden Cross" or the "RR et AC").

In October 1887, Westcott claimed to have written to a German countess and prominent Rosicrucian named Anna Sprengel, whose address was said to have been found in the decoded Cipher Manuscripts. According to Westcott, Sprengel claimed the ability to contact certain supernatural entities, known as the Secret Chiefs, that were considered the authorities over any magical order or esoteric organization. Westcott purportedly received a reply from Sprengel granting permission to establish a Golden Dawn temple and conferring honorary grades of Adeptus Exemptus on Westcott, Mathers, and Woodman. The temple was to consist of the five grades outlined in the manuscripts.

In 1888, the Isis-Urania Temple was founded in London. In contrast to the S.R.I.A. and Masonry, women were allowed and welcome to participate in the Order in "perfect equality" with men. The Order was more of a philosophical and metaphysical teaching order in its early years. Other than certain rituals and meditations found in the Cipher manuscripts and developed further, "magical practices" were generally not taught at the first temple.

For the first four years, the Golden Dawn was one cohesive group later known as "the Outer Order" or "First Order." An "Inner Order" was established and became active in 1892. The Inner Order consisted of members known as "adepts," who had completed the entire course of study for the Outer Order. This group of adepts eventually became known as the Second Order.

Eventually, the Osiris temple in Weston-super-Mare, the Horus temple in Bradford (both in 1888), and the Amen-Ra temple in Edinburgh (1893) were founded. In 1893 Mathers founded the Ahathoor temple in Paris.

In 1891, Westcott's alleged correspondence with Anna Sprengel suddenly ceased. He claimed to have received word from Germany that she was either dead or that her companions did not approve of the founding of the Order and no further contact was to be made. If the founders were to contact the Secret Chiefs, apparently, it had to be done on their own. In 1892, Mathers professed that a link to the Secret Chiefs had been established. Subsequently, he supplied rituals for the Second Order, calling them the Red Rose and Cross of Gold. The rituals were based on the tradition of the tomb of Christian Rosenkreuz, and a "Vault of Adepts" became the controlling force behind the Outer Order. Later in 1916, Westcott claimed that Mathers also constructed these rituals from materials he received from Frater Lux ex Tenebris, a purported "Continental Adept".

Some followers of the Golden Dawn tradition believe that the Secret Chiefs were not human or supernatural beings but, rather, symbolic representations of actual or legendary sources of spiritual esotericism. The term came to stand for a great leader or teacher of a spiritual path or practice that found its way into the teachings of the Order.

By the mid-1890s, the Golden Dawn was well established in Great Britain, with over one hundred members from every class of Victorian society. Many celebrities belonged to the Golden Dawn, such as the actress Florence Farr, the Irish revolutionary Maud Gonne, the Irish poet William Butler Yeats, the Welsh author Arthur Machen, and the English authors Evelyn Underhill and Aleister Crowley.

In 1896 or 1897, Westcott broke all ties to the Golden Dawn, leaving Mathers in control. It has been speculated that his departure was due to his having lost a number of occult-related papers in a hansom cab. Apparently, when the papers were found, Westcott's connection to the Golden Dawn was discovered and brought to the attention of his employers. He may have been told to either resign from the Order or to give up his occupation as coroner. After Westcott's departure, Mathers appointed Florence Farr to be Chief Adept in Anglia. Dr. Henry B. Pullen Burry succeeded Westcott as Cancellarius—one of the three Chiefs of the Order.

Mathers was the only active founding member after Westcott's departure. Due to personality clashes with other members and frequent absences from the center of Lodge activity in Great Britain, however, challenges to Mathers's authority as leader developed among the members of the Second Order.

Toward the end of 1899, the Adepts of the Isis-Urania and Amen-Ra temples had become dissatisfied with Mathers' leadership, as well as his growing friendship with Aleister Crowley. They had also become anxious to make contact with the Secret Chiefs themselves, instead of relying on Mathers as an intermediary. Within the Isis-Urania temple, disputes were arising between Farr's "The Sphere", a secret society within the Isis-Urania, and the rest of the Adepti Minores.

Crowley was refused initiation into the Adeptus Minor grade by the London officials. Mathers overrode their decision and quickly initiated him at the Ahathoor temple in Paris on January 16, 1900. Upon his return to the London temple, Crowley requested from Miss Cracknell, the acting secretary, the papers acknowledging his grade, to which he was now entitled. To the London Adepts, this was the final straw. Farr, already of the opinion that the London temple should be closed, wrote to Mathers expressing her wish to resign as his representative, although she was willing to carry on until a successor was found. Mathers believed Westcott was behind this turn of events and replied on February 16. On March 3, a committee of seven Adepts was elected in London, and requested a full investigation of the matter. Mathers sent an immediate reply, declining to provide proof, refusing to acknowledge the London temple, and dismissing Farr as his representative on March 23. In response, a general meeting was called on March 29 in London to remove Mathers as chief and expel him from the Order.

In 1901, W. B. Yeats privately published a pamphlet titled "Is the Order of R. R. & A. C. to Remain a Magical Order?"
After the Isis-Urania temple claimed its independence, there were even more disputes, leading to Yeats resigning. A committee of three was to temporarily govern, which included P.W. Bullock, M.W. Blackden and J. W. Brodie-Innes. After a short time, Bullock resigned, and Dr. Robert Felkin took his place.

In 1903, A. E. Waite and Blackden joined forces to retain the name Isis-Urania, while Felkin and other London members formed the Stella Matutina. Yeats remained in the Stella Matutina until 1921, while Brodie-Innes continued his Amen-Ra membership in Edinburgh.

Once Mathers realised that reconciliation was impossible, he made efforts to reestablish himself in London. The Bradford and Weston-super-Mare temples remained loyal to him, but their numbers were few. He then appointed Edward Berridge as his representative. According to Francis King, historical evidence shows that there were "twenty three members of a flourishing Second Order under Berridge-Mathers in 1913."

J.W. Brodie-Innes continued leading the Amen-Ra temple, deciding that the revolt was unjustified. By 1908, Mathers and Brodie-Innes were in complete accord. According to sources that differ regarding the actual date, sometime between 1901 and 1913 Mathers renamed the branch of the Golden Dawn remaining loyal to his leadership to Alpha et Omega. Brodie-Innes assumed command of the English and Scottish temples, while Mathers concentrated on building up his Ahathoor temple and extending his American connections. According to occultist Israel Regardie, the Golden Dawn had spread to the United States of America before 1900 and a Thoth-Hermes temple had been founded in Chicago. By the beginning of the First World War in 1914, Mathers had established two to three American temples.

Most temples of the Alpha et Omega and Stella Matutina closed or went into abeyance by the end of the 1930s, with the exceptions of two Stella Matutina temples: Hermes Temple in Bristol, which operated sporadically until 1970, and the Smaragdum Thallasses Temple (commonly referred to as Whare Ra) in Havelock North, New Zealand, which operated regularly until its closure in 1978.

Much of the hierarchical structure for the Golden Dawn came from the Societas Rosicruciana in Anglia, which was itself derived from the Order of the Golden and Rosy Cross.




The paired numbers attached to the Grades relate to positions on the Tree of Life. The Neophyte Grade of "0=0" indicates no position on the Tree. In the other pairs, the first numeral is the number of steps up from the bottom (Malkuth), and the second numeral is the number of steps down from the top (Kether).

The First Order Grades were related to the four elements of Earth, Air, Water, and Fire, respectively. The Aspirant to a Grade received instruction on the metaphysical meaning of each of these Elements and had to pass a written examination and demonstrate certain skills to receive admission to that Grade.

The Portal Grade was an "Invisible" or in-between grade separating the First Order from the Second Order.


While no temples in the original chartered lineage of the Golden Dawn survived past the 1970s, several organizations have since revived its teachings and rituals. Among these, the following are notable:





</doc>
<doc id="13790" url="https://en.wikipedia.org/wiki?curid=13790" title="Hash function">
Hash function

A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. Hash functions are often used in combination with a hash table, a common data structure used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file. One such application is finding similar stretches in DNA sequences. They are also useful in cryptography. A cryptographic hash function allows one to easily verify whether some input data map onto a given hash value, but if the input data is unknown it is deliberately difficult to reconstruct it (or any equivalent alternatives) by knowing the stored hash value. This is used for assuring integrity of transmitted data, and is the building block for HMACs, which provide message authentication.

Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The HashKeeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalogue of file fingerprints than of hash values.

Hash functions are used in hash tables, to quickly locate a data record (e.g., a dictionary definition) given its search key (the headword). Specifically, the hash function is used to map the search key to a list; the index gives the place in the hash table where the corresponding record should be stored. Hash tables are also used to implement associative arrays and dynamic sets.

Typically, the domain of a hash function (the set of possible keys) is larger than its range (the number of different table indices), and so it will map several different keys to the same index which could result in collisions. So then, each slot of a hash table is associated with (implicitly or explicitly) a set of records, rather than a single record. For this reason, each slot of a hash table is often called a "bucket", and hash values are also called "bucket listing" or a "bucket index".

Thus, the hash function only hints at the record's location. Still, in a half-full table, a good hash function will typically narrow the search down to only one or two entries.

People who write complete hash table implementations choose a specific hash function—such as a Jenkins hash or Zobrist hashing—and independently choose a hash-table collision resolution scheme—such as
coalesced hashing,
cuckoo hashing, or
hopscotch hashing.

Hash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items. This is also used in file comparison.

Hash functions are an essential ingredient of the Bloom filter, a space-efficient probabilistic data structure that is used to test whether an element is a member of a set.

When storing records in a large unsorted file, one may use a hash function to map each record to an index into a table "T", and to collect in each bucket "T"["i"] a list of the numbers of all records with the same hash value "i". Once the table is complete, any two duplicate records will end up in the same bucket. The duplicates can then be found by scanning every bucket "T"["i"] which contains two or more members, fetching those records, and comparing them. With a table of appropriate size, this method is likely to be much faster than any alternative approach (such as sorting the file and comparing all consecutive pairs).

A hash value can be used to uniquely identify secret information. This requires that the hash function is collision-resistant, which means that it is very hard to find data that will generate the same hash value. These functions are categorized into cryptographic hash functions and provably secure hash functions. Functions in the second category are the most secure but also too slow for most practical purposes. Collision resistance is accomplished in part by generating very large hash values. For example, SHA-2, one of the most widely used cryptographic hash functions, generates 256-bit values.

Hash functions can also be used to locate table records whose key is similar, but not identical, to a given key; or pairs of records in a large file which have similar keys. For that purpose, one needs a hash function that maps similar keys to hash values that differ by at most "m", where "m" is a small integer (say, 1 or 2). If one builds a table "T" of all record numbers, using such a hash function, then similar records will end up in the same bucket, or in nearby buckets. Then one need only check the records in each bucket "T"["i"] against those in buckets "T"["i"+"k"] where "k" ranges between −"m" and "m".

This class includes the so-called acoustic fingerprint algorithms, that are used to locate similar-sounding entries in large collection of audio files. For this application, the hash function must be as insensitive as possible to data capture or transmission errors, and to trivial changes such as timing and volume changes, compression, etc.

The same techniques can be used to find equal or similar stretches in a large collection of strings, such as a document repository or a genomic database. In this case, the input strings are broken into many small pieces, and a hash function is used to detect potentially equal pieces, as above.

The Rabin–Karp algorithm is a relatively fast string searching algorithm that works in O("n") time on average. It is based on the use of hashing to compare strings.

This principle is widely used in computer graphics, computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space, such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database, and so on. In these applications, the set of all inputs is some sort of metric space, and the hashing function can be interpreted as a partition of that space into a grid of "cells". The table is often an array with two or more indices (called a "grid file", "grid index", "bucket grid", and similar names), and the hash function returns an index tuple. This special case of hashing is known as geometric hashing or "the grid method". Geometric hashing is also used in telecommunications (usually under the name vector quantization) to encode and compress multi-dimensional signals.

Some standard applications that employ hash functions include authentication, message integrity (using an HMAC (Hashed MAC)), message fingerprinting, data corruption detection, and digital signature efficiency.

Good hash functions, in the original sense of the term, are usually required to satisfy certain properties listed below. The exact requirements are dependent on the application. For example, a hash function well suited to indexing data will probably be a poor choice for a cryptographic hash function.

A hash procedure must be deterministic—meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection), although sometimes rehashing of the item is possible.

The determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash is still a valid hash function when used within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ.

A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability. The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of "collisions"—pairs of inputs that are mapped to the same hash value—increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries.

Note that this criterion only requires the value to be "uniformly distributed", not "random" in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function, but the converse need not be true.

Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries.

In other words, if a typical set of "m" records is hashed to "n" table slots, the probability of a bucket receiving many more than "m"/"n" records should be vanishingly small. In particular, if "m" is less than "n", very few buckets should have more than one or two records. (In an ideal "perfect hash function", no bucket should have more than one record; but a small number of collisions is virtually inevitable, even if "n" is much larger than "m" – see the birthday problem).

When testing a hash function, the uniformity of the distribution of hash values can be evaluated by the chi-squared test.

It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. On the other hand, cryptographic hash functions produce much larger hash values, in order to ensure the computational complexity of brute-force inversion. For example, SHA-1, one of the most widely used cryptographic hash functions, produces a 160-bit value.

Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value. In cryptographic hash functions, these chunks are processed by a one-way compression function, with the last chunk being padded if necessary. In this case, their size, which is called "block size", is much bigger than the size of the hash value. For example, in SHA-1, the hash value is 160 bits and the block size 512 bits.

In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters—the input data "z", and the number "n" of allowed hash values.

A common solution is to compute a fixed hash function with a very large range (say, 0 to 2 − 1), divide the result by "n", and use the division's remainder. If "n" is itself a power of 2, this can be done by bit masking and bit shifting. When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and "n" − 1, for any value of "n" that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of "n", e.g. odd or prime numbers.

We can allow the table size "n" to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let "n" be significantly less than 2. Consider a pseudorandom number generator (PRNG) function "P"(key) that is uniform on the interval [0, 2 − 1]. A hash function uniform on the interval [0, n-1] is "n" "P"(key)/2. We can replace the division by a (possibly faster) right bit shift: "nP"(key) » "b".

When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table.

A hash function that will relocate the minimum number of records when the table is – where "z" is the key being hashed and "n" is the number of allowed hash values – such that "H"("z","n" + 1) = "H"("z","n") with probability close to "n"/("n" + 1).

Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property.

Extendible hashing uses a dynamic hash function that requires space proportional to "n" to compute the hash function, and it becomes a function of the previous keys that have been inserted.

Several algorithms that preserve the uniformity property but require time proportional to "n" to compute the value of "H"("z","n") have been invented.

A hash function with minimal movement is especially useful in distributed hash tables.

In some applications, the input data may contain features that are irrelevant for comparison purposes. For example, when looking up a personal name, it may be desirable to ignore the distinction between upper and lower case letters. For such data, one must use a hash function that is compatible with the data equivalence criterion being used: that is, any two inputs that are considered equivalent must yield the same hash value. This can be accomplished by normalizing the input before hashing it, as by upper-casing all letters.

"A hash function that is used to search for similar (as opposed to equivalent) data must be as continuous as possible; two inputs that differ by a little should be mapped to equal or nearly equal hash values."

Note that continuity is usually considered a fatal flaw for checksums, cryptographic hash functions, and other related concepts. Continuity is desirable for hash functions only in some applications, such as hash tables used in Nearest neighbor search.

In cryptographic applications, hash functions are typically expected to be practically non-invertible, meaning that it is not realistic to reconstruct the input datum from its hash value () alone without spending great amounts of computing time (see also One-way function).

For most types of hashing functions, the choice of the function depends strongly on the nature of the input data, and their probability distribution in the intended application.

If the data to be hashed is small enough, one can use the data itself (reinterpreted as an integer) as the hashed value. The cost of computing this "trivial" (identity) hash function is effectively zero. This hash function is perfect, as it maps each input to a distinct hash value.

The meaning of "small enough" depends on the size of the type that is used as the hashed value. For example, in Java, the hash code is a 32-bit integer. Thus the 32-bit integer codice_1 and 32-bit floating-point codice_2 objects can simply use the value directly; whereas the 64-bit integer codice_3 and 64-bit floating-point codice_4 cannot use this method.

Other types of data can also use this perfect hashing scheme. For example, when mapping character strings between upper and lower case, one can use the binary encoding of each character, interpreted as an integer, to index a table that gives the alternative form of that character ("A" for "a", "8" for "8", etc.). If each character is stored in 8 bits (as in extended ASCII or ISO Latin 1), the table has only 2 = 256 entries; in the case of Unicode characters, the table would have 17×2 = entries.

The same technique can be used to map two-letter country codes like "us" or "za" to country names (26 = 676 table entries), 5-digit zip codes like 13083 to city names ( entries), etc. Invalid data values (such as the country code "xx" or the zip code 00000) may be left undefined in the table or mapped to some appropriate "null" value.

A hash function that is injective—that is, maps each valid input to a different hash value—is said to be perfect. With such a function one can directly locate the desired entry in a hash table, without any additional searching.

A perfect hash function for "n" keys is said to be minimal if its range consists of "n" "consecutive" integers, usually from 0 to "n"−1. Besides providing single-step lookup, a minimal perfect hash function also yields a compact hash table, without any vacant slots. Minimal perfect hash functions are much harder to find than perfect ones with a wider range.

If the inputs are bounded-length strings and each input may independently occur with uniform probability (such as telephone numbers, car license plates, invoice numbers, etc.), then a hash function needs to map roughly the same number of inputs to each hash value. For instance, suppose that each input is an integer "z" in the range 0 to "N"−1, and the output must be an integer "h" in the range 0 to "n"−1, where "N" is much larger than "n". Then the hash function could be "h" = "z" mod "n" (the remainder of "z" divided by "n"), or "h" = ("z" × "n") ÷ "N" (the value "z" scaled down by "n"/"N" and truncated to an integer), or many other formulas.

These simple formulas will not do if the input values are not equally likely, or are not independent. For instance, most patrons of a supermarket will live in the same geographic area, so their telephone numbers are likely to begin with the same 3 to 4 digits. In that case, if "m" is 10000 or so, the division formula ("z" × "m") ÷ "M", which depends mainly on the leading digits, will generate a lot of collisions; whereas the remainder formula "z" mod "m", which is quite sensitive to the trailing digits, may still yield a fairly even distribution.

When the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, vary characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.

In cryptographic hash functions, a Merkle–Damgård construction is usually used. In general, the scheme for hashing such data is to break the input into a sequence of small units (bits, bytes, words, etc.) and combine all the units "b"[1], "b"[2], ..., "b"["m"] sequentially, as follows

This schema is also used in many text checksum and fingerprint algorithms. The state variable "S" may be a 32- or 64-bit unsigned integer; in that case, "S0" can be 0, and F("S","n") can be just "S" mod "n". The best choice of "F" is a complex issue and depends on the nature of the data. If the units "b"["k"] are single bits, then "F"("S","b") could be, for instance

Here "highbit"("S") denotes the most significant bit of "S"; the '*' operator denotes unsigned integer multiplication with lost overflow; '^' is the bitwise exclusive or operation applied to words; and "P" is a suitable fixed word.

In many cases, one can design a special-purpose (heuristic) hash function that yields many fewer collisions than a good general-purpose hash function. For example, suppose that the input data are file names such as FILE0000.CHK, FILE0001.CHK, FILE0002.CHK, etc., with mostly sequential numbers. For such data, a function that extracts the numeric part "k" of the file name and returns "k" mod "n" would be nearly optimal. Needless to say, a function that is exceptionally good for a specific kind of data may have dismal performance on data with different distribution.

In some applications, such as substring search, one must compute a hash function "h" for every "k"-character substring of a given "n"-character string "t"; where "k" is a fixed integer, and "n" is greater than "k". The straightforward solution, which is to extract every such substring "s" of "t" and compute "h"("s") separately, requires a number of operations proportional to "k"·"n". However, with the proper choice of "h", one can use the technique of rolling hash to compute all those hashes with an effort proportional to "k" + "n".

A universal hashing scheme is a randomized algorithm that selects a hashing function "h" among a family of such functions, in such a way that the probability of a collision of any two distinct keys is 1/"n", where "n" is the number of distinct hash values desired—independently of the two keys. Universal hashing ensures (in a probabilistic sense) that the hash function application will behave as well as if it were using a random function, for any distribution of the input data. It will, however, have more collisions than perfect hashing and may require more operations than a special-purpose hash function. See also unique permutation hashing.

One can adapt certain checksum or fingerprinting algorithms for use as hash functions. Some of those algorithms will map arbitrarily long string data "z", with any typical real-world distribution—no matter how non-uniform and dependent—to a 32-bit or 64-bit string, from which one can extract a hash value in 0 through "n" − 1.

This method may produce a sufficiently uniform distribution of hash values, as long as the hash range size "n" is small compared to the range of the checksum or fingerprint function. However, some checksums fare poorly in the avalanche test, which may be a concern in some applications. In particular, the popular CRC32 checksum provides only 16 bits (the higher half of the result) that are usable for hashing. Moreover, each bit of the input has a deterministic effect on each bit of the CRC32 output; that is, one can tell without looking at the rest of the input which bits of the output will flip if the input bit is flipped, so care must be taken to use all 32 bits when computing the hash from the checksum.

Multiplicative hashing is a simple type of hash function often used by teachers introducing students to hash tables. Standard multiplicative hashing uses the formula formula_1 which produces a hash value in formula_2. The value formula_3 is an appropriately chosen value that should be relatively prime to formula_4. An important practical special case occurs when formula_5 and formula_6 are powers of 2 and formula_7 is the machine word size. In this case this formula becomes formula_8. This is special because arithmetic modulo formula_9 is done by default in low-level programming languages and integer division by a power of 2 is simply a right-shift, so, in C, for example, this function becomes
and for fixed formula_10 and formula_7 this translates into a single integer multiplication and right-shift making it one of the fastest hash functions to compute. Furthermore, if formula_3 is a uniformly random "odd" integer in formula_13 then this hash function is nearly universal in the sense that, for any formula_14, formula_15.

In many applications, such as hash tables, collisions make the system a little slower but are otherwise harmless.
In such systems, it is often better to use hash functions based on multiplication—such as MurmurHash and the SBoxHash—or even simpler hash functions such as CRC32—and tolerate more collisions; rather than use a more complex hash function that avoids many of those collisions but takes longer to compute. Multiplicative hashing is susceptible to a "common mistake" that leads to poor diffusion—higher-value input bits do not affect lower-value output bits.

Some cryptographic hash functions, such as SHA-1, have even stronger uniformity guarantees than checksums or fingerprints, and thus can provide very good general-purpose hashing functions.

In ordinary applications, this advantage may be too small to offset their much higher cost. However, this method can provide uniformly distributed hashes even when the keys are chosen by a malicious agent. This feature may help to protect services against denial of service attacks.

Tables of random numbers (such as 256 random 32-bit integers) can provide high-quality nonlinear functions to be used as hash functions or for other purposes such as cryptography. The key to be hashed is split into 8-bit (one-byte) parts, and each part is used as an index for the nonlinear table. The table values are then added by arithmetic or XOR addition to the hash output value. Because the table is just 1024 bytes in size, it fits into the cache of modern microprocessors and allows very fast execution of the hashing algorithm. As the table value is on average much longer than 8 bits, one bit of input affects nearly all output bits.

This algorithm has proven to be very fast and of high quality for hashing purposes (especially hashing of integer-number keys).

Modern microprocessors will allow for much faster processing, if 8-bit character strings are not hashed by processing one character at a time, but by interpreting the string as an array of 32 bit or 64 bit integers and hashing/accumulating these "wide word" integer values by means of arithmetic operations (e.g. multiplication by constant and bit-shifting). The remaining characters of the string which are smaller than the word length of the CPU must be handled differently (e.g. being processed one character at a time).

This approach has proven to speed up hash code generation by a factor of five or more on modern microprocessors of
a word size of 64 bit.

Another approach is to convert strings to a 32 or 64 bit numeric value and then apply a hash function. One method that avoids the problem of strings having great similarity ("Aaaaaaaaaa" and "Aaaaaaaaab") is to use a Cyclic redundancy check (CRC) of the string to compute a 32- or 64-bit value. While it is possible that two different strings will have the same CRC, the likelihood is very small and only requires that one check the actual string found to determine whether one has an exact match. CRCs will be different for strings such as "Aaaaaaaaaa" and "Aaaaaaaaab". Although CRC codes can be used as hash values, they are not cryptographically secure, because they are not collision-resistant.

Locality-sensitive hashing (LSH) is a method of performing probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). This is different from the conventional hash functions, such as those used in cryptography, as in this case the goal is to minimize the probability of "collision" of every item.

One example of LSH is MinHash algorithm used for finding similar documents (such as web-pages):

Let "h" be a hash function that maps the members of and to distinct integers, and for any set "S" define to be the member of with the minimum value of . Then exactly when the minimum hash value of the union lies in the intersection .
Therefore,
In other words, if is a random variable that is one when and zero otherwise, then is an unbiased estimator of , although it has too high a variance to be useful on its own. The idea of the MinHash scheme is to reduce the variance by averaging together several variables constructed in the same way.

The term "hash" offers a natural analogy with its non-technical meaning (to "chop" or "make a mess" out of something), given how hash functions scramble their input data to derive their output. In his research for the precise origin of the term, Donald Knuth notes that, while Hans Peter Luhn of IBM appears to have been the first to use the concept of a hash function in a memo dated January 1953, the term itself would only appear in published literature in the late 1960s, on Herbert Hellerman's "Digital Computer System Principles", even though it was already widespread jargon by then.



</doc>
<doc id="13791" url="https://en.wikipedia.org/wiki?curid=13791" title="High jump">
High jump

The high jump is a track and field event in which competitors must jump unaided over a horizontal bar placed at measured heights without dislodging it. In its modern most practised format, a bar is placed between two standards with a crash mat for landing. In the modern era, athletes run towards the bar and use the Fosbury Flop method of jumping, leaping head first with their back to the bar. Since ancient times, competitors have introduced increasingly effective techniques to arrive at the current form.

The discipline is, alongside the pole vault, one of two vertical clearance events to feature on the Olympic athletics programme. It is contested at the World Championships in Athletics and IAAF World Indoor Championships, and is a common occurrence at track and field meetings. The high jump was among the first events deemed acceptable for women, having been held at the 1928 Olympic Games.

Javier Sotomayor (Cuba) is the current men's record holder with a jump of set in 1993 – the longest standing record in the history of the men's high jump. Stefka Kostadinova (Bulgaria) has held the women's world record at since 1987, also the longest-held record in the event.
The rules for the high jump are set internationally by the International Association of Athletics Federations (IAAF). Jumpers must take off on one foot. A jump is considered a failure if the bar is dislodged by the action of the jumper whilst jumping or the jumper touches the ground or breaks the plane of the near edge of the bar before clearance. The technique one uses for the jump must be almost flawless in order to have a chance of clearing a high bar.

Competitors may begin jumping at any height announced by the chief judge, or may pass, at their own discretion. Most competitions state that three consecutive missed jumps, at any height or combination of heights, will eliminate the jumper from competition.

The victory goes to the jumper who clears the greatest height during the final. Tie-breakers are used for any place in which scoring occurs. If two or more jumpers tie for one of these places, the tie-breakers are: 1) the fewest misses at the height at which the tie occurred; and 2) the fewest misses throughout the competition.

If the event remains tied for first place (or a limited advancement position to a subsequent meet), the jumpers have a jump-off, beginning at the next greater height. Each jumper has one attempt. The bar is then alternately lowered and raised until only one jumper succeeds at a given height.

The first recorded high jump event took place in Scotland in the 19th century. Early jumpers used either an elaborate straight-on approach or a scissors technique. In latter years, soon then after, the bar was approached diagonally, and the jumper threw first the inside leg and then the other over the bar in a scissoring motion. Around the turn of the 20th century, techniques began to change, beginning with the Irish-American Michael Sweeney's "Eastern cut-off". By taking off like the scissors and extending his spine and flattening out over the bar, Sweeney raised the world record to in 1895.

Another American, George Horine, developed an even more efficient technique, the "Western roll". In this style, the bar again is approached on a diagonal, but the inner leg is used for the take-off, while the outer leg is thrust up to lead the body sideways over the bar. Horine increased the world standard to in 1912. His technique was predominant through the Berlin Olympics of 1936, in which the event was won by Cornelius Johnson at .

American and Soviet jumpers were the most successful for the next four decades, and they pioneered the evolution of the straddle technique. Straddle jumpers took off as in the Western roll, but rotated their (belly-down) torso around the bar, obtaining the most efficient and highest clearance (of the bar) up to that time. Straddle-jumper, Charles Dumas, was the first to clear 7 feet (2.13 m), in 1956, and American John Thomas pushed the world mark to in 1960. Valeriy Brumel took over the event for the next four years. The elegant Soviet jumper radically sped up his approach run, took the record up to , and won the Olympic gold medal in 1964, before a motorcycle accident ended his career.

American coaches, including two-time NCAA champion Frank Costello of the University of Maryland, flocked to Russia to learn from Brumel and his coaches. However, it would be a solitary innovator at Oregon State University, Dick Fosbury, who would bring the high jump into the next century. Taking advantage of the raised, softer landing areas by then in use, Fosbury added a new twist to the outmoded Eastern Cut-off. He directed himself over the bar head and shoulders first, sliding over on his back and landing in a fashion which would likely have broken his neck in the old, sawdust landing pits. After he used this Fosbury flop to win the 1968 Olympic gold medal, the technique began to spread around the world, and soon "floppers" were dominating international high jump competitions. The last straddler to set a world record was Vladimir Yashchenko, who cleared in 1977 and then indoors in 1978.

Among renowned high jumpers following Fosbury's lead were Americans Dwight Stones and his rival, tall Franklin Jacobs of Paterson, NJ, who cleared , over his head (a feat equalled 27 years later by Sweden's Stefan Holm); Chinese record-setters Ni-chi Chin and Zhu Jianhua; Germans Gerd Wessig and Dietmar Mögenburg; Swedish Olympic medalist and former world record holder Patrik Sjöberg; and female jumpers Iolanda Balaş of Romania, Ulrike Meyfarth of Germany and Italy's Sara Simeoni.

The approach run of the high jump may actually be more important than the take-off. If a high jumper runs with bad timing or without enough aggression, clearing a high bar becomes more of a challenge. The approach requires a certain shape or curve, the right amount of speed, and the correct number of strides. The approach angle is also critical for optimal height.

Most great straddle jumpers have a run at angles of about 30 to 40 degrees. The length of the run is determined by the speed of the person's approach. A slower run requires about 8 strides. However, a faster high jumper might need about 13 strides. A greater run speed allows a greater part of the body's forward momentum to be converted upward.

The J type approach, favored by Fosbury floppers, allows for horizontal speed, the ability to turn in the air (centripetal force), and good take-off position. This allows for horizontal momentum to turn into vertical momentum, propelling the jumper off the ground and over the bar. The approach should be a hard controlled stride so that a person does not fall from creating an angle with speed. Athletes should run tall and lean on the curve, from the ankles and not the hips. This allows the correct angle to force their hips to rotate during take-off, which allows their center of gravity to pass under the bar.

Unlike the classic straddle technique, where the take-off foot is "planted" in the same spot at every height, flop-style jumpers must adjust their take-off as the bar is raised. Their approach run must be adjusted slightly so that their take-off spot is slightly further out from the bar in order to allow their hips to clear the bar while still maintaining enough momentum to carry their legs across the bar. Jumpers attempting to reach record heights commonly fail when most of their energy is directed into the vertical effort, and they brush the bar off the standards with the backs of their legs as they stall out in mid-air.

An effective approach shape can be derived from physics. For example, the rate of backward spin required as the jumper crosses the bar to facilitate shoulder clearance on the way up and foot clearance on the way down can be determined by computer simulation. This rotation rate can be back-calculated to determine the required angle of lean away from the bar at plant, based on how long the jumper is on the take-off foot. This information, together with the jumper's speed in the curve, can be used to calculate the radius of the curved part of the approach. This is a lot of work and requires measurements of running speed and time of take-off foot on the ground. However, one can work in the opposite direction by assuming an approach radius and watching the resulting backward rotation. This only works if some basic rules are followed in how one executes the approach and take-off. 
Drills can be practiced to solidify the approach. One drill is to run in a straight line (the linear part of the approach) and then run two to three circles spiraling into one another. Another is to run or skip a circle of any size, two to three times in a row. It is important to train to leap upwards without first leaning into the bar, allowing the momentum of the J approach to carry the body across the bar.

In competition the winner is the person who cleared the highest height. In case of a tie, fewer failed attempts at that height are better: "i.e.", the jumper who makes a height on his or her first attempt is placed ahead of someone who clears the same height on the second or third attempt. If there still is a tie, all the failed attempts at lower heights are added up, and the one with the fewest total misses is declared the winner. If still tied, a playoff is held. Starting height is the next higher height after the overjumped one. If all the competitors clear the height, the bar is raised , and if they fail, the bar is lowered 2 cm. That continues until only one competitor succeeds in overjumping that height, and he or she is declared the winner.


In high jump, it helps if the athlete is tall, has long legs, and limited weight on their body. They must have a strong lower body and flexibility helps a lot as well. High jumpers tend to go through very vigorous training methods to achieve this ideal body frame.

High jumpers must have a fast approach so it is crucial to work on speed and also speed endurance. Lots of high jump competitions may take hours and athletes must make sure they have the endurance to last the entire competition. Common sprint endurance workouts for high jumpers include 200-, 400-, and 800-meter training. Other speed endurance training methods such as hill training or a ladder workout may also be used.

It is crucial for high jumpers to have strong lower bodies and cores, as the bar progressively gets higher, the strength of an athlete's legs (along with speed and technique) will help propel them over the bar. Squats, deadlifts, and core exercises will help a high jumper achieve these goals. It is important, however, for a high jumper to keep a slim figure as any unnecessary weight makes it difficult to jump higher.

Arguably the most important training for a high jumper is plyometric training. Because high jump is such a technical event, any mistake in the technique could either lead to failure, injury, or both. To prevent these from happening, high jumpers tend to focus heavily on plyometrics. This includes hurdle jumps, flexibility training, skips, or scissor kick training. Plyometric workouts tend to be performed at the beginning of the workout.




Athletes who have won multiple titles at the two most important competitions, the Olympic Games and the World Championships:


Kostadinova and Sotomayor are the only high jumpers to have been Olympic Champion, World Champion and broken the world record.


All time lists of athletes with the highest recorded jumps above their own height.

, 67 different female athletes had ever been able to jump .





</doc>
<doc id="13792" url="https://en.wikipedia.org/wiki?curid=13792" title="Heraclitus">
Heraclitus

Heraclitus of Ephesus (; ; () was a pre-Socratic Greek philosopher, and a native of the city of Ephesus, then part of the Persian Empire. He was of distinguished parentage. Little is known about his early life and education, but he regarded himself as self-taught and a pioneer of wisdom. From the lonely life he led, and still more from the apparently riddled and allegedly paradoxical nature of his philosophy and his stress upon the heedless unconsciousness of humankind, he was called "The Obscure" and the "Weeping Philosopher".

Heraclitus was famous for his insistence on ever-present change as being the fundamental essence of the universe, as stated in the famous saying, "No man ever steps in the same river twice" (see "panta rhei" below). This is commonly considered to be a key contribution in the development of the philosophical concept of becoming, as contrasted with "being", and has sometimes been seen in a dialectical relationship with Parmenides' statement that "whatever is, is, and what is not cannot be", the latter being understood as a key contribution in the development of the philosophical concept of being. For this reason, Parmenides and Heraclitus are commonly considered to be two of the founders of ontology. Scholars have generally believed that either Parmenides was responding to Heraclitus, or Heraclitus to Parmenides, though opinion on who was responding to whom has varied over the course of the 20th and 21st centuries. Heraclitus' position was complemented by his stark commitment to a unity of opposites in the world, stating that "the path up and down are one and the same". Through these doctrines Heraclitus characterized all existing entities by pairs of contrary properties, whereby no entity may ever occupy a single state at a single time. This, along with his cryptic utterance that "all entities come to be in accordance with this "Logos"" (literally, "word", "reason", or "account") has been the subject of numerous interpretations.

The main source for the life of Heraclitus is Diogenes Laërtius, although some have questioned the validity of his account as "a tissue of Hellenistic anecdotes, most of them obviously fabricated on the basis of statements in the preserved fragments". Diogenes said that Heraclitus flourished in the 69th Olympiad, 504–501 BCE. All the rest of the evidence—the people Heraclitus is said to have known, or the people who were familiar with his work—confirms the "floruit". His dates of birth and death are based on a life span of 60 years, the age at which Diogenes says he died, with the floruit in the middle.

Heraclitus was born to an aristocratic family in Ephesus, in the Persian Empire, in what is now called present-day Efes, Turkey. His father was named either Blosôn or Herakôn. Diogenes says that he abdicated the kingship ("basileia") in favor of his brother and Strabo confirms that there was a ruling family in Ephesus descended from the Ionian founder, Androclus, which still kept the title and could sit in the chief seat at the games, as well as a few other privileges. How much power the king had is another question. Ephesus had been part of the Persian Empire since 547 and was ruled by a satrap, a more distant figure, as the Great King allowed the Ionians considerable autonomy. Diogenes says that Heraclitus used to play knucklebones with the youths in the temple of Artemis and when asked to start making laws he refused saying that the constitution ("politeia") was "ponêra", which can mean either that it was fundamentally wrong or that he considered it toilsome. Two extant letters between Heraclitus and Darius I, quoted by Diogenes, are undoubtedly later forgeries.

With regard to education, Diogenes says that Heraclitus was "wondrous" ("thaumasios", which, as Socrates explains in Plato's "Theaetetus" and "Gorgias", is the beginning of philosophy) from childhood. Diogenes relates that Sotion said he was a "hearer" of Xenophanes, which contradicts Heraclitus' statement (so says Diogenes) that he had taught himself by questioning himself. Burnet states in any case that "... Xenophanes left Ionia before Herakleitos was born." Diogenes relates that as a boy Heraclitus had said he "knew nothing" but later claimed to "know everything". His statement that he "heard no one" but "questioned himself", can be placed alongside his statement that "the things that can be seen, heard and learned are what I prize the most."

Diogenes relates that Heraclitus had a poor opinion of human affairs. He believed that Hesiod and Pythagoras lacked understanding though learned and that Homer and Archilochus deserved to be beaten. Laws needed to be defended as though they were city walls. Timon of Phlius is said to have called him a "mob-reviler". Heraclitus hated the Athenians and his fellow Ephesians, wishing the latter wealth in punishment for their wicked ways. According to Diogenes Laërtius: "Finally, he became a hater of his kind ("misanthrope") and wandered the mountains [...] making his diet of grass and herbs."

Heraclitus' life as a philosopher was interrupted by dropsy. The physicians he consulted were unable to prescribe a cure. Diogenes lists various stories about Heraclitus' death: In two versions, Heraclitus was cured of the dropsy and died of another disease. In one account, however, the philosopher "buried himself in a cowshed, expecting that the noxious damp humour would be drawn out of him by the warmth of the manure", while another says he treated himself with a liniment of cow manure and, after a day prone in the sun, died and was interred in the marketplace. According to Neathes of Cyzicus, after smearing himself with dung, Heraclitus was devoured by dogs.

He died after 478 BC from a hydropsy.

Diogenes states that Heraclitus' work was "a continuous treatise "On Nature", but was divided into three discourses, one on the universe, another on politics, and a third on theology." Theophrastus says (in Diogenes) "...some parts of his work [are] half-finished, while other parts [made] a strange medley."

Diogenes also tells us that Heraclitus deposited his book as a dedication in the great temple of Artemis, the Artemisium, one of the largest temples of the 6th century BCE and one of the Seven Wonders of the Ancient World. Ancient temples were regularly used for storing treasures, and were open to private individuals under exceptional circumstances; furthermore, many subsequent philosophers in this period refer to the work. Says Kahn: "Down to the time of Plutarch and Clement, if not later, the little book of Heraclitus was available in its original form to any reader who chose to seek it out." Diogenes says: "the book acquired such fame that it produced partisans of his philosophy who were called Heracliteans."

As with the other pre-Socratics, his writings survive now only in quoted by other authors. These are catalogued using the Diels–Kranz numbering system.

At some time in antiquity he acquired this epithet denoting that his major sayings were difficult to understand. According to Diogenes Laërtius, Timon of Phlius called him "the Riddler" (; ), and explained that Heraclitus wrote his book "rather unclearly" ("asaphesteron") so that only the "capable" should attempt it. By the time of Cicero he had become "the dark" (; ) because he had spoken "nimis obscurē", "too obscurely", concerning nature and had done so deliberately in order to be misunderstood. The customary English translation of follows the Latin, "the Obscure".

Diogenes Laërtius ascribes the theory that Heraclitus did not complete some of his works because of melancholia to Theophrastus. Later he was referred to as the "weeping philosopher", as opposed to Democritus, who is known as the "laughing philosopher". If Stobaeus writes correctly, Sotion in the early 1st century CE was already combining the two in the imaginative duo of weeping and laughing philosophers: "Among the wise, instead of anger, Heraclitus was overtaken by tears, Democritus by laughter." The view is expressed by the satirist Juvenal:

The motif was also adopted by Lucian of Samosata in his "Sale of Creeds", in which the duo is sold together as a complementary product in the satirical auction of philosophers. Subsequently, they were considered an indispensable feature of philosophic landscapes. Montaigne proposed two archetypical views of human affairs based on them, selecting Democritus' for himself. The weeping philosopher may have been mentioned in William Shakespeare's "The Merchant of Venice". Donato Bramante painted a fresco, "Democritus and Heraclitus," in Casa Panigarola in Milan.

"The idea that all things come to pass in accordance with this "Logos"" and "the "Logos" is common," is expressed in two famous but obscure fragments:
This "Logos" holds always but humans always prove unable to understand it, both before hearing it and when they have first heard it. For though all things come to be in accordance with this "Logos", humans are like the inexperienced when they experience such words and deeds as I set out, distinguishing each in accordance with its nature and saying how it is. But other people fail to notice what they do when awake, just as they forget what they do while asleep. (DK 22B1)
For this reason it is necessary to follow what is common. But although the "Logos" is common, most people live as if they had their own private understanding. (DK 22B2)
The meaning of "Logos" also is subject to interpretation: "word", "account", "principle", "plan", "formula", "measure", "proportion", "reckoning." Though Heraclitus "quite deliberately plays on the various meanings of "logos"", there is no compelling reason to suppose that he used it in a special technical sense, significantly different from the way it was used in ordinary Greek of his time.

The later Stoics understood it as "the account which governs everything," and Hippolytus, in the 3rd century CE, identified it as meaning the Christian "Word of God".

The phrase (panta rhei) "everything flows" either was spoken by Heraclitus or survived as a quotation of his. This famous aphorism used to characterize Heraclitus' thought comes from Simplicius, a neoplatonist, and from Plato's "Cratylus". The word "rhei" (as in rheology) is the Greek word for "to stream", and is etymologically related to Rhea according to Plato's "Cratylus".

The philosophy of Heraclitus is summed up in his cryptic utterance: 
<br>"Potamoisi toisin autoisin embainousin, hetera kai hetera hudata epirrei"<br>"Ever-newer waters flow on those who step into the same rivers." 
The quote from Heraclitus appears in Plato's "Cratylus" twice; in 401d as:
<br>"Ta onta ienai te panta kai menein ouden"<br>"All entities move and nothing remains still"and in 402a

<br>"Panta chōrei kai ouden menei kai dis es ton auton potamon ouk an embaies"
"Everything changes and nothing remains still ... and ... you cannot step twice into the same stream"
Instead of "flow" Plato uses "chōrei", "to change place" (; ).

The assertions of flow are coupled in many fragments with the enigmatic river image:

<br>"We both step and do not step in the same rivers. We are and are not."

Compare with the Latin adages "Omnia mutantur" and "Tempora mutantur" (8 CE) and the Japanese tale "Hōjōki," (1200 CE) which contains the same image of the changing river, and the central Buddhist doctrine of impermanence.

However, the German classicist and philosopher interprets this fragment as an indication by Heraclitus, for the world as a steady constant: "You will not find anything, in which the river remains constant. [...] Just the fact, that there is a particular river bed, that there is a source and a estuary etc. is something, that stays identical. And this is [...] the concept of a river".

In the structure "anō katō" is more accurately translated as a hyphenated word: "the upward-downward path". They go on simultaneously and instantaneously and result in "hidden harmony". A way is a series of transformations: the , "turnings of fire", first into sea, then half of sea to earth and half to rarefied air.

The transformation is a replacement of one element by another: "The death of fire is the birth of air, and the death of air is the birth of water."
This world, which is the same for all, no one of gods or men has made. But it always was and will be: an ever-living fire, with measures of it kindling, and measures going out. This latter phraseology is further elucidated:All things are an interchange for fire, and fire for all things, just like goods for gold and gold for goods.

Heraclitus considered fire as the most fundamental element. He believed fire gave rise to the other elements and thus to all things. He regarded the soul as being a mixture of fire and water, with fire being the noble part of the soul, and water the ignoble part. A soul should therefore aim toward becoming more full of fire and less full of water: a "dry" soul was best. According to Heraclitus, worldly pleasures made the soul "moist", and he considered mastering one's worldly desires to be a noble pursuit which purified the soul's fire. Norman Melchert interpreted Heraclitus as using "fire" metaphorically, in lieu of "Logos", as the origin of all things.

If objects are new from moment to moment so that one can never touch the same object twice, then each object must dissolve and be generated continually momentarily and an object is a harmony between a building up and a tearing down. Heraclitus calls the oppositional processes (), "strife", and hypothesizes that the apparently stable state, (), or "justice", is a harmony of it:We must know that war ( "polemos") is common to all and strife is justice, and that all things come into being through strife necessarily.
As Diogenes explains:All things come into being by conflict of opposites, and the sum of things ( "ta hola", "the whole") flows like a stream.
In the bow metaphor Heraclitus compares the resultant to a strung bow held in shape by an equilibrium of the string tension and spring action of the bow:There is a harmony in the bending back ( "palintropos") as in the case of the bow and the lyre.

People must "follow the common" ( ) and not live having "their own judgement ("phronēsis")". He distinguishes between human laws and divine law ( ). By "God" Heraclitus does not mean the Judeo-Christian version of a single God as "primum movens" of all things, God as Creator, but the divine as opposed to human; the immortal as opposed to the mortal, the cyclical as opposed to the transient. It is more accurate to speak of "the Divine" and not of "God".

He removes the human sense of justice from his concept of God; i.e., humanity is not the image of God: "To God all things are fair and good and just, but people hold some things wrong and some right." God's custom has wisdom but human custom does not, and yet both humans and God are childish (inexperienced): "human opinions are children's toys" and "Eternity is a child moving counters in a game; the kingly power is a child's."

Wisdom is "to know the thought by which all things are steered through all things", which must not imply that people are or can be wise. Only Zeus is wise. To some degree then Heraclitus seems to be in the mystic's position of urging people to follow God's plan without much of an idea what that may be. In fact there is a note of despair: "The fairest universe ( ) is but a heap of rubbish ( ) piled up ( , i.e. "poured out") at random ( "aimlessly")."

This influential quote by Heraclitus "" (DK 22B119) has led to numerous interpretations. Whether in this context "daimon" can indeed be translated to mean "fate" is disputed; however, it lends much sense to Heraclitus' observations and conclusions about human nature in general. While the translation with "fate" is generally accepted as in Kahn's "a man's character is his divinity", in some cases, it may also stand for the soul of the departed.

To Heraclitus, a perceived object is a harmony between two fundamental units of change, a waxing and a waning. He typically uses the ordinary word "to become" ("gignesthai" or "ginesthai", present tense or aorist tense of the verb, with the root sense of "being born"), which led to his being characterized as the philosopher of becoming rather than of being. He recognizes the fundamental changing of objects with the flow of time.

Plato argues against Heraclitus as follows:How can that be a real thing which is never in the same state? ... for at the moment that the observer approaches, then they become other ... so that you cannot get any further in knowing their nature or state ... but if that which knows and that which is known exist ever ... then I do not think they can resemble a process or flux ...

In Plato one experienced unit is a state, or object existing, which can be observed. The time parameter is set at "ever"; that is, the state is to be presumed present between observations. Change is to be deduced by comparing observations and is thus presumed a function that "happens to" objects already in being, rather than something ontologically essential to them (such that something that does not change cannot exist) as in Heraclitus. In Plato, no matter how many of those experienced units you are able to tally, you cannot get through the mysterious gap between them to account for the change that must be occurring there. This limitation is considered a fundamental limitation of reality by Plato and in part underpins his differentiation between imperfect experience from more perfect Forms. The fact that this is no limitation for Heraclitus motivates Plato's condemnation.

Stoicism was a philosophical school which flourished between the 3rd century BC and about the 3rd century AD. It began among the Greeks and became the major philosophy of the Roman Empire before declining with the rise of Christianity in the 3rd century.

Throughout their long tenure the Stoics believed that the major tenets of their philosophy derived from the thought of Heraclitus. According to Long, "the importance of Heraclitus to later Stoics is evident most plainly in Marcus Aurelius." Explicit connections of the earliest Stoics to Heraclitus showing how they arrived at their interpretation are missing but they can be inferred from the Stoic fragments, which Long concludes are "modifications of Heraclitus."

The Stoics were interested in Heraclitus' treatment of fire. In addition to seeing it as the most fundamental of the four elements and the one that is quantified and determines the quantity ("logos") of the other three, he presents fire as the cosmos, which was not made by any of the gods or men, but "was and is and ever shall be ever-living fire." Fire is both a substance and a motivator of change, it is active in altering other things quantitatively and performing an activity Heraclitus describes as "the judging and convicting of all things." It is "the thunderbolt that steers the course of all things." There is no reason to interpret the judgement, which is actually "to separate" (κρίνειν "krinein"), as outside of the context of "strife is justice" (see subsection above).

The earliest surviving Stoic work, the "Hymn to Zeus" of Cleanthes, though not explicitly referencing Heraclitus, adopts what appears to be the Heraclitean logos modified. Zeus rules the universe with law ("nomos") wielding on its behalf the "forked servant", the "fire" of the "ever-living lightning." So far nothing has been said that differs from the Zeus of Homer. But then, says Cleanthes, Zeus uses the fire to "straighten out the common logos" that travels about ("phoitan", "to frequent") mixing with the greater and lesser lights (heavenly bodies). This is Heraclitus' logos, but now it is confused with the "common "nomos"", which Zeus uses to "make the wrong ("perissa", left or odd) right ("artia", right or even)" and "order ("kosmein") the disordered ("akosma")."

The Stoic modification of Heraclitus' idea of the Logos was also influential on Jewish philosophers such as Philo of Alexandria, who connected it to "Wisdom personified" as God's creative principle. Philo uses the term Logos throughout his treatises on Hebrew Scripture in a manner clearly influenced by the Stoics.

The church fathers were the leaders of the early Christian Church during its first five centuries of existence, roughly contemporaneous to Stoicism under the Roman Empire. The works of dozens of writers in hundreds of pages have survived.

All of them had something to say about the Christian form of the Logos. The Catholic Church found it necessary to distinguish between the Christian logos and that of Heraclitus as part of its ideological distancing from paganism. The necessity to convert by defeating paganism was of paramount importance. Hippolytus of Rome therefore identifies Heraclitus along with the other Pre-Socratics (and Academics) as sources of heresy. Church use of the methods and conclusions of ancient philosophy as such was as yet far in the future, even though many were converted philosophers.

In "Refutation of All Heresies" Hippolytus says: "What the blasphemous folly is of Noetus, and that he devoted himself to the tenets of Heraclitus the Obscure, not to those of Christ." Hippolytus then goes on to present the inscrutable DK B67: "God ("theos") is day and night, winter and summer, ... but he takes various shapes, just as fire, when it is mingled with spices, is named according to the savor of each." The fragment seems to support pantheism if taken literally. German physicist and philosopher Max Bernard Weinstein classed these views with pandeism.

Hippolytus condemns the obscurity of it. He cannot accuse Heraclitus of being a heretic so he says instead: "Did not (Heraclitus) the Obscure anticipate Noetus in framing a system ...?" The apparent pantheist deity of Heraclitus (if that is what DK B67 means) must be equal to the union of opposites and therefore must be corporeal and incorporeal, divine and not-divine, dead and alive, etc., and the Trinity can only be reached by some sort of illusory shape-shifting.

The Christian apologist Justin Martyr, however, took a much more positive view of him. In his First Apology, he said both Socrates and Heraclitus were Christians before Christ: "those who lived reasonably are Christians, even though they have been thought atheists; as, among the Greeks, Socrates and Heraclitus, and men like them." 

The following articles on other topics contain non-trivial information that relates to Heraclitus in some way.





</doc>
<doc id="13793" url="https://en.wikipedia.org/wiki?curid=13793" title="Harrison Schmitt">
Harrison Schmitt

Harrison Hagan "Jack" Schmitt (born July 3, 1935) is an American geologist, retired NASA astronaut, university professor, former U.S. senator from New Mexico, and, as a crew member of Apollo 17, the most recent living person to have walked on the Moon.

In December 1972, as one of the crew on board Apollo 17, Schmitt became the first member of NASA's first scientist-astronaut group to fly in space. As Apollo 17 was the last of the Apollo missions, he also became the twelfth and second-youngest person to set foot on the Moon, and the second-to-last person to step off of the Moon (he boarded the Lunar Module shortly before commander Eugene Cernan). Schmitt also remains the first and only professional scientist to have flown beyond low Earth orbit and to have visited the Moon. He was influential within the community of geologists supporting the Apollo program and, before starting his own preparations for an Apollo mission, had been one of the scientists training those Apollo astronauts chosen to visit the lunar surface.

Schmitt resigned from NASA in August 1975 in order to run for election to the United States Senate as a member from New Mexico. As the Republican candidate in the 1976 election, he defeated the two-term Democratic incumbent Joseph Montoya. In 1982, Schmitt was defeated by Democrat Jeff Bingaman.

Born July 3, 1935, in Santa Rita, New Mexico, Schmitt grew up in nearby Silver City, and is a graduate of the Western High School (class of 1953). He received a B.S. degree in geology from the California Institute of Technology in 1957 and then spent a year studying geology at the University of Oslo in Norway. He received a Ph.D. in geology from Harvard University in 1964, based on his geological field studies in Norway.

Before joining NASA as a member of the first group of scientist-astronauts in June 1965, he worked at the U.S. Geological Survey's Astrogeology Center at Flagstaff, Arizona, developing geological field techniques that would be used by the Apollo crews. Following his selection, Schmitt spent his first year at Air Force UPT learning to become a jet pilot. Upon his return to the astronaut corps in Houston, he played a key role in training Apollo crews to be geologic observers when they were in lunar orbit and competent geologic field workers when they were on the lunar surface. After each of the landing missions, he participated in the examination and evaluation of the returned lunar samples and helped the crews with the scientific aspects of their mission reports.

Schmitt spent considerable time becoming proficient in the CSM and LM systems. In March 1970 he became the first of the scientist-astronauts to be assigned to space flight, joining Richard F. Gordon, Jr. (Commander) and Vance Brand (Command Module Pilot) on the Apollo 15 backup crew. The flight rotation put these three in line to fly as prime crew on the third following mission, Apollo 18. When Apollo 18 and Apollo 19 were cancelled in September 1970, the community of lunar geologists supporting Apollo felt so strongly about the need to land a professional geologist on the Moon, that they pressured NASA to reassign Schmitt to a remaining flight. As a result, Schmitt was assigned in August 1971 to fly on the last mission, Apollo 17, replacing Joe Engle as Lunar Module Pilot. Schmitt landed on the Moon with commander Gene Cernan in December 1972.

Schmitt claims to have taken the photograph of the Earth known as "The Blue Marble", possibly one of the most widely distributed photographic images in existence. NASA officially credits the image to the entire Apollo 17 crew.

While on the Moon's surface, Schmitt — the only geologist in the astronaut corps — collected the rock sample designated Troctolite 76535, which has been called "without doubt the most interesting sample returned from the Moon". Among other distinctions, it is the central piece of evidence suggesting that the Moon once possessed an active magnetic field.

As he returned to the Lunar Module before Cernan, Schmitt is the next-to-last person to have walked on the Moon's surface.

After the completion of Apollo 17, Schmitt played an active role in documenting the Apollo geologic results and also took on the task of organizing NASA's Energy Program Office.

On August 30, 1975, Schmitt resigned from NASA to seek election as a Republican to the United States Senate representing New Mexico in the 1976 election. Schmitt campaigned for fourteen months, and his campaign focused on the future.

In the Republican primary, held on June 1, 1976, Schmitt defeated Eugene Peirce. In the election, Schmitt opposed two-term Democratic incumbent Joseph Montoya. He defeated Montoya 57% to 42%.

He served one term and, notably, was the ranking Republican member of the Science, Technology, and Space Subcommittee.

He sought a second term in 1982, facing state Attorney General Jeff Bingaman. Bingaman attacked Schmitt for not paying enough attention to local matters; his campaign slogan asked, "What on Earth has he done for you lately?" This, combined with the deep recession, proved too much for Schmitt to overcome; he was defeated, 54% to 46%.

Following his Senate term, Schmitt has been a consultant in business, geology, space, and public policy.

Schmitt is an adjunct professor of engineering physics at the University of Wisconsin–Madison, and has long been a proponent of lunar resource utilization. In 1997 he proposed the Interlune InterMars Initiative, listing among its goals the advancement of private-sector acquisition and use of lunar resources, particularly lunar helium-3 as a fuel for notional nuclear fusion reactors.

Schmitt was chair of the NASA Advisory Council, whose mandate is to provide technical advice to the NASA Administrator, from November 2005 until his abrupt resignation on October 16, 2008. In November 2008, he quit the Planetary Society over policy advocacy differences, citing the organization's statements on "focusing on Mars as the driving goal of human spaceflight" (Schmitt said that going back to the Moon would speed progress toward a manned Mars mission), on "accelerating research into global climate change through more comprehensive Earth observations" (Schmitt voiced objections to the notion of a present "scientific consensus" on climate change as any policy guide), and on international cooperation (which he felt would retard rather than accelerate progress), among other points of divergence.

In January 2011, he was appointed as secretary of the New Mexico Energy, Minerals and Natural Resources Department in the cabinet of Governor Susana Martinez, but was forced to give up the appointment the following month after refusing to submit to a required background investigation. "El Paso Times" called him the "most celebrated" candidate for New Mexico energy secretary.

Schmitt wrote a book entitled "Return to the Moon: Exploration, Enterprise, and Energy in the Human Settlement of Space" in 2006.

He lives in Silver City, New Mexico, and spends some of his summer at his northern Minnesota lake cabin.

Schmitt is also involved in several civic projects, including the improvement of the Senator Harrison H. Schmitt Big Sky Hang Glider Park in Albuquerque, New Mexico.

Schmitt's view on climate change diverges from the frequently reported scientific consensus, as he emphasizes natural over human factors as driving climate. Schmitt has expressed the view that the risks posed by climate change are overrated, and suggests instead that climate change is a tool for people who are trying to increase the size of government. He resigned his membership in the Planetary Society primarily because of its Mars-first policy, but also because of its stance on global warming, writing in his resignation letter that the "'global warming scare' is being used as a political tool to increase government control over American lives, incomes and decision making. It has no place in the Society's activities." Schmitt spoke at the March 2009 International Conference on Climate Change sponsored by the Heartland Institute. He appeared in December that year on the Fox Business Network, saying "[t]he CO scare is a red herring".

In a 2009 interview with conspiracy theorist and talk-radio host Alex Jones, Schmitt asserted a link between Soviet Communism and the American environmental movement: "I think the whole trend really began with the fall of the Soviet Union. Because the great champion of the opponents of liberty, namely communism, had to find some other place to go and they basically went into the environmental movement." At the Heartland Institute's sixth International Conference on Climate Change Schmitt said that climate change was a stalking horse for National Socialism.

Schmitt co-authored a May 8, 2013 "Wall Street Journal" opinion column with William Happer, contending that increasing levels of carbon dioxide in the atmosphere are not significantly correlated with global warming, attributing the "single-minded demonization of this natural and essential atmospheric gas" to advocates of government control of energy production. Noting a positive relationship between crop resistance to drought and increasing carbon dioxide levels, the authors argued, "Contrary to what some would have us believe, increased carbon dioxide in the atmosphere will benefit the increasing population on the planet by increasing agricultural productivity."



Schmitt was inducted into the International Space Hall of Fame in 1977. He was one of 24 Apollo astronauts who were inducted into the U.S. Astronaut Hall of Fame in 1997.

Schmitt is one of the astronauts featured in the 2007 documentary "In the Shadow of the Moon". He also contributed to the book "NASA's Scientist-Astronauts" by David Shayler and Colin Burgess.




</doc>
<doc id="13795" url="https://en.wikipedia.org/wiki?curid=13795" title="Hilaire Rouelle">
Hilaire Rouelle

Hilaire Marin Rouelle (15 February 1718 – 7 April 1779) was an 18th-century French chemist. Commonly cited as the 1773 discoverer of urea, he was not the first to do so. Dutch scientist Herman Boerhaave had discovered this chemical as early as 1727. Rouelle is known as "le cadet" (the younger) to distinguish him from his older brother, Guillaume-François Rouelle, who was also a chemist.


</doc>
<doc id="13798" url="https://en.wikipedia.org/wiki?curid=13798" title="Halon">
Halon

Halon may refer to:



</doc>
<doc id="13800" url="https://en.wikipedia.org/wiki?curid=13800" title="Harrisonburg">
Harrisonburg

Harrisonburg may refer to a place in the United States:



</doc>
<doc id="13802" url="https://en.wikipedia.org/wiki?curid=13802" title="Hammer">
Hammer

A modern day hammer is a tool consisting of a weighted "head" fixed to a long handle that is swung to deliver an impact to a small area of an object. This can be, for example, to drive nails into wood, to shape metal (as with a forge), or to crush rock. Hammers are used for a wide range of driving, shaping, and breaking applications. 

The modern hammer head is typically made of steel which has been heat treated for hardness, and the handle (also called a haft or helve) is typically made of wood or plastic. The term "hammer" also applies to a mechanism's part that delivers a blow, such as the hammer of a firearm or of a piano. 

The claw hammer has a "claw" to pull nails out of wood, and is commonly found in an inventory of household tools in North America. Other types of hammer vary in shape, size, and structure, depending on their purposes. Hammers used in many trades include sledgehammers, mallets, and ball-peen hammers. Although most hammers are hand tools, powered hammers, such as steam hammers and trip hammers, are used to deliver forces beyond the capacity of the human arm. There are over 40 different types of hammers that have many different types of uses.

The use of simple hammers dates to around 3.3 million years ago according to the 2012 find made by Sonia Harmand and Jason Lewis of Stony Brook University, who while excavating a site near Kenya's Lake Turkana discovered a very large deposit of various shaped stones including those used to strike wood, bone, or other stones to break them apart and shape them. The first hammers were without handles. <https://langs.co.uk/blog/2017/06/30/the-history-of-the-hammer-from-its-prehistoric-beginnings/> Later stones attached to sticks with strips of leather or animal sinew were being used as hammers with handles by about 30,000 BCE during the middle of the Paleolithic Stone Age. The addition of a handle gave the user better control abilities and less accidents. <https://langs.co.uk/blog/2017/06/30/the-history-of-the-hammer-from-its-prehistoric-beginnings/>. The hammer became the number one tool. Used for building, food and protection.

The hammer's archeological record shows that it may be the oldest tool for which definite evidence exists of its early existence.

A traditional hand-held hammer consists of a separate head and a handle, which can be fastened together by means of a special wedge made for the purpose, or by glue, or both. This two-piece design is often used to combine a dense metallic striking head with a non-metallic mechanical-shock-absorbing handle (to reduce user fatigue from repeated strikes). If wood is used for the handle, it is often hickory or ash, which are tough and long-lasting materials that can dissipate shock waves from the hammer head. Rigid fiberglass resin may be used for the handle; this material does not absorb water or decay but does not dissipate shock as well as wood.

A loose hammer head is hazardous because it can literally "fly off the handle" when in use, becoming a dangerous uncontrolled missile. Wooden handles can often be replaced when worn or damaged; specialized kits are available covering a range of handle sizes and designs, plus special wedges for attachment.

Some hammers are one-piece designs made mostly of a single material. A one-piece metallic hammer may optionally have its handle coated or wrapped in a resilient material such as rubber, for improved grip and to reduce user fatigue.

The hammer head may be surfaced with a variety of materials including brass, bronze, wood, plastic, rubber, or leather. Some hammers have interchangeable striking surfaces, which can be selected as needed or replaced when worn out.

A large hammer-like tool is a "maul" (sometimes called a "beetle"), a wood- or rubber-headed hammer is a "mallet", and a hammer-like tool with a cutting blade is usually called a "hatchet". The essential part of a hammer is the head, a compact solid mass that is able to deliver a blow to the intended target without itself deforming. The impacting surface of the tool is usually flat or slightly rounded; the opposite end of the impacting mass may have a ball shape, as in the ball-peen hammer. Some upholstery hammers have a magnetized face, to pick up tacks. In the hatchet, the flat hammer head may be secondary to the cutting edge of the tool.

The impact between steel hammer heads and the objects being hit can create sparks, which may ignite flammable or explosive gases. These are a hazard in some industries such as underground coal mining (due to the presence of methane gas), or in other hazardous environments such as petroleum refineries and chemical plants. In these environments, a variety of non-sparking metal tools are used, primarily made of aluminium or beryllium copper. In recent years, the handles have been made of durable plastic or rubber, though wood is still widely used because of its shock-absorbing qualities and repair-ability.


Mechanically-powered hammers often look quite different from the hand tools, but nevertheless, most of them work on the same principle. They include:
In professional framing carpentry, the manual hammer has almost been completely replaced by the nail gun. In professional upholstery, its chief competitor is the staple gun.


A hammer is a simple force amplifier that works by converting mechanical work into kinetic energy and back.

In the swing that precedes each blow, the hammer head stores a certain amount of kinetic energy—equal to the length "D" of the swing times the force "f" produced by the muscles of the arm and by gravity. When the hammer strikes, the head is stopped by an opposite force coming from the target, equal and opposite to the force applied by the head to the target. If the target is a hard and heavy object, or if it is resting on some sort of anvil, the head can travel only a very short distance "d" before stopping. Since the stopping force "F" times that distance must be equal to the head's kinetic energy, it follows that "F" is much greater than the original driving force "f"—roughly, by a factor "D"/"d". In this way, great strength is not needed to produce a force strong enough to bend steel, or crack the hardest stone.

The amount of energy delivered to the target by the hammer-blow is equivalent to one half the mass of the head times the square of the head's speed at the time of impact formula_1. While the energy delivered to the target increases linearly with mass, it increases quadratically with the speed (see the effect of the handle, below). High tech titanium heads are lighter and allow for longer handles, thus increasing velocity and delivering the same energy with less arm fatigue than that of a heavier steel head hammer. A titanium head has about 3% recoil energy and can result in greater efficiency and less fatigue when compared to a steel head with up to 30% recoil. Dead blow hammers use special rubber or steel shot to absorb recoil energy, rather than bouncing the hammer head after impact.

The handle of the hammer helps in several ways. It keeps the user's hands away from the point of impact. It provides a broad area that is better-suited for gripping by the hand. Most importantly, it allows the user to maximize the speed of the head on each blow. The primary constraint on additional handle length is the lack of space to swing the hammer. This is why sledgehammers, largely used in open spaces, can have handles that are much longer than a standard carpenter's hammer. The second most important constraint is more subtle. Even without considering the effects of fatigue, the longer the handle, the harder it is to guide the head of the hammer to its target at full speed.

Most designs are a compromise between practicality and energy efficiency. With too long a handle, the hammer is inefficient because it delivers force to the wrong place, off-target. With too short a handle, the hammer is inefficient because it doesn't deliver enough force, requiring more blows to complete a given task. Modifications have also been made with respect to the effect of the hammer on the user. Handles made of shock-absorbing materials or varying angles attempt to make it easier for the user to continue to wield this age-old device, even as nail guns and other powered drivers encroach on its traditional field of use.

As hammers must be used in many circumstances, where the position of the person using them cannot be taken for granted, trade-offs are made for the sake of practicality. In areas where one has plenty of room, a long handle with a heavy head (like a sledgehammer) can deliver the maximum amount of energy to the target. It is not practical to use such a large hammer for all tasks, however, and thus the overall design has been modified repeatedly to achieve the optimum utility in a wide variety of situations.

Gravity exerts a force on the hammer head. If hammering downwards, gravity increases the acceleration during the hammer stroke and increases the energy delivered with each blow. If hammering upwards, gravity reduces the acceleration during the hammer stroke and therefore reduces the energy delivered with each blow. Some hammering methods, such as traditional mechanical pile drivers, rely entirely on gravity for acceleration on the down stroke.

A hammer may cause significant injury if it strikes the body. Both manual and powered hammers can cause peripheral neuropathy or a variety of other ailments when used improperly. Awkward handles can cause repetitive stress injury (RSI) to hand and arm joints, and uncontrolled shock waves from repeated impacts can injure nerves and the skeleton. Additionally, striking metal objects with a hammer may produce small metallic projectiles which can become lodged in the eye. It is therefore recommended to wear safety glasses.

A war hammer is a late medieval weapon of war intended for close combat action.

The hammer, being one of the most used tools by "Homo sapiens", has been used very much in symbols such as flags and heraldry. In the Middle Ages, it was used often in blacksmith guild logos, as well as in many family symbols. The hammer and pick are used as a symbol of mining.

A well-known symbol with a hammer in it is the Hammer and Sickle, which was the symbol of the former Soviet Union and is strongly linked to communism and early socialism. Mythology includes hammers for their Gods. Thor, Hercules and Sucello all had hammers that appear in their lore and carried different meanings. The hammer in this symbol represents the industrial working class (and the sickle represents the agricultural working class). The hammer is used in some coat of arms in (former) socialist countries like East Germany. Similarly, the Hammer and Sword symbolizes Strasserism, a strand of National Socialism seeking to appeal to the working class.

The gavel, a small wooden mallet, is used to symbolize a mandate to preside over a meeting or judicial proceeding, and a graphic image of one is used as a symbol of legislative or judicial decision-making authority.

In Norse mythology, Thor, the god of thunder and lightning, wields a hammer named Mjölnir. Many artifacts of decorative hammers have been found, leading modern practitioners of this religion to often wear reproductions as a sign of their faith.

Judah Maccabee was nicknamed "The Hammer", possibly in recognition of his ferocity in battle. The name "Maccabee" may derive from the Aramaic "maqqaba". (see .)

In American folklore, the hammer of John Henry represents the strength and endurance of a man.

The hammer in the song "If I Had a Hammer" represents a relentless message of justice broadcast across the land. The song became a symbol of the civil rights movement.




</doc>
<doc id="13804" url="https://en.wikipedia.org/wiki?curid=13804" title="Hiragana">
Hiragana

Hiragana and katakana are both kana systems. With one or two minor exceptions, each sound in the Japanese language (strictly, each mora) is represented by one character (or one digraph) in each system. This may be either a vowel such as ""a"" (hiragana あ); a consonant followed by a vowel such as ""ka"" (か); or ""n"" (ん), a nasal sonorant which, depending on the context, sounds either like English "m", "n", or "ng" (), or like the nasal vowels of French. Because the characters of the kana do not represent single consonants (except in the case of ん "n"), the kana are referred to as syllabaries and not alphabets.

Hiragana is used to write "okurigana" (kana suffixes following a kanji root, for example to inflect verbs and adjectives), various grammatical and function words including particles, as well as miscellaneous other native words for which there are no kanji or whose kanji form is obscure or too formal for the writing purpose. Words that do have common kanji renditions may also sometimes be written instead in hiragana, according to an individual author's preference, for example to impart an informal feel. Hiragana is also used to write "furigana", a reading aid that shows the pronunciation of kanji characters.

There are two main systems of ordering hiragana: the old-fashioned iroha ordering and the more prevalent gojūon ordering.

The modern hiragana syllabary consists of 46 base characters:

These are conceived as a 5×10 grid ("gojūon", , "Fifty Sounds"), as illustrated in the adjacent table, read and so forth, with the singular consonant appended to the end. Of the 50 theoretically possible combinations, "yi" and "wu" do not exist in the language, and "ye", "wi" and "we" are obsolete (or virtually obsolete) in modern Japanese. "wo" is usually pronounced as a vowel ("o") in modern Japanese, and is preserved in only one use, as a particle.

Romanization of the kana does not always strictly follow the consonant-vowel scheme laid out in the table. For example, ち, nominally "ti", is very often romanised as "chi" in an attempt to better represent the actual sound in Japanese.

These basic characters can be modified in various ways. By adding a "dakuten" marker ( ゛), a voiceless consonant is turned into a voiced consonant: "k"→"g", "ts/s"→"z", "t"→"d", "h"→"b" and "ch"/"sh"→"j". For example, か ("ka") becomes が ("ga"). Hiragana beginning with an "h" can also add a "handakuten" marker ( ゜) changing the "h" to a "p". For example, は ("ha") becomes ぱ ("pa").

A small version of the hiragana for "ya", "yu", or "yo" (ゃ, ゅ or ょ respectively) may be added to hiragana ending in "i". This changes the "i" vowel sound to a glide (palatalization) to "a", "u" or "o". For example, き ("ki") plus ゃ (small "ya") becomes ("kya"). Addition of the small "y" kana is called "yōon".

A small "tsu" っ, called a "sokuon", indicates that the following consonant is geminated (doubled). In Japanese this is an important distinction in pronunciation; for example, compare "saka" "hill" with "sakka" "author". The "sokuon" also sometimes appears at the end of utterances, where it denotes a glottal stop, as in ( Ouch!). However, it cannot be used to double the "na", "ni", "nu", "ne", "no" syllables' consonants – to double these, the singular "n" (ん) is added in front of the syllable, as in みんな ("minna", "all").

Hiragana usually spells long vowels with the addition of a second vowel kana; for example, おかあさん ("o-ka-a-sa-n", "mother"). The "chōonpu" (long vowel mark) (ー) used in katakana is rarely used with hiragana, for example in the word , "rāmen", but this usage is considered non-standard in Japanese; the Okinawan language uses chōonpu with hiragana. In informal writing, small versions of the five vowel kana are sometimes used to represent trailing off sounds ( "haa", "nee"). Standard and voiced iteration marks are written in hiragana as ゝ and ゞ respectively.

The following table shows the complete hiragana together with the Hepburn romanization and IPA transcription in the "gojūon" order. Hiragana with "dakuten" or "handakuten" follow the "gojūon" kana without them, with the "yōon" kana following. Obsolete and normally unused kana are shown in brackets and . Those in bold do not use the initial sound for that row. For all syllables besides ん, the pronunciation indicated is for word-initial syllables, for mid-word pronunciations see below.

In the middle of words, the "g" sound (normally ) may turn into a velar nasal or velar fricative . An exception to this is numerals; 15 "jūgo" is considered to be one word, but is pronounced as if it was "jū" and "go" stacked end to end: .

In many accents, the "j" and "z" sounds are pronounced as affricates ( and , respectively) at the beginning of utterances and fricatives in the middle of words. For example, "sūji" 'number', "zasshi" 'magazine'.

In archaic forms of Japanese, there existed the "kwa" ( ) and "gwa" ( ) digraphs. In modern Japanese, these phonemes have been phased out of usage and only exist in the extended katakana digraphs for approximating foreign language words.

The singular "n" is pronounced before "t", "ch", "ts", "n", "r", "z", "j" and "d", before "m", "b" and "p", before "k" and "g", at the end of utterances, and some kind of high nasal vowel before vowels, palatal approximants ("y"), fricative consonants "s", "sh", "h", "f" and "w".

In kanji readings, the diphthongs "ou" and "ei" are today usually pronounced (long o) and (long e) respectively. For example, (lit. "toukyou") is pronounced 'Tokyo', and "sensei" is 'teacher'. However, "tou" is pronounced 'to inquire', because the "o" and "u" are considered distinct, "u" being the verb ending in the dictionary form. Similarly, "shite iru" is pronounced 'is doing'.

For a more thorough discussion on the sounds of Japanese, please refer to Japanese phonology.

An early, now obsolete, hiragana-esque form of "ye" may have existed (𛀁 ) in pre-Classical Japanese (prior to the advent of kana), but is generally represented for purposes of reconstruction by the kanji 江, and its hiragana form is not present in any known orthography. In modern orthography, "ye" can also be written as いぇ (イェ in katakana).

It's true that in early periods of kana, hiragana and katakana letters for "ye" were used, but soon after the distinction between /ye/ and /e/ went away, and letters and glyphs were not established.
Though "ye" did appear in some textbooks during the Meiji period along with another kana for "yi" in the form of cursive 以. Today it is considered a Hentaigana by scholars and was encoded in Unicode 10 (𛀆).

"wu" also appeared in different Meiji textbooks () utilizing the cursive form of 汙. It was never commonly used.

With a few exceptions for sentence particles は, を, and へ (normally "ha", "wo", and "he", but instead pronounced as "wa", "o", and "e", respectively), and a few other arbitrary rules, Japanese, when written in kana, is phonemically orthographic, i.e. there is a one-to-one correspondence between kana characters and sounds, leaving only words' pitch accent unrepresented. This has not always been the case: a previous system of spelling, now referred to as historical kana usage, differed substantially from pronunciation; the three above-mentioned exceptions in modern usage are the legacy of that system.

There are two hiragana pronounced "ji" (じ and ぢ) and two hiragana pronounced "zu" (ず and づ), but to distinguish them, particularly when typing Japanese, sometimes "ぢ" is written as "di" and "づ" is written as "du". These pairs are not interchangeable. Usually, "ji" is written as じ and "zu" is written as ず. There are some exceptions. If the first two syllables of a word consist of one syllable without a "dakuten" and the same syllable with a "dakuten", the same hiragana is used to write the sounds. For example, "chijimeru" ('to boil down' or 'to shrink') is spelled ちぢめる and "tsuzuku" ('to continue') is . For compound words where the dakuten reflects "rendaku" voicing, the original hiragana is used. For example, "chi" ( 'blood') is spelled ち in plain hiragana. When "hana" ('nose') and "chi" ('blood') combine to make "hanaji" ( 'nose bleed'), the sound of changes from "chi" to "ji". So "hanaji" is spelled according to ち: the basic hiragana used to transcribe . Similarly, "tsukau" (; 'to use') is spelled in hiragana, so "kanazukai" (; 'kana use', or 'kana orthography') is spelled in hiragana.

However, this does not apply when kanji are used phonetically to write words that do not relate directly to the meaning of the kanji (see also ateji). The Japanese word for 'lightning', for example, is "inazuma" (). The component means 'rice plant', is written in hiragana and is pronounced: "ina". The component means 'wife' and is pronounced "tsuma" (つま) when written in isolation—or frequently as "zuma" when it features after another syllable. Neither of these components have anything to do with 'lightning', but together they do when they compose the word for 'lightning'. In this case, the default spelling in hiragana rather than is used.

Officially, ぢ and づ do not occur word-initially pursuant to modern spelling rules. There were words such as "jiban" 'ground' in the historical kana usage, but they were unified under じ in the modern kana usage in 1946, so today it is spelled exclusively . However, "zura" 'wig' (from "katsura") and "zuke" (a sushi term for lean tuna soaked in soy sauce) are examples of word-initial づ today. Some people write the word for hemorrhoids as ぢ (normally じ) for emphasis.

No standard Japanese words begin with the kana ん ("n"). This is the basis of the word game shiritori. ん "n" is normally treated as its own syllable and is separate from the other "n"-based kana ("na", "ni" etc.). A notable exception to this is the colloquial negative verb conjugation; for example "wakaranai" meaning "[I] don't understand" is rendered as "wakaran". It is however not a contraction of the former, but instead comes from the classic negative verb conjugation ぬ "nu" ( "wakaranu").

ん is sometimes directly followed by a vowel ("a", "i", "u", "e" or "o") or a palatal approximant ("ya", "yu" or "yo"). These are clearly distinct from the "na", "ni" etc. syllables, and there are minimal pairs such as "kin'en" 'smoking forbidden', "kinen" 'commemoration', "kinnen" 'recent years'. In Hepburn romanization, they are distinguished with an apostrophe, but not all romanization methods make the distinction. For example, past prime minister Junichiro Koizumi's first name is actually "Jun'ichirō" pronounced 

There are a few hiragana that are rarely used. ゐ "wi" and ゑ "we" are obsolete outside of Okinawan orthography. 𛀁 "e" was an alternate version of え "e" before spelling reform, and was briefly reused for "ye" during initial spelling reforms, but is now completely obsolete. ゔ "vu" is a modern addition used to represent the /v/ sound in foreign languages such as English, but since Japanese from a phonological standpoint does not have a /v/ sound, it is pronounced as /b/ and mostly serves as a more accurate indicator of a word's pronunciation in its original language. However, it is rarely seen because loanwords and transliterated words are usually written in katakana, where the corresponding character would be written as ヴ. , , for "ja"/"ju"/"jo" are theoretically possible in rendaku, but are practically never used. For example, 'throughout Japan' could be written , but is practically always 

The "myu" kana is extremely rare in originally Japanese words; linguist Haruhiko Kindaichi raises the example of the Japanese family name Omamyūda and claims it is the only occurrence amongst pure Japanese words. Its katakana counterpart is used in many loanwords, however.

Hiragana developed from "man'yōgana", Chinese characters used for their pronunciations, a practice that started in the 5th century. The oldest examples of Man'yōgana include the Inariyama Sword, an iron sword excavated at the Inariyama Kofun in 1968. This sword is thought to be made in year of (which is A.D. 471 in commonly accepted theory).
The forms of the hiragana originate from the cursive script style of Chinese calligraphy. The figure below shows the derivation of hiragana from manyōgana via cursive script. The upper part shows the character in the regular script form, the center character in red shows the cursive script form of the character, and the bottom shows the equivalent hiragana. Note also that the cursive script forms are not strictly confined to those in the illustration.

Male authors came to write literature using hiragana. Hiragana was used for unofficial writing such as personal letters, while katakana and Chinese were used for official documents. In modern times, the usage of hiragana has become mixed with katakana writing. Katakana is now relegated to special uses such as recently borrowed words (i.e., since the 19th century), names in transliteration, the names of animals, in telegrams, and for emphasis.

Originally, for all syllables there was more than one possible hiragana. In 1900, the system was simplified so each syllable had only one hiragana. The deprecated hiragana are now known as .

The pangram poem "Iroha-uta" ("ABC song/poem"), which dates to the 10th century, uses every hiragana once (except "n" ん, which was just a variant of む before the Muromachi era).

The following table shows the method for writing each hiragana character. It is arranged in the traditional way, beginning top right and reading columns down. The numbers and arrows indicate the stroke order and direction respectively. <br>

Hiragana was added to the Unicode Standard in October, 1991 with the release of version 1.0.

The Unicode block for Hiragana is U+3040–U+309F:

The Unicode hiragana block contains precomposed characters for all hiragana in the modern set, including small vowels and yōon kana for compound syllables, plus the archaic ゐ "wi" and ゑ "we" and the rare ゔ "vu"; the archaic 𛀁 "ye" is included in plane 1 at U+1B001 (see below). All combinations of hiragana with "dakuten" and "handakuten" used in modern Japanese are available as precomposed characters, and can also be produced by using a base hiragana followed by the combining dakuten and handakuten characters (U+3099 and U+309A, respectively). This method is used to add the diacritics to kana that are not normally used with them, for example applying the dakuten to a pure vowel or the handakuten to a kana not in the h-group.

Characters U+3095 and U+3096 are small か ("ka") and small け ("ke"), respectively. U+309F is a ligature of より ("yori") occasionally used in vertical text. U+309B and U+309C are spacing (non-combining) equivalents to the combining dakuten and handakuten characters, respectively.

Historic and variant forms of Japanese kana characters were first added to the Unicode Standard in October, 2010 with the release of version 6.0, with significantly more added in 2017 as part of Unicode 10.

The Unicode block for Kana Supplement is U+1B000–U+1B0FF, and is immediately followed by the Kana Extended-A block (U+1B100–U+1B12F). These blocks include mainly hentaigana (historic or variant hiragana):

The Unicode block for Small Kana Extension is U+1B130–U+1B16F:

In the following character sequences a kana from the /k/ row is modified by a "handakuten" combining mark to indicate that a syllable starts with an initial nasal, known as "bidakuon". As of Unicode 12.0, these character combinations are explicitly called out as Named Sequences.




</doc>
<doc id="13805" url="https://en.wikipedia.org/wiki?curid=13805" title="Hohenstaufen">
Hohenstaufen

The Hohenstaufen (), also known as Staufer, were a dynasty of German kings (1138–1254) during the Middle Ages. Before ascending to the kingship, they were Dukes of Swabia from 1079. As kings of Germany, they had a claim to Italy, Burgundy and the Holy Roman Empire. Three members of the dynasty—Frederick I (1155), Henry VI (1191) and Frederick II (1220)—were crowned emperor. Besides Germany, they also ruled the Kingdom of Sicily (1194–1268) and the Kingdom of Jerusalem (1225–1268)

The dynasty is named after a castle, which in turn is named after a mountain. The names used by scholars today, however, are conventional and somewhat anachronistic.

The name Hohenstaufen was first used in the 14th century to distinguish the "high" ("hohen") conical hill named Staufen in the Swabian Jura, in the district of Göppingen, from the village of the same name in the valley below. The new name was only applied to the hill castle of Staufen by historians in the 19th century, to distinguish it from other castles of the same name. The name of the dynasty followed, but in recent decades the trend in German historiography has been to prefer the name Staufer, which is closer to contemporary usage.

The name "Staufen" itself derives from "Stauf" (OHG "stouf", akin to Early Modern English stoup), meaning "chalice". This term was commonly applied to conical hills in Swabia in the Middle Ages. It is a contemporary term for both the hill and the castle, although its spelling in the Latin documents of the time varies considerably: "Sthouf", "Stophe", "Stophen", "Stoyphe", "Estufin" etc. The castle was built or at least acquired by Duke Frederick I of Swabia in the latter half of the 11th century.

Members of the family occasionally used the toponymic surname "de Stauf" or variants thereof. Only in the 13th century does the name come to be applied to the family as a whole. Around 1215 a chronicler referred to the "emperors of Stauf". In 1247, the Emperor Frederick II himself referred to his family as the "domus Stoffensis" (Staufer house), but this was an isolated instance. Otto of Freising (d. 1158) associated the Staufer with the town of Waiblingen and around 1230 Burchard of Ursberg referred to the Staufer as of the "royal lineage of the Waiblingens" ("regia stirps Waiblingensium"). The exact connection between the family and Waiblingen is not clear, but as a name for the family it became very popular. The pro-imperial Ghibelline faction of the Italian civic rivalries of the 13th and 14th centuries took its name from Waiblingen.

In Italian historiography, the Staufer are known as the "Svevi" (Swabians).

The noble family first appeared in the late 10th century in the Swabian "Riesgau" region around the former Carolingian court of Nördlingen. A local count Frederick (d. about 1075) is mentioned as progenitor in a pedigree drawn up by Abbot Wibald of Stavelot at the behest of Emperor Frederick Barbarossa in 1153. He held the office of a Swabian count palatine; his son Frederick of Buren (c.1020–1053) married Hildegard of Egisheim-Dagsburg (d. 1094/95), a niece of Pope Leo IX. Their son Frederick I was appointed Duke of Swabia at Hohenstaufen Castle by the Salian king Henry IV of Germany in 1079.

At the same time, Duke Frederick I was engaged to the king's approximately seventeen-year-old daughter, Agnes. Nothing is known about Frederick's life before this event, but he proved to be an imperial ally throughout Henry's struggles against other Swabian lords, namely Rudolf of Rheinfelden, Frederick's predecessor, and the Zähringen and Welf lords. Frederick's brother Otto was elevated to the Strasbourg bishopric in 1082.

Upon Frederick's death, he was succeeded by his son, Duke Frederick II, in 1105. Frederick II remained a close ally of the Salians, he and his younger brother Conrad were named the king's representatives in Germany when the king was in Italy. Around 1120, Frederick II married Judith of Bavaria from the rival House of Welf.

When the last male member of the Salian dynasty, Emperor Henry V, died without heirs in 1125, a controversy arose about the succession. Duke Frederick II and Conrad, the two current male Staufers, by their mother Agnes, were grandsons of late Emperor Henry IV and nephews of Henry V. Frederick attempted to succeed to the throne of the Holy Roman Emperor (formally known as the King of the Romans) through a customary election, but lost to the Saxon duke Lothair of Supplinburg. A civil war between Frederick's dynasty and Lothair's ended with Frederick's submission in 1134. After Lothair's death in 1137, Frederick's brother Conrad was elected King as Conrad III.

Because the Welf duke Henry the Proud, son-in-law and heir of Lothair and the most powerful prince in Germany, who had been passed over in the election, refused to acknowledge the new king, Conrad III deprived him of all his territories, giving the Duchy of Saxony to Albert the Bear and that of Bavaria to Leopold IV, Margrave of Austria. In 1147, Conrad heard Bernard of Clairvaux preach the Second Crusade at Speyer, and he agreed to join King Louis VII of France in a great expedition to the Holy Land which failed.

Conrad's brother Duke Frederick II died in 1147, and was succeeded in Swabia by his son, Duke Frederick III. When King Conrad III died without adult heir in 1152, Frederick also succeeded him, taking both German royal and Imperial titles.

Frederick I, known as Frederick Barbarossa because of his red beard, struggled throughout his reign to restore the power and prestige of the German monarchy against the dukes, whose power had grown both before and after the Investiture Controversy under his Salian predecessors. As royal access to the resources of the church in Germany was much reduced, Frederick was forced to go to Italy to find the finances needed to restore the king's power in Germany. He was soon crowned emperor in Italy, but decades of warfare on the peninsula yielded scant results. The Papacy and the prosperous city-states of the Lombard League in northern Italy were traditional enemies, but the fear of Imperial domination caused them to join ranks to fight Frederick. Under the skilled leadership of Pope Alexander III, the alliance suffered many defeats but ultimately was able to deny the emperor a complete victory in Italy. Frederick returned to Germany. He had vanquished one notable opponent, his Welf cousin, Duke Henry the Lion of Saxony and Bavaria in 1180, but his hopes of restoring the power and prestige of the monarchy seemed unlikely to be met by the end of his life.

During Frederick's long stays in Italy, the German princes became stronger and began a successful colonization of Slavic lands. Offers of reduced taxes and manorial duties enticed many Germans to settle in the east in the course of the "Ostsiedlung". In 1163 Frederick waged a successful campaign against the Kingdom of Poland in order to re-install the Silesian dukes of the Piast dynasty. With the German colonization, the Empire increased in size and came to include the Duchy of Pomerania. A quickening economic life in Germany increased the number of towns and Imperial cities, and gave them greater importance. It was also during this period that castles and courts replaced monasteries as centers of culture. Growing out of this courtly culture, Middle High German literature reached its peak in lyrical love poetry, the Minnesang, and in narrative epic poems such as "Tristan", "Parzival", and the "Nibelungenlied".
Frederick died in 1190 while on the Third Crusade and was succeeded by his son, Henry VI. Elected king even before his father's death, Henry went to Rome to be crowned emperor. He married Princess Constance of Sicily, and deaths in his wife's family gave him claim of succession and possession of the Kingdom of Sicily in 1189 and 1194 respectively, a source of vast wealth. Henry failed to make royal and Imperial succession hereditary, but in 1196 he succeeded in gaining a pledge that his infant son Frederick would receive the German crown. Faced with difficulties in Italy and confident that he would realize his wishes in Germany at a later date, Henry returned to the south, where it appeared he might unify the peninsula under the Hohenstaufen name. After a series of military victories, however, he fell ill and died of natural causes in Sicily in 1197. His underage son Frederick could only succeed him in Sicily and Malta, while in the Empire the struggle between the House of Staufen and the House of Welf erupted once again.

Because the election of a three-year-old boy to be German king appeared likely to make orderly rule difficult, the boy's uncle, Duke Philip of Swabia, brother of late Henry VI, was designated to serve in his place. Other factions however favoured a Welf candidate. In 1198, two rival kings were chosen: the Hohenstaufen Philip of Swabia and the son of the deprived Duke Henry the Lion, the Welf Otto IV. A long civil war began; Philip was about to win when he was murdered by the Bavarian count palatine Otto VIII of Wittelsbach in 1208. Pope Innocent III initially had supported the Welfs, but when Otto, now sole elected monarch, moved to appropriate Sicily, Innocent changed sides and accepted young Frederick II and his ally, King Philip II of France, who defeated Otto at the 1214 Battle of Bouvines. Frederick had returned to Germany in 1212 from Sicily, where he had grown up, and was elected king in 1215. When Otto died in 1218, Fredrick became the undisputed ruler, and in 1220 was crowned Holy Roman Emperor.

Philip changed the coat of arms from a black lion on a gold shield to three leopards, probably derived from the arms of his Welf rival Otto IV.

The conflict between the Staufer dynasty and the Welf had irrevocably weakened the Imperial authority and the Norman kingdom of Sicily became the base for Staufer rule.

Emperor Frederick II spent little time in Germany as his main concerns lay in Southern Italy. He founded the University of Naples in 1224 to train future state officials and reigned over Germany primarily through the allocation of royal prerogatives, leaving the sovereign authority and imperial estates to the ecclesiastical and secular princes. He made significant concessions to the German nobles, such as those put forth in an imperial statute of 1232, which made princes virtually independent rulers within their territories. These measures favoured the further fragmentation of the Empire.

By the 1226 Golden Bull of Rimini, Frederick had assigned the military order of the Teutonic Knights to complete the conquest and conversion of the Prussian lands. A reconciliation with the Welfs took place in 1235, whereby Otto the Child, grandson of the late Saxon duke Henry the Lion, was named Duke of Brunswick and Lüneburg. The power struggle with the popes continued and resulted in Fredrick's excommunication in 1227. In 1239, Pope Gregory IX excommunicated Fredrick again, and in 1245 he was condemned as a heretic by a church council. Although Frederick was one of the most energetic, imaginative, and capable rulers of the time, he was not concerned with drawing the disparate forces in Germany together. His legacy was thus that local rulers had more authority after his reign than before it. The clergy also had become more powerful.

By the time of Frederick's death in 1250, little centralized power remained in Germany. The Great Interregnum, a period in which there were several elected rival kings, none of whom was able to achieve any position of authority, followed the death of Frederick's son King Conrad IV of Germany in 1254. The German princes vied for individual advantage and managed to strip many powers away from the diminished monarchy. Rather than establish sovereign states however, many nobles tended to look after their families. Their many male heirs created more and smaller estates, and from a largely free class of officials previously formed, many of these assumed or acquired hereditary rights to administrative and legal offices. These trends compounded political fragmentation within Germany. The period was ended in 1273 with the election of Rudolph of Habsburg, a godson of Frederick.

Conrad IV was succeeded as duke of Swabia by his only son, two-year-old Conradin. By this time, the office of duke of Swabia had been fully subsumed into the office of the king, and without royal authority had become meaningless. In 1261, attempts to elect young Conradin king were unsuccessful. He also had to defend Sicily against an invasion, sponsored by Pope Urban IV (Jacques Pantaléon) and Pope Clement IV (Guy Folques), by Charles of Anjou, a brother of the French king. Charles had been promised by the popes the Kingdom of Sicily, where he would replace the relatives of Frederick II. Charles had defeated Conradin's uncle Manfred, King of Sicily, in the Battle of Benevento on 26 February 1266. The king himself, refusing to flee, rushed into the midst of his enemies and was killed. Conradin's campaign to retake control ended with his defeat in 1268 at the Battle of Tagliacozzo, after which he was handed over to Charles, who had him publicly executed at Naples. With Conradin, the direct line of the Dukes of Swabia finally ceased to exist, though most of the later emperors were descended from the Staufer dynasty indirectly.

During the political decentralization of the late Staufer period, the population had grown from an estimated 8 million in 1200 to about 14 million in 1300, and the number of towns increased tenfold. The most heavily urbanized areas of Germany were located in the south and the west. Towns often developed a degree of independence, but many were subordinate to local rulers if not immediate to the emperor. Colonization of the east also continued in the thirteenth century, most notably through the efforts of the Teutonic Knights. German merchants also began trading extensively on the Baltic.

The first ruling Hohenstaufen, Conrad III, like the last one, Conrad IV, was never crowned emperor. After a 20-year period (Great interregnum 1254–1273), the first Habsburg was elected king.

"Note: The following kings are already listed above as German Kings"

"Note: Some of the following kings are already listed above as German Kings"

"Note: Some of the following dukes are already listed above as German Kings"

Modern history


</doc>
<doc id="13806" url="https://en.wikipedia.org/wiki?curid=13806" title="History of Malaysia">
History of Malaysia

Malaysia is a Southeast Asian country located on a strategic sea-lane that exposes it to global trade and foreign culture. An early western account of the area is seen in Ptolemy's book "Geographia," which mentions a "Golden Khersonese," now identified as the Malay Peninsula. Hinduism and Buddhism from India and China dominated early regional history, reaching their peak during the reign of the Sumatra-based Srivijaya civilisation, whose influence extended through Sumatra, Java, the Malay Peninsula and much of Borneo from the 7th to the 13th centuries.

Although Muslims had passed through the Malay Peninsula as early as the 10th century, it was not until the 14th century that Islam first firmly established itself. The adoption of Islam in the 14th century saw the rise of a number of sultanates, the most prominent of which was the Sultanate of Malacca. Islam had a profound influence on the Malay people, but has also been influenced by them. The Portuguese were the first European colonial powers to establish themselves on the Malay Peninsula and Southeast Asia, capturing Malacca in 1511, followed by the Dutch in 1641. However, it was the British who, after initially establishing bases at Jesselton, Kuching, Penang and Singapore, ultimately secured their hegemony across the territory that is now Malaysia. The Anglo-Dutch Treaty of 1824 defined the boundaries between British Malaya and the Netherlands East Indies (which became Indonesia). A fourth phase of foreign influence was immigration of Chinese and Indian workers to meet the needs of the colonial economy created by the British in the Malay Peninsula and Borneo.

Japanese invasion during World War II ended British domination in Malaysia. The subsequent occupation of Malaya, North Borneo and Sarawak from 1942 to 1945 unleashed nationalism. In the Peninsula, the Malayan Communist Party took up arms against the British. A tough military response was needed to end the insurgency and bring about the establishment of an independent, multi-racial Federation of Malaya on 31 August 1957. On 22 July 1963, Sarawak was granted self-governance. The following month on 31 August 1963, both North Borneo and Singapore were also granted self-governance and all states formed Malaysia on 16 September 1963. Approximately two years later, the Malaysian parliament passed a bill without the consent of signatory of Malaysia Agreement 1963 to separate Singapore from the Federation. A confrontation with Indonesia occurred in the early-1960s. Race riots in 1969 led to the imposition of emergency rule, and a curtailment of political life and civil liberties which has never been fully reversed. Since 1970 the "Barisan Nasional coalition" headed by United Malays National Organisation (UMNO) has governed Malaysia until beaten by the competing party led by Mahathir Mohamad in 10 May 2018.

Stone hand-axes from early hominoids, probably Homo erectus, have been unearthed in Lenggong. They date back 1.83 million years, the oldest evidence of hominid habitation in Southeast Asia. The earliest evidence of modern human habitation in Malaysia is the 40,000-year-old skull excavated from the Niah Caves in today's Sarawak, nicknamed "Deep Skull". It was excavated from a deep trench uncovered by Barbara and Tom Harrisson (a British ethnologist) in 1958. this is also the oldest modern human skull in Southeast Asia. The skull probably belongs to a 16-to 17-year-old adolescent girl. The first foragers visited the West Mouth of Niah Caves (located southwest of Miri) 40,000 years ago when Borneo was connected to the mainland of Southeast Asia. The landscape around the Niah Caves was drier and more exposed than it is now. Prehistorically, the Niah Caves were surrounded by a combination of closed forests with bush, parkland, swamps, and rivers. The foragers were able to survive in the rainforest through hunting, fishing, and gathering molluscs and edible plants. Mesolithic and Neolithic burial sites have also been found in the area. The area around the Niah Caves has been designated the Niah National Park.

A study of Asian genetics points to the idea that the original humans in East Asia came from Southeast Asia. The oldest complete skeleton found in Malaysia is 11,000-year-old Perak Man unearthed in 1991. The indigenous groups on the peninsula can be divided into three ethnicities, the Negritos, the Senoi, and the proto-Malays. The first inhabitants of the Malay Peninsula were most probably Negritos. These Mesolithic hunters were probably the ancestors of the Semang, an ethnic Negrito group who have a long history in the Malay Peninsula.

The Senoi appear to be a composite group, with approximately half of the maternal mitochondrial DNA lineages tracing back to the ancestors of the Semang and about half to later ancestral migrations from Indochina. Scholars suggest they are descendants of early Austroasiatic-speaking agriculturalists, who brought both their language and their technology to the southern part of the peninsula approximately 4,000 years ago. They united and coalesced with the indigenous population.

The Proto Malays have a more diverse origin and had settled in Malaysia by 1000 BC. Although they show some connections with other inhabitants in Maritime Southeast Asia, some also have an ancestry in Indochina around the time of the Last Glacial Maximum about 20,000 years ago. Anthropologists support the notion that the Proto-Malays originated from what is today Yunnan, China. This was followed by an early-Holocene dispersal through the Malay Peninsula into the Malay Archipelago. Around 300 BC, they were pushed inland by the Deutero-Malays, an Iron Age or Bronze Age people descended partly from the Chams of Cambodia and Vietnam. The first group in the peninsula to use metal tools, the Deutero-Malays were the direct ancestors of today's Malaysian Malays, and brought with them advanced farming techniques. The Malays remained politically fragmented throughout the Malay archipelago, although a common culture and social structure was shared.

In the first millennium CE, Malays became the dominant race on the peninsula. The small early states that were established were greatly influenced by Indian culture. Indian influence in the region dates back to at least the 3rd century BCE. South Indian culture was spread to Southeast Asia by the south Indian Pallava dynasty in the 4th and 5th century.
In ancient Indian literature, the term "Suvarnadvipa" or the "Golden Peninsula" is used in "Ramayana", and some argued that it may be a reference to the Malay Peninsula. The ancient Indian text "Vayu Purana" also mentioned a place named "Malayadvipa" where gold mines may be found, and this term has been proposed to mean possibly Sumatra and the Malay Peninsula. The Malay Peninsula was shown on Ptolemy's map as the "Golden Khersonese". He referred to the Straits of Malacca as "Sinus Sabaricus".

Trade relations with China and India were established in the 1st century BC. Shards of Chinese pottery have been found in Borneo dating from the 1st century following the southward expansion of the Han Dynasty. In the early centuries of the first millennium, the people of the Malay Peninsula adopted the Indian religions of Hinduism and Buddhism, religions which had a major effect on the language and culture of those living in Malaysia. The Sanskrit writing system was used as early as the 4th century.

There were numerous Malay kingdoms in the 2nd and 3rd century, as many as 30, mainly based on the Eastern side of the Malay peninsula. Among the earliest kingdoms known to have been based in the Malay Peninsula is the ancient kingdom of Langkasuka, located in the northern Malay Peninsula and based somewhere on the west coast. It was closely tied to Funan in Cambodia, which also ruled part of northern Malaysia until the 6th century. In the 5th century, the Kingdom of Pahang was mentioned in the "Book of Song". According to the Sejarah Melayu ("Malay Annals"), the Khmer prince Raja Ganji Sarjuna founded the kingdom of Gangga Negara (modern-day Beruas, Perak) in the 700s. Chinese chronicles of the 5th century CE speak of a great port in the south called Guantoli, which is thought to have been in the Straits of Malacca. In the 7th century, a new port called Shilifoshi is mentioned, and this is believed to be a Chinese rendering of Srivijaya.

Between the 7th and the 13th century, much of the Malay peninsula was under the Buddhist Srivijaya empire. The site of Srivijaya's centre is thought be at a river mouth in eastern Sumatra, based near what is now Palembang. For over six centuries the Maharajahs of Srivijaya ruled a maritime empire that became the main power in the archipelago. The empire was based around trade, with local kings (dhatus or community leaders) swearing allegiance to the central lord for mutual profit.

The relation between Srivijaya and the Chola Empire of south India was friendly during the reign of Raja Raja Chola I but during the reign of Rajendra Chola I the Chola Empire attacked Srivijaya cities.
In 1025 and 1026 Gangga Negara was attacked by Rajendra Chola I of the Chola Empire, the Tamil emperor who is now thought to have laid Kota Gelanggi to waste. Kedah—known as "Kedaram", "Cheh-Cha" (according to "I-Ching") or "Kataha", in ancient Pallava or Sanskrit—was in the direct route of the invasions and was ruled by the Cholas from 1025. A second invasion was led by Virarajendra Chola of the Chola dynasty who conquered Kedah in the late 11th century. The senior Chola's successor, Vira Rajendra Chola, had to put down a Kedah rebellion to overthrow other invaders. The coming of the Chola reduced the majesty of Srivijaya, which had exerted influence over Kedah, Pattani and as far as Ligor. During the reign of Kulothunga Chola I Chola overlordship was established over the Sri Vijaya province kedah in the late 11th century. The expedition of the Chola Emperors had such a great impression to the Malay people of the medieval period that their name was mentioned in the corrupted form as Raja Chulan in the medieval Malay chronicle Sejarah Melaya. Even today the Chola rule is remembered in Malaysia as many Malaysian princes have names ending with Cholan or Chulan, one such was the Raja of Perak called Raja Chulan.
Pattinapalai, a Tamil poem of the 2nd century CE, describes goods from Kedaram heaped in the broad streets of the Chola capital. A 7th-century Indian drama, "Kaumudhimahotsva", refers to Kedah as Kataha-nagari. The "Agnipurana" also mentions a territory known as Anda-Kataha with one of its boundaries delineated by a peak, which scholars believe is Gunung Jerai. Stories from the "Katasaritasagaram" describe the elegance of life in Kataha. The Buddhist kingdom of Ligor took control of Kedah shortly after. Its king Chandrabhanu used it as a base to attack Sri Lanka in the 11th century and ruled the northern parts, an event noted in a stone inscription in Nagapattinum in Tamil Nadu and in the Sri Lankan chronicles, "Mahavamsa".

At times, the Khmer kingdom, the Siamese kingdom, and even Cholas kingdom tried to exert control over the smaller Malay states. The power of Srivijaya declined from the 12th century as the relationship between the capital and its vassals broke down. Wars with the Javanese caused it to request assistance from China, and wars with Indian states are also suspected. In the 11th century, the centre of power shifted to Malayu, a port possibly located further up the Sumatran coast near the Jambi River. The power of the Buddhist Maharajas was further undermined by the spread of Islam. Areas which were converted to Islam early, such as Aceh, broke away from Srivijaya's control. By the late 13th century, the Siamese kings of Sukhothai had brought most of Malaya under their rule. In the 14th century, the Hindu Java-based Majapahit empire came into possession of the peninsula.

An excavation by Tom Harrisson in 1949 unearthed a series of Chinese ceramics at Santubong (near Kuching) that date to the Tang and the Song dynasties in the 8th to 13th century AD. It is possible that Santubong was an important seaport in Sarawak during the period, but its importance declined during the Yuan dynasty, and the port was deserted during the Ming dynasty. Other archaeological sites in Sarawak can be found inside the Kapit, Song, Serian, and Bau districts.

Islam came to the Malay Archipelago through the Arab and Indian traders in the 13th century, ending the age of Hinduism and Buddhism. It arrived in the region gradually, and became the religion of the elite before it spread to the commoners. The Islam in Malaysia was influenced by previous religions and was originally not orthodox.

The port of Malacca on the west coast of the Malay Peninsula was founded in 1402 by Parameswara, a Srivijaya prince fleeing Temasek (now Singapore), Parameswara in particular sailed to Temasek to escape persecution. There he came under the protection of Temagi, a Malay chief from Patani who was appointed by the king of Siam as regent of Temasek. Within a few days, Parameswara killed Temagi and appointed himself regent. Some five years later he had to leave Temasek, due to threats from Siam. During this period, a Javanese fleet from Majapahit attacked Temasek.
Parameswara headed north to found a new settlement. At Muar, Parameswara considered siting his new kingdom at either Biawak Busuk or at Kota Buruk. Finding that the Muar location was not suitable, he continued his journey northwards. Along the way, he reportedly visited Sening Ujong (former name of present-day Sungai Ujong) before reaching a fishing village at the mouth of the Bertam River (former name of the Melaka River), and founded what would become the Malacca Sultanate. Over time this developed into modern-day Malacca Town. According to the "Malay Annals", here Parameswara saw a mouse deer outwitting a dog resting under a Malacca tree. Taking this as a good omen, he decided to establish a kingdom called Malacca. He built and improved facilities for trade. The Malacca Sultanate is commonly considered the first independent state in the peninsula.

At the time of Melaka's founding, the emperor of Ming Dynasty China was sending out fleets of ships to expand trade. Admiral Zheng He called at Malacca and brought Parameswara with him on his return to China, a recognition of his position as legitimate ruler of Malacca. In exchange for regular tribute, the Chinese emperor offered Melaka protection from the constant threat of a Siamese attack. The Chinese and Indians who settled in the Malay Peninsula before and during this period are the ancestors of today's Baba-Nyonya and Chetti community. According to one theory, Parameswara became a Muslim when he married a Princess of Pasai and he took the fashionable Persian title "Shah", calling himself Iskandar Shah. Chinese chronicles mention that in 1414, the son of the first ruler of Malacca visited the Ming emperor to inform them that his father had died. Parameswara's son was then officially recognised as the second ruler of Melaka by the Chinese Emperor and styled Raja Sri Rama Vikrama, Raja of Parameswara of Temasek and Malacca and he was known to his Muslim subjects as Sultan Sri Iskandar Zulkarnain Shah or Sultan Megat Iskandar Shah. He ruled Malacca from 1414 to 1424. Through the influence of Indian Muslims and, to a lesser extent, Hui people from China, Islam became increasingly common during the 15th century.

After an initial period paying tribute to the Ayutthaya, the kingdom rapidly assumed the place previously held by Srivijaya, establishing independent relations with China, and exploiting its position dominating the Straits to control the China-India maritime trade, which became increasingly important when the Mongol conquests closed the overland route between China and the west.
Within a few years of its establishment, Malacca officially adopted Islam. Parameswara became a Muslim, and because Malacca was under a Muslim prince, the conversion of Malays to Islam accelerated in the 15th century. The political power of the Malacca Sultanate helped Islam's rapid spread through the archipelago. Malacca was an important commercial centre during this time, attracting trade from around the region. By the start of the 16th century, with the Malacca Sultanate in the Malay peninsula and parts of Sumatra, the Demak Sultanate in Java, and other kingdoms around the Malay archipelago increasingly converting to Islam, it had become the dominant religion among Malays, and reached as far as the modern-day Philippines, leaving Bali as an isolated outpost of Hinduism today.

Malacca's reign lasted little more than a century, but during this time became the established centre of Malay culture. Most future Malay states originated from this period. Malacca became a cultural centre, creating the matrix of the modern Malay culture: a blend of indigenous Malay and imported Indian, Chinese and Islamic elements. Malacca's fashions in literature, art, music, dance and dress, and the ornate titles of its royal court, came to be seen as the standard for all ethnic Malays. The court of Malacca also gave great prestige to the Malay language, which had originally evolved in Sumatra and been brought to Malacca at the time of its foundation. In time Malay came to be the official language of all the Malaysian states, although local languages survived in many places. After the fall of Malacca, the Sultanate of Brunei became the major centre of Islam.

From the 15th century onwards, the Portuguese started seeking a maritime route towards Asia. In 1511, Afonso de Albuquerque led an expedition to Malaya which seized Malacca with the intent of using it as a base for activities in southeast Asia. This was the first colonial claim on what is now Malaysia. The son of the last Sultan of Malacca, Sultan Alauddin Riayat Shah II fled to the southern tip of the peninsula, where he founded a state that which became the Sultanate of Johor. Another son created the Perak Sultanate to the north. By the late 16th century, the tin mines of northern Malaya had been discovered by European traders, and Perak grew wealthy on the proceeds of tin exports. Portuguese influence was strong, as they aggressively tried to convert the population of Malacca to Catholicism. In 1571, the Spanish captured Manila and established a colony in the Philippines, reducing the Sultanate of Brunei's power.
After the fall of Malacca to Portugal, the Johor Sultanate and the Sultanate of Aceh on northern Sumatra moved to fill the power vacuum left behind. The three powers struggled to dominate the Malay peninsula and the surrounding islands. Johor founded in the wake of Malacca's conquest grew powerful enough to rival the Portuguese, although it was never able to recapture the city. Instead it expanded in other directions, building in 130 years one of the largest Malay states. In this time the numerous attempts to recapture Malacca led to a strong backlash from the Portuguese, whose raids even reached Johor's capital of Johor Lama in 1587.

In 1607, the Sultanate of Aceh rose as the powerful and wealthiest state in the Malay archipelago. Under Iskandar Muda's reign, the sultanate's control was extended over a number of Malay states. A notable conquest was Perak, a tin-producing state on the Peninsula. In Iskandar Muda's disastrous campaign against Malacca in 1629, the combined Portuguese and Johor forces managed to destroy all the ships of his formidable fleet and 19,000 troops according to a Portuguese account. Aceh forces were not destroyed, however, as Aceh was able to conquer Kedah within the same year and took many of its citizens to Aceh. The Sultan's son-in-law, Iskandar Thani, the former prince of Pahang later became Iskandar Muda's successor. The conflict over control of the straits went on until 1641, when the Dutch (allied to Johor) gained control of Malacca.

In the early 17th century, the Dutch East India Company ("Vereenigde Oost-Indische Compagnie", or VOC) was established. During this time the Dutch were at war with Spain, which absorbed the Portuguese Empire due to the Iberian Union. The Dutch expanded across the archipelago, forming an alliance with Johor and using this to push the Portuguese out of Malacca in 1641. Backed by the Dutch, Johor established a loose hegemony over the Malay states, except Perak, which was able to play off Johor against the Siamese to the north and retain its independence. The Dutch did not interfere in local matters in Malacca, but at the same time diverted most trade to its colonies on Java.

The weakness of the small coastal Malay states led to the immigration of the Bugis, escaping from Dutch colonisation of Sulawesi, who established numerous settlements on the peninsula which they used to interfere with Dutch trade. They seized control of Johor following the assassination of the last Sultan of the old Melaka royal line in 1699. Bugis expanded their power in the states of Johor, Kedah, Perak, and Selangor. The Minangkabau from central Sumatra migrated into Malaya, and eventually established their own state in Negeri Sembilan. The fall of Johor left a power vacuum on the Malay Peninsula which was partly filled by the Siamese kings of Ayutthaya kingdom, who made the five northern Malay states—Kedah, Kelantan, Patani, Perlis, and Terengganu — their vassals. Johor's eclipse also left Perak as the unrivalled leader of the Malay states.

The economic importance of Malaya to Europe grew rapidly during the 18th century. The fast-growing tea trade between China and United Kingdom increased the demand for high-quality Malayan tin, which was used to line tea-chests. Malayan pepper also had a high reputation in Europe, while Kelantan and Pahang had gold mines. The growth of tin and gold mining and associated service industries led to the first influx of foreign settlers into the Malay world — initially Arabs and Indians, later Chinese.

Before its conversion to Islam, Brunei was known as Poni and it was a vassal-state to the Majapahit Empire. By the 15th century, the empire became a Muslim state, when the King of Brunei converted to Islam, brought by Muslim Indians and Arab merchants from other parts of Maritime Southeast Asia, who came to trade and spread Islam. During the rule of Bolkiah, the fifth Sultan, the empire controlled over coastal areas of northwest Borneo (present-day Brunei, Sarawak and Sabah) and reached the Philippines at Seludong (present-day Manila), Sulu Archipelago and included parts of the island of Mindanao. In the 16th century, the Brunei empire's influence also extended as far as Kapuas River delta in West Kalimantan. The Malay Sultanate of Sambas in West Kalimantan and Sultanate of Sulu in Southern Philippines in particular has developed dynastic relations with the royal house of Brunei. Other Malay sultans of Pontianak, Samarinda as far as Banjarmasin, treated the Sultan of Brunei as their leader. The Kuching area was known to Portuguese cartographers as "Cerava", one of the five great seaports on the island of Borneo. It was under the influence of the Bruneian Empire and was self-governed under Sultan Tengah. The Bruneian empire began to decline during the arrival of western powers. Spain sent several expeditions from Mexico to invade Brunei's territories in the Philippines. They conquered the Bruneian colony of Islamic Manila and Christianized its people and they also laid siege to Sulu. Eventually the Spanish, their Visayan allies and their Latin-American recruits assaulted Brunei itself during the Castilian War. The invasion was only temporary as the Spanish then retreated.
However, Brunei was unable to regain the territory it lost in the Philippines. Yet, it still maintained sway in Borneo. By the early 19th century, Sarawak had become a loosely governed territory under the control of the Brunei Sultanate. The Bruneian Empire had authority only along the coastal regions of Sarawak held by semi-independent Malay leaders. Meanwhile, the interior of Sarawak suffered from tribal wars fought by Iban, Kayan, and Kenyah peoples, who aggressively fought to expand their territories. Following the discovery of antimony ore in the Kuching region, Pangeran Indera Mahkota (a representative of the Sultan of Brunei) began to develop the territory between 1824 and 1830. When antimony production increased, the Brunei Sultanate demanded higher taxes from Sarawak; this led to civil unrest and chaos. In 1839, Sultan Omar Ali Saifuddin II (1827–1852), ordered his uncle Pengiran Muda Hashim to restore order. It was around this time that James Brooke (who would later become the first White Rajah of Sarawak) arrived in Sarawak, and Pengiran Muda Hashim requested his assistance in the matter, but Brooke refused. However, he agreed to a further request during his next visit to Sarawak in 1841. Pangeran Muda Hashim signed a treaty in 1841 surrendering Sarawak to Brooke. On 24 September 1841, Pengiran Muda Hashim bestowed the title of governor on James Brooke. This appointment was later confirmed by the Sultan of Brunei in 1842. In 1843, James Brooke decided to create a pro-British Brunei government by installing Pengiran Muda Hashim into the Brunei Court as he would be taking the Brooke's advice. James Brooke forced Brunei to appoint Hashim under the guns of East India Company's steamer "Phlegethon". The Brunei Court was unhappy with Hashim's appointment and had him assassinated in 1845. In retaliation, James Brooke attacked the Kampong Ayer, the capital of Brunei. After the incident, the Sultan of Brunei sent an apology letter to Queen Victoria. The sultan also confirmed James Brooke's possession of Sarawak and his mining rights of antimony without paying tribute to Brunei. In 1846 Brooke effectively became the Rajah of Sarawak and founded the White Rajah Dynasty of Sarawak.

English traders had been present in Malay waters since the 17th century. However, with the arrival of the British, European power became dominant in Malaysia. Before the mid-19th-century British interests in the region were predominantly economic, with little interest in territorial control. Already the most powerful coloniser in India, the British were looking towards southeast Asia for new resources. The growth of the China trade in British ships increased the East India Company’s desire for bases in the region. Various islands were used for this purpose, but the first permanent acquisition was Penang, leased from the Sultan of Kedah in 1786. This was followed soon after by the leasing of a block of territory on the mainland opposite Penang (known as Province Wellesley). In 1795, during the Napoleonic Wars, the British with the consent of the Netherlands occupied Dutch Melaka to forestall possible French encroachment in the area.

When Malacca was handed back to the Dutch in 1815, the British governor, Stamford Raffles, looked for an alternative base, and in 1819 he acquired Singapore from the Sultan of Johor. The exchange of the British colony of Bencoolen for Malacca with the Dutch left the British as the sole colonial power on the peninsula. The territories of the British were set up as free ports, attempting to break the monopoly held by other colonial powers at the time, and making them large bases of trade. They allowed Britain to control all trade through the straits of Malacca. British influence was increased by Malayan fears of Siamese expansionism, to which Britain made a useful counterweight. During the 19th century the Malay Sultans aligned themselves with the British Empire, due to the benefits of associations with the British and the belief in superior British civilisation.

In 1824, British hegemony in Malaya (before the name Malaysia) was formalised by the Anglo-Dutch Treaty, which divided the Malay archipelago between Britain and the Netherlands. The Dutch evacuated Melaka and renounced all interest in Malaya, while the British recognised Dutch rule over the rest of the East Indies. By 1826 the British controlled Penang, Malacca, Singapore, and the island of Labuan, which they established as the crown colony of the Straits Settlements, administered first under the East India Company until 1867, when they were transferred to the Colonial Office in London.

Initially, the British followed a policy of non-intervention in relations between the Malay states. The commercial importance of tin mining in the Malay states to merchants in the Straits Settlements led to infighting between the aristocracy on the peninsula. The destabilisation of these states damaged the commerce in the area, causing British intervention. The wealth of Perak's tin mines made political stability there a priority for British investors, and Perak was thus the first Malay state to agree to the supervision of a British resident. British gunboat diplomacy was employed to bring about a peaceful resolution to civil disturbances caused by Chinese and Malay gangsters employed in a political fight between Ngah Ibrahim and Raja Muda Abdullah. The Pangkor Treaty of 1874 paved the way for the expansion of British influence in Malaya. The British concluded treaties with some Malay states, installing “residents” who advised the Sultans and soon became the effective rulers of their states. These advisors held power in everything except to do with Malay religion and customs.

Johor alone resisted, by modernising and giving British and Chinese investors legal protection. By the turn of the 20th century, the states of Pahang, Selangor, Perak, and Negeri Sembilan, known together as the Federated Malay States, had British advisors. In 1909 the Siamese kingdom was compelled to cede Kedah, Kelantan, Perlis and Terengganu, which already had British advisors, over to the British. Sultan Abu Bakar of Johor and Queen Victoria were personal acquaintances who recognised each other as equals. It was not until 1914 that Sultan Abu Bakar's successor, Sultan Ibrahim, accepted a British adviser. The four previously Thai states and Johor were known as the Unfederated Malay States. The states under the most direct British control developed rapidly, becoming the largest suppliers in the world of first tin, then rubber.

By 1910, the pattern of British rule in the Malay lands was established. The Straits Settlements were a Crown colony, ruled by a governor under the supervision of the Colonial Office in London. Their population was about half Chinese, but all residents, regardless of race, were British subjects. The first four states to accept British residents, Perak, Selangor, Negeri Sembilan, and Pahang, were termed the Federated Malay States: while technically independent, they were placed under a Resident-General in 1895, making them British colonies in all but name. The Unfederated Malay States (Johore, Kedah, Kelantan, Perlis, and Terengganu) had a slightly larger degree of independence, although they were unable to resist the wishes of their British residents for long. Johor, as Britain's closest ally in Malay affairs, had the privilege of a written constitution, which gave the Sultan the right to appoint his own Cabinet, but he was generally careful to consult the British first.

During the late 19th century the British also gained control of the north coast of Borneo, where Dutch rule had never been established. Development on the Peninsula and Borneo were generally separate until the 19th century. The eastern part of this region (now Sabah) was under the nominal control of the Sultan of Sulu, who later became a vassal of the Spanish East Indies. The rest was the territory of the Sultanate of Brunei. In 1841, British adventurer James Brooke helped the Sultan of Brunei suppress a revolt, and in return received the title of raja and the right to govern the Sarawak River District. In 1846, his title was recognised as hereditary, and the "White Rajahs" began ruling Sarawak as a recognised independent state. The Brookes expanded Sarawak at the expense of Brunei.

In 1881, the British North Borneo Company was granted control of the territory of British North Borneo, appointing a governor and legislature. It was ruled from the office in London. Its status was similar to that of a British Protectorate, and like Sarawak it expanded at the expense of Brunei. Until the Philippine independence on 1946, seven British-controlled islands in the north-eastern part of Borneo named Turtle Islands and Cagayan de Tawi-Tawi were ceded to the Philippine government by the Crown colony government of North Borneo. The Philippines then under its irredentism motive since the administration of President Diosdado Macapagal laying claim to eastern Sabah in a basis the territory was part of the present-defunct Sultanate of Sulu's territory. In 1888, what was left of Brunei was made a British protectorate, and in 1891 another Anglo-Dutch treaty formalised the border between British and Dutch Borneo.

Unlike some colonial powers, the British always saw their empire as primarily an economic concern, and its colonies were expected to turn a profit for British shareholders. Malaya's obvious attractions were its tin and gold mines, but British planters soon began to experiment with tropical plantation crops—tapioca, gambier, pepper, and coffee. But in 1877 the rubber plant was introduced from Brazil, and rubber soon became Malaya's staple export, stimulated by booming demand from European industry. Rubber was later joined by palm oil as an export earner. All these industries required a large and disciplined labour force, and the British did not regard the Malays as reliable workers. The solution was the importation of plantation workers from India, mainly Tamil-speakers from South India. A small group of Malabaris were brought from the current place called Kerala to help with the rubber plantations, resulting in the small Malabari population seen in Malaysia today. The mines, mills and docks also attracted a flood of immigrant workers from southern China. Soon towns like Singapore, Penang, and Ipoh were majority Chinese, as was Kuala Lumpur, founded as a tin-mining centre in 1857. By 1891, when Malaya's first census was taken, Perak and Selangor, the main tin-mining states, had Chinese majorities.

The Chinese mostly arrived poor; yet their belief in industriousness and frugality, their emphasis in their children's education and their maintenance of Confucian family hierarchy, as well as their voluntary connection with tightly knit networks of mutual aid societies (run by "Hui-Guan" 會館, or non-profit organisations with nominal geographic affiliations from different parts of China) all contributed to their prosperity. In the 1890s Yap Ah Loy, who held the title of Kapitan China of Kuala Lumpur, was the richest man in Malaya, owning a chain of mines, plantations and shops. Malaya's banking and insurance industries were run by the Chinese from the start, and Chinese businesses, usually in partnership with London firms, soon had a stranglehold on the economy. Since the Malay Sultans tended to spend well beyond their means, they were soon indebted to Chinese bankers, and this gave the Chinese political as well as economic leverage. At first the Chinese immigrants were mostly men, and many intended to return home when they had made their fortunes. Many did go home, but many more stayed. At first they married Malay women, producing a community of Sino-Malayans or baba people, but soon they began importing Chinese brides, establishing permanent communities and building schools and temples.

The Indians were initially less successful, since unlike the Chinese they came mainly as indentured labourers to work in the rubber plantations, and had few of the economic opportunities that the Chinese had. They were also a less united community, since they were divided between Hindus and Muslims and along lines of language and caste. An Indian commercial and professional class emerged during the early 20th century, but the majority of Indians remained poor and uneducated in rural ghettos in the rubber-growing areas.

Traditional Malay society had great difficulty coping with both the loss of political sovereignty to the British and of economic power to the Chinese. By the early 20th century it seemed possible that the Malays would become a minority in their own country. The Sultans, who were seen as collaborators with both the British and the Chinese, lost some of their traditional prestige, particularly among the increasing number of Malays with a western education, but the mass of rural Malays continued to revere the Sultans and their prestige was thus an important prop for colonial rule. A small class of Malay nationalist intellectuals began to emerge during the early 20th century, and there was also a revival of Islam in response to the perceived threat of other imported religions, particularly Christianity. In fact few Malays converted to Christianity, although many Chinese did. The northern regions, which were less influenced by western ideas, became strongholds of Islamic conservatism, as they have remained.

The one consolation to Malay pride was that the British allowed them a virtual monopoly of positions in the police and local military units, as well as a majority of those administrative positions open to non-Europeans. While the Chinese mostly built and paid for their own schools and colleges, importing teachers from China, the colonial government fostered education for Malays, opening Malay College in 1905 and creating the Malay Administrative Service in 1910. (The college was dubbed “Bab ud-Darajat” – the Gateway to High Rank.) A Malay Teachers College followed in 1922, and a Malay Women's Training College in 1935. All this reflected the official British policy that Malaya belonged to the Malays, and that the other races were but temporary residents. This view was increasingly out of line with reality, and contained the seeds of much future trouble.

The Malay teacher's college had lectures and writings that nurtured Malay nationalism and anti-colonialist sentiments. Due to this it is known as the birthplace of Malay nationalism. In 1938, Ibrahim Yaacob, an alumnus of Sultan Idris College, established the Kesatuan Melayu Muda (Young Malays Union or KMM) in Kuala Lumpur. It was the first nationalist political organisation in British Malaya, advocating for the union of all Malays regardless of origin, and fighting for Malay rights and against British Imperialism. A specific ideal the KMM held was "Panji Melayu Raya", which called for the unification of British Malaya and Dutch East Indies.

In the years before World War II, the British were concerned with finding the balance between a centralised state and maintaining the power of the Sultans in Malaya. There were no moves to give Malaya a unitary government, and in fact in 1935 the position of Resident-General of the Federated States was abolished, and its powers decentralised to the individual states. With their usual tendency to racial stereotyping, the British regarded the Malays as amiable but unsophisticated and rather lazy, incapable of self-government, although making good soldiers under British officers. They regarded the Chinese as clever but dangerous—and indeed during the 1920s and 1930s, reflecting events in China, the Chinese Nationalist Party (the Kuomintang) and the Communist Party of China built rival clandestine organisations in Malaya, leading to regular disturbances in the Chinese towns. The British saw no way that Malaya's disparate collection of states and races could become a nation, let alone an independent one.

Although a belligerent as part of the British Empire, Malaya saw little action during World War I, except for the sinking of the Russian cruiser Zhemchug by the German cruiser SMS Emden on 28 October 1914 during the Battle of Penang.

The outbreak of war in the Pacific in December 1941 found the British in Malaya completely unprepared. During the 1930s, anticipating the rising threat of Japanese naval power, they had built a great naval base at Singapore, but never anticipated an invasion of Malaya from the north. Because of the demands of the war in Europe, there was virtually no British air capacity in the Far East. The Japanese were thus able to attack from their bases in French Indo-China with impunity, and despite stubborn resistance from British, Australian, and Indian forces, they overran Malaya in two months. Singapore, with no landward defences, no air cover, and no water supply, was forced to surrender in February 1942, doing irreparable damage to British prestige. British North Borneo and Brunei were also occupied.

The Japanese had a racial policy just as the British did. They regarded the Malays as a colonial people liberated from British imperialist rule, and fostered a limited form of Malay nationalism, which gained them some degree of collaboration from the Malay civil service and intellectuals. (Most of the Sultans also collaborated with the Japanese, although they maintained later that they had done so unwillingly.) The Malay nationalist Kesatuan Melayu Muda, advocates of "Melayu Raya", collaborated with the Japanese, based on the understanding that Japan would unite the Dutch East Indies, Malaya and Borneo and grant them independence. The occupiers regarded the Chinese, however, as enemy aliens, and treated them with great harshness: during the so-called "sook ching" (purification through suffering), up to 80,000 Chinese in Malaya and Singapore were killed. Chinese businesses were expropriated and Chinese schools either closed or burned down. Not surprisingly the Chinese, led by the Malayan Communist Party (MCP), became the backbone of the Malayan Peoples' Anti-Japanese Army (MPAJA), which with British assistance became the most effective resistance force in the occupied Asian countries.

Although the Japanese argued that they supported Malay nationalism, they offended Malay nationalism by allowing their ally Thailand to re-annex the four northern states, Kedah, Perlis, Kelantan, and Terengganu that had been surrendered to the British in 1909. The loss of Malaya's export markets soon produced mass unemployment which affected all races and made the Japanese increasingly unpopular.

During occupation, ethnic tensions were raised and nationalism grew. The Malayans were thus on the whole glad to see the British back in 1945, but things could not remain as they were before the war, and a stronger desire for independence grew. Britain was bankrupt and the new Labour government was keen to withdraw its forces from the East as soon as possible. Colonial self-rule and eventual independence were now British policy. The tide of colonial nationalism sweeping through Asia soon reached Malaya. But most Malays were more concerned with defending themselves against the MCP which was mostly made up of Chinese, than with demanding independence from the British; indeed, their immediate concern was that the British not leave and abandon the Malays to the armed Communists of the MPAJA, which was the largest armed force in the country.

In 1944, the British drew up plans for a Malayan Union, which would turn the Federated and Unfederated Malay States, plus Penang and Malacca (but not Singapore), into a single Crown colony, with a view towards independence. The Bornean territories and Singapore were left out as it was thought this would make union more difficult to achieve. There was however strong opposition from the Malays, who opposed the weakening of the Malay rulers and the granting of citizenship to the ethnic Chinese and other minorities. The British had decided on equality between races as they perceived the Chinese and Indians as more loyal to the British during the war than the Malays. The Sultans, who had initially supported it, backed down and placed themselves at the head of the resistance.

In 1946, the United Malays National Organisation (UMNO) was founded by Malay nationalists led by Dato Onn bin Jaafar, the Chief Minister of Johor. UMNO favoured independence for Malaya, but only if the new state was run exclusively by the Malays. Faced with implacable Malay opposition, the British dropped the plan for equal citizenship. The Malayan Union was thus established in 1946, and was dissolved in 1948 and replaced by the Federation of Malaya, which restored the autonomy of the rulers of the Malay states under British protection.

Meanwhile, the Communists were moving towards open insurrection. The MPAJA had been disbanded in December 1945, and the MCP organised as a legal political party, but the MPAJA's arms were carefully stored for future use. The MCP policy was for immediate independence with full equality for all races. This meant it recruited very few Malays. The Party's strength was in the Chinese-dominated trade unions, particularly in Singapore, and in the Chinese schools, where the teachers, mostly born in China, saw the Communist Party of China as the leader of China's national revival. In March 1947, reflecting the international Communist movement's “turn to left” as the Cold War set in, the MCP leader Lai Tek was purged and replaced by the veteran MPAJA guerrilla leader Chin Peng, who turned the party increasingly to direct action. These rebels, under the leadership of the MCP, launched guerrilla operations designed to force the British out of Malaya. In July, following a string of assassinations of plantation managers, the colonial government struck back, declaring a State of Emergency, banning the MCP and arresting hundreds of its militants. The Party retreated to the jungle and formed the Malayan Peoples’ Liberation Army, with about 13,000 men under arms, all Chinese.

The Malayan Emergency as it was known, lasted from 1948 to 1960, and involved a long anti-insurgency campaign by Commonwealth troops in Malaya. The British strategy, which proved ultimately successful, was to isolate the MCP from its support base by a combination of economic and political concessions to the Chinese and the resettlement of Chinese squatters into “New Villages” in “white areas” free of MCP influence. In December 1948, 24 villagers were executed by British troops. From 1949 the MCP campaign lost momentum and the number of recruits fell sharply. Although the MCP succeeded in assassinating the British High Commissioner, Sir Henry Gurney, in October 1951, this turn to terrorist tactics alienated many moderate Chinese from the Party. The arrival of Lt.-Gen Sir Gerald Templer as British commander in 1952 was the beginning of the end of the Emergency. Templer invented the techniques of counter-insurgency warfare in Malaya and applied them ruthlessly. Although the insurgency was defeated Commonwealth troops remained with the backdrop of the Cold War. Against this backdrop, independence for the Federation within the Commonwealth was granted on 31 August 1957, with Tunku Abdul Rahman as the first prime minister.

Chinese reaction against the MCP was shown by the formation of the Malayan Chinese Association (MCA) in 1949 as a vehicle for moderate Chinese political opinion. Its leader Tan Cheng Lock favoured a policy of collaboration with UMNO to win Malayan independence on a policy of equal citizenship, but with sufficient concessions to Malay sensitivities to ease nationalist fears. Tan formed a close collaboration with Tunku (Prince) Abdul Rahman, the Chief Minister of Kedah and from 1951 successor to Datuk Onn as leader of UMNO. Since the British had announced in 1949 that Malaya would soon become independent whether the Malayans liked it or not, both leaders were determined to forge an agreement their communities could live with as a basis for a stable independent state. The UMNO-MCA Alliance, which was later joined by the Malayan Indian Congress (MIC), won convincing victories in local and state elections in both Malay and Chinese areas between 1952 and 1955.

The introduction of elected local government was another important step in defeating the Communists. After Joseph Stalin’s death in 1953, there was a split in the MCP leadership over the wisdom of continuing the armed struggle. Many MCP militants lost heart and went home, and by the time Templer left Malaya in 1954 the Emergency was over, although Chin Peng led a diehard group that lurked in the inaccessible country along the Thai border for many years.

During 1955 and 1956 UMNO, the MCA and the British hammered out a constitutional settlement for a principle of equal citizenship for all races. In exchange, the MCA agreed that Malaya's head of state would be drawn from the ranks of the Malay Sultans, that Malay would be the official language, and that Malay education and economic development would be promoted and subsidised. In effect this meant that Malaya would be run by the Malays, particularly since they continued to dominate the civil service, the army and the police, but that the Chinese and Indians would have proportionate representation in the Cabinet and the parliament, would run those states where they were the majority, and would have their economic position protected. The difficult issue of who would control the education system was deferred until after independence. This came on 31 August 1957, when Tunku Abdul Rahman became the first Prime Minister of independent Malaya.

This left the unfinished business of the other British-ruled territories in the region. After the Japanese surrender the Brooke family and the British North Borneo Company gave up their control of Sarawak and North Borneo respectively, and these became British Crown Colonies. They were much less economically developed than Malaya, and their local political leaderships were too weak to demand independence. Singapore, with its large Chinese majority, achieved autonomy in 1955, and in 1959 the young socialist leader Lee Kuan Yew became Prime Minister. The Sultan of Brunei remained as a British client in his oil-rich enclave. Between 1959 and 1962 the British government orchestrated complex negotiations between these local leaders and the Malayan government.

On 24 April 1961 Lee Kuan Yew proposed the idea of forming Malaysia during a meeting to Tunku Abdul Rahman, after which Tunku invited Lee to prepare a paper elaborating on this idea. On 9 May, Lee sent the final version of the paper to Tunku and then deputy Malayan Prime Minister Abdul Razak. There were doubts about the practicality of the idea but Lee assured the Malayan government of continued Malay political dominance in the new federation. Razak supported the idea of the new federation and worked to convince Tunku to back it. On 27 May 1961, Abdul Rahman proposed the idea of forming "Malaysia", which would consist of Brunei, Malaya, North Borneo, Sarawak, and Singapore, all except Malaya still under British rule. It was stated that this would allow the central government to better control and combat communist activities, especially in Singapore. It was also feared that if Singapore became independent, it would become a base for Chinese chauvinists to threaten Malayan sovereignty. The proposed inclusion of British territories besides Singapore was intended to keep the ethnic composition of the new nation similar to that of Malaya, with the Malay and indigenous populations of the other territories cancelling out the Chinese majority in Singapore.

Although Lee Kuan Yew supported the proposal, his opponents from the Singaporean Socialist Front (Barisan Sosialis) resisted, arguing that this was a ploy for the British to continue controlling the region. Most political parties in Sarawak were also against the merger, and in North Borneo, where there were no political parties, community representatives also stated their opposition. Although the Sultan of Brunei supported the merger, the Parti Rakyat Brunei opposed it as well. At the Commonwealth Prime Ministers Conference in 1961, Abdul Rahman explained his proposal further to its opponents. In October, he obtained agreement from the British government to the plan, provided that feedback be obtained from the communities involved in the merger.
The Cobbold Commission, named after its head, Lord Cobbold, conducted a study in the Borneo territories and approved a merger with North Borneo and Sarawak; however, it was found that a substantial number of Bruneians opposed merger. North Borneo drew up a list of points, referred to as the 20-point agreement, proposing terms for its inclusion in the new federation. Sarawak prepared a similar memorandum, known as the 18-point agreement. Some of the points in these agreements were incorporated into the eventual constitution, some were instead accepted orally. These memoranda are often cited by those who believe that Sarawak's and North Borneo's rights have been eroded over time. A referendum was conducted in Singapore to gauge opinion, and 70% supported merger with substantial autonomy given to the state government. The Sultanate of Brunei withdrew from the planned merger due to opposition from certain segments of its population as well as arguments over the payment of oil royalties and the status of the sultan in the planned merger. Additionally, the Bruneian Parti Rakyat Brunei staged an armed revolt, which, though it was put down, was viewed as potentially destabilising to the new nation.

After reviewing the Cobbold Commission's findings, the British government appointed the Landsdowne Commission to draft a constitution for Malaysia. The eventual constitution was essentially the same as the 1957 constitution, albeit with some rewording; for instance, giving recognition to the special position of the natives of the Borneo States. North Borneo, Sarawak and Singapore were also granted some autonomy unavailable to the states of Malaya. After negotiations in July 1963, it was agreed that Malaysia would come into being on 31 August 1963, consisting of Malaya, North Borneo, Sarawak and Singapore. The date was to coincide with the independence day of Malaya and the British giving self-rule to Sarawak and North Borneo. However, Indonesia and the Philippines strenuously objected to this development, with Indonesia claiming Malaysia represented a form of "neocolonialism" and the Philippines claiming North Borneo as its territory. The opposition from the Indonesian government led by Sukarno and attempts by the Sarawak United People's Party delayed the formation of Malaysia. Due to these factors, an eight-member UN team was formed to re-ascertain whether North Borneo and Sarawak truly wanted to join Malaysia. Malaysia formally came into being on 16 September 1963, consisting of Malaya, North Borneo, Sarawak, and Singapore. In 1963 the total population of Malaysia was about 10 million.

At the time of independence Malaya had great economic advantages. It was among the world's leading producers of three valuable commodities; rubber, tin, and palm oil, and was also a significant iron ore producer. These export industries gave the Malayan government a healthy surplus to invest in industrial development and infrastructure projects. Like other developing nations in the 1950s and 1960s, Malaya (and later Malaysia) placed great stress on state planning, although UMNO was never a socialist party. The First and Second Malayan Plans (1956–60 and 1961–65 respectively) stimulated economic growth through state investment in industry and repairing infrastructure such as roads and ports, which had been damaged and neglected during the war and the Emergency. The government was keen to reduce Malaya's dependence on commodity exports, which put the country at the mercy of fluctuating prices. The government was also aware that demand for natural rubber was bound to fall as the production and use of synthetic rubber expanded. Since a third of the Malay workforce worked in the rubber industry it was important to develop alternative sources of employment. Competition for Malaya's rubber markets meant that the profitability of the rubber industry increasingly depended on keeping wages low, which perpetuated rural Malay poverty.

Both Indonesia and the Philippines withdrew their ambassadors from Malaya on 15 September 1963, the day before Malaysia's formation. In Jakarta the British and Malayan embassies were stoned, and the British consulate in Medan was ransacked with Malaya's consul taking refuge in the US consulate. Malaysia withdrew its ambassadors in response, and asked Thailand to represent Malaysia in both countries.

Indonesian President Sukarno, backed by the powerful Communist Party of Indonesia (PKI), chose to regard Malaysia as a "neocolonialist" plot against his country, and backed a Communist insurgency in Sarawak, mainly involving elements of the local Chinese community. Indonesian irregular forces were infiltrated into Sarawak, where they were contained by Malaysian and Commonwealth of Nations forces. This period of "Konfrontasi", an economic, political, and military confrontation lasted until the downfall of Sukarno in 1966. The Philippines objected to the formation of the federation, claiming North Borneo was part of Sulu, and thus the Philippines. In 1966 the new president, Ferdinand Marcos, dropped the claim, although it has since been revived and is still a point of contention marring Philippine-Malaysian relations.

The Depression of the 1930s, followed by the outbreak of the Sino-Japanese War, had the effect of ending Chinese emigration to Malaya. This stabilised the demographic situation and ended the prospect of the Malays becoming a minority in their own country. At the time of independence in 1957, Malays comprised 55% of the population, Chinese 35% and Indians 10%. This balance was altered by the inclusion of the majority-Chinese Singapore, upsetting many Malays. The federation increased the Chinese proportion to close to 40%. Both UMNO and the MCA were nervous about the possible appeal of Lee's People's Action Party (then seen as a radical socialist party) to voters in Malaya, and tried to organise a party in Singapore to challenge Lee's position there. Lee in turn threatened to run PAP candidates in Malaya at the 1964 federal elections, despite an earlier agreement that he would not do so (see PAP-UMNO Relations). Racial tensions intensified as PAP created an opposition alliance aiming for equality between races. This provoked Tunku Abdul Rahman to demand that Singapore withdraw from Malaysia. While the Singaporean leaders attempted to keep Singapore as a part of the Federation, the Malaysian Parliament voted 126–0 on 9 August 1965 in favor of the expulsion of Singapore.

The most vexed issues of independent Malaysia were education and the disparity of economic power among the ethnic communities. The Malays felt unhappy with the wealth of the Chinese community, even after the expulsion of Singapore. Malay political movements emerged based around this. However, since there was no effective opposition party, these issues were contested mainly within the coalition government, which won all but one seat in the first post-independence Malayan Parliament. The two issues were related, since the Chinese advantage in education played a large part in maintaining their control of the economy, which the UMNO leaders were determined to end. The MCA leaders were torn between the need to defend their own community's interests and the need to maintain good relations with UMNO. This produced a crisis in the MCA in 1959, in which a more assertive leadership under Lim Chong Eu defied UMNO over the education issue, only to be forced to back down when Tunku Abdul Rahman threatened to break up the coalition.

The Education Act of 1961 put UMNO's victory on the education issue into legislative form. Henceforward Malay and English would be the only teaching languages in secondary schools, and state primary schools would teach in Malay only. Although the Chinese and Indian communities could maintain their own Chinese and Tamil-language primary schools, all their students were required to learn Malay, and to study an agreed "Malayan curriculum". Most importantly, the entry exam to the University of Malaya (which moved from Singapore to Kuala Lumpur in 1963) would be conducted in Malay, even though most teaching at the university was in English until the 1970s. This had the effect of excluding many Chinese students. At the same time Malay schools were heavily subsidised, and Malays were given preferential treatment. This obvious defeat for the MCA greatly weakened its support in the Chinese community.

As in education, the UMNO government's unspoken agenda in the field of economic development aimed to shift economic power away from the Chinese and towards the Malays. The two Malayan Plans and the First Malaysian Plan (1966–1970) directed resources heavily into developments which would benefit the rural Malay community, such as village schools, rural roads, clinics, and irrigation projects. Several agencies were set up to enable Malay smallholders to upgrade their production and to increase their incomes. The Federal Land Development Authority (FELDA) helped many Malays to buy farms or to upgrade ones they already owned. The state also provided a range of incentives and low-interest loans to help Malays start businesses, and government tendering systematically favoured Malay companies, leading many Chinese-owned businesses to "Malayanise" their management. All this certainly tended to reduce to gap between Chinese and Malay standards of living, although some argued that this would have happened anyway as Malaysia's trade and general prosperity increased.

The collaboration of the MCA and the MIC in these policies weakened their hold on the Chinese and Indian electorates. At the same time, the effects of the government's affirmative action policies of the 1950s and 1960s had been to create a discontented class of educated but underemployed Malays. This was a dangerous combination, and led to the formation of a new party, the Malaysian People's Movement (Gerakan Rakyat Malaysia) in 1968. Gerakan was a deliberately non-communal party, bringing in Malay trade unionists and intellectuals as well as Chinese and Indian leaders. At the same time, an Islamist party, the Islamic Party of Malaysia (PAS) and a Democratic socialist party, the Democratic Action Party (DAP), gained increasing support, at the expense of UMNO and the MCA respectively.

Following the end of the Malayan Emergency in 1960, the predominantly ethnic Chinese Malayan National Liberation Army, armed wing of the Malayan Communist Party, had retreated to the Malaysian-Thailand border where it had regrouped and retrained for future offensives against the Malaysian government. The insurgency officially began when the MCP ambushed security forces in Kroh–Betong, in the northern part of Peninsular Malaysia, on 17 June 1968. Instead of declaring a "state of emergency" as the British had done previously, the Malaysian government responded to the insurgency by introducing several policy initiatives including the Security and Development Program (KESBAN), "Rukun Tetangga" (Neighbourhood Watch), and the RELA Corps (People’s Volunteer Group).

At the May 1969 federal elections, the UMNO-MCA-MIC Alliance polled only 48% of the vote, although it retained a majority in the legislature. The MCA lost most of the Chinese-majority seats to Gerakan or DAP candidates. The victorious opposition celebrated by holding a motorcade on the main streets of Kuala Lumpur with supporters holding up brooms as a signal of its intention to make sweeping changes. Fear of what the changes might mean for them (as much of the country's businesses were Chinese-owned), a Malay backlash resulted, leading rapidly to riots and inter-communal violence in which about 6,000 Chinese homes and businesses were burned and at least 184 people were killed. The government declared a state of emergency, and a National Operations Council, headed by Deputy Prime Minister Tun Abdul Razak, took power from the government of Tunku Abdul Rahman, who, in September 1970, was forced to retire in favour of Abdul Razak. It consisted of nine members, mostly Malay, and wielded full political and military power.

Using the Emergency-era Internal Security Act (ISA), the new government suspended Parliament and political parties, imposed press censorship and placed severe restrictions on political activity. The ISA gave the government power to intern any person indefinitely without trial. These powers were widely used to silence the government's critics, and have never been repealed. The Constitution was changed to make illegal any criticism, even in Parliament, of the Malaysian monarchy, the special position of Malays in the country, or the status of Malay as the national language.

In 1971 Parliament reconvened, and a new government coalition, the National Front (Barisan Nasional), was formed in 1973 to replace the Alliance party. The coalition consisted of UMNO, the MCA, the MIC, Gerakan, PPP, and regional parties in Sabah and Sarawak. The PAS also joined the Front but was expelled in 1977. The DAP was left outside as the only significant opposition party. Abdul Razak held office until his death in 1976. He was succeeded by Datuk Hussein Onn, the son of UMNO's founder Onn Jaafar, and then by Tun Mahathir Mohamad, who had been Education Minister since 1981, and who held power for 22 years. During these years policies were put in place which led to the rapid transformation of Malaysia's economy and society, such as the controversial New Economic Policy, which was intended to increase proportionally the share of the economic "pie" of the bumiputras as compared to other ethnic groups—was launched by Prime Minister Tun Abdul Razak. Malaysia has since maintained a delicate ethno-political balance, with a system of government that has attempted to combine overall economic development with political and economic policies that promote equitable participation of all races.

In 1970 three quarters of Malaysians living below the poverty line were Malays, the majority of Malays were still rural workers, and Malays were still largely excluded from the modern economy. The government's response was the New Economic Policy of 1971, which was to be implemented through a series of four five-year plans from 1971 to 1990. The plan had two objectives: the elimination of poverty, particularly rural poverty, and the elimination of the identification between race and prosperity. This latter policy was understood to mean a decisive shift in economic power from the Chinese to the Malays, who until then made up only 5% of the professional class.

Poverty was tackled through an agricultural policy which resettled 250,000 Malays on newly cleared farmland, more investment in rural infrastructure, and the creation of free trade zones in rural areas to create new manufacturing jobs. Little was done to improve the living standards of the low-paid workers in plantation agriculture, although this group steadily declined as a proportion of the workforce. By 1990 the poorest parts of Malaysia were rural Sabah and Sarawak, which lagged significantly behind the rest of the country. During the 1970s and ‘80s rural poverty did decline, particularly in the Malayan Peninsula, but critics of the government's policy contend that this was mainly due to the growth of overall national prosperity (due in large part to the discovery of important oil and gas reserves) and migration of rural people to the cities rather than to state intervention. These years saw rapid growth in Malaysian cities, particularly Kuala Lumpur, which became a magnet for immigration both from rural Malaya and from poorer neighbours such as Indonesia, Bangladesh, Thailand and the Philippines. Urban poverty became a problem for the first time, with shanty towns growing up around the cities.

The second arm of government policy, driven mainly by Mahathir first as Education Minister and then as Prime Minister, was the transfer of economic power to the Malays. Mahathir greatly expanded the number of secondary schools and universities throughout the country, and enforced the policy of teaching in Malay rather than English. This had the effect of creating a large new Malay professional class. It also created an unofficial barrier against Chinese access to higher education, since few Chinese are sufficiently fluent in Malay to study at Malay-language universities. Chinese families therefore sent their children to universities in Singapore, Australia, Britain or the United States – by 2000, for example, 60,000 Malaysians held degrees from Australian universities. This had the unintended consequence of exposing large numbers of Malaysians to life in Western countries, creating a new source of discontent. Mahathir also greatly expanded educational opportunities for Malay women – by 2000 half of all university students were women.
To find jobs for all these new Malay graduates, the government created several agencies for intervention in the economy. The most important of these were PERNAS (National Corporation Ltd.), PETRONAS (National Petroleum Ltd.), and HICOM (Heavy Industry Corporation of Malaysia), which not only directly employed many Malays but also invested in growing areas of the economy to create new technical and administrative jobs which were preferentially allocated to Malays. As a result, the share of Malay equity in the economy rose from 1.5% in 1969 to 20.3% in 1990, and the percentage of businesses of all kinds owned by Malays rose from 39 percent to 68 percent. This latter figure was deceptive because many businesses that appeared to be Malay-owned were still indirectly controlled by Chinese, but there is no doubt that the Malay share of the economy considerably increased. The Chinese remained disproportionately powerful in Malaysian economic life, but by 2000 the distinction between Chinese and Malay business was fading as many new corporations, particularly in growth sectors such as information technology, were owned and managed by people from both ethnic groups.

Malaysia's rapid economic progress since 1970, which was only temporarily disrupted by the Asian financial crisis of 1997, has not been matched by change in Malaysian politics. The repressive measures passed in 1970 remain in place. Malaysia has had regular elections since 1974, and although campaigning is reasonably free at election time, it is in effect a one-party state, with the UMNO-controlled National Front usually winning nearly all the seats, while the DAP wins some Chinese urban seats and the PAS some rural Malay ones. Since the DAP and the PAS have diametrically opposed policies, they have been unable to form an effective opposition coalition. There is almost no criticism of the government in the media and public protest remains severely restricted. The ISA continues to be used to silence dissidents, and the members of the UMNO youth movement are deployed to physically intimidate opponents.

The restoration of democracy after the 1969 crisis caused disputes in the UMNO, a struggle of power which increased after the death of Tun Abdul Razak. The ailing Datuk Hussein Bin Onn replaced him, but the fight for control shifted to appointing the deputy prime minister. Mahathir Mohamad was chosen, an advocate of Bumiputra who also tried to benefit the other ethnic communities.

Under the premiership of Mahathir Mohamad, Malaysia experienced economic growth from the 1980s, a 1985–86 property market depression, and returned to growth through to the mid-1990s. Mahathir increased privatisation and introduced the New Development Policy (NDP), designed to increase economic wealth for all Malaysians, rather than just Malays. The period saw a shift from an agriculture-based economy to one based on manufacturing and industry in areas such as computers and consumer electronics. It was during this period, too, that the physical landscape of Malaysia changed with the emergence of numerous mega-projects. Notable amongst these projects were the construction of the Petronas Twin Towers (at the time the tallest building in the world, and, as of 2016, still the tallest twin building), Kuala Lumpur International Airport (KLIA), the North–South Expressway, the Sepang International Circuit, the Multimedia Super Corridor (MSC), the Bakun hydroelectric dam, and Putrajaya, the new federal administrative capital.

Under Mahathir Mohamad’s long Prime Ministership (1981–2003), Malaysia's political culture became increasingly centralised and authoritarian, due to Mahathir's belief that the multiethnic Malaysia could only remain stable through controlled democracy. In 1986–87, he faced leadership challenges among his own party. There were also attacks by the government on several non-governmental organisations (NGO) which were critical of various government policies. There were also issues such the questioning by MCA's Lee Kim Sai over the use of the term "pendatang" (immigrants) that was seen as challenging Malay's bumiputra status, as well as rumours of forced conversion to or from Islam. Mahathir initiated a crackdown on opposition dissidents with the use of the Internal Security Act named Operation Lalang. The Internal Security Act was invoked in October 1987 arresting 106 people, including opposition leaders. The head of the judiciary and five members of the supreme court who had questioned his use of the ISA were also arrested, and a clampdown on Malaysia's press occurred.

This culminated in the dismissal and imprisonment on unsubstantiated charges of the Deputy Prime Minister, Anwar Ibrahim, in 1997 after an internal dispute within the government. The complicity of the judiciary in this piece of persecution was seen as a particularly clear sign of the decline of Malaysian democracy. The Anwar affair led to the formation of a new party, the People's Justice Party, or Keadilan, led by Anwar's wife, Wan Azizah Wan Ismail. At the 1999 elections Keadilan formed a coalition with the DAP and the PAS known as the Alternative Front (Barisan Alternatif). The result of this was that the PAS won a number of Malay seats from UMNO, but many Chinese voters disapproved of this unnatural alliance with the Islamist PAS, causing the DAP to lose many of its seats to the MCA, including that of its veteran leader, Lim Kit Siang. Wan Azizah won her husband's former constituency in Penang but otherwise Keadilan made little impact.

In the late 1990s, Malaysia was shaken by the Asian financial crisis, which damaged Malaysia's assembly line-based economy. Mahathir combated it initially with IMF approved policies. However, the devaluation of the Ringgit and the deepening recession caused him to create his own programme, based on protecting Malaysia from foreign investors and reinvigorating the economy through construction projects and the lowering of interest rates. The policies caused Malaysia's economy to rebound by 2002, but brought disagreement between Mahathir and his deputy, Anwar Ibrahim, who backed the IMF policies. This led to the sacking of the Anwar, causing political unrest. Anwar was arrested and banned from politics on what are considered trumped up charges. In 2003 Mahathir, Malaysia's longest serving prime minister, voluntarily retired in favour of his new deputy, Abdullah Ahmad Badawi. In November 2007 two anti-government rallies occurred, precipitated by allegations of corruption and discrepancies in the election system that heavily favoured the ruling political party, National Front, which has been in power since Malaya achieved independence.

Dato Seri Abdullah Ahmad Badawi freed Anwar, which was seen as a portent of a mild liberalisation. At the 2004 election, the National Front led by Abdullah had a massive victory, virtually wiping out the PAS and Keadilan, although the DAP recovered the seats it had lost in 1999. This victory was seen as the result mainly of Abdullah's personal popularity and the strong recovery of Malaysia's economy, which has lifted the living standards of most Malaysians to almost first world standards, coupled with an ineffective opposition. The government's objective is for Malaysia to become a fully developed country by 2020 as expressed in "Wawasan 2020". It leaves unanswered, however, the question of when and how Malaysia will acquire a first world political system (a multi-party democracy, a free press, an independent judiciary and the restoration of civil and political liberties) to go with its new economic maturity.

In November 2007, Malaysia was rocked by two anti-government rallies. The 2007 Bersih Rally which was attended by 40,000 people was held in Kuala Lumpur on 10 November 2007, to campaign for electoral reform. It was precipitated by allegations of corruption and discrepancies in the Malaysian election system that heavily favour the ruling political party, Barisan Nasional, which has been in power since Malaysia achieved its independence in 1957. Another rally was held on 25 November 2007, in Kuala Lumpur led by HINDRAF. The rally organiser, the Hindu Rights Action Force, had called the protest over alleged discriminatory policies favouring ethnic Malays. The crowd was estimated to be between 5,000 and 30,000. In both cases the government and police tried to prevent the gatherings from taking place.

On 16 October 2008, HINDRAF was banned when the government labelled the organisation as "a threat to national security".

Najib Razak entered office as Prime Minister with a sharp focus on domestic economic issues and political reform. On his first day as Prime Minister, Najib announced as his first actions the removal of bans on two opposition newspapers, "Suara Keadilan" and "Harakahdaily", run by the opposition leader Datuk Seri Anwar Ibrahim-led People's Justice Party and the Pan Islamic Party, respectively, and the release of 13 people held under the Internal Security Act. Among the released detainees were two ethnic Indian activists who were arrested in December 2007 for leading an anti-government campaign, three foreigners and eight suspected Islamic militants. Najib also pledged to conduct a comprehensive review of the much-criticised law which allows for indefinite detention without trial. In the speech, he emphasised his commitment to tackling poverty, restructuring Malaysian society, expanding access to quality education for all, and promoting renewed "passion for public service". He also deferred and abandoned the digital television transition plan of all free-to-air broadcasters such as Radio Televisyen Malaysia.

Malaysia Day, celebrating the formation of Malaysia on 16 September 1963, was declared a public holiday in 2010 in complement to the existing 31 August celebration of Hari Merdeka.

In September 2016 Mahathir submitted a request to the King requesting Najib be dismissed, although no action was taken on this.

Tun Dr Mahathir Mohamad, who left UMNO in 2016 and formed his own political party, which teamed up with three other political parties to form Pakatan Harapan, was sworn in as the Prime Minister of Malaysia after winning the election on the 10th of May, 2018. He defeated Najib Razak who led Barisan Nasional political party. Najib Razak was defeated by Tun Dr Mahathir Mohamad due to the factors such as the ongoing political scandal which is 1Malaysia Development Berhad scandal that has arisen since 2015, the introduction of Goods and Services Tax (Malaysia) of 6% since 1 April 2015, high cost of living and openly extreme criticism against the Tun Dr. Mahathir Mohamad.
The unpopular tax was reduced to 0% on 1 June 2018. Government of Malaysia under Tun Dr Mahathir tabled for first reading Bill to repeal GST in Parliament on 31 July 2018 (Dewan Rakyat). GST was successfully replaced with Sales Tax and Service Tax starting 1 September 2018.





</doc>
<doc id="13808" url="https://en.wikipedia.org/wiki?curid=13808" title="History of Israel">
History of Israel

The Land of Israel (also known as the Holy Land or Palestine) is the birthplace of the Jewish people, the place where the Hebrew Bible was composed and the birthplace of Judaism and Christianity. It contains sites sacred to Judaism, Christianity, Islam, Samaritanism, Druze and the Bahá'í Faith.

The region has come under the sway of various empires and been home to a variety of ethnicities, but was predominantly Jewish from roughly 1,000 years before the Common Era (BCE) until the 3rd century of the Common Era (CE). The adoption of Christianity by the Roman Empire in the 4th century led to a Greco-Roman Christian majority which lasted until the 7th century when the area was conquered by the Arab Muslim Empires. It gradually became predominantly Muslim until the Crusades between 1096 and 1291, when it was the focal point of conflict between Christianity and Islam. From the 13th century it was mainly Muslim with Arabic as the dominant language and was first part of the Syrian province of the Mamluk Sultanate and then part of the Ottoman Empire until the British conquest in 1917.

A Jewish national movement, Zionism, emerged in the late-19th century (partially in response to growing anti-Semitism), as part of which Aliyah (Jewish immigration) increased. During World War I, the British government publicly committed to create a Jewish National Home and was granted a Mandate to rule Palestine by the League of Nations for this purpose. A rival Arab nationalism also claimed rights over the former Ottoman territories and sought to prevent Jewish migration into Palestine, leading to growing Arab–Jewish tensions. Israeli independence in 1948 was accompanied by an exodus of Arabs from Israel, the Arab–Israeli conflict and a subsequent Jewish exodus from Arab and Muslim countries and Europe to Israel. About 43% of the world's Jews live in Israel today, the largest Jewish community in the world.

Since about 1970, the United States has become the principal ally of Israel. In 1979, an uneasy Egypt–Israel Peace Treaty was signed, based on the Camp David Accords. In 1993, Israel signed Oslo I Accord with the Palestine Liberation Organization, followed by establishment of the Palestinian National Authority and in 1994 Israel–Jordan peace treaty was signed. Despite efforts to finalize the peace agreement, the conflict continues to play a major role in Israeli and international political, social and economic life.

The economy of Israel was initially primarily socialist and the country dominated by social democratic parties until the 1970s. Since then the Israeli economy has gradually moved to capitalism and a free market economy, partially retaining the social welfare system.

Between 2.6 and 0.9 million years ago, at least four episodes of hominine dispersal from Africa to the Levant are known, each culturally distinct. The oldest evidence of early humans in the territory of modern Israel, dating to 1.5 million years ago, was found in Ubeidiya near the Sea of Galilee. The flint tool artefacts have been discovered at Yiron, the oldest stone tools found anywhere outside Africa. Other groups include 1.4 million years old Acheulean industry, the Bizat Ruhama group and Gesher Bnot Yaakov.

In the Carmel mountain range at el-Tabun, and Es Skhul, Neanderthal and early modern human remains were found, including the skeleton of a Neanderthal female, named Tabun I, which is regarded as one of the most important human fossils ever found. The excavation at el-Tabun produced the longest stratigraphic record in the region, spanning 600,000 or more years of human activity, from the Lower Paleolithic to the present day, representing roughly a million years of human evolution. Other notable Paleolithic sites include caves Qesem and Manot. The oldest fossils of anatomically modern humans found outside Africa are the Skhul and Qafzeh hominids, who lived in northern Israel 120,000 years ago. Around 10th millennium BCE, the Natufian culture existed in the area.

During the 2nd millennium BCE, Canaan, part of which later became known as Israel, was dominated by the New Kingdom of Egypt from c.1550 to c. 1180.

The first record of the name Israel (as "") occurs in the Merneptah stele, erected for Egyptian Pharaoh Merneptah (son of Ramses II) c. 1209 BCE, "Israel is laid waste and his seed is not." William G. Dever sees this "Israel" in the central highlands as a cultural and probably political entity, more an ethnic group rather than an organized state.

Ancestors of the Israelites may have included Semites native to Canaan and the Sea Peoples. McNutt says, "It is probably safe to assume that sometime during Iron Age I a population began to identify itself as 'Israelite'", differentiating itself from the Canaanites through such markers as the prohibition of intermarriage, an emphasis on family history and genealogy, and religion.

The archaeological evidence indicates a society of village-like centres, but with more limited resources and a small population. Villages had populations of up to 300 or 400, which lived by farming and herding, and were largely self-sufficient; economic interchange was prevalent. Writing was known and available for recording, even in small sites.

The first use of grapheme-based writing originated in the area, probably among Canaanite peoples resident in Egypt. This evolved into the Phoenician alphabet from which all modern alphabetical writing systems are descended. The Paleo-Hebrew alphabet was one of the first to develop and evidence of its use exists from about 1000 BCE (see the Gezer calendar), the language spoken was probably Biblical Hebrew.

Monotheism, the belief in a single all-powerful law-giving God is thought to have evolved among the Hebrew speakers gradually, over the next few centuries, from a number of separate cults, leading to the first versions of the religion now known as Judaism.

The Hebrew Bible describes constant warfare between the Israelites and the Philistines whose capital was Gaza. The Phillistines were Greek refugee-settlers who inhabited the southern Levantine coast. The Bible states that King David founded a dynasty of kings and that his son Solomon built a temple. Both David and Solomon are widely referenced in Jewish, Christian and Islamic texts. Standard Biblical chronology suggests that around 930 BCE, following the death of Solomon, the kingdom split into a southern Kingdom of Judah and a northern Kingdom of Israel. The Bible's Books of Kings state that soon after the split Pharoh "Shishaq" invaded the country plundering Jerusalem. An inscription over a gate at Karnak in Egypt recounts such an invasion by Pharoh Sheshonq I.

The archaeological evidence for this period is extremely sparse, leading some scholars to suggest that this section of the Hebrew Bible, which includes texts written two centuries later, exaggerates the importance of David and Solomon. The earliest references to the "House of David" have been found in two inscriptions, on the Tel Dan Stele and the Mesha Stele; the latter is a Moabite stele, now in the Louvre, which describes an 840 BCE invasion of Moab by Omri, king of Israel. Jehu, son of Omri, is referenced by Assyrian records (now in the British Museum). Modern archaeological findings show that Omri's capital city, Samaria, was large and Finkelstein has suggested that the Biblical account of David and Solomon are an attempt by later Judean rulers to ascribe Israel's successes to their dynasty.

In 854 BCE, according to Assyrian records (the Kurkh Monoliths) an alliance between Ahab of Israel and Ben Hadad II of Aram Damascus managed to repulse the incursions of the Assyrians, with a victory at the Battle of Qarqar. This is not included in the Bible which describes conflict between Ahab and Ben Hadad. Around 750 BCE, the Kingdom of Israel was destroyed by Assyrian king Tiglath-Pileser III. The Philistine kingdom was also destroyed. The Assyrians sent most of the population of the northern Israelite kingdom into exile, thus creating the "Lost Tribes of Israel". The Samaritans claim to be descended from survivors of the Assyrian conquest. An Israelite revolt (724–722 BCE) was crushed after the siege and capture of Samaria by the Assyrian king Sargon II.

Modern scholars believe that refugees from the destruction of Israel moved to Judah, massively expanding Jerusalem and leading to construction of the Siloam Tunnel during the rule of King Hezekiah (ruled 715–686 BCE). The tunnel could provide water during a siege and its construction is described in the Bible. A Hebrew plaque left by the construction team still exists.

Sargon's son, Sennacherib, tried and failed to conquer Judah, during Hezekiah's reign. Assyrian records say that Sennacherib leveled 46 walled cities and besieged Jerusalem, leaving after receiving extensive tribute. The Bible also refers to tribute, and suggests that Hezekiah was aided by Taharqa, king of Kush (now Sudan), in repulsing the Assyrians. The Twenty-fifth Dynasty of Egypt were Nubian Pharohs and they probably defeated the Assyrians. Sennacherib had a 12 meter by 5 meter frieze erected in his palace in Nineveh (now in Iraq) depicting his victory at Lachish, the second largest city in Judah.

The Bible describes a tradition of religious men ("prophets") exercising some form of free speech and criticizing rulers. The most famous of these was Isaiah, who witnessed the Assyrian invasion and warned of its consequences.

Under King Josiah (ruler from 641 - 619), the book of Deuteronomy was either rediscovered or written. The Book of Joshua and the accounts of the kingship of David and Solomon in the book of Kings are believed to have the same author. The books are known as Deuteronomist and considered to be a key step in the emergence of Monotheism in Judah. They emerged at a time that Assyria was weakened by the emergence of Babylon and may be a committing to text of pre-writing verbal traditions.

In 586 BCE King Nebuchadnezzar II of Babylon conquered Judah. According to the Hebrew Bible, he destroyed Solomon's Temple and exiled the Jews to Babylon. The Phillistines were also driven into exile. The defeat of Judah was recorded by the Babylonians (see the Babylonian Chronicles). Babylonian and Biblical sources suggest that the Judean king, Jehoiachin, switched allegiances between the Egyptians and the Babylonians and that invasion was a punishment for allying with Babylon's principal rival, Egypt. The exiled Jews may have been restricted to the elite. Jehoiachin was eventually released by the Babylonians. Tablets which seem to describe his rations were found in the ruins of Babylon (see Jehoiachin's Rations Tablets). According to both the Bible and the Talmud, the Judean royal family (the Davidic line) continued as head of Babylonian Jewry, called the "Rosh Galut" (head of exile). Arab and Jewish sources show that the "Rosh Galut" continued to exist (in what is now Iraq) for another 1,500 years, ending in the eleventh century.
In 538 BCE, Cyrus the Great of Persia conquered Babylon and took over its empire. Cyrus issued a proclamation granting subjugated nations (including the people of Judah) religious freedom (for the original text see the Cyrus Cylinder). According to the Hebrew Bible 50,000 Judeans, led by Zerubabel, returned to Judah and rebuilt the temple. A second group of 5,000, led by Ezra and Nehemiah, returned to Judah in 456 BCE although non-Jews wrote to Cyrus to try to prevent their return. Modern scholars believe that the final Hebrew versions of the Torah and Books of Kings date from this period, that the returning Israelites adopted an Aramaic script (also known as the Ashuri alphabet), which they brought back from Babylon; this is the current Hebrew script. The Hebrew calendar closely resembles the Babylonian calendar and probably dates from this period.

The Persians also conquered Egypt, posting a Judean military garrison on Elephantine Island near Aswan. In the early 20th century 175 papyrus documents were discovered, recording activity in this community, including the "Passover Papyrus", a letter instructing the garrison on how to correctly conduct the Passover feast.

In 333 BCE, the Macedonian ruler Alexander the Great defeated Persia and conquered the region. Sometime thereafter, the first translation of the Hebrew Bible, the Septuagint, was begun in Alexandria. After Alexander's death, his generals fought over the territory he had conquered. Judah became the frontier between the Seleucid Empire and Ptolemaic Egypt, eventually becoming part of the Seleucid Empire in 200 BCE at the battle of Panium (fought near Banias on the Golan Heights).

In the 2nd century BCE, Seleucid ruler Antiochus IV Epiphanes tried to eradicate Judaism in favour of Hellenistic religion. This provoked the 174–135 BCE Maccabean Revolt led by Judas Maccabeus (whose victory is celebrated in the Jewish festival of Hanukkah). The Books of the Maccabees describe the uprising and the end of Greek rule.
A Jewish party called the Hasideans opposed both Hellenism "and" the revolt, but eventually gave their support to the Maccabees. Modern interpretations see the initial stages of the uprising as a civil war between Hellenised and orthodox forms of Judaism.

The Hasmonean dynasty of Jewish priest-kings ruled Judea with the Pharisees, Sadducees and Essenes as the principal Jewish social movements. As part of the struggle against Hellenistic civilisation, the Pharisee leader Simeon ben Shetach established the first schools based around meeting houses. This led to Rabbinical Judaism. Justice was administered by the Sanhedrin, which was a Rabbincal assembly and law court whose leader was known as the Nasi. The Nasi's religious authority gradually superseded that of the Temple's high priest, who under the Hasmoneans was the king himself.

The Hasmoneans continually extended their control over much of the region. In 125 BCE the Hasmonean ethnarch John Hyrcanus subjugated Edom and forcibly converted its population to Judaism.

Hyrcanus' son Alexander Jannaeus established good relations with the Roman Republic, however there was growing tension between Pharisees and Sadducces and a conflict over the succession to Janneus, in which the warring parties invited foreign intervention on their behalf.

In 64 BCE the Roman general Pompey conquered Syria and intervened in the Hasmonean civil war in Jerusalem. During the siege of Alexandria in 47 BCE, the lives of Julius Caesar and his protege Cleopatra were saved by 3,000 Jewish troops sent by King Hyrcanus II and commanded by Antipater, whose descendants Caesar made kings of Judea.

From 37 BCE to 6 CE, the Herodian dynasty, Jewish-Roman client kings, descended from Antipater, ruled Judea. Herod the Great considerably enlarged the temple (see Herod's Temple), making it one of the largest religious structures in the world. Despite its fame, it was in this period that Rabbinical Judaism, led by Hillel the Elder, began to assume popular prominence over the Temple priesthood. The Jewish Temple in Jerusalem was granted special permission not to display an effigy of the emperor, becoming the only religious structure in the Roman Empire that did not do so. Special dispensation was granted for Jewish citizens of the Roman Empire to pay a tax to the temple.

Augustus made Judea a Roman province in 6 CE, deposing the last Jewish king, Herod Archelaus, and appointing a Roman governor. There was a small revolt against Roman taxation led by Judas of Galilee and over the next decades tensions grew between the Greco-Roman and Judean population centered on attempts to place effigies of the Emperor Caligula in Synagogues and in the Jewish temple.

Jesus was born in the last years of Herod's rule, probably in the Judean city of Bethlehem. Jesus is thought to have been a Galilean Jewish reformer (from Nazareth), and was executed in Jerusalem by the Roman governor Pontius Pilate between 25 and 35 CE. All his key followers, the Twelve Apostles, were Jews including Paul the Apostle (5–67 CE) who took critical steps towards creating a new religion, defining Jesus as the "Son of God". In the year 50 CE, the Council of Jerusalem led by Paul, decided to abandon the Jewish requirement of circumcision and the Torah, creating a form of Judaism highly accessible to non-Jews and with a more universal notion of God. Another Jewish follower, Peter is believed to have become the first Pope.

In 64 CE, the Temple High Priest Joshua ben Gamla introduced a religious requirement for Jewish boys to learn to read from the age of six. Over the next few hundred years this requirement became steadily more ingrained in Jewish tradition.

In 66 CE, the Jews of Judea rose in revolt against Rome, naming their new state as "Israel". The events were described by the Jewish leader and historian Josephus, including the defence of Jotapata, the siege of Jerusalem (69–70 CE) and the desperate last stand at Masada under Eleazar ben Yair (72–73 CE).

Josephus estimated that over a million people died in the siege of Jerusalem. The Temple and most of Jerusalem was destroyed. During the Jewish revolt, most Christians, at this time a sub-sect of Judaism, removed themselves from Judea. The rabbinical/Pharisee movement led by Yochanan ben Zakai, who opposed the Sadducee temple priesthood, made peace with Rome and survived. After the war Jews continued to be taxed in the Fiscus Judaicus, which was used to fund a temple to Jupiter. A victory arch erected in Rome can still be seen today.

Tensions and attacks on Jews around the Roman Empire led to a massive Jewish uprising against Rome from 115 to 117. Jews in Libya, Egypt, Cyprus and Mesopotamia fought against Rome. This conflict was accompanied by large-scale massacres of both sides. Cyprus was so severely depopulated that new settlers were imported and Jews banned from living there.

In 131, the Emperor Hadrian renamed Jerusalem "Aelia Capitolina" and constructed a Temple of Jupiter on the site of the former Jewish temple. Jews were banned from living in Jerusalem itself (a ban that persisted until the Arab conquest), and the Roman province, until then known as Iudaea Province, was renamed Palaestina, no other revolt led to a province being renamed. The names "Palestine" (in English) and "Filistin" (in Arabic) are derived from this.
From 132 to 136, the Jewish leader Simon Bar Kokhba led another major revolt against the Romans, again renaming the country "Israel" (see Bar Kochba Revolt coinage). The Bar-Kochba revolt probably caused more trouble for the Romans than the better documented revolt of 70. Christians refused to participate in the revolt and from this point the Jews regarded Christianity as a separate religion. The revolt was eventually crushed by Emperor Hadrian himself. During the Bar Kokhba revolt a rabbinical assembly decided which books could be regarded as part of the Hebrew Bible: the Jewish apocrypha and Christian books were excluded. As a result, the original text of some Hebrew texts, including the Books of Maccabees were lost (Greek translations survived).

A rabbi of this period, Simeon bar Yochai, is regarded as the author of the Zohar, the foundational text for Kabbalistic thought. However, modern scholars believe it was written in Medieval Spain.

After suppressing the Bar Kochba revolt, the Romans exiled the Jews of Judea, but not of Galilee and permitted a hereditary Rabbinical Patriarch (from the House of Hillel, based in Galilee) to represent the Jews in dealings with the Romans. The most famous of these was Judah haNasi who is credited with compiling the final version of the Mishnah (a massive body of Jewish religious texts interpreting the Bible) and with strengthening the educational demands of Judaism by requiring that illiterate Jews be treated as outcasts. As a result, many illiterate Jews may have converted to Christianity. Jewish seminaries, such as those at Shefaram and Bet Shearim continued to produce scholars and the best of these became members of the Sanhedrin, which was located first at Sepphoris and later at Tiberias. Before the Bar-Kochba uprising, an estimated 2/3 of the population of Gallilee and 1/3 of the coastal region were Jewish. In the Galillee, many synagogues have been found dating from this period. However, economic mismanagement of the Roman economy in the third century led to a collapse of Roman trade and empire, as well as increased taxation and persecution, which caused most Jews to migrate to the more tolerant Persian Sassanid Empire, where a prosperous Jewish community with extensive seminaries existed in the area of Babylon.

Early in the 4th century, the Emperor Constantine made Constantinople the capital of the East Roman Empire and made Christianity an accepted religion. His mother, Helena made a pilgrimage to Jerusalem (326–328) and led the construction of the Church of the Nativity (Bethlehem), the Church of the Holy Sepulchre (Jerusalem) and other key churches that still exist. The name Jerusalem was restored to Aelia Capitolina and it became a Christian city. Jews were still banned from living in Jerusalem, but were allowed to visit, and it is in this period that the surviving Western Wall of the Temple became sacred to Judaism.

In 351–2, another Jewish revolt in the Galilee erupted against a corrupt Roman governor. In 362, the last pagan Roman Emperor, Julian the Apostate, announced plans to rebuild the Jewish Temple. He died while fighting the Persians in 363 and the project was discontinued.

In 380 Emperor Theodosius I, the last Emperor of a united Roman Empire, made Christianity the official religion of the Roman Empire.

The Roman Empire split in 390 CE and the region became part of the (Christian) East Roman Empire, known as the Byzantine Empire. Byzantine Christianity was dominated by the (Greek) Eastern Orthodox Church whose massive land ownership has extended into the present. In the 5th century, the Western Roman Empire collapsed leading to Christian migration into the Roman province of Palaestina Prima and development of a Christian majority. Jews numbered 10–15% of the population, concentrated largely in the Galilee. Judaism was the only non-Christian religion tolerated, but restrictions on Jews slowly increased to include a ban on building new places of worship, holding public office or owning slaves. In 425, following the death of the last Nasi, Gamliel VI, the Sanhedrin was officially abolished and the title of Nasi banned. Several Samaritan Revolts erupted in this period, resulting in the decrease of Samaritan community from about a million to a near extinction. Sacred Jewish texts written in Palestine at this time are the Gemara (400), the Jerusalem Talmud (500) and the Passover Haggadah.

In 495 Mar-Zutra II (the Exilarch), set up an independent Jewish city-state in what is now Iraq. It lasted seven years and after its fall, his son Mar-Zutra III moved to Tiberias where he became head of the local religious academy in 520.

The Jewish Menorah, which the Romans took when the temple was destroyed, was reportedly taken to Carthage by the Vandals after the sacking of Rome in 455. According to the Byzantine historian, Procopius, the Byzantine army recovered it in 533 and brought it to Constantinople.

In 611, Sassanid Persia invaded the Byzantine Empire and, after a long siege, Khosrau II captured Jerusalem in 614, with Jewish help, including possibly the Jewish Himyarite Kingdom in Yemen. Jews briefly governed Jerusalem when the Persians took over. The Byzantine Emperor Heraclius reportedly revoked his brother's decision to extirpate the Jews of Edessa for fighting on the Persian side during the war by giving them a blanket pardon, and provided protection to the Jews of the Galilee. He allegedly managed to convince Benjamin of Tiberias to convert, after the latter admitted Jews had massacred Christians during the Persian interregnum. On the other hand, on hearing of the slaughter perpetrated by Jews on Christians at the Mamilla Pool after the Sassanian conquest, he reneged on his prior commitment to guarantee their safety, permitting the massacre of any Jew who might be caught in the neighbourhood of Jerusalem or the Galilee,> and is said in one source to have taken the unprecedented step of decreeing the forced conversion of Jews (and Samaritans), something which met opposition from ecclesiastical authorities, and is unambiguously attested only in Carthage. (Egyptian) Coptic Christians took responsibility for this broken pledge and fasted in penance.

According to Muslim tradition, on the last night of his life in 620, Muhammed was taken on spiritual journey from Mecca to the "farthest mosque", whose location many consider to be the Temple Mount, returning the same night. In 634–636 the Arabs conquered Palaestina Prima and renamed it Jund Filastin, ending the Byzantine ban on Jews living in Jerusalem. Over the next few centuries, Islam replaced Christianity as the dominant religion of the region.

From 636 until the beginning of the Crusades, Jund Filastin was ruled first by Medinah-based Rashidun Caliphs, then by the Damascus-based Umayyad Caliphate and after that the Baghdad-based Abbasid Caliphs. In 691, Umayyad Caliph Abd al-Malik (685–705) constructed the Dome of the Rock shrine on the Temple Mount. Jews consider it to contain the Foundation Stone (see also Holy of Holies), which is the holiest site in Judaism. A second building, the Al-Aqsa Mosque, was also erected on the Temple Mount in 705.

Between the 7th and 11th centuries, Jewish scribes, called the Masoretes and located in Galilee and Jerusalem, established the Masoretic Text, the final text of the Hebrew Bible.

In 1099, the First Crusade took Jerusalem and established a Catholic kingdom, known as the Kingdom of Jerusalem. During the conquest, both Muslims and Jews were indiscriminately massacred or sold into slavery. The murder of Jews began as the Crusaders travelled across Europe and continued when they reached the Holy Land. Ashkenazi orthodox Jews still recite a prayer in memory of the death and destruction caused by the Crusades.

Around 1180, Raynald of Châtillon, ruler of Transjordan, caused increasing conflict with the Ayyubid Sultan Saladin (Salah-al-Din), leading to the defeat of the Crusaders in the 1187 Battle of Hattin (above Tiberias). Saladin was able to peacefully take Jerusalem and conquered most of the former Kingdom of Jerusalem. Saladin's court physician was Maimonides, a refugee from persecution in Córdoba, Spain. This was the end of the Golden age of Jewish culture in Spain and Maimonides possessed extensive knowledge of Greek and Arab medicine. His religious writings (in Hebrew and Judeo-Arabic) are still studied by Orthodox Jews. Maimonides was buried in Tiberias. A Crusader state centred round Acre survived in weakened form for another century.

From 1260 to 1291 the area became the frontier between Mongol invaders (occasional Crusader allies) and the Mamluks of Egypt. The conflict impoverished the country and severely reduced the population. Sultan Qutuz of Egypt eventually defeated the Mongols in the Battle of Ain Jalut ("Goliath's spring" near Ein Harod), ending the Mongol advances, and his successors eliminated the Crusader states. The last Crusader state, the Kingdom of Acre, fell in 1291, ending the Crusades.

Egyptian Mamluk sultan, Baibars (1260–1277), conquered the last outposts of Crusader rule in 1291. The Mamluks ruled Palestine until 1516, regarding it as part of Syria. In Hebron, Baibars banned Jews from worshipping at the Cave of the Patriarchs (the second-holiest site in Judaism); the ban remained in place until its conquest by Israel 700 years later.

The Mamluks, continuing the policy of the Ayyubids, made the strategic decision to destroy the coastal area and to bring desolation to many of its cities, from Tyre in the north to Gaza in the south. Ports were destroyed and various materials were dumped to make them inoperable. The goal was to prevent attacks from the sea, given the fear of the return of the crusaders. This had a long-term effect on those areas, which remained sparsely populated for centuries. The activity in that time concentrated more inland.

The collapse of the Crusades was followed by increased persecution and expulsions of Jews in Europe. Expulsions began in England (1290) and were followed by France (1306). In Spain, persecution of the highly integrated and successful Jewish community began, including massacres and forced conversions. During the Black Death, many Jews were murdered after being accused of poisoning wells. The completion of the Christian reconquest of Spain led to expulsion of the Jews of Spain in 1492 and Portugal in 1497. These were the wealthiest and most integrated Jewish communities in Europe. Many Jews converted to Christianity, however many secretly practised Judaism and prejudice against converts (regardless of their sincerity) persisted, leading many former Jews to move to the New World (see History of the Jews in Latin America). Most of the expelled Spanish Jews moved to North Africa, Poland, to the Ottoman Empire and to the region of Bilad a-Sham, which roughly corresponds to the ancient Kingdom of Israel (united monarchy). In Italy, Jews living in the Papal States were required to live in ghettos (see Cum nimis absurdum). The last compulsory Ghetto, in Rome, was abolished in the 1880s.

Under the Mamluks, the area was a province of Bilad a-Sham (Syria). It was conquered by Turkish Sultan Selim I in 1516–17, becoming a part of the province of Ottoman Syria for the next four centuries, first as the Damascus Eyalet and later as the Syria Vilayet (following the Tanzimat reorganization of 1864).

The Ottoman Sultans encouraged Jews fleeing the inquisition in Catholic Europe to settle in the Ottoman Empire. Some of the Sultans' favorite sex-slaves may have been Jewish girls enslaved in Eastern Europe, and at least one vizier was of Jewish origin. Suleiman the Magneficent's personal physician was Moses Hamon, an inquisition survivor and Jewish businesswomen dominated communication beween the Harem and the outside world (see Esther Handali). Between 1535 and 1538 Suleiman the Magnificent (ruled 1520 - 1566) built the current city walls of Jerusalem; Jerusalem had been without walls since the early 13th century. The construction followed the historical outline of the city, but left out a key section of the City of David (today part of Silwan) and what is now known as Mount Zion.

In 1558 Selim II (1566 - 1574), successor to Suleiman, (his wife Nurbanu Sultan may have been of Jewish origin) gave control of Tiberias to Doña Gracia Mendes Nasi, one of the richest women in Europe and an escapee from the inquisition. She encouraged Jewish refugees to settle in the area and established a Hebrew printing press. Safed became a centre for study of the Kabbalah. Doña Nasi's nephew, Joseph Nasi, was made governor of Tiberias and he encouraged Jewish settlement from Italy. 

Jewish population was concentrated in Jerusalem, Hebron, Safed and Tiberias, known in Jewish tradition as the Four Holy Cities. Further migration occurred during the Khmelnytsky Uprising in Ukraine, which was accompanied by brutal massacres of tens of thousands of Jews (and possibly more).

In 1660, a Druze revolt led to the destruction of Safed and Tiberias. In 1663 Sabbatai Zevi settled in Jerusalem, and was proclaimed as the Jewish Messiah by Nathan of Gaza. He acquired a large number of followers before going to Istanbul in 1666, where Sultan Suleiman II forced him to convert to Islam. Many of his followers converted, forming a sect that still exists in Turkey, known as the Dönmeh. In the late 18th century a local Arab "sheikh" Zahir al-Umar created a "de facto" independent Emirate in the Galilee. Ottoman attempts to subdue the Sheikh failed, but after Zahir's death the Ottomans restored their rule in the area.

In 1799 Napoleon briefly occupied the country and planned a proclamation inviting Jews to create a state. The proclamation was shelved following his defeat at Acre. In 1831, Muhammad Ali of Egypt, an Ottoman ruler who left the Empire and tried to modernize Egypt, conquered Ottoman Syria and tried to revive and resettle much of its regions. His conscription policies led to a popular Arab revolt in 1834, resulting in major casualties for the local Arab peasants, and massacres of Christian and Jewish communities by the rebels. Following the revolt, Muhammad Pasha, the son of Muhammad Ali, expelled nearly 10,000 of the local peasants to Egypt, while bringing loyal Egyptian peasants and discharged soldiers to settle the coastline of Ottoman Syria. Northern Jordan Valley was settled by his Sudanese troops.
In 1838 there was another revolt by the Druze. In 1839 Moses Montefiore met with Muhammed Pasha in Egypt and signed an agreement to establish 100–200 Jewish villages in the Damascus Eyalet of Ottoman Syria, but in 1840 the Egyptians withdrew before the deal was implemented, returning the area to Ottoman governorship. In 1844, Jews constituted the largest population group in Jerusalem and by 1890 an absolute majority in the city, but as a whole the Jewish population made up far less than 10% of the country. In 1868, the Ottomans banished the Bahá'u'lláh, one of the founders of the Bahá'í Faith, to Acre where he is buried, and the movement subsequently established its global administrative centre in nearby Haifa. In 1874, Ottoman reforms led to the area of Jerusalem gaining special status as the Mutasarrifate of Jerusalem.

During the 19th century, Jews in Western Europe were increasingly granted citizenship and equality before the law; however, in Eastern Europe, they faced growing persecution and legal restrictions, including widespread pogroms in which thousands were murdered, raped or lost their property. Half the world's Jews lived in the Russian Empire, where they were severely persecuted and restricted to living in the Pale of Settlement. National groups in the Empire, such as the Poles, Lithuanians and Ukrainians were agitating for independence and often regarded the Jews as undesirable aliens. The Jews were usually the only non-Christian minority and spoke a distinct language (Yiddish). An independent Jewish national movement first began to emerge in the Russian Empire and the millions of Jews who were fleeing the country (mostly to the USA) carried the seeds of this nationalism wherever they went.

In 1870, an agricultural school, the Mikveh Israel, was founded near Jaffa by the Alliance Israelite Universelle, a French Jewish association. In 1878, "Russian" Jewish emigrants established the village of Petah Tikva, followed by Rishon LeZion in 1882. "Russian" Jews established the Bilu and Hovevei Zion ("Lovers of Zion") movements to assist settlers and these created communities that, unlike the traditional Ashkenazi-Jewish communities, sought to be self-reliant rather than dependent on donations from abroad. Existing Ashkenazi-Jewish communities were concentrated in the Four Holy Cities, extremely poor and lived on donations from Europe. The new migrants created small agricultural settlements. In Jaffa a vibrant commercial community developed in which Ashkenazi and Sephardi Jews inter-mingled. Many early migrants left due to difficulty finding work and the early settlements often remained dependent on foreign donations. Despite the difficulties, more settlements arose and the community grew.

The new migration was accompanied by a revival of the Hebrew language and attracted Jews of all kinds; religious, secular, nationalists and left-wing socialists. Socialists aimed to reclaim the land by becoming peasants or workers and forming collectives. In Zionist history, the different waves of Jewish settlement are known as "aliyah". During the First Aliyah, between 1882 and 1903, approximately 35,000 Jews moved to what is now Israel. The first wave coincided with a wave of Jewish migration and Messianism among Yemenite Jews and Bukharan Jews. By 1890, Jews were a majority in Jerusalem, although the country as a whole was populated mainly by Muslim (settled and nomad Bedouins) and Christian Arabs.

In 1896 Theodor Herzl published "Der Judenstaat" ("The Jewish State"), in which he asserted that the solution to growing antisemitism in Europe (the so-called "Jewish Question") was to establish a Jewish state. In 1897, the Zionist Organisation was founded and the First Zionist Congress proclaimed its aim "to establish a home for the Jewish people in Palestine secured under public law." However, Zionism was regarded with suspicion by the Ottoman rulers and was unable to make major progress.

Between 1904 and 1914, around 40,000 Jews settled in the area now known as Israel (the Second Aliyah). In 1908 the Zionist Organisation set up the Palestine Bureau (also known as the "Eretz Israel Office") in Jaffa and began to adopt a systematic Jewish settlement policy. Migrants were mainly from Russia (which then included part of Poland), escaping persecution. The first Kibbutz, Degania, was founded by nine Russian socialists in 1909. In 1909 residents of Jaffa established the first entirely Hebrew-speaking city, Ahuzat Bayit (later renamed Tel Aviv). Hebrew newspapers and books were published, Hebrew schools, Jewish political parties and workers organizations were established.

During World War I, most Jews supported the Germans because they were fighting the Russians who were regarded as the Jews' main enemy. In Britain, the government sought Jewish support for the war effort for a variety of reasons including an erroneous antisemitic perception of "Jewish power" over the Ottoman Empire's Young Turks movement, and a desire to secure American Jewish support for US intervention on Britain's behalf.

There was already sympathy for the aims of Zionism in the British government, including the Prime Minister Lloyd George. In late 1917, the British Army drove the Turks out of Southern Syria, and the British foreign minister, Lord Balfour, sent a public letter to Lord Rothschild, a leading member of his party and leader of the Jewish community. The letter subsequently became known as the Balfour Declaration of 1917. It stated that the British Government "view[ed] with favour the establishment in Palestine of a national home for the Jewish people". The declaration provided the British government with a pretext for claiming and governing the country. New Middle Eastern boundaries were decided by an agreement between British and French bureaucrats. The agreement gave Britain control over what parties would begin to call "Palestine".

A Jewish Legion composed largely of Zionist volunteers organized by Jabotinsky and Trumpeldor participated in the British invasion. It also participated in the failed Gallipoli Campaign. A Zionist spy network provided the British with details of Ottoman troops.

The British Mandate (in effect, British rule) of Palestine, including the Balfour Declaration, was confirmed by the League of Nations in 1922 and came into effect in 1923. The boundaries of Palestine initially included modern Jordan, which was removed from the territory by Churchill a few years later. Britain signed a treaty with the United States (which did not join the League of Nations) in which the United States endorsed the terms of the Mandate.

Between 1919 and 1923, another 40,000 Jews arrived in Palestine, mainly escaping the post-revolutionary chaos of Russia (Third Aliyah), as over 100,000 Jews were massacred in this period in Ukraine and Russia. Many of these immigrants became known as "pioneers" (halutzim), experienced or trained in agriculture and capable of establishing self-sustaining economies. Malarial marshes in the Jezreel Valley and Hefer Plain were drained and converted to agricultural use. Land was bought by the Jewish National Fund, a Zionist charity that collected money abroad for that purpose. A mainly socialist underground Jewish militia, Haganah ("Defense"), was established to defend outlying Jewish settlements.
The French victory over the Arab Kingdom of Syria and the Balfour Declaration led to the emergence of Palestinian Nationalism and Arab rioting in 1920 and 1921. In response, the British authorities imposed immigration quotas for Jews. Exceptions were made for Jews with over 1,000 pounds in cash (roughly 100,000 pounds at year 2000 rates) or Jewish professionals with over 500 pounds. The Jewish Agency issued the British entry permits and distributed funds donated by Jews abroad. Between 1924 and 1929, 82,000 more Jews arrived (Fourth Aliyah), fleeing antisemitism in Poland and Hungary, and because the United States Immigration Act of 1924 now kept Jews out. The new arrivals were mainly middle-class families who moved into towns and established small businesses and workshops—although lack of economic opportunities meant that approximately a quarter later left. The first electricity generator was built in Tel Aviv in 1923 under the guidance of Pinhas Rutenberg, a former Commissar of St Petersburg in Russia's pre-Bolshevik Kerensky Government. In 1925 the Jewish Agency established the Hebrew University in Jerusalem and the Technion (technological university) in Haifa. British authorities introduced the Palestine pound (worth 1000 "Mills") in 1927, replacing the Egyptian pound as the unit of currency in the Mandate.

From 1928, the democratically elected Va'ad Leumi (Jewish National Council or JNC) became the main institution of the Palestine Jewish community ("Yishuv") and included non-Zionist Jews. As the Yishuv grew, the JNC adopted more government-type functions, such as education, health care and security. With British permission, the Va'ad raised its own taxes and ran independent services for the Jewish population. From 1929 its leadership was elected by Jews from 26 countries.

In 1929 tensions grew over the Kotel (Wailing Wall), a narrow alleyway where Jews were banned from using chairs or any furniture (many of the worshipers were elderly). The Mufti claimed it was Muslim property and that the Jews were seeking control of the Temple Mount. This (and general animosity) led to the August 1929 Palestine riots. The main victims were the ancient Jewish community at Hebron, which came to an end. The riots led to right-wing Zionists establishing their own militia in 1931, the Irgun Tzvai Leumi (National Military Organization, known in Hebrew by its acronym "Etzel").

Zionist political parties provided private education and health care: the General Zionists, the Mizrahi and the Socialist Zionists, each established independent health and education services and operated sports organizations funded by local taxes, donations and fees (the British administration did not invest in public services). During the whole interwar period, the British, appealing to the terms of the Mandate, rejected the principle of majority rule or any other measure that would give the Arab population, who formed the majority of the population, control over Palestinian territory.

In 1933, the Jewish Agency and the Nazis negotiated the Ha'avara Agreement (transfer agreement), under which 50,000 Jews would be transferred to Palestine. The Jews' possessions were confiscated and in return the Nazis allowed the Ha'avara organization to purchase 14 million pounds worth of German goods for export to Palestine and use it to compensate the immigrants. Although many Jews wanted to leave Nazi Germany, the Nazis prevented Jews from taking any money and restricted them to two suitcases so few could pay the British entry tax and many were afraid to leave. The agreement was controversial and the Labour Zionist leader who negotiated the agreement, Haim Arlosoroff, was assassinated in Tel Aviv in 1933. The assassination was used by the British to create tension between the Zionist left and the Zionist right. Arlosoroff had been the boyfriend of Magda Ritschel some years before she married Joseph Goebbels. There has been speculation that he was assassinated by the Nazis to hide the connection but there is no evidence for it. In Palestine, Jewish immigration (and the Ha'avara goods) helped the economy to flourish. The British used the taxes paid by the Jewish population to build a port and oil refineries at Haifa and to fund their government in Transjordan. Industrialization began to change the predominantly agricultural Palestinian economy.

Between 1929 and 1938, 250,000 Jews arrived in Palestine (Fifth Aliyah). 174,000 arrived between 1933 and 1936, after which the British increasingly prevented immigration. Migrants were mainly from Germany and included professionals, doctors, lawyers and professors. German architects of the Bauhaus school made Tel-Aviv the world's only city with purely Bauhaus neighbourhoods and Palestine had the highest per-capita percentage of doctors in the world.

Fascist regimes were emerging across Europe and persecution of Jews increased. In many countries, Jews reverted to being non-citizens deprived of civil and economic rights, subject to arbitrary persecution. Significantly antisemitic governments came to power in Poland (the government increasingly boycotted Jews and by 1937 had totally excluded all Jews), Hungary, Romania and the Nazi created states of Croatia and Slovakia, while Germany annexed Austria and the Czech territories.

Jewish immigration and Nazi propaganda contributed to the large-scale 1936–1939 Arab revolt in Palestine, a largely nationalist uprising directed at ending British rule. The head of the Jewish Agency, Ben-Gurion, responded to the Arab Revolt with a policy of "Havlagah"—self-restraint and a refusal to be provoked by Arab attacks in order to prevent polarization. The Etzel group broke off from the Haganah in opposition to this policy.

The British responded to the revolt with the Peel Commission (1936–37), a public inquiry that recommended that an exclusively Jewish territory be created in the Galilee and western coast (including the population transfer of 225,000 Arabs); the rest becoming an exclusively Arab area. The two main Jewish leaders, Chaim Weizmann and David Ben-Gurion, had convinced the Zionist Congress to approve equivocally the Peel recommendations as a basis for more negotiation. The plan was rejected outright by the Palestinian Arab leadership and they renewed the revolt, which caused the British to appease the Arabs, and to abandon the plan as unworkable.

Testifying before the Peel Commission, Weizmann said "There are in Europe 6,000,000 people ... for whom the world is divided into places where they cannot live and places where they cannot enter." In 1938, the US called an international conference to address the question of the vast numbers of Jews trying to escape Europe. Britain made its attendance contingent on Palestine being kept out of the discussion. No Jewish representatives were invited. The Nazis proposed their own solution: that the Jews of Europe be shipped to Madagascar (the Madagascar Plan).

With millions of Jews trying to leave Europe and every country in the world closed to Jewish migration, the British decided to close Palestine. The White Paper of 1939, recommended that an independent Palestine, governed jointly by Arabs and Jews, be established within 10 years. The White Paper agreed to allow 75,000 Jewish immigrants into Palestine over the period 1940–44, after which migration would require Arab approval. Both the Arab and Jewish leadership rejected the White Paper. In March 1940 the British High Commissioner for Palestine issued an edict banning Jews from purchasing land in 95% of Palestine. Jews now resorted to illegal immigration: (Aliyah Bet or "Ha'apalah"), often organized by the Mossad Le'aliyah Bet and the Irgun. Very few Jews managed to escape Europe between 1939 and 1945. Those caught by the British were mostly sent to Mauritius.

During the Second World War, the Jewish Agency worked to establish a Jewish army that would fight alongside the British forces. Churchill supported the plan but British Military and government opposition led to its rejection. The British demanded that the number of Jewish recruits match the number of Arab recruits, but few Arabs would fight for Britain, and the Palestinian leader, the Mufti of Jerusalem, allied with Nazi Germany.

In May 1941, the Palmach was established to defend the Yishuv against the planned Axis invasion through North Africa. The British refusal to provide arms to the Jews, even when Rommel's forces were advancing through Egypt in June 1942 (intent on occupying Palestine) and the 1939 White Paper, led to the emergence of a Zionist leadership in Palestine that believed conflict with Britain was inevitable. Despite this, the Jewish Agency called on Palestine's Jewish youth to volunteer for the British Army (both men and women). 30,000 Palestinian Jews and 6,000 Palestinian Arabs enlisted in the British armed forces during the war. In June 1944 the British agreed to create a Jewish Brigade that would fight in Italy.

Approximately 1.5 million Jews around the world served in every branch of the allied armies, mainly in the Soviet and US armies. 200,000 Jews died serving in the Soviet army alone. Many of these war veterans later volunteered to fight for Israel or were active in its support.

A small group (about 200 activists), dedicated to resisting the British administration in Palestine, broke away from the Etzel (which advocated support for Britain during the war) and formed the "Lehi" (Stern Gang), led by Avraham Stern. In 1943, the USSR released the Revisionist Zionist leader Menachem Begin from the Gulag and he went to Palestine, taking command of the Etzel organization with a policy of increased conflict against the British. At about the same time Yitzhak Shamir escaped from the camp in Eritrea where the British were holding Lehi activists without trial, taking command of the Lehi (Stern Gang).

Jews in the Middle East were also affected by the war. Most of North Africa came under Nazi control and many Jews were used as slaves. The 1941 pro-Axis coup in Iraq was accompanied by massacres of Jews. The Jewish Agency put together plans for a last stand in the event of Rommel invading Palestine (the Nazis planned to exterminate Palestine's Jews).

Between 1939 and 1945, the Nazis, aided by local forces, led systematic efforts to kill every person of Jewish extraction in Europe (The Holocaust), causing the deaths of approximately 6 million Jews. A quarter of those killed were children. The Polish and German Jewish communities, which played an important role in defining the pre-1945 Jewish world, mostly ceased to exist. In the United States and Palestine, Jews of European origin became disconnected from their families and roots. Sepharadi and Mizrahi Jews, who had been a minority, became a much more significant factor in the Jewish world. Those Jews who survived in central Europe, were displaced persons (refugees); an Anglo-American Committee of Inquiry, established to examine the Palestine issue, surveyed their ambitions and found that over 95% wanted to migrate to Palestine.

In the Zionist movement the moderate Pro-British (and British citizen) Weizmann, whose son died flying in the RAF, was undermined by Britain's anti-Zionist policies. Leadership of the movement passed to the Jewish Agency in Palestine, now led by the anti-British Socialist-Zionist party (Mapai) and led by David Ben-Gurion. In the diaspora, US Jews now dominated the Zionist movement.

The British Empire was severely weakened by the war. In the Middle East, the war had made Britain conscious of its dependence on Arab oil. British firms controlled Iraqi oil and Britain ruled Kuwait, Bahrain and the Emirates. Shortly after VE Day, the Labour Party won the general election in Britain. Although Labour Party conferences had for years called for the establishment of a Jewish state in Palestine, the Labour government now decided to maintain the 1939 White Paper policies.
Illegal migration (Aliyah Bet) became the main form of Jewish entry into Palestine. Across Europe Bricha ("flight"), an organization of former partisans and ghetto fighters, smuggled Holocaust survivors from Eastern Europe to Mediterranean ports, where small boats tried to breach the British blockade of Palestine. Meanwhile, Jews from Arab countries began moving into Palestine overland. Despite British efforts to curb immigration, during the 14 years of the Aliyah Bet, over 110,000 Jews entered Palestine. By the end of World War II, the Jewish population of Palestine had increased to 33% of the total population.

In an effort to win independence, Zionists now waged a guerrilla war against the British. The main underground Jewish militia, the Haganah, formed an alliance called the Jewish Resistance Movement with the Etzel and Stern Gang to fight the British. In June 1946, following instances of Jewish sabotage, the British launched Operation Agatha, arresting 2700 Jews, including the leadership of the Jewish Agency, whose headquarters were raided. Those arrested were held without trial.

On July 4, 1946 a massive pogrom in Poland led to a wave of Holocaust survivors fleeing Europe for Palestine. Three weeks later (21 July), Etzel bombed the British Military Headquarters in the King David Hotel (Jerusalem) killing 91 people. In the days following the bombing, Tel Aviv was placed under curfew and over 120,000 Jews, nearly 20% of the Jewish population of Palestine, were questioned by the police. In the US, Congress criticized British handling of the situation and considered delaying loans that were vital to British post-war recovery.

Between 1945 and 1948, 100,000–120,000 Jews left Poland. Their departure was largely organized by Zionist activists in Poland under the umbrella of the semi-clandestine organization "Berihah" ("Flight"). "Berihah" was also responsible for the organized emigration of Jews from Romania, Hungary, Czechoslovakia and Yugoslavia, totalling 250,000 (including Poland) Holocaust survivors. The British imprisoned the Jews trying to enter Palestine in the Atlit detainee camp and Cyprus internment camps. Those held were mainly Holocaust survivors, including large numbers of children and orphans. In response to Cypriot fears that the Jews would never leave (since they lacked a state or documentation) and because the 75,000 quota established by the 1939 White Paper had never been filled, the British allowed the refugees to enter Palestine at a rate of 750 per month.

By 1947 the Labour Government was ready to refer the Palestine problem to the newly created United Nations.

On 2 April 1947, the United Kingdom requested that the question of Palestine be handled by the General Assembly. The General Assembly created a committee, United Nations Special Committee on Palestine (UNSCOP), to report on "the question of Palestine". In July 1947 the UNSCOP visited Palestine and met with Jewish and Zionist delegations. The Arab Higher Committee boycotted the meetings. During the visit the British Foreign Secretary Ernest Bevin ordered an illegal immigrant ship, the "Exodus 1947", to be sent back to Europe. The migrants on the ship were forcibly removed by British troops at Hamburg.

The principal non-Zionist Orthodox Jewish (or Haredi) party, Agudat Israel, recommended to UNSCOP that a Jewish state be set up after reaching a religious status quo agreement with Ben-Gurion regarding the future Jewish state. The agreement granted an exemption from military service to a quota of yeshiva (religious seminary) students and to all orthodox women, made the Sabbath the national weekend, guaranteed Kosher food in government institutions and allowed Orthodox Jews to maintain a separate education system.

The majority report of UNSCOP proposed "an independent Arab State, an independent Jewish State, and the City of Jerusalem", the last to be under "an International Trusteeship System". On 29 November 1947, in Resolution 181 (II), the General Assembly adopted the majority report of UNSCOP, but with slight modifications. The Plan also called for the British to allow "substantial" Jewish migration by 1 February 1948.

Neither Britain nor the UN Security Council took any action to implement the resolution and Britain continued detaining Jews attempting to enter Palestine. Concerned that partition would severely damage Anglo-Arab relations, Britain denied UN representatives access to Palestine during the period between the adoption of Resolution 181 (II) and the termination of the British Mandate. The British withdrawal was finally completed in May 1948. However, Britain continued to hold (formerly illegal) Jewish immigrants of "fighting age" and their families on Cyprus until March 1949.

The General Assembly's vote caused joy in the Jewish community and discontent among the Arab community. Violence broke out between the sides, escalating into civil war. From January 1948, operations became increasingly militarized, with the intervention of a number of Arab Liberation Army regiments inside Palestine, each active in a variety of distinct sectors around the different coastal towns. They consolidated their presence in Galilee and Samaria. Abd al-Qadir al-Husayni came from Egypt with several hundred men of the Army of the Holy War. Having recruited a few thousand volunteers, he organized the blockade of the 100,000 Jewish residents of Jerusalem. The Yishuv tried to supply the city using convoys of up to 100 armoured vehicles, but largely failed. By March, almost all Haganah's armoured vehicles had been destroyed, the blockade was in full operation, and hundreds of Haganah members who had tried to bring supplies into the city were killed.

Up to 100,000 Arabs, from the urban upper and middle classes in Haifa, Jaffa and Jerusalem, or Jewish-dominated areas, evacuated abroad or to Arab centres eastwards. This situation caused the US to withdraw their support for the Partition plan, thus encouraging the Arab League to believe that the Palestinian Arabs, reinforced by the Arab Liberation Army, could put an end to the plan for partition. The British, on the other hand, decided on 7 February 1948 to support the annexation of the Arab part of Palestine by Transjordan.
David Ben-Gurion reorganized Haganah and made conscription obligatory. Every Jewish man and woman in the country had to receive military training. Thanks to funds raised by Golda Meir from sympathisers in the United States, and Stalin's decision to support the Zionist cause, the Jewish representatives of Palestine were able to purchase important arms in Eastern Europe.

Ben-Gurion gave Yigael Yadin the responsibility to plan for the announced intervention of the Arab states. The result of his analysis was Plan Dalet, in which Haganah passed from the defensive to the offensive. The plan sought to establish Jewish territorial continuity by conquering mixed zones. Tiberias, Haifa, Safed, Beisan, Jaffa and Acre fell, resulting in the flight of more than 250,000 Palestinian Arabs. The situation pushed the leaders of the neighbouring Arab states to intervene.

On 14 May 1948, on the day the last British forces left from Haifa, the Jewish People's Council gathered at the Tel Aviv Museum and proclaimed the establishment of a Jewish state in Eretz Israel, to be known as the State of Israel.

Immediately following the declaration of the new state, both superpower leaders, US President Harry S. Truman and Soviet leader Joseph Stalin, recognized the new state.
The Arab League members Egypt, Transjordan, Syria, Lebanon and Iraq refused to accept the UN partition plan and proclaimed the right of self-determination for the Arabs across the whole of Palestine. The Arab states marched their forces into what had, until the previous day, been the British Mandate for Palestine, starting the first Arab–Israeli War. The Arab states had heavy military equipment at their disposal and were initially on the offensive (the Jewish forces were not a state before 15 May and could not buy heavy arms). On 29 May 1948, the British initiated United Nations Security Council Resolution 50 declaring an arms embargo on the region. Czechoslovakia violated the resolution, supplying the Jewish state with critical military hardware to match the (mainly British) heavy equipment and planes already owned by the invading Arab states. On 11 June, a month-long UN truce was put into effect.

Following independence, the Haganah became the Israel Defense Forces (IDF). The Palmach, Etzel and Lehi were required to cease independent operations and join the IDF. During the ceasefire, Etzel attempted to bring in a private arms shipment aboard a ship called "Altalena". When they refused to hand the arms to the government, Ben-Gurion ordered that the ship be sunk. Several Etzel members were killed in the fighting.

Large numbers of Jewish immigrants, many of them World War II veterans and Holocaust survivors, now began arriving in the new state of Israel, and many joined the IDF.

After an initial loss of territory by the Jewish state and its occupation by the Arab armies, from July the tide gradually turned in the Israelis' favour and they pushed the Arab armies out and conquered some of the territory that had been included in the proposed Arab state. At the end of November, tenuous local ceasefires were arranged between the Israelis, Syrians and Lebanese. On 1 December King Abdullah announced the union of Transjordan with Arab Palestine west of the Jordan; only Britain recognized the annexation.

Israel signed armistices with Egypt (24 February), Lebanon (23 March), Jordan (3 April) and Syria (20 July). No actual peace agreements were signed. With permanent ceasefire coming into effect, Israel's new borders, later known as the Green Line, were established. These borders were not recognized by the Arab states as international boundaries. The IDF had overrun Galilee, Jezreel Valley, West Jerusalem, the coastal plain and the Negev. The Syrians remained in control of a strip of territory along the Sea of Galilee originally allocated to the Jewish state, the Lebanese occupied a tiny area at Rosh Hanikra, and the Egyptians retained the Gaza strip and still had some forces surrounded inside Israeli territory. Jordanian forces remained in the West Bank, where the British had stationed them before the war. Jordan annexed the areas it occupied while Egypt kept Gaza as an occupied zone.

Following the ceasefire declaration, Britain released over 2,000 Jewish detainees it was still holding in Cyprus and recognized the state of Israel. On 11 May 1949, Israel was admitted as a member of the United Nations. Out of an Israeli population of 650,000, some 6,000 men and women were killed in the fighting, including 4,000 soldiers in the IDF. According to United Nations figures, 726,000 Palestinians had fled or were evicted by the Israelis between 1947 and 1949. Except in Jordan, the Palestinian refugees were settled in large refugee camps in poor, overcrowded conditions and denied citizenship by their host countries. In December 1949, the UN (in response to a British proposal) established an agency (UNRWA) to provide aid to the Palestinian refugees. It became the largest single UN agency and is the only UN agency that serves a single people.

A 120-seat parliament, the Knesset, met first in Tel Aviv then moved to Jerusalem after the 1949 ceasefire. In January 1949, Israel held its first elections. The Socialist-Zionist parties Mapai and Mapam won the most seats (46 and 19 respectively). Mapai's leader, David Ben-Gurion, was appointed Prime Minister, he formed a coalition which did not include Mapam who were Stalinist and loyal to the USSR (another Stalinist party, non-Zionist Maki won 4 seats). The Knesset elected Chaim Weizmann as the first (largely ceremonial) President of Israel. Hebrew and Arabic were made the official languages of the new state. All governments have been coalitions—no party has ever won a majority in the Knesset. From 1948 until 1977 all governments were led by Mapai and the Alignment, predecessors of the Labour Party. In those years Labour Zionists, initially led by David Ben-Gurion, dominated Israeli politics and the economy was run on primarily socialist lines.

Within three years (1948 to 1951), immigration doubled the Jewish population of Israel and left an indelible imprint on Israeli society. Overall, 700,000 Jews settled in Israel during this period. Some 300,000 arrived from Asian and North African nations as part of the Jewish exodus from Arab and Muslim countries. Among them, the largest group (over 100,000) was from Iraq. The rest of the immigrants were from Europe, including more than 270,000 who came from Eastern Europe, mainly Romania and Poland (over 100,000 each). Nearly all the Jewish immigrants could be described as refugees, however only 136,000 who immigrated to Israel from Central Europe, had international certification because they belonged to the 250,000 Jews registered by the allies as displaced after World War II and living in displaced persons camps in Germany, Austria and Italy.

In 1950 the Knesset passed the Law of Return, which granted to all Jews and those of Jewish ancestry, and their spouses, the right to settle in Israel and gain citizenship. That year, 50,000 Yemenite Jews (99%) were secretly flown to Israel. In 1951 Iraqi Jews were granted temporary permission to leave the country and 120,000 (over 90%) opted to move to Israel. Jews also fled from Lebanon, Syria and Egypt. By the late sixties, about 500,000 Jews had left Algeria, Morocco and Tunisia. Over the course of twenty years, some 850,000 Jews from Arab countries (99%) relocated to Israel (680,000), France and the Americas. The land and property left behind by the Jews (much of it in Arab city centres) is still a matter of some dispute. Today there are about 9,000 Jews living in Arab states, of whom 75% live in Morocco and 15% in Tunisia.
Between 1948 and 1958, the population of Israel rose from 800,000 to two million. During this period, food, clothes and furniture had to be rationed in what became known as the Austerity Period ("Tkufat haTsena"). Immigrants were mostly refugees with no money or possessions and many were housed in temporary camps known as ma'abarot. By 1952, over 200,000 immigrants were living in tents or prefabricated shacks built by the government. Israel received financial aid from private donations from outside the country (mainly the United States). The pressure on the new state's finances led Ben-Gurion to sign a reparations agreement with West Germany. During the Knesset debate some 5,000 demonstrators gathered and riot police had to cordon the building. Israel received several billion marks and in return agreed to open diplomatic relations with Germany.

At the end of 1953, Ben-Gurion retired to Kibbutz Sde Boker in the Negev.

In 1949, education was made free and compulsory for all citizens until the age of 14. The state now funded the party-affiliated Zionist education system and a new body created by the Haredi Agudat Israel party. A separate body was created to provide education for the remaining Palestinian-Arab population. The major political parties now competed for immigrants to join their education systems. The government banned the existing educational bodies from the transit camps and tried to mandate a unitary secular socialist education under the control of "camp managers" who also had to provide work, food and housing for the immigrants. There were attempts to force orthodox Yemenite children to adopt a secular life style by teachers, including many instances of Yemenite children having their side-curls cut by teachers. This led to the first Israeli public inquiry (the Fromkin Inquiry), the collapse of the coalition, and an election in 1951, with little change in the results. In 1953 the party-affiliated education system was scrapped and replaced by a secular state education system and a state-run Modern Orthodox system. Agudat Israel were allowed to maintain their existing school system.

In its early years Israel sought to maintain a non-aligned position between the super-powers. However, in 1952, an antisemitic public trial was staged in Moscow in which a group of Jewish doctors were accused of trying to poison Stalin (the Doctors' plot), followed by a similar trial in Czechoslovakia (Slánský trial). This, and the failure of Israel to be included in the Bandung Conference (of non-aligned states), effectively ended Israel's pursuit of non-alignment. On 19 May 1950, Egypt announced that the Suez Canal was closed to Israeli ships and commerce. In 1952 a military coup in Egypt brought Abdel Nasser to power. The United States pursued close relations with the new Arab states, particularly the Nasser-led Egyptian Free Officers Movement and Ibn Saud of Saudi Arabia. Israel's solution to diplomatic isolation was to establish good relations with newly independent states in Africa and with France, which was engaged in the Algerian War.

In the January 1955 elections Mapai won 40 seats and the Labour Party 10, Moshe Sharett became prime minister of Israel at the head of a left-wing coalition. Between 1953 and 1956, there were intermittent clashes along all of Israel's borders as Arab terrorism and breaches of the ceasefire resulted in Israeli counter-raids. Palestinian fedayeen attacks, often organized and sponsored by the Egyptians, were made from (Egyptian occupied) Gaza. Fedayeen attacks led to a growing cycle of violence as Israel launched reprisal attacks against Gaza. In 1954 the Uzi submachine gun first entered use by the Israel Defense Forces. In 1955 the Egyptian government began recruiting former Nazi rocket scientists for a missile program.

Archaeologist and General Yigael Yadin purchased the Dead Sea Scrolls on behalf of the State of Israel. The entire first batch to be discovered were now owned by Israel and housed in the Shrine of the Book at the Israel Museum.

Sharett's government was brought down by the Lavon Affair, a crude plan to disrupt US–Egyptian relations, involving Israeli agents planting bombs at American sites in Egypt. The plan failed when eleven agents were arrested. Defense Minister Lavon was blamed despite his denial of responsibility. The Lavon affair led to Sharett's resignation and Ben-Gurion returned to the post of prime minister.

In 1956, the increasingly pro-Soviet President Nasser of Egypt, announced the nationalization of the (French and British owned) Suez Canal, which was Egypt's main source of foreign currency. Egypt also blockaded the Gulf of Aqaba preventing Israeli access to the Red Sea. Israel made a secret agreement with the French at Sèvres to co-ordinate military operations against Egypt. Britain and France had already begun secret preparations for military action. It has been alleged that the French also agreed to build a nuclear plant for the Israelis and that by 1968 this was able to produce nuclear weapons. Britain and France arranged for Israel to give them a pretext for seizing the Suez Canal. Israel was to attack Egypt, and Britain and France would then call on both sides to withdraw. When, as expected, the Egyptians refused, Anglo-French forces would invade to take control of the Canal.
Israeli forces, commanded by General Moshe Dayan, attacked Egypt on 29 October 1956. On 30 October Britain and France made their pre-arranged call for both sides to stop fighting and withdraw from the Canal area, and for them to be allowed to take up positions at key points on the Canal. Egypt refused and the allies commenced air strikes on 31 October aimed at neutralizing the Egyptian air force. By 5 November the Israelis had overrun the Sinai. The Anglo-French invasion began that day. There was uproar in the UN, with the United States and USSR for once in agreement in denouncing the actions of Israel, Britain and France. A demand for a ceasefire was reluctantly accepted on 7 November.

At Egypt's request, the UN sent an Emergency Force (UNEF), consisting of 6,000 peacekeeping troops from 10 nations to supervise the ceasefire. This was the first ever UN peacekeeping operation. From 15 November the UN troops marked out a zone across the Sinai to separate the Israeli and Egyptian forces. Upon receiving US guarantees of Israeli access to the Suez Canal, freedom of access out of the Gulf of Aqaba and Egyptian action to stop Palestinian raids from Gaza, the Israelis withdrew to the Negev. In practice the Suez Canal remained closed to Israeli shipping. The conflict marked the end of West-European dominance in the Middle East.

Nasser emerged as the victor in the conflict, having won the political battle, however the Israeli military learnt that it did not need British or French support in order to conquer Sinai and that it could conquer the Sinai peninsula in a few days. The Israeli political leadership learnt that Israel had a limited time frame within which to operate militarily after which international political pressure would restrict Israel's freedom of action.

In 1956, two modern-orthodox (and religious-zionist) parties, Mizrachi and Hapoel HaMizrachi, joined to form the National Religious Party. The party was a component of every Israeli coalition until 1992, usually running the Ministry of Education. Mapai was once again victorious in the 1959 elections, increasing its number of seats to 47, Labour had 7. Ben-Gurion remained Prime Minister.

In 1959, there were renewed skirmishes along Israel's borders that continued throughout the early 1960s. The Arab League continued to maintain an economic boycott and there was a dispute over water rights in the River Jordan basin. With Soviet backing, the Arab states, particularly Egypt, were continuing to build up their forces. Israel's main military hardware supplier was France.
Rudolph Kastner, a minor political functionary, was accused of collaborating with the Nazis and sued his accuser. Kastner lost the trial and was assassinated two years later. In 1958 the Supreme Court exonerated him. In May 1960 Adolf Eichmann, one of the chief administrators of the Nazi Holocaust, was located in Argentina by the Mossad, which later kidnapped him to Israel. In 1961 he was put on trial, and after several months found guilty and sentenced to death. He was hanged in 1962 and is the only person ever sentenced to death by an Israeli court. Testimonies by Holocaust survivors at the trial and the extensive publicity that surrounded it has led the trial to be considered a turning point in public awareness of the Holocaust.

In 1961 a Herut no-confidence motion over the Lavon affair led to Ben-Gurion's resignation. Ben-Gurion declared that he would only accept office if Lavon was fired from the position of the head of Histadrut, Israel's labour union organization. His demands were accepted and Mapai won the 1961 election (42 seats keeping Ben-Gurion as PM) with a slight reduction in its share of the seats. Menachem Begin's Herut party and the Liberals came next with 17 seats each. In 1962 the Mossad began assassinating German rocket scientists working in Egypt after one of them reported the missile program was designed to carry chemical warheads. This action was condemned by Ben-Gurion and led to the Mossad director, Isser Harel, resignation. In 1963 Ben-Gurion quit again over the Lavon scandal. His attempts to make his party Mapai support him over the issue failed. Levi Eshkol became leader of Mapai and the new prime minister.

In 1963 Yigael Yadin began excavating Masada. In 1964, Egypt, Jordan and Syria developed a unified military command. Israel completed work on a national water carrier, a huge engineering project designed to transfer Israel's allocation of the Jordan river's waters towards the south of the country in realization of Ben-Gurion's dream of mass Jewish settlement of the Negev desert. The Arabs responded by trying to divert the headwaters of the Jordan, leading to growing conflict between Israel and Syria.

In 1964, Israeli Rabbinical authorities accepted that the Bene Israel of India were indeed Jewish and most of the remaining Indian Jews migrated to Israel. The 2,000-strong Jewish community of Cochin had already migrated in 1954. Ben-Gurion quit Mapai to form the new party Rafi, he was joined by Shimon Peres and Moshe Dayan. Begin's Herut party joined with the Liberals to form Gahal. Mapai and Labour united for the 1965 elections, winning 45 seats and maintaining Levi Eshkol as Prime Minister. Ben-Gurion's Rafi party received 10 seats, Gahal got 26 seats becoming the second largest party.

Until 1966, Israel's principal arms supplier was France, however in 1966, following the withdrawal from Algeria, Charles de Gaulle announced France would cease supplying Israel with arms (and refused to refund money paid for 50 warplanes). On 5 February 1966, the United States announced that it was taking over the former French and West German obligations, to maintain military "stabilization" in the Middle East. Included in the military hardware would be over 200 M48 tanks. In May of that year the US also agreed to provide A-4 Skyhawk tactical aircraft to Israel. In 1966 security restrictions placed on Arab-Israelis were eased and efforts made to integrate them into Israeli life.

In 1966, Black and white TV broadcasts began. On 15 May 1967, the first public performance of Naomi Shemer's classic song "Jerusalem of Gold" took place and over the next few weeks it dominated the Israeli airwaves. Two days later Syria, Egypt and Jordan amassed troops along the Israeli borders, and Egypt closed the Straits of Tiran to Israeli shipping. Nasser demanded that the UNEF leave Sinai, threatening escalation to a full war. Egyptian radio broadcasts talked of a coming genocide. On 26 May Nasser declared, ""The battle will be a general one and our basic objective will be to destroy Israel"". Israel considered the Straits of Tiran closure a Casus belli. Egypt, Syria, Jordan and Iraq signed defence pacts and Iraqi troops began deploying to Jordan, Syria and Egypt. Algeria also announced that it would send troops to Egypt. Between 1963 and 1967 Egyptian troops had tested chemical weapons on Yemenite civilians as part of an Egyptian intervention in support of rebels.

Israel responded by calling up its civilian reserves, bringing much of the Israeli economy to a halt. The Israelis set up a national unity coalition, including for the first time Menachem Begin's party, Herut, in a coalition. During a national radio broadcast, Prime Minister Levi Eshkol stammered, causing widespread fear in Israel. To calm public concern Moshe Dayan (Chief of Staff during the Sinai war) was appointed Defence Minister.
On the morning before Dayan was sworn in, 5 June 1967, the Israeli air force launched pre-emptive attacks destroying first the Egyptian air force, and then later the same day destroying the air forces of Jordan and Syria. Israel then defeated (almost successively) Egypt, Jordan and Syria. By 11 June the Arab forces were routed and all parties had accepted the cease-fire called for by UN Security Council Resolutions 235 and 236. Israel gained control of the Sinai Peninsula, the Gaza Strip, the Golan Heights, and the formerly Jordanian-controlled West Bank of the Jordan River. East Jerusalem was arguably annexed by Israel. Residents were given permanent residency status and the option of applying for Israeli citizenship. The annexation was not recognized internationally (the Jordanian annexation of 1948 was also unrecognized).

Other areas occupied remained under military rule (Israeli civil law did not apply to them) pending a final settlement. The Golan was also annexed in 1981. On 22 November 1967, the Security Council adopted Resolution 242, the "land for peace" formula, which called for the establishment of a just and lasting peace based on Israeli withdrawal from territories occupied in 1967 in return for the end of all states of belligerency, respect for the sovereignty of all states in the area, and the right to live in peace within secure, recognized boundaries. The resolution was accepted by both sides, though with different interpretations, and has been the basis of all subsequent peace negotiations. After 1967 the US began supplying Israel with aircraft and the Soviet block (except Romania) broke off relations with Israel. Antisemitic purges led to the final migration of the last Polish Jews to Israel.

For the first time since the end of the British Mandate, Jews could visit the Old City of Jerusalem and pray at the Western Wall (the holiest site in modern Judaism), to which they had been denied access by the Jordanians in contravention of the 1949 Armistice agreement. The four-meter-wide public alley beside the Wall was expanded into a massive plaza and worshippers were allowed to sit, or use other furniture, for the first time in centuries. In Hebron, Jews gained access to the Cave of the Patriarchs (the second most holy site in Judaism) for the first time since the 14th century (previously Jews were only allowed to pray at the entrance). A third Jewish holy site, Rachel's Tomb, in Bethlehem, also became accessible. Sinai oil fields made Israel self-sufficient in energy.

In 1968 Moshe Levinger led a group of Religious Zionists who created the first Jewish settlement, a town near Hebron called Kiryat Arba. There were no other religious settlements until after 1974. Ben-Gurion's Rafi party merged with the Labour-Mapai alliance. Ben-Gurion remained outside as an independent. In 1968, compulsory education was extended until the age of 16 for all citizens (it had been 14) and the government embarked on an extensive program of integration in education. In the major cities children from mainly Sephardi/Mizrahi neighbourhoods were bused to newly established middle schools in better areas. The system remained in place until after 2000.

In March 1968, Israeli forces attacked the Palestinian militia, Fatah, at its base in the Jordanian town of Karameh. The attack was in response to land mines placed on Israeli roads. The Israelis retreated after destroying the camp, however the Israelis sustained unexpectedly high casualties and the attack was not viewed as a success. Despite heavy casualties, the Palestinians claimed victory, while Fatah and the PLO (of which it formed part) became famous across the Arab world. In early 1969, fighting broke out between Egypt and Israel along the Suez Canal. In retaliation for repeated Egyptian shelling of Israeli positions along the Suez Canal, Israeli planes made deep strikes into Egypt in the 1969–1970 "War of Attrition".

In early 1969, Levi Eshkol died in office of a heart attack and Golda Meir became Prime Minister with the largest percentage of the vote ever won by an Israeli party, winning 56 of the 120 seats after the 1969 election. Meir was the first female prime minister of Israel and the first woman to have headed a Middle Eastern state in modern times. Gahal remained on 26 seats, and was the second largest party.
In December 1969, Israeli naval commandos took five missile boats during the night from Cherbourg Harbour in France. Israel had paid for the boats but the French had refused to supply them. In July 1970 the Israelis shot down five Soviet fighters that were aiding the Egyptians in the course of the War of Attrition. Following this, the US worked to calm the situation and in August 1970 a cease fire was agreed.

In September 1970 King Hussein of Jordan drove the Palestine Liberation Organization out of his country. On September 18, 1970, Syrian tanks invaded Jordan, intending to aid the PLO. At the request of the US, Israel moved troops to the border and threatened Syria, causing the Syrians to withdraw. The centre of PLO activity then shifted to Lebanon, where the 1969 Cairo agreement gave the Palestinians autonomy within the south of the country. The area controlled by the PLO became known by the international press and locals as "Fatahland" and contributed to the 1975–1990 Lebanese Civil War. The event also led to Hafez al-Assad taking power in Syria. Egyptian President Nasser died immediately after and was succeeded by Anwar Sadat.

Increased Soviet antisemitism and enthusiasm generated by the 1967 victory led to a wave of Soviet Jews applying to emigrate to Israel. Those who left could only take two suitcases. Most Jews were refused exit visas and persecuted by the authorities. Some were arrested and sent to Gulag camps, becoming known as Prisoners of Zion. During 1971, violent demonstrations by the Israeli Black Panthers, made the Israeli public aware of resentment among Mizrahi Jews at ongoing discrimination and social gaps. In 1972 the US Jewish Mafia leader, Meyer Lansky, who had taken refuge in Israel, was deported to the United States.

At the 1972 Munich Olympics, two members of the Israeli team were killed, and nine members taken hostage by Palestinian terrorists. A botched German rescue attempt led to the death of the rest along with five of the eight hijackers. The three surviving Palestinians were released by the West German authorities eight weeks later without charge, in exchange for the hostages of hijacked Lufthansa Flight 615. The Israeli government responded with a bombing, an assassination campaign against the organizers of the massacre and a raid on the PLO headquarters in Lebanon (led by future Prime Minister, Ehud Barak).

In 1972 the new Egyptian President Anwar Sadat expelled the Soviet advisers from Egypt. This and frequent invasion exercises by Egypt and Syria led to Israeli complacency about the threat from these countries. In addition the desire not to be held responsible for initiating conflict and an election campaign highlighting security, led to an Israeli failure to mobilize, despite receiving warnings of an impending attack.
The Yom Kippur War (also known as the October War) began on 6 October 1973 (the Jewish Day of Atonement), the holiest day in the Jewish calendar and a day when adult Jews are required to fast. The Syrian and Egyptian armies launched a well-planned surprise attack against the unprepared Israeli Defense Forces. For the first few days there was a great deal of uncertainty about Israel's capacity to repel the invaders. Both the Soviets and the Americans (at the orders of Richard Nixon) rushed arms to their allies. The Syrians were repulsed by the tiny remnant of the Israeli tank force on the Golan and, although the Egyptians captured a strip of territory in Sinai, Israeli forces crossed the Suez Canal, trapping the Egyptian Third Army in Sinai and were 100 kilometres from Cairo. The war cost Israel over 2,000 dead, resulted in a heavy arms bill (for both sides) and made Israelis more aware of their vulnerability. It also led to heightened superpower tension. Following the war, both Israelis and Egyptians showed greater willingness to negotiate. On 18 January 1974, extensive diplomacy by US Secretary of State Henry Kissinger led to a Disengagement of Forces agreement with the Egyptian government and on 31 May with the Syrian government.

The war led the Saudi government to initiate the 1973 oil crisis, an oil embargo in conjunction with OPEC, against countries trading with Israel. Severe shortages led to massive increases in the price of oil, and as a result, many countries broke off relations with Israel or downgraded relations, and Israel was banned from participation in the Asian Games and other Asian sporting events.

State funding was introduced for elected parties. The new system made parties independent of wealthy donors and gave Knesset members more power over party funding, however it also made them less dependent on existing party structures and able to take their funding elsewhere. Prior to the December 1973 elections, Gahal and a number of right-wing parties united to form the Likud (led by Begin). In the December 1973 elections, Labour won 51 seats, leaving Golda Meir as Prime Minister. The Likud won 39 seats.

In May 1974, Palestinians attacked a school in Ma'alot, holding 102 children hostage. Twenty-two children were killed. In November 1974 the PLO was granted observer status at the UN and Yasser Arafat addressed the General Assembly. Later that year the Agranat Commission, appointed to assess responsibility for Israel's lack of preparedness for the war, exonerated the government of responsibility, and held the Chief of Staff and head of military intelligence responsible. Despite the report, public anger at the Government led to Golda Meir's resignation.

Following Meir's resignation, Yitzhak Rabin (Chief of Staff during the Six Day War) became prime minister. Modern Orthodox Jews (Religious Zionist followers of the teachings of Rabbi Kook), formed the Gush Emunim movement, and began an organized drive to settle the West Bank and Gaza Strip. In November 1975 the United Nations General Assembly, under the guidance of Austrian Secretary General Kurt Waldheim, adopted Resolution 3379, which asserted Zionism to be a form of racism. The General Assembly rescinded this resolution in December 1991 with Resolution 46/86. In March 1976 there was a massive strike by Israeli-Arabs in protest at a government plan to expropriate land in the Galilee.

In July 1976, an Air France plane carrying 260 people was hijacked by Palestinian and German terrorists and flown to Uganda, then ruled by Idi Amin Dada. There, the Germans separated the Jewish passengers from the non-Jewish passengers, releasing the non-Jews. The hijackers threatened to kill the remaining, 100-odd Jewish passengers (and the French crew who had refused to leave). Despite the distances involved, Rabin ordered a daring rescue operation in which the kidnapped Jews were freed. UN Secretary General Waldheim described the raid as "a serious violation of the national sovereignty of a United Nations member state" (meaning Uganda). Waldheim was a former Nazi and suspected war criminal, with a record of offending Jewish sensibilities.

In 1976, the ongoing Lebanese Civil War led Israel to allow South Lebanese to cross the border and work in Israel. In January 1977, French authorities arrested Abu Daoud, the planner of the Munich massacre, releasing him a few days later. In March 1977 Anatoly Sharansky, a prominent Refusenik and spokesman for the Moscow Helsinki Group, was sentenced to 13 years' hard labour.

Rabin resigned on April 1977 after it emerged that his wife maintained a dollar account in the United States (illegal at the time), which had been opened while Rabin was Israeli ambassador. The incident became known as the Dollar Account affair. Shimon Peres informally replaced him as prime minister, leading the Alignment in the subsequent elections.

In a surprise result, the Likud led by Menachem Begin won 43 seats in the 1977 elections (Labour got 32 seats). This was the first time in Israeli history that the government was not led by the left. A key reason for the victory was anger among Mizrahi Jews at discrimination, which was to play an important role in Israeli politics for many years. Talented small town Mizrahi social activists, unable to advance in the Labour party, were readily embraced by Begin. Moroccan-born David Levy and Iranian-born Moshe Katzav were part of a group who won Mizrahi support for Begin. Many Labour voters voted for the Democratic Movement for Change (15 seats) in protest at high-profile corruption cases. The party joined in coalition with Begin and disappeared at the next election.

In addition to starting a process of healing the Mizrahi–Ashkenazi divide, Begin's government included Ultra-Orthodox Jews and was instrumental in healing the Zionist–Ultra-Orthodox rift, however it did so at the cost of expanding the exemption from military service to all Haredi Jewish students of military age. This led to creation of a huge class of unemployed Haredi Jews (the exemption was conditional on attendance of a religious seminary, so they kept studying until they were too old for military service). By remaining students, they were a massive burden on the state, while also failing to participate in the military burden.

Begin's liberalization of the economy led to hyper-inflation (around 150% inflation) but enabled Israel to begin receiving US financial aid. Begin actively supported Gush Emunim's efforts to settle the West Bank and Jewish settlements in the occupied territories received government support, thus laying the grounds for intense conflict with the Palestinian population of the occupied territories.

In November 1977, Egyptian President Anwar Sadat broke 30 years of hostility with Israel by visiting Jerusalem at the invitation of Israeli Prime Minister Menachem Begin. Sadat's two-day visit included a speech before the Knesset and was a turning point in the history of the conflict. The Egyptian leader created a new psychological climate in the Middle East in which peace between Israel and its Arab neighbours seemed possible. Sadat recognized Israel's right to exist and established the basis for direct negotiations between Egypt and Israel. Following Sadat's visit, 350 Yom Kippur War veterans organized the Peace Now movement to encourage Israeli governments to make peace with the Arabs.

In March 1978, eleven armed Lebanese Palestinians reached Israel in boats and hijacked a bus carrying families on a day outing, killing 38 people, including 13 children. The attackers opposed the Egyptian–Israeli peace process. Three days later, Israeli forces crossed into Lebanon beginning Operation Litani. After passage of United Nations Security Council Resolution 425, calling for Israeli withdrawal and the creation of the United Nations Interim Force in Lebanon (UNIFIL) peace-keeping force, Israel withdrew its troops.
In September 1978, US President Jimmy Carter invited President Sadat and Prime Minister Begin to meet with him at Camp David, and on 11 September they agreed on a framework for peace between Israel and Egypt, and a comprehensive peace in the Middle East. It set out broad principles to guide negotiations between Israel and the Arab states. It also established guidelines for a West Bank–Gaza transitional regime of full autonomy for the Palestinians residing in these territories, and for a peace treaty between Egypt and Israel. The treaty was signed 26 March 1979 by Begin and Sadat, with President Carter signing as witness. Under the treaty, Israel returned the Sinai peninsula to Egypt in April 1982. The final piece of territory to be repatriated was Taba, adjacent to Eilat, returned in 1989. The Arab League reacted to the peace treaty by suspending Egypt from the organization and moving its headquarters from Cairo to Tunis. Sadat was assassinated in 1981 by Islamic fundamentalist members of the Egyptian army who opposed peace with Israel. Following the agreement Israel and Egypt became the two largest recipients of US military and financial aid (Iraq and Afghanistan have now overtaken them).

In December 1978 the Israeli Merkava battle tank entered use with the IDF. In 1979, over 40,000 Iranian Jews migrated to Israel, escaping the Islamic Revolution there. On 30 June 1981, the Israeli air force destroyed the Osirak nuclear reactor that France was building for Iraq. Three weeks later, Begin won yet again, in the 1981 elections (48 seats Likud, 47 Labour). Ariel Sharon was made defence minister. The new government annexed the Golan Heights and banned the national airline from flying on Shabbat. By the 1980s a diverse set of high-tech industries had developed in Israel.

In the decades following the 1948 war, Israel's border with Lebanon was quiet compared to its borders with other neighbours. But the 1969 Cairo agreement gave the PLO a free hand to attack Israel from South Lebanon. The area was governed by the PLO independently of the Lebanese Government and became known as "Fatahland" (Fatah was the largest faction in the PLO). Palestinian irregulars constantly shelled the Israeli north, especially the town of Kiryat Shmona, which was a Likud stronghold inhabited primarily by Jews who had fled the Arab world. Lack of control over Palestinian areas was an important factor in causing civil war in Lebanon.

In June 1982, the attempted assassination of Shlomo Argov, the ambassador to Britain, was used as a pretext for an Israeli invasion aiming to drive the PLO out of the southern half of Lebanon. Sharon agreed with Chief of Staff Raphael Eitan to expand the invasion deep into Lebanon even though the cabinet had only authorized a 40 kilometre deep invasion. The invasion became known as the 1982 Lebanon War and the Israeli army occupied Beirut, the only time an Arab capital has been occupied by Israel. Some of the Shia and Christian population of South Lebanon welcomed the Israelis, as PLO forces had maltreated them, but Lebanese resentment of Israeli occupation grew over time and the Shia became gradually radicalized under Iranian guidance. Constant casualties among Israeli soldiers and Lebanese civilians led to growing opposition to the war in Israel.

In August 1982, the PLO withdrew its forces from Lebanon (moving to Tunisia). Israel helped engineer the election of a new Lebanese president, Bashir Gemayel, who agreed to recognize Israel and sign a peace treaty. Gemayal was assassinated before an agreement could be signed, and one day later Phalangist Christian forces led by Elie Hobeika entered two Palestinian refugee camps and massacred the occupants. The massacres led to the biggest demonstration ever in Israel against the war, with as many as 400,000 people (almost 10% of the population) gathering in Tel Aviv. In 1983, an Israeli public inquiry found that Israel's defence minister, Sharon, was indirectly but personally responsible for the massacres. It also recommended that he never again be allowed to hold the post (it did not forbid him from being Prime Minister). In 1983, the May 17 Agreement was signed between Israel and Lebanon, paving the way for an Israeli withdrawal from Lebanese territory through a few stages. Israel continued to operate against the PLO until its eventual departure in 1985, and kept a small force stationed in Southern Lebanon in support of the South Lebanon Army until May 2000.

In September 1983, Begin resigned and was succeeded by Yitzhak Shamir as prime minister. The 1984 election was inconclusive, and led to a power sharing agreement between Shimon Peres of the Alignment (44 seats) and Shamir of Likud (41 seats). Peres was prime minister from 1984 to 1986 and Shamir from 1986 to 1988. In 1984, continual discrimination against Sephardi Ultra-Orthodox Jews by the Ashkenazi Ultra-Orthodox establishment led political activist Aryeh Deri to leave the Agudat Israel party and join former chief Rabbi Ovadia Yosef in forming Shas, a new party aimed at the non-Ashkenazi Ultra-Orthodox vote. The party won 4 seats in the first election it contested and over the next twenty years was the third largest party in the Knesset. Shas established a nationwide network of free Sephardi Orthodox schools. In 1984, during a severe famine in Ethiopia, 8,000 Ethiopian Jews were secretly transported to Israel. In 1986 Natan Sharansky, a famous Russian human rights activist and Zionist refusenik (denied an exit visa), was released from the Gulag in return for two Soviet spies.

In June 1985, Israel withdrew most of its troops from Lebanon, leaving a residual Israeli force and an Israeli-supported militia in southern Lebanon as a "security zone" and buffer against attacks on its northern territory. Since then, IDF fought for many years against the Shia organization Hezbollah, which became a growing threat to Israel. By July 1985, Israel's inflation, buttressed by complex index linking of salaries, had reached 480% per annum and was the highest in the world. Peres introduced emergency control of prices and cut government expenditure successfully bringing inflation under control. The currency (known as the old Israeli shekel) was replaced and renamed the Israeli new shekel at a rate of 1,000 old shkalim = 1 new shekel. In October 1985, Israel responded to a Palestinian terrorist attack in Cyprus by bombing the PLO headquarters in Tunis. Growing Israeli settlement and continuing occupation of the West Bank and Gaza Strip, led to the first Palestinian Intifada (uprising) in 1987, which lasted until the Madrid Conference of 1991, despite Israeli attempts to suppress it. Human rights abuses by Israeli troops led a group of Israelis to form B'Tselem, an organization devoted to improving awareness and compliance with human rights requirements in Israel.

In August 1987, the Israeli government cancelled the IAI Lavi project, an attempt to develop an independent Israeli fighter aircraft. The Israelis found themselves unable to sustain the huge development costs, and faced US opposition to a project that threatened US influence in Israel and US global military ascendancy. In September 1988, Israel launched an Ofeq reconnaissance satellite into orbit, using a Shavit rocket, thus becoming one of only eight countries possessing a capacity to independently launch satellites into space (two more have since developed this ability). The Alignment and Likud remained neck and neck in the 1988 elections (39:40 seats). Shamir successfully formed a national unity coalition with the Labour Alignment. In March 1990, Alignment leader Shimon Peres engineered a defeat of the government in a non-confidence vote and then tried to form a new government. He failed and Shamir became prime minister at the head of a right-wing coalition.

In 1990, the Soviet Union finally permitted free emigration of Soviet Jews to Israel. Prior to this, Jews trying to leave the USSR faced persecution; those who succeeded arrived as refugees. Over the next few years some one million Soviet citizens migrated to Israel. Although there was concern that some of the new immigrants had only a very tenuous connection to Judaism, and many were accompanied by non-Jewish relatives, this massive wave of migration slowly transformed Israel, bringing large numbers of highly educated Soviet Jews and creating a powerful Russian culture in Israel.

In August 1990, Iraq invaded Kuwait, triggering the Gulf War between Iraq and a large allied force, led by the United States. Iraq attacked Israel with 39 Scud missiles. Israel did not retaliate at request of the US, fearing that if Israel responded against Iraq, other Arab nations might desert the allied coalition. Israel provided gas masks for both the Palestinian population and Israeli citizens, while Netherlands and the United States deployed Patriot defence batteries in Israel as protection against the Scuds. In May 1991, during a 36-hour period, 15,000 Beta Israel (Ethiopian Jews) were secretly airlifted to Israel. The coalition's victory in the Gulf War opened new possibilities for regional peace, and in October 1991 the US President, George H.W. Bush, and Soviet Union Premier, Mikhail Gorbachev, jointly convened a historic meeting in Madrid of Israeli, Lebanese, Jordanian, Syrian, and Palestinian leaders. Shamir opposed the idea but agreed in return for loan guarantees to help with absorption of immigrants from the former Soviet Union. His participation in the conference led to the collapse of his (right-wing) coalition.

In the 1992 elections, the Labour Party, led by Yitzhak Rabin, won a significant victory (44 seats) promising to pursue peace while promoting Rabin as a "tough general" and pledging not to deal with the PLO in any way. The pro-peace Zionist party Meretz won 12 seats, and the Arab and communist parties a further 5, meaning that parties supporting a peace treaty had a full (albeit small) majority in the Knesset. Later that year, the Israeli electoral system was changed to allow for direct election of the prime minister. It was hoped this would reduce the power of small parties (mainly the religious parties) to extract concessions in return for coalition agreements. The new system had the opposite effect; voters could split their vote for prime minister from their (interest based) party vote, and as a result larger parties won fewer votes and smaller parties becoming more attractive to voters. It thus increased the power of the smaller parties. By the 2006 election the system was abandoned.
On 25 July 1993, Israel carried out a week-long military operation in Lebanon to attack Hezbollah positions. On 13 September 1993, Israel and the Palestine Liberation Organization (PLO) signed the Oslo Accords (a Declaration of Principles) on the South Lawn of the White House. The principles established objectives relating to a transfer of authority from Israel to an interim Palestinian Authority, as a prelude to a final treaty establishing a Palestinian state, in exchange for mutual recognition. The DOP established May 1999 as the date by which a permanent status agreement for the West Bank and Gaza Strip would take effect. In February 1994, Baruch Goldstein, a follower of the Kach party, killed 29 Palestinians and wounded 125 at the Cave of the Patriarchs in Hebron, which became known as the Cave of the Patriarchs massacre. Kach had been barred from participation in the 1992 elections (on the grounds that the movement was racist). It was subsequently made illegal. Israel and the PLO signed the Gaza–Jericho Agreement in May 1994, and the Agreement on Preparatory Transfer of Powers and Responsibilities in August, which began the process of transferring authority from Israel to the Palestinians. On 25 July 1994, Jordan and Israel signed the Washington Declaration, which formally ended the state of war that had existed between them since 1948 and on 26 October the Israel–Jordan Treaty of Peace, witnessed by US President Bill Clinton.

Prime Minister Yitzhak Rabin and PLO Chairman Yasser Arafat signed the Israeli–Palestinian Interim Agreement on the West Bank and the Gaza Strip on 28 September 1995 in Washington. The agreement was witnessed by President Bill Clinton on behalf of the United States and by Russia, Egypt, Norway and the European Union, and incorporates and supersedes the previous agreements, marking the conclusion of the first stage of negotiations between Israel and the PLO. The agreement allowed the PLO leadership to relocate to the occupied territories and granted autonomy to the Palestinians with talks to follow regarding final status. In return the Palestinians promised to abstain from use of terror and changed the Palestinian National Covenant, which had called for the expulsion of all Jews who migrated after 1917 and the elimination of Israel.

The agreement was opposed by Hamas and other Palestinian factions, which launched suicide bomber attacks at Israel. Rabin had a barrier constructed around Gaza to prevent attacks. The growing separation between Israel and the "Palestinian Territories" led to a labour shortage in Israel, mainly in the construction industry. Israeli firms began importing labourers from the Philippines, Thailand, China and Romania; some of these labourers stayed on without visas. In addition, a growing number of Africans began illegally migrating to Israel. On 4 November 1995, a far-right-wing religious Zionist opponent of the Oslo Accords, assassinated Prime Minister Yitzhak Rabin. In February 1996 Rabin's successor, Shimon Peres, called early elections. In April 1996, Israel launched an operation in southern Lebanon as a result of Hezbollah's Katyusha rocket attacks on Israeli population centres along the border.

The May 1996 elections were the first featuring direct election of the prime minister and resulted in a narrow election victory for Likud leader Binyamin Netanyahu. A spate of suicide bombings reinforced the Likud position for security. Hamas claimed responsibility for most of the bombings. Despite his stated differences with the Oslo Accords, Prime Minister Netanyahu continued their implementation, but his prime ministership saw a marked slow-down in the Peace Process. Netanyahu also pledged to gradually reduce US aid to Israel.

In September 1996, a Palestinian riot broke out against the creation of an exit in the Western Wall tunnel. Over the subsequent few weeks, around 80 people were killed as a result. In January 1997 Netanyahu signed the Hebron Protocol with the Palestinian Authority, resulting in the redeployment of Israeli forces in Hebron and the turnover of civilian authority in much of the area to the Palestinian Authority.

In the election of July 1999, Ehud Barak of the Labour Party became Prime Minister. His party was the largest in the Knesset with 26 seats. In September 1999 the Supreme Court of Israel ruled that the use of torture in interrogation of Palestinian prisoners was illegal. On 21 March 2000, Pope John Paul II arrived in Israel for a historic visit.

On 25 May 2000, Israel unilaterally its remaining forces from the "security zone" in southern Lebanon. Several thousand members of the South Lebanon Army (and their families) left with the Israelis. The UN Secretary-General concluded that, as of 16 June 2000, Israel had withdrawn its forces from Lebanon in accordance with UN Security Council Resolution 425. Lebanon claims that Israel continues to occupy Lebanese territory called "Sheba'a Farms" (however this area was governed by Syria until 1967 when Israel took control). The Sheba'a Farms provided Hezbollah with a ruse to maintain warfare with Israel. The Lebanese government, in contravention of the UN Security Council resolution, did not assert sovereignty in the area, which came under Hezbollah control. In the Fall of 2000, talks were held at Camp David to reach a final agreement on the Israel/Palestine conflict. Ehud Barak offered to meet most of the Palestinian teams requests for territory and political concessions, including Arab parts of east Jerusalem; however, Arafat abandoned the talks without making a counterproposal.

Following its withdrawal from South Lebanon, Israel became a member of the Western European and Others Group at the United Nations. Prior to this Israel was the only nation at the UN which was not a member of any group (The Arab states would not allow it to join the Asia group), which meant it could not be a member of the Security Council or appoint anyone to the International Court and other key UN roles. Since December 2013 it has been a permanent member of the group.

In July 2000, Aryeh Deri was sentenced to 3 years in prison for bribe taking. Deri is regarded as the mastermind behind the rise of Shas and was a government minister at the age of 24. Political manipulation meant the investigation lasted for years. Deri subsequently sued a Police Officer who alleged that he was linked to the traffic-accident death of a witness, who was run over in New York by a driver who had once been in the employ of an associate of Deri.

On 28 September 2000, Israeli opposition leader Ariel Sharon visited the Al-Aqsa compound, or Temple Mount, the following day the Palestinians launched the al-Aqsa Intifada. David Samuels and Khaled Abu Toameh have stated that the uprising was planned much earlier. In October 2000, Palestinians destroyed Joseph's Tomb, a Jewish shrine in Nablus.

The Arrow missile, a missile designed to destroy ballistic missiles, including Scud missiles, was first deployed by Israel. In 2001, with the Peace Process increasingly in disarray, Ehud Barak called a special election for Prime Minister. Barak hoped a victory would give him renewed authority in negotiations with the Palestinians. Instead opposition leader Ariel Sharon was elected PM. After this election, the system of directly electing the Premier was abandoned.

The failure of the peace process, increased Palestinian terror and occasional attacks by Hezbollah from Lebanon, led much of the Israeli public and political leadership to lose confidence in the Palestinian Authority as a peace partner. Most felt that many Palestinians viewed the peace treaty with Israel as a temporary measure only. Many Israelis were thus anxious to disengage from the Palestinians. In response to a wave of suicide bomb attacks, culminating in the "Passover massacre" (see List of Israeli civilian casualties in the Second Intifada), Israel launched Operation Defensive Shield in March 2002, and Sharon began the construction of a barrier around the West Bank. Around the same time, the Israeli town of Sderot and other Israeli communities near Gaza became subject to constant shelling and mortar bomb attacks from Gaza.

Thousands of Jews from Latin America began arriving in Israel due to economic crises in their countries of origin. In January 2003 separate elections were held for the Knesset. Likud won the most seats (27). An anti-religion party, Shinui, led by media pundit Tommy Lapid, won 15 seats on a secularist platform, making it the third largest party (ahead of orthodox Shas). Internal fighting led to Shinui's demise at the next election. In 2004, the Black Hebrews were granted permanent residency in Israel. The group had begun migrating to Israel 25 years earlier from the United States, but had not been recognized as Jews by the state and hence not granted citizenship under Israel's Law of Return. They had settled in Israel without official status. From 2004 onwards, they received citizen's rights.

The Sharon government embarked on an extensive program of construction of desalinization plants that freed Israel of the fear of drought. Some of the Israeli desalinization plants are the largest of their kind in the world.

In May 2004, Israel launched Operation Rainbow in southern Gaza to create a safer environment for the IDF soldiers along the Philadelphi Route. On September 30, 2004, Israel carried out Operation Days of Penitence in northern Gaza to destroy the launching sites of Palestinian rockets which were used to attack Israeli towns. In 2005, all Jewish settlers were evacuated from Gaza (some forcibly) and their homes demolished. Disengagement from the Gaza Strip was completed on 12 September 2005. Military disengagement from the northern West Bank was completed ten days later.

In 2005 Sharon left the Likud and formed a new party called Kadima, which accepted that the peace process would lead to creation of a Palestinian state. He was joined by many leading figures from both Likud and Labour.

Hamas won the Palestinian legislative election, 2006, the first and only genuinely free Palestinian elections. Hamas' leaders rejected all agreements signed with Israel, refused to recognize Israel's right to exist, refused to abandon terror, and occasionally claimed the Holocaust was a Jewish conspiracy. The withdrawal and Hamas victory left the status of Gaza unclear, Israel claimed it was no longer an occupying power but continued to control air and sea access to Gaza although it did not exercise sovereignty on the ground. Egypt insisted that it was still occupied and refused to open border crossings with Gaza, although it was free to do so.

On April 2006 Ariel Sharon was incapacitated by a severe haemorrhagic stroke and Ehud Olmert became Prime Minister.

Ehud Olmert was elected Prime Minister after his party, Kadima, won the most seats (29) in the Israeli legislative election, 2006. In 2005 Mahmoud Ahmadinejad was officially elected president of Iran; since then, Iranian policy towards Israel has grown more confrontational. Israeli analysts believe Ahmadinejad has worked to undermine the peace process with arms supplies and aid to Hezbullah in South Lebanon and Hamas in Gaza, and is developing nuclear weapons, possibly for use against Israel. Iranian support for Hezbollah and its nuclear arms program are in contravention of UN Security Council resolutions 1559 and 1747. Iran also encourages Holocaust denial. Following the Israeli withdrawal from Lebanon, Hezbollah had mounted periodic attacks on Israel, which did not lead to Israeli retaliation. Similarly, the withdrawal from Gaza led to incessant shelling of towns around the Gaza area with only minimal Israeli response. The failure to react led to criticism from the Israeli right and undermined the government.

On 14 March 2006, Israel carried out an operation in the Palestinian Authority prison of Jericho in order to capture Ahmad Sa'adat and several Palestinian Arab prisoners located there who assassinated Israeli politician Rehavam Ze'evi in 2001. The operation was conducted as a result of the expressed intentions of the newly elected Hamas government to release these prisoners. On 25 June 2006, a Hamas force crossed the border from Gaza and attacked a tank, capturing Israeli soldier Gilad Shalit, sparking clashes in Gaza.
On 12 July, Hezbollah attacked Israel from Lebanon, shelled Israeli towns and attacked a border patrol, taking two dead or badly wounded Israeli soldiers. These incidents led Israel to initiate the Second Lebanon War, which lasted through August 2006. Israeli forces entered some villages in Southern Lebanon, while the air force attacked targets all across the country. Israel only made limited ground gains until the launch of Operation Changing Direction 11, which lasted for 3 days with disputed results. Shortly before a UN ceasefire came into effect, Israeli troops captured Wadi Saluki. The war concluded with Hezbollah evacuating its forces from Southern Lebanon, while the IDF remained until its positions could be handed over to the Lebanese Armed Forces and UNIFIL.

In 2007 education was made compulsory until the age of 18 for all citizens (it had been 16). Refugees from the genocide in Darfur, mostly Muslim, arrived in Israel illegally, with some given Asylum. Illegal immigrants arrived mainly from Africa in addition to foreign workers overstaying their visas. The numbers of such migrants are not known, and estimates vary between 30,000 and over 100,000.

An American billionaire casino owner, Sheldon Adelson, set up a free newspaper Israel Hayom with the express intention of reducing the influence of the dominant (centre-left) newspaper Yediot Ahronot and causing a right-ward shift in Israeli politics by supporting Netanyahu.

In June 2007, Hamas took control of the Gaza Strip in the course of the Battle of Gaza, seizing government institutions and replacing Fatah and other government officials with its own. Following the takeover, Egypt and Israel imposed a partial blockade, on the grounds that Fatah had fled and was no longer providing security on the Palestinian side, and to prevent arms smuggling by terrorist groups. On 6 September 2007, the Israeli Air Force destroyed a nuclear reactor in Syria. On 28 February 2008, Israel launched a military campaign in Gaza in response to the constant firing of Qassam rockets by Hamas militants. On July 16, 2008, Hezbollah swapped the bodies of Israeli soldiers Ehud Goldwasser and Eldad Regev, kidnapped in 2006, in exchange for the Lebanese terrorist Samir Kuntar, four Hezbollah prisoners, and the bodies of 199 Palestinian Arab and Lebanese fighters.

Olmert came under investigation for corruption and this led him to announce on 30 July 2008, that he would be stepping down as Prime Minister following election of a new leader of the Kadima party in September 2008. Tzipi Livni won the election, but was unable to form a coalition and Olmert remained in office until the general election. Israel carried out Operation Cast Lead in the Gaza Strip from 27 December 2008 to 18 January 2009 in response to rocket attacks from Hamas militants, leading to a decrease of Palestinian rocket attacks.

In the 2009 legislative election Likud won 27 seats and Kadima 28; however, the right-wing camp won a majority of seats, and President Shimon Peres called on Netanyahu to form the government. Russian immigrant-dominated Yisrael Beiteinu came third with 15 seats, and Labour was reduced to fourth place with 13 seats. In 2009, Israeli billionaire Yitzhak Tshuva announced the discovery of huge natural gas reserves off the coast of Israel.

On 31 May 2010, an international incident broke out in the Mediterranean Sea when foreign activists trying to break the maritime blockade over Gaza, clashed with Israeli troops. During the struggle, nine Turkish activists were killed. In late September 2010 took place direct negotiations between Israel and the Palestinians without success. As a defensive countermeasure to the rocket threat against Israel's civilian population, at the end of March 2011 Israel began to operate the advanced mobile air defence system "Iron Dome" in the southern region of Israel and along the border with the Gaza Strip.
On 14 July 2011, the largest social protest in the history of Israel began in which hundreds of thousands of protesters from a variety of socio-economic and religious backgrounds in Israel protested against the continuing rise in the cost of living (particularly housing) and the deterioration of public services in the country (such as health and education). The peak of the demonstrations took place on 3 September 2011, in which about 400,000 people demonstrated across the country.

In October 2011, a deal was reached between Israel and Hamas, by which the kidnapped Israeli soldier Gilad Shalit was released in exchange for 1,027 Palestinians and Arab-Israeli prisoners. In March 2012, Secretary-general of the Popular Resistance Committees, Zuhir al-Qaisi, a senior PRC member and two additional Palestinian militants were assassinated during a targeted killing carried out by Israeli forces in Gaza. The Palestinian armed factions in the Gaza Strip, led by the Islamic Jihad and the Popular Resistance Committees, fired a massive amount of rockets towards southern Israel in retaliation, sparking five days of clashes along the Gaza border.

In May 2012, Prime Minister Benjamin Netanyahu reached an agreement with the Head of Opposition Shaul Mofaz for Kadima to join the government, thus cancelling the early election supposed to be held in September. However, in July, the Kadima party left Netanyahu's government due to a dispute concerning military conscription for ultra-Orthodox Jews in Israel.

In June 2012, Israel transferred the bodies of 91 Palestinian suicide bombers and other militants as part of what Mark Regev, spokesman for Netanyahu, described as a "humanitarian gesture" to PA chairman Mahmoud Abbas to help revive the peace talks, and reinstate direct negotiations between Israel and the Palestinians. On 21 October 2012, United States and Israel began their biggest joint air and missile defence exercise, known as Austere Challenge 12, involving around 3,500 US troops in the region along with 1,000 IDF personnel, expected to last three weeks. Germany and Britain also participated. In response to over a hundred rocket attacks on southern Israeli cities, Israel began an operation in Gaza on 14 November 2012, with the targeted killing of Ahmed Jabari, chief of Hamas military wing, and airstrikes against twenty underground sites housing long-range missile launchers capable of striking Tel Aviv. In January 2013, construction of the barrier on the Israeli-Egyptian border was completed in its main section.

Benjamin Netanyahu was elected Prime Minister again after the Likud Yisrael Beiteinu alliance won the most seats (31) in the 2013 legislative election and formed a coalition government with secular centrist Yesh Atid party (19), rightist The Jewish Home (12) and Livni's Hatnuah (6), excluding Haredi parties. Labour came in third with 15 seats. In July 2013, as a "good will gesture" to restart peace talks with the Palestinian Authority, Israel agreed to release 104 Palestinian prisoners, most of whom had been in jail since before the 1993 Oslo Accords, including militants who had killed Israeli civilians. In April 2014, Israel suspended peace talks after Hamas and Fatah agreed to form a unity government.

Following an escalation of rocket attacks by Hamas, Israel started an operation in the Gaza Strip on 8 July 2014, which included a ground incursion aimed at destroying the cross-border tunnels. Differences over the budget and a triggered early elections in December 2014. After the 2015 Israeli elections, Netanyahu renewed his mandate as Prime Minister when Likud obtained 30 seats and formed a right-wing coalition government with Kulanu (10), The Jewish Home (8), and Orthodox parties Shas (7) and United Torah Judaism (6), the bare minimum of seats required to form a coalition. The Zionist Union alliance came second with 24 seats.

On 6 December 2017, President Donald Trump of the United States formally announced United States recognition of Jerusalem as the capital of Israel.



</doc>
<doc id="13810" url="https://en.wikipedia.org/wiki?curid=13810" title="Harvey Mudd College">
Harvey Mudd College

Harvey Mudd College (HMC) is a private residential undergraduate science and engineering college in Claremont, California. It is one of the institutions of the contiguous Claremont Colleges which share adjoining campus grounds. Harvey Mudd College shares university resources such as libraries, dining halls, health services and campus security with the other Claremont Colleges, although each college is independently managed, with their own faculty, board of trustees, endowment, and admissions procedures. Students at Harvey Mudd College may take classes (acceptable for academic credit at Harvey Mudd College) at the other four undergraduate Claremont colleges. The Bachelor of Science diploma received at graduation is issued by Harvey Mudd College.

The college is named after Harvey Seeley Mudd, one of the initial investors in the Cyprus Mines Corporation. Although involved in planning of the new institution, Mudd died before it opened. The college was funded by Mudd's friends and family, and named in his honor.

HMC offers four-year degrees in chemistry, mathematics, physics, computer science, biology, and engineering, interdisciplinary degrees in mathematical biology, and joint majors in computer science and mathematics; or in biology and chemistry. Students may also elect an Individual Program of Study (IPS) or an off-campus major offered by any of the other Claremont Colleges, provided one also completes a minor in one of the technical fields that Harvey Mudd offers as a major.

In 2018, the "Chronicle of Higher Education" reported that, in response to student " complaints first to mental-health counselors and then to outside evaluators" the college was " considering how to ease pressure on students without sacrificing rigor. " 

For the class of 2021, the college received 4,078 applications and admitted 529 applicants (a 13.0% acceptance rate). Of the 225 freshmen who enrolled, the middle 50% of SAT scores were 750–800 in mathematics and 720–770 in critical reading, while the ACT Composite range was 33–35.

Harvey Mudd, along with Wake Forest University, long held out as the last four-year colleges or universities in the U.S. to accept only SAT and not ACT test scores for admission. In August 2007, at the beginning of the application process for the class of 2012, HMC began accepting ACT results, a year after Wake Forest abandoned its former SAT-only policy.

In 2016, Harvey Mudd was for the second year in a row the most expensive college in the United States, with the total annual cost of attendance (tuition, fees, and room and board) being $69,717. About 70% of freshmen receive financial aid.

The official names for the dormitories of Harvey Mudd College are (listed in order of construction):
Until the addition of the Linde and Sontag dorms, Atwood and Case dorms were occasionally referred to as New Dorm and New Dorm II; Mildred E. Mudd Hall and Marks Hall are almost invariably referred to as East dorm and South dorm.

During the construction of Case Dorm some students decided as a prank to move all of the survey stakes exactly six inches in one direction.

South Dorm is in the northwest corner of the quad. "East" was the first dorm, but it wasn't until "West" was built west of it that it was actually referred to as "East". Then "North" was built, directly north of "East". When the fourth dorm (Marks) was built, there was one corner of the quad available (the northwest) and one directional name, "South", remaining. To this day "South" dorm is the northernmost HMC dorm.

The fifth, sixth, seventh, eighth, and ninth dorms built are Atwood, Case, Linde, Sontag, and Drinkward, respectively. They were initially referred to as "the colonies" by some students, a reference to the fact that they were newer and at the farthest end of the campus; these dorms are now more commonly referred to as "the outer dorms." The college had initially purchased an apartment building adjacent to the newer dorms to house additional students, but it was demolished to make room for Sontag.

Since any HMC student, regardless of class year, can live in any of the dormitories, several of the dorms have accumulated long-standing traditions and so-called 'personalities'. 

A student-led organization, "Increasing Harvey Mudd's Traditional Practices" (IHTP), works to revive college traditions that have slowly faded over the years, and also starts new traditions that the group hopes to see take root on campus. It hosts annual events such as the 5-Class Competition, Friday Nooners, Wednesday Nighters, Frosh/Soph Games, and the Thomas-Garrett Affair.

Athletes from Harvey Mudd compete alongside athletes from Claremont McKenna College and Scripps College as the Claremont-Mudd-Scripps Stags and Athenas. The teams participate in NCAA Division III in the Southern California Intercollegiate Athletic Conference. The mascot for the men's teams is Stanley the Stag, and the women's teams are the Athenas. Their colors are cardinal and gold.

According to the Division III Fall Learfield Director's Cup Standings for the 2016-2017 year, CMS ranks 12th among all Division III programs, and first among SCIAC colleges.

There are 21 men's and women's teams.

Men's sports


Women's sports



The other sports combination of the Claremont Colleges, and CMS' primary rival, is the team made up of Pomona College and Pitzer College known as the Pomona-Pitzer Sagehens (P-P).

The original buildings of campus, designed by Edward Durell Stone and completed in 1955, features "knobbly concrete squares that students of Harvey Mudd affectionately call “warts” and use as hooks for skateboards." The school's unofficial mascot "Wally Wart" is an anthropomorphic concrete wart.

In 2013, "Travel and Leisure" named the college as one of "America's ugliest college campuses" and noted that while Stone regarded his design as a "Modernist masterpiece" the result was "layering drab, slab-sided buildings with Beaux-Arts decoration."

The California Institute of Technology, another school known for its strength in the natural sciences and engineering, is located away (nearly the distance of a marathon) from Harvey Mudd College. From time to time, Mudders have been known to amuse themselves by pranking Caltech. For example, in 1986, students from Mudd stole a memorial cannon from Fleming House at Caltech (originally from the National Guard) by dressing as maintenance people and carting it off on a flatbed truck for "cleaning". Harvey Mudd eventually returned the cannon after Caltech threatened to take legal action. In 2006, MIT replicated the prank and moved the same cannon to their campus in Cambridge, Massachusetts.

Harvey Mudd maintains the highest rate of science and engineering Ph.D. production among all undergraduate colleges and second highest (Caltech ranks first and MIT third) compared to all universities and colleges, according to a 2008 report by the National Science Foundation. 

"Money" magazine ranked Harvey Mudd 79th in the country out of the nearly 1500 schools for its 2016 Best Colleges ranking. The Daily Beast ranked Harvey Mudd 78th in the country out of nearly 2000 schools for its 2013 Best Colleges ranking. In "U.S. News & World Report"'s 2017 America's Best Colleges, Harvey Mudd College is tied for the 12th best U.S. liberal arts college, and is 1st among undergraduate engineering schools in the U.S. whose highest degree is a Master's. "Forbes" in 2017 rated it No. 18 in its "America's Top Colleges" ranking, which includes 660 military academies, national universities and liberal arts colleges.

As of 2007, the median GPA of Harvey Mudd students was 3.35, behind other peer institutions; only seven students have achieved a perfect 4.0 GPA.

In 1997, Harvey Mudd College became the sole American undergraduate-only institution to win 1st place in the ACM International Collegiate Programming Contest. As of 2017, no American school has won the world competition since.

In 2006, the Harvey Mudd College mathematics department received the American Mathematical Society award for an Exemplary Program or Achievement in a Mathematics Department. Two of the department's alumni, Joshua Greene and Aaron Archer, were winners and honorable mention for its undergraduate Morgan Prize in 2002 and 1998 respectively.

Notable Harvey Mudd College alumni include co-inventor of SQL Donald D. Chamberlin (1966), astronauts George "Pinky" Nelson (1972) and Stan Love (1987), and diplomat Richard H. Jones (1972).




</doc>
<doc id="13811" url="https://en.wikipedia.org/wiki?curid=13811" title="Heaven">
Heaven

Heaven, or the heavens, is a common religious, cosmological, or transcendent place where beings such as gods, angels, spirits, saints, or venerated ancestors are said to originate, be enthroned, or live. According to the beliefs of some religions, heavenly beings can descend to earth or incarnate, and earthly beings can ascend to heaven in the afterlife, or in exceptional cases enter heaven alive.

Heaven is often described as a "higher place", the holiest place, a Paradise, in contrast to hell or the Underworld or the "low places", and universally or conditionally accessible by earthly beings according to various standards of divinity, goodness, piety, faith, or other virtues or right beliefs or simply the will of God. Some believe in the possibility of a heaven on Earth in a World to Come.

Another belief is in an axis mundi or world tree which connects the heavens, the terrestrial world, and the underworld. In Indian religions, heaven is considered as "Svarga loka", and the soul is again subjected to rebirth in different living forms according to its "karma". This cycle can be broken after a soul achieves "Moksha" or "Nirvana". Any place of existence, either of humans, souls or deities, outside the tangible world (heaven, hell, or other) is referred to as "otherworld."

The modern English word "heaven" is derived from the earlier (Middle English) "heven" (attested 1159); this in turn was developed from the previous Old English form "heofon". By about 1000, "heofon" was being used in reference to the Christianized "place where God dwells", but originally, it had signified "sky, firmament" (e.g. in "Beowulf", c. 725). The English term has cognates in the other Germanic languages: Old Saxon "heƀan" "sky, heaven" (hence also Middle Low German "heven" "sky"), Old Icelandic "himinn", Gothic "himins"; and those with a variant final "-l": Old Frisian "himel, himul" "sky, heaven", Old Saxon and Old High German "himil", Old Saxon and Middle Low German "hemmel", Old Dutch and Dutch "hemel", and modern German "Himmel". All of these have been derived from a reconstructed Proto-Germanic form *"hemina-". or "*hemō".

The further derivation of this form is uncertain. A connection to Proto-Indo-European "*ḱem-" "cover, shroud", via a reconstructed "*k̑emen-" or "*k̑ōmen-" "stone, heaven", has been proposed. Others endorse the derivation from a Proto-Indo-European root "*h₂éḱmō" "stone" and, possibly, "heavenly vault" at the origin of this word, which then would have as cognates Ancient Greek ἄκμων (ákmōn "anvil, pestle; meteorite"), Persian آسمان‎ ("âsemân, âsmân" "stone, sling-stone; sky, heaven") and Sanskrit अश्मन् ("aśman" "stone, rock, sling-stone; thunderbolt; the firmament"). In the latter case English "hammer" would be another cognate to the word.

The ancient Mesopotamians regarded the sky as a series of domes (usually three, but sometimes seven) covering the flat earth. Each dome was made of a different kind of precious stone. The lowest dome of heaven was made of jasper and was the home of the stars. The middle dome of heaven was made of "saggilmut" stone and was the abode of the Igigi. The highest and outermost dome of heaven was made of "luludānītu" stone and was personified as An, the god of the sky. The celestial bodies were equated with specific deities as well. The planet Venus was believed to be Inanna, the goddess of love, sex, and war. The sun was her brother Utu, the god of justice, and the moon was their father Nanna.

In ancient Near Eastern cultures in general and in Mesopotamia in particular, humans had little to no access to the divine realm. Heaven and earth were separated by their very nature; humans could see and be affected by elements of the lower heaven, such as stars and storms, but ordinary mortals could not go to heaven because it was the abode of the gods alone. In the "Epic of Gilgamesh", Gilgamesh says to Enkidu, "Who can go up to heaven, my friend? Only the gods dwell with Shamash forever." Instead, after a person died, his or her soul went to Kur (later known as Irkalla), a dark shadowy underworld, located deep below the surface of the earth.

All souls went to the same afterlife, and a person's actions during life had no impact on how he would be treated in the world to come. Nonetheless, funerary evidence indicates that some people believed that Inanna had the power to bestow special favors upon her devotees in the afterlife. Despite the separation between heaven and earth, humans sought access to the gods through oracles and omens. The gods were believed to live in heaven, but also in their temples, which were seen as the channels of communication between earth and heaven, which allowed mortal access to the gods. The Ekur temple in Nippur was known as the "Dur-an-ki", the "mooring-rope" of heaven and earth. It was widely thought to have been built and established by Enlil himself.

Almost nothing is known of Bronze Age (pre-1200 BC) Canaanite views of heaven, and the archaeological findings at Ugarit (destroyed c. 1200 BC) have not provided information. The 1st century Greek author Philo of Byblos may preserve elements of Iron Age Phoenician religion in his "Sanchuniathon".

The ancient Hittites believed that some deities lived in Heaven, while others lived in remote places on earth, such as mountains, where humans had little access. In the Middle Hittite myths, heaven is the abode of the gods. In the Song of Kumarbi, Alalu was king in heaven for nine years before giving birth to his son, Anu. Anu was himself overthrown by his son, Kumarbi.
As in other ancient Near Eastern cultures, in the Hebrew Bible, the universe is commonly divided into two realms: heaven ("šāmayim") and earth ("’ereṣ"). Sometimes a third realm is added: either "sea" (, ), "water under the earth" (, ), or sometimes a vague "land of the dead" that is never described in depth (, , ). The structure of heaven itself is never fully described in the Hebrew Bible, but the fact that the Hebrew word "šāmayim" is plural has been interpreted by scholars as an indication that the ancient Israelites envisioned the heavens as having multiple layers, much like the ancient Mesopotamians. This reading is also supported by the use of the phrase "heaven of heavens" in verses such as , , and and .

In line with the typical view of most Near Eastern cultures, the Hebrew Bible depicts heaven as a place that is inaccessible to humans. Although some prophets are occasionally granted temporary visionary access to heaven, such as in , and , and , they hear only God's deliberations concerning the earth and learn nothing of what heaven is like. There is almost no mention in the Hebrew Bible of heaven as a possible afterlife destination for human beings, who are instead described as "resting" in Sheol (, , ). The only two possible exceptions to this are Enoch, who is described in as having been "taken" by God, and the prophet Elijah, who is described in as having ascended to heaven in a chariot of fire. According to Michael B. Hundley, the text in both of these instances is ambiguous regarding the significance of the actions being described and in neither of these cases does the text explain what happened to the subject afterwards.

The God of the Israelites is described as ruling both heaven and earth ( , ). Other passages, such as state that even the vastness of heaven cannot contain God's majesty. A number of passages throughout the Hebrew Bible indicate that heaven and earth will one day come to an end (, , , , , , and and ). This view is paralleled in other ancient Near Eastern cultures, which also regarded heaven and earth as vulnerable and subject to dissolution. However, the Hebrew Bible differs from other ancient Near Eastern cultures in that it portrays the God of Israel as independent of creation and unthreatened by its potential destruction. Because most of the Hebrew Bible concerns the God of Israel's relationship with his people, most of the events described in it take place on earth, not in heaven. The Deuteronomistic source, Deuteronomistic History, and Priestly source all portray the Temple in Jerusalem as the sole channel of communication between earth and heaven.

During the period of the Second Temple ( 515 BC – 70 AD), the Hebrew people lived under the rule of first the Persian Achaemenid Empire, then the Greek kingdoms of the Diadochi, and finally the Roman Empire. Their culture was profoundly influenced by those of the peoples who ruled them. Consequently, their views on existence after death were profoundly shaped by the ideas of the Persians, Greeks, and Romans. The idea of the immortality of the soul is derived from Greek philosophy and the idea of the resurrection of the dead is derived from Persian cosmology. By the early first century AD, these two seemingly incompatible ideas were often conflated by Hebrew thinkers. The Hebrews also inherited from the Persians, Greeks, and Romans the idea that the human soul originates in the divine realm and seeks to return there. The idea that a human soul belongs in heaven and that the earth is merely a temporary abode in which the soul is tested to prove its worthiness became increasingly popular during the Hellenistic period (323 – 31 BC). Gradually, some Hebrews began to adopt the idea of heaven as the eternal home of the righteous dead.

Descriptions of heaven in the New Testament are more fully developed than those in the Old Testament, but are still generally vague. As in the Old Testament, in the New Testament God is described as the ruler of heaven and earth, but his power over the earth is challenged by Satan. Sayings of Jesus recorded in the Gospels of Mark and Luke speak of the "Kingdom of God" (; ), while the Gospel of Matthew more commonly uses the term "Kingdom of Heaven" (; ). Both phrases have exactly the same meaning, but the author of the Gospel of Matthew changed the name "Kingdom of God" to "Kingdom of Heaven" in most instances because it was the more acceptable phrase in his own cultural and religious context in the late first century.

Modern scholars agree that the Kingdom of God was an essential part of the teachings of the historical Jesus. In spite of this, none of the gospels ever record Jesus as having explained exactly what the phrase "Kingdom of God" means. The most likely explanation for this apparent omission is that the Kingdom of God was a commonly understood concept that required no explanation. Jews in Judea during the early first century believed that God reigns eternally in Heaven, but many also believed that God would eventually establish his kingdom on earth as well. This belief is referenced in the first petition of the Lord's Prayer, taught by Jesus to his disciples and recorded in both and : "Your kingdom come, your will be done, on earth as it is in heaven."

Because God's Kingdom was believed to be superior to any human kingdom, this meant that God would necessarily drive out the Romans, who ruled Judea, and establish his own direct rule over the Jewish people. In the teachings of the historical Jesus, people are expected to prepare for the coming of the Kingdom of God by living moral lives. Jesus's commands for his followers to adopt lifestyles of moral perfectionism are found in many passages throughout the Synoptic Gospels, particularly in the Sermon on the Mount in . Jesus also taught that, in the Kingdom of Heaven, there would be a reversal of roles in which "the last will be first and the first will be last" (, , , and ). This teaching recurs throughout the recorded teachings of Jesus, including in the admonition to be like a child in , , and , the Parable of the Rich Man and Lazarus in , the Parable of the Workers in the Vineyard in , the Parable of the Great Banquet in , and the Parable of the Prodigal Son in .

Traditionally, Christianity has taught that heaven is the location of the throne of God as well as the holy angels, although this is in varying degrees considered metaphorical. In traditional Christianity, it is considered a state or condition of existence (rather than a particular place somewhere in the cosmos) of the supreme fulfillment of theosis in the beatific vision of the Godhead. In most forms of Christianity, heaven is also understood as the abode for the redeemed dead in the afterlife, usually a temporary stage before the resurrection of the dead and the saints' return to the New Earth.

The resurrected Jesus is said to have ascended to heaven where he now sits at the Right Hand of God and will return to earth in the Second Coming. Various people have been said to have entered heaven while still alive, including Enoch, Elijah and Jesus himself, after his resurrection. According to Roman Catholic teaching, Mary, mother of Jesus, is also said to have been assumed into heaven and is titled the Queen of Heaven.

In the 2nd century AD, Irenaeus of Lyons recorded a belief that, in accordance with , those who in the afterlife see the Saviour are in different mansions, some dwelling in the heavens, others in paradise and others in "the city".

While the word used in all these writings, in particular the New Testament Greek word οὐρανός ("ouranos"), applies primarily to the sky, it is also used metaphorically of the dwelling place of God and the blessed. Similarly, though the English word "heaven" still keeps its original physical meaning when used, for instance, in allusions to the stars as "lights shining through from heaven", and in phrases such as heavenly body to mean an astronomical object, the heaven or happiness that Christianity looks forward to is, according to Pope John Paul II, "neither an abstraction nor a physical place in the clouds, but a living, personal relationship with the Holy Trinity. It is our meeting with the Father which takes place in the risen Christ through the communion of the Holy Spirit."

While the concept of heaven ("malkuth hashamaim" מלכות השמים, the Kingdom of Heaven) is much discussed in Christian thought, the Jewish concept of the afterlife, sometimes known as "olam haba", the World-to-come, is not discussed so often. The Torah has little to say on the subject of survival after death, but by the time of the rabbis two ideas had made inroads among the Jews: one, which is probably derived from Greek thought, is that of the immortal soul which returns to its creator after death; the other, which is thought to be of Persian origin, is that of resurrection of the dead.

Jewish writings refer to a "new earth" as the abode of mankind following the resurrection of the dead. Originally, the two ideas of immortality and resurrection were different but in rabbinic thought they are combined: the soul departs from the body at death but is returned to it at the resurrection. This idea is linked to another rabbinic teaching, that men's good and bad actions are rewarded and punished not in this life but after death, whether immediately or at the subsequent resurrection. Around 1 CE, the Pharisees are said to have maintained belief in resurrection but the Sadducees are said to have denied it (Matt. 22:23).

The Mishnah has many sayings about the World to Come, for example, "Rabbi Yaakov said: This world is like a lobby before the World to Come; prepare yourself in the lobby so that you may enter the banquet hall."

Judaism holds that the righteous of all nations have a share in the World-to-come.

According to Nicholas de Lange, Judaism offers no clear teaching about the destiny which lies in wait for the individual after death and its attitude to life after death has been expressed as follows: "For the future is inscrutable, and the accepted sources of knowledge, whether experience, or reason, or revelation, offer no clear guidance about what is to come. The only certainty is that each man must die - beyond that we can only guess."

According to Tracey R. Rich of the website "Judaism 101", Judaism, unlike other world-religions, is not focused on the quest of getting into heaven but on life and how to live it.

In order from lowest to highest, the seven heavens, "Shamayim" (שָׁמַיִם), according to the Talmud, are listed alongside the angels who govern them:


Similar to Jewish traditions such as the Talmud, the Qur'an and Hadith frequently mention the existence of seven "samāwāt" (سماوات), the plural of "samāʾ" (سماء), meaning 'heaven, sky, celestial sphere', and cognate with Hebrew "shamāyim" (שמים). Some of the verses in the Qur'an mentioning the "samaawat" are , , . Sidrat al-Muntaha, a large enigmatic Lote tree, marks the end of the seventh heaven and the utmost extremity for all of God's creatures and heavenly knowledge.

One interpretation of "heavens" is that all the stars and galaxies (including the Milky Way) are all part of the "first heaven", and "beyond that six still bigger worlds are there," which have yet to be discovered by scientists.

According to Shi'ite sources, Ali mentioned the names of the seven heavens as below:

Still an afterlife destination of the righteous is conceived in Islam as "Jannah" ( "Garden [of Eden]" translated as "paradise"). Regarding Eden or paradise the Quran says, "The parable of the Garden which the righteous are promised: Beneath it flow rivers; perpetual is the fruits thereof and the shade therein. Such is the end of the righteous; and the end of the unbelievers is the Hellfire." Islam rejects the concept of original sin, and Muslims believe that all human beings are born pure. Children automatically go to paradise when they die, regardless of the religion of their parents.

Paradise is described primarily in physical terms as a place where every wish is immediately fulfilled when asked. Islamic texts describe immortal life in Jannah as happy, without negative emotions. Those who dwell in Jannah are said to wear costly apparel, partake in exquisite banquets, and recline on couches inlaid with gold or precious stones. Inhabitants will rejoice in the company of their parents, spouses, and children. In Islam if one's good deeds outweigh one's sins then one may gain entrance to paradise. Conversely, if one's sins outweigh their good deeds they are sent to hell. The more good deeds one has performed the higher the level of Jannah one is directed to.

Verses which describe paradise include: , , , , .

The Quran refer to Jannah with different names: "Al-Firdaws", "Jannātu-′Adn" ("Garden of Eden" or "Everlasting Gardens"), "Jannatu-n-Na'īm" ("Garden of Delight"), "Jannatu-l-Ma'wa" ("Garden of Refuge"), "Dāru-s-Salām" ("Abode of Peace"), "Dāru-l-Muqāma" ("Abode of Permanent Stay"), "al-Muqāmu-l-Amin" ("The Secure Station") and "Jannātu-l-Khuld" ("Garden of Immortality"). In the Hadiths, these are the different regions in paradise.

According to the Ahmadiyya view, much of the imagery presented in the Quran regarding heaven, but also hell, is in fact metaphorical. They propound the verse which describes, according to them how the life to come after death is very different from the life here on earth. The Quran says: According to Mirza Ghulam Ahmad, the founder of Ahmadiyya sect in Islam, the soul will give birth to another rarer entity and will resemble the life on this earth in the sense that this entity will bear a similar relationship to the soul, as the soul bears relationship with the human existence on earth. On earth, if a person leads a righteous life and submits to the will of God, his or her tastes become attuned to enjoying spiritual pleasures as opposed to carnal desires. With this, an "embryonic soul" begins to take shape. Different tastes are said to be born which a person given to carnal passions finds no enjoyment. For example, sacrifice of one's own's rights over that of other's becomes enjoyable, or that forgiveness becomes second nature. In such a state a person finds contentment and Peace at heart and at this stage, according to Ahmadiyya beliefs, it can be said that a soul within the soul has begun to take shape.

The Bahá'í Faith regards the conventional description of heaven (and hell) as a specific place as symbolic. The Bahá'í writings describe heaven as a "spiritual condition" where closeness to God is defined as heaven; conversely hell is seen as a state of remoteness from God. Bahá'u'lláh, the founder of the Bahá'í Faith, has stated that the nature of the life of the soul in the afterlife is beyond comprehension in the physical plane, but has stated that the soul will retain its consciousness and individuality and remember its physical life; the soul will be able to recognize other souls and communicate with them.

For Bahá'ís, entry into the next life has the potential to bring great joy. Bahá'u'lláh likened death to the process of birth. He explains: "The world beyond is as different from this world as this world is different from that of the child while still in the womb of its mother." The analogy to the womb in many ways summarizes the Bahá'í view of earthly existence: just as the womb constitutes an important place for a person's initial physical development, the physical world provides for the development of the individual soul. Accordingly, Bahá'ís view life as a preparatory stage, where one can develop and perfect those qualities which will be needed in the next life. The key to spiritual progress is to follow the path outlined by the current Manifestation of God, which Bahá'ís believe is currently Bahá'u'lláh. Bahá'u'lláh wrote, "Know thou, of a truth, that if the soul of man hath walked in the ways of God, it will, assuredly return and be gathered to the glory of the Beloved."

The Bahá'í teachings state that there exists a hierarchy of souls in the afterlife, where the merits of each soul determines their place in the hierarchy, and that souls lower in the hierarchy cannot completely understand the station of those above. Each soul can continue to progress in the afterlife, but the soul's development is not entirely dependent on its own conscious efforts, the nature of which we are not aware, but also augmented by the grace of God, the prayers of others, and good deeds performed by others on Earth in the name of that person.

In the native Chinese Confucian traditions, heaven (Tian) is an important concept, where the ancestors reside and from which emperors drew their mandate to rule in their dynastic propaganda, for example.

Heaven is a key concept in Chinese mythology, philosophies and religions, and is on one end of the spectrum a synonym of "Shangdi" ("Supreme Deity") and on the other naturalistic end, a synonym for nature and the sky. The Chinese term for "heaven", "Tian" (天), derives from the name of the supreme deity of the Zhou Dynasty. After their conquest of the Shang Dynasty in 1122 BC, the Zhou people considered their supreme deity "Tian" to be identical with the Shang supreme deity "Shangdi". The Zhou people attributed heaven with anthropomorphic attributes, evidenced in the etymology of the Chinese character for heaven or sky, which originally depicted a person with a large cranium. heaven is said to see, hear and watch over all men. heaven is affected by man's doings, and having personality, is happy and angry with them. Heaven blesses those who please it and sends calamities upon those who offend it. Heaven was also believed to transcend all other spirits and gods, with Confucius asserting, "He who offends against Heaven has none to whom he can pray."

Other philosophers born around the time of Confucius such as Mozi took an even more theistic view of heaven, believing that heaven is the divine ruler, just as the Son of Heaven (the King of Zhou) is the earthly ruler. Mozi believed that spirits and minor gods exist, but their function is merely to carry out the will of heaven, watching for evil-doers and punishing them. Thus they function
as angels of heaven and do not detract from its monotheistic government of the world. With such a high monotheism, it is not surprising that Mohism championed a concept called "universal love" ("jian'ai", 兼愛), which taught that heaven loves all people equally and that each person should similarly love all human beings without distinguishing between his own relatives and those of others. In Mozi's "Will of Heaven" (天志), he writes:

Mozi criticized the Confucians of his own time for not following the teachings of Confucius. By the time of the later Han Dynasty, however, under the influence of Xunzi, the Chinese concept of heaven and Confucianism itself had become mostly naturalistic, though some Confucians argued that heaven was where ancestors reside. Worship of heaven in China continued with the erection of shrines, the last and greatest being the Temple of Heaven in Beijing, and the offering of prayers. The ruler of China in every Chinese dynasty would perform annual sacrificial rituals to heaven, usually by slaughtering two healthy bulls as a sacrifice.

In Buddhism there are several heavens, all of which are still part of "samsara" (illusionary reality). Those who accumulate good karma may be reborn in one of them. However, their stay in heaven is not eternal—eventually they will use up their good karma and will undergo rebirth into another realm, as a human, animal or other being. Because heaven is temporary and part of "samsara", Buddhists focus more on escaping the cycle of rebirth and reaching enlightenment ("nirvana"). Nirvana is not a heaven but a mental state.

According to Buddhist cosmology the universe is impermanent and beings transmigrate through a number of existential "planes" in which this human world is only one "realm" or "path". These are traditionally envisioned as a vertical continuum with the heavens existing above the human realm, and the realms of the animals, hungry ghosts and hell beings existing beneath it. According to Jan Chozen Bays in her book, "Jizo: Guardian of Children, Travelers, and Other Voyagers", the realm of the "asura" is a later refinement of the heavenly realm and was inserted between the human realm and the heavens. One important Buddhist heaven is the "Trāyastriṃśa", which resembles Olympus of Greek mythology.

In the Mahayana world view, there are also pure lands which lie outside this continuum and are created by the Buddhas upon attaining enlightenment. Rebirth in the pure land of Amitabha is seen as an assurance of Buddhahood, for once reborn there, beings do not fall back into cyclical existence unless they choose to do so to save other beings, the goal of Buddhism being the obtainment of enlightenment and freeing oneself and others from the birth–death cycle.

The Tibetan word "Bardo" means literally "intermediate state". In Sanskrit the concept has the name "antarabhāva".

Brahmāloka

Here the denizens are Brahmās, and the ruler is Mahābrahmā

After developing the four Brahmavihāras, King Makhādeva rebirths here after death. The monk Tissa and Brāhmana Jānussoni were also reborn here.

For a monk, the next best thing to Nirvana is to be reborn in this Brahmāloka.

The lifespan of a Brahmās is not stated but is not eternal.

Parinirmita-vaśavartin or Paranimmita-vasavatti

The heaven of devas "with power over (others') creations". These devas do not create pleasing forms that they desire for themselves, but their desires are fulfilled by the acts of other devas who wish for their favor. The ruler of this world is called Vaśavartin (Pāli: Vasavatti), who has longer life, greater beauty, more power and happiness and more delightful sense-objects than the other devas of his world. This world is also the home of the devaputra (being of divine race) called Māra, who endeavors to keep all beings of the Kāmadhātu in the grip of sensual pleasures. Māra is also sometimes called Vaśavartin, but in general these two dwellers in this world are kept distinct. The beings of this world are tall and live for 9,216,000,000 years (Sarvāstivāda tradition).

Nimmānarati

The world of devas "delighting in their creations". The devas of this world are capable of making any appearance to please themselves. The lord of this world is called Sunirmita (Pāli Sunimmita); his wife is the rebirth of Visākhā, formerly the chief upāsikā (female lay devotee) of the Buddha. The beings of this world are tall and live for 2,304,000,000 years (Sarvāstivāda tradition).

The world of the "joyful" devas. This world is best known for being the world in which a Bodhisattva lives before being reborn in the world of humans. Until a few thousand years ago, the Bodhisattva of this world was Śvetaketu (Pāli: Setaketu), who was reborn as Siddhārtha, who would become the Buddha Śākyamuni; since then the Bodhisattva has been Nātha (or Nāthadeva) who will be reborn as Ajita and will become the Buddha Maitreya (Pāli Metteyya). While this Bodhisattva is the foremost of the dwellers in , the ruler of this world is another deva called (Pāli: Santusita). The beings of this world are tall and live for 576,000,000 years (Sarvāstivāda tradition). Anāthapindika, a Kosālan householder and benefactor to the Buddha's order was reborn here.

Yāma

The denizens here have a lifespan of 144,000,000 years.

Tāvatimsa

The ruler of this heaven is Indra or Shakra, and the realm is also called Trayatrimia.

Each denizen addresses other denizens as the title "mārisa".

The governing hall of this heaven is called Sudhamma Hall.

This heaven has a garden Nandanavana with damsels, as its most magnificent sight.

Ajita the Licchavi army general was reborn here. Gopika the Sākyan girl was reborn as a male god in this realm.

Any Buddhist reborn in this realm can outshine any of the previously dwelling denizens because of the extra merit acquired for following the Buddha's teachings.

The denizens here have a lifespan of 36,000,000 years.

Cātummahārājika

The heaven "of the Four Great Kings". Its rulers are the four Great Kings of the name, , , , and their leader . The devas who guide the Sun and Moon are also considered part of this world, as are the retinues of the four kings, composed of (dwarfs), Gandharva गन्धर्वs (fairies), Nāgas (snakes) and (goblins). The beings of this world are tall and live for 9,000,000 years (Sarvāstivāda tradition) or 90,000 years (Vibhajyavāda tradition).

There are 5 major types of heavens.

The Six Desire Heaven
The cause for birth in the Six Desire Heavens are the 10 good conducts.

1. The Heaven of the Four Kings 
Those with no interest in deviant sexual activity and so develop a purity and produce light. When their life ends, they draw near the sun and moon and are among those born in the Heaven of the Four Kings.
Master Ou Yi Zhixu explains that the Shurangama sutra only emphasized on avoiding deviant sexual desire, but one would naturally also need to avoid killing and abide by the 10 good conducts to be born in this heaven.

2. The Trayastrimsha Heaven
Those whose sexual love for their wives is slight, but who have not yet obtained the entire flavor of dwelling in purity, transcend the light of sun and moon at the end of their lives, and reside at the summit of the human realm. They are among those born in the Trayastrimsha Heaven

3. The Suyama Heaven
”Those who become temporarily involved when they meet with desire but who forget about it when it is finished, and who, while in the human realm, are active less and quiet more, abide at the end of their lives in light and emptiness where the illumination of sun and moon does not reach. These beings have their own light, and they are among those born in the Suyama Heaven.

4. The Tushita Heaven
Those who are quiet all the time, but who are not yet able to resist when stimulated by contact, ascend at the end of their lives to a subtle and ethereal place; they will not be drawn into the lower realms. The destruction of the realms of humans and gods and the obliteration of kalpas by the three disasters will not reach them, for they are among those born in the Tushita Heaven.

5. The Heaven of Bliss by Transformation
Those who are devoid of desire, but who will engage in it for the sake of their partner, even though the flavor of doing so is like the flavor of chewing wax, are born at the end of their lives in a place of transcending transformations. They are among those born in the Heaven of Bliss by Transformation.

6. The Heaven of the Comfort from Others’ Transformations
Those who have no kind of worldly thoughts while doing what worldly people do, who are lucid and beyond such activity while involved in it, are capable at the end of their lives of entirely transcending states where transformations may be present and may be lacking. They are among those born in the Heaven of the Comfort from Others’ Transformations.

The Form Realm
The First Dhyana, the Second Dhyana, the Third Dhyana and the Fourth Dhyana.

The First Dhyana

Those who flow to these levels will not be oppressed by any suffering or affliction. Although they have not developed proper samadhi, their minds are pure to the point that they are not moved by outflows.

1. The Heaven of the Multitudes of Brahma

Those in the world who cultivate their minds but do not avail themselves of dhyana and so have no wisdom, can only control their bodies so as to not engage in sexual desire. Whether walking or sitting, or in their thoughts, they are totally devoid of it. Since they do not give rise to defiling love, they do not remain in the realm of desire. These people can, in response to their thought, take on the bodies of Brahma beings. They are among those in the Heaven of the Multitudes of Brahma.

2. The Heaven of the Ministers of Brahma

Those whose hearts of desire have already been cast aside, the mind apart from desire manifests. They have a fond regard for the rules of discipline and delight in being in accord with them. These people can practice the Brahma virtue at all times, and they are among those in the Heaven of the Ministers of Brahma.

3. The Great Brahma Heaven

Those whose bodies and minds are wonderfully perfect, and whose awesome deportment is not in the least deficient, are pure in the prohibitive precepts and have a thorough understanding of them as well. At all times these people can govern the Brahma multitudes as great Brahma lords, and they are among those in the Great Brahma Heaven.

The Second Dhyana

Those who flow to these levels will not be oppressed by worries or vexations. Although they have not developed proper samadhi, their minds are pure to the point that they have subdued their coarser outflows

1. The Heaven of Lesser Light

Those beyond the Brahma heavens gather in and govern the Brahma beings, for their Brahma conduct is perfect and fulfilled. Unmoving and with settled minds, they produce light in profound stillness, and they are among those in the Heaven of Lesser Light.

2. The Heaven of Limitless Light

Those whose lights illumine each other in an endless dazzling blaze shine throughout the realms of the ten directions so that everything becomes like crystal. They are among those in the Heaven of Limitless Light.

3. The Light-Sound Heaven

Those who take in and hold the light to perfection accomplish the substance of the teaching. Creating and transforming the purity into endless responses and functions, they are among those in the Light-Sound Heaven.

The Third Dhyana

1. The Heaven of Lesser Purity

The heavenly beings for whom the perfection of light has become sound and who further open out the sound to disclose its wonder discover a subtler level of practice. They penetrate to the bliss of still extinction and are among those in the Heaven of Lesser Purity.

2. The Heaven of Limitless Purity

Those in whom the emptiness of purity manifests are led to discover its boundlessness. Their bodies and minds experience light ease, and they accomplish the bliss of still extinction. They are among those in the Heaven of Limitless Purity.

3. The Heaven of Pervasive Purity

Those for whom the world, the body, and the mind are all perfectly pure have accomplished the virtue of purity, and a superior level emerges. They return to the bliss of still extinction, and they are among those in the Heaven of Pervasive Purity.

Attaining heaven is not the final pursuit in Hinduism as heaven itself is ephemeral and related to physical body. Only being tied by the bhoot-tatvas, heaven cannot be perfect either and is just another name for pleasurable and mundane material life. According to Hindu cosmology, above the earthly plane, are other planes: (1) Bhuva Loka, (2) Swarga Loka, meaning Good Kingdom, is the general name for heaven in Hinduism, a heavenly paradise of pleasure, where most of the Hindu Devatas (Deva) reside along with the king of Devas, Indra, and beatified mortals. Some other planes are Mahar Loka, Jana Loka, Tapa Loka and Satya Loka. Since heavenly abodes are also tied to the cycle of birth and death, any dweller of heaven or hell will again be recycled to a different plane and in a different form per the karma and "maya" i.e. the illusion of Samsara. This cycle is broken only by self-realization by the Jivatma. This self-realization is Moksha (Turiya, Kaivalya).

The concept of moksha is unique to Hinduism and is unparalleled. Moksha stands for liberation from the cycle of birth and death and final communion with Brahman. With moksha, a liberated soul attains the stature and oneness with Brahman or Paramatma. Different schools such as Vedanta, Mimansa, Sankhya, Nyaya, Vaisheshika, and Yoga offer subtle differences in the concept of Brahman, obvious Universe, its genesis and regular destruction, Jivatma, Nature (Prakriti) and also the right way in attaining perfect bliss or moksha.

In the Vaishnava traditions the highest heaven is Vaikuntha, which exists above the six heavenly lokas and outside of the mahat-tattva or mundane world. It's where eternally liberated souls who have attained moksha reside in eternal sublime beauty with Lakshmi and Narayana (a manifestation of Vishnu).

In the Nasadiya Sukta, the heavens/sky Vyoman is mentioned as a place from which an overseeing entity surveys what has been created. However, the Nasadiya Sukta questions the omniscience of this overseer.

The shape of the Universe as described in Jainism is shown alongside. Unlike the current convention of using North direction as the top of map, this uses South as the top. The shape is similar to a part of human form standing upright.

The "Deva Loka" (heavens) are at the symbolic "chest", where all souls enjoying the positive karmic effects reside. The heavenly beings are referred to as "devas" (masculine form) and "devis" (feminine form). According to Jainism, there is not one heavenly abode, but several layers to reward appropriately the souls of varying degree of karmic merits. Similarly, beneath the "waist" are the "Narka Loka" (hell). Human, animal, insect, plant and microscopic life forms reside on the middle.

The pure souls (who reached Siddha status) reside at the very south end (top) of the Universe. They are referred to in Tamil literature as தென்புலத்தார் (Kural 43).

As per Sikh thought, heaven and hell are not places for living hereafter, they are part of spiritual topography of man and do not exist otherwise. They refer to good and evil stages of life respectively and can be lived now and here during our earthly existence. For example, Bhagat Kabir rejects the otherworldly heaven in Guru Granth Sahib and says that one can experience heaven on this Earth by doing company of holy people.

The Nahua people such as the Aztecs, Chichimecs and the Toltecs believed that the heavens were constructed and separated into 13 levels. Each level had from one to many Lords living in and ruling these heavens. Most important of these heavens was Omeyocan (Place of Two). The Thirteen Heavens were ruled by Ometeotl, the dual Lord, creator of the Dual-Genesis who, as male, takes the name Ometecuhtli (Two Lord), and as female is named Omecihuatl (Two Lady).

In the creation myths of Polynesian mythology are found various concepts of the heavens and the underworld. These differ from one island to another. What they share is the view of the universe as an egg or coconut that is divided between the world of humans (earth), the upper world of heavenly gods, and the underworld. Each of these is subdivided in a manner reminiscent of Dante's Divine Comedy, but the number of divisions and their names differs from one Polynesian culture to another.

In Māori mythology, the heavens are divided into a number of realms. Different tribes number the heaven differently, with as few as two and as many as fourteen levels. One of the more common versions divides heaven thus:


The Māori believe these heavens are supported by pillars. Other Polynesian peoples see them being supported by gods (as in Hawaii). In one Tahitian legend, heaven is supported by an octopus.

The Polynesian conception of the universe and its division is nicely illustrated by a famous drawing made by a Tuomotuan chief in 1869. Here, the nine heavens are further divided into left and right, and each stage is associated with a stage in the evolution of the earth that is portrayed below. The lowest division represents a period when the heavens hung low over the earth, which was inhabited by animals that were not known to the islanders. In the third division is shown the first murder, the first burials, and the first canoes, built by Rata. In the fourth division, the first coconut tree and other significant plants are born.

It is believed in Theosophy of Helena Blavatsky that each religion (including Theosophy) has its own individual heaven in various regions of the upper astral plane that fits the description of that heaven that is given in each religion, which a soul that has been good in their previous life on Earth will go to. The area of the upper astral plane of Earth in the upper atmosphere where the various heavens are located is called "Summerland" (Theosophists believe hell is located in the lower astral plane of Earth which extends downward from the surface of the earth down to its center). However, Theosophists believe that the soul is recalled back to Earth after an average of about 1400 years by the "Lords of Karma" to incarnate again. The final heaven that souls go to billions of years in the future after they finish their cycle of incarnations is called "Devachan".

Anarchist Emma Goldman expressed this view when she wrote, "Consciously or unconsciously, most theists see in gods and devils, heaven and hell; reward and punishment, a whip to lash the people into obedience, meekness and contentment."

Many people consider George Orwell's use of Sugarcandy Mountain in his novel "Animal Farm" to be a literary expression of this view. In the book, the animals were told that after their miserable lives were over they would go to a place in which "it was Sunday seven days a week, clover was in season all the year round, and lump sugar and linseed cake grew on the hedges".

Some have argued that a belief in a reward after death is poor motivation for moral behavior while alive. Sam Harris wrote, "It is rather more noble to help people purely out of concern for their suffering than it is to help them because you think the Creator of the Universe wants you to do it, or will reward you for doing it, or will punish you for not doing it. The problem with this linkage between religion and morality is that it gives people bad reasons to help other human beings when good reasons are available."

In Inside the Neolithic Mind, Lewis-Williams and Pearce argue that a tiered structure of heaven, along with similarly structured circles of hell, is neurally perceived by members of many cultures around the world and through history. The reports are so similar across time and space that Lewis-Williams and Pearce argue for a neuroscientific explanation, accepting the percepts as real neural activations and subjective percepts during particular altered states of consciousness.

Many people who come close to death and have near death experiences report meeting relatives or entering "the Light" in an otherworldly dimension, which share similarities with the religious concept of heaven. Even though there are also reports of distressing experiences and negative life-reviews, which share some similarities with the concept of hell, the positive experiences of meeting or entering "the Light" is reported as an immensely intense feeling state of love, peace and joy beyond human comprehension. Together with this intensely positive-feeling state, people who have near death experiences also report that consciousness or a heightened state of awareness seems as if it is at the heart of experiencing a taste of "heaven".

Works of fiction have included numerous different conceptions of heaven and hell. The two most famous descriptions of heaven are given in Dante Alighieri's "Paradiso" (of the "Divine Comedy") and John Milton's "Paradise Lost".





</doc>
<doc id="13812" url="https://en.wikipedia.org/wiki?curid=13812" title="History of Libya">
History of Libya

Libya's history covers its rich mix of ethnic groups added to the indigenous Berber tribes. Berbers have been present throughout the entire history of the country. For most of its history, Libya has been subjected to varying degrees of foreign control, from Europe, Asia, and Africa. The modern history of independent Libya began in 1951.

The history of Libya comprises six distinct periods: Ancient Libya, the Roman era, the Islamic era, Ottoman rule, Italian rule, and the Modern era.

Tens of thousands of years ago, the Sahara Desert, which now covers roughly 90% of Libya, was lush with green vegetation. It was home to lakes, forests, diverse wildlife and a temperate Mediterranean climate. Archaeological evidence indicates that the coastal plain was inhabited by Neolithic peoples from as early as 8000 BCE. These peoples were perhaps drawn by the climate, which enabled their culture to grow, subsisting on the domestication of cattle and the cultivation of crops.

Rock paintings at Wadi Mathendous and the mountainous region of Jebel Acacus are the best sources of information about prehistoric Libya, and the pastoralist culture that settled there. The paintings reveal that the Libyan Sahara contained rivers, grassy plateaus and an abundance of wildlife such as giraffes, elephants and crocodiles.

The onset of the Piora Oscillation's intense aridification resulted in the "green Sahara" rapidly transforming into the Sahara Desert. Dispersal in Africa from the Atlantic coast to the Siwa Oasis in Egypt seems to have followed, due to climatic changes which caused increasing desertification.

The Afro-Asiatic ancestors of the Berber people are assumed to have spread into the area by the Late Bronze Age. The earliest known name of such a tribe is that of the Garamantes, who were based in Germa. The Garamantes were a Saharan people of Berber origin who used an elaborate underground irrigation system; they were probably present as tribal people in the Fezzan by about 1000 BCE, and were a local power in the Sahara between 500 BCE and 500 CE. By the time of contact with the Phoenicians, the first of the Semitic civilizations to arrive in Libya from the East, the Lebu, Garamantes, Berbers and other tribes that lived in the Sahara were already well established.

The Phoenicians were the first to establish trading posts in Libya, when the merchants of Tyre (in present-day Lebanon) developed commercial relations with the Berber tribes and made treaties with them to ensure their cooperation in the exploitation of raw materials. By the 5th century BCE, the greatest of the Phoenician colonies, Carthage, had extended its hegemony across much of North Africa, where a distinctive civilization, known as Punic, came into being. Punic settlements on the Libyan coast included Oea (later Tripoli), Libdah (later Leptis Magna) and Sabratha. These cities were in an area that was later called Tripolis, or "Three Cities", from which Libya's modern capital Tripoli takes its name.

In 630 BCE, the Ancient Greeks colonized Eastern Libya and founded the city of Cyrene. Within 200 years, four more important Greek cities were established in the area that became known as Cyrenaica: Barce (later Marj); Euhesperides (later Berenice, present-day Benghazi); Taucheira (later Arsinoe, present-day Taucheria); Balagrae (later Bayda and Beda Littoria under Italian occupation, present-day Bayda); and Apollonia (later Susa), the port of Cyrene. Together with Cyrene, they were known as the Pentapolis (Five Cities). Cyrene became one of the greatest intellectual and artistic centers of the Greek world, and was famous for its medical school, learned academies, and architecture. The Greeks of the Pentapolis resisted encroachments by the Ancient Egyptians from the East, as well as by the Carthaginians from the West.

In 525 BCE the Persian army of Cambyses II overran Cyrenaica, which for the next two centuries remained under Persian or Egyptian rule. Alexander was greeted by the Greeks when he entered Cyrenaica in 331 BCE, and Eastern Libya again fell under the control of the Greeks, this time as part of the Ptolemaic Kingdom. Later, a federation of the Pentapolis was formed that was customarily ruled by a king drawn from the Ptolemaic royal house.

After the fall of Carthage the Romans did not occupy immediately Tripolitania (the region around Tripoli), but left it under control of the kings of Numidia, until the coastal cities asked and obtained its protection. Ptolemy Apion, the last Greek ruler, bequeathed Cyrenaica to Rome, which formally annexed the region in 74 BCE and joined it to Crete as a Roman province. During the Roman civil wars Tripolitania (still not formally annexed) and Cyrenaica sustained Pompey and Marc Antony against respectively Caesar and Octavian. The Romans completed the conquest of the region under Augustus, occupying northern Fezzan ("Fasania") with Cornelius Balbus Minor. As part of the Africa Nova province, Tripolitania was prosperous, and reached a golden age in the 2nd and 3rd centuries, when the city of Leptis Magna, home to the Severan dynasty, was at its height. On the other side, Cyrenaica's first Christian communities were established by the time of the Emperor Claudius but was heavily devastated during the Kitos War and almost depopulated of Greeks and Jews alike, and, although repopulated by Trajan with military colonies, from then started its decadence.
Regardless, for more than 400 years Tripolitania and Cyrenaica were part of a cosmopolitan state whose citizens shared a common language, legal system, and Roman identity. Roman ruins like those of Leptis Magna and Sabratha, extant in present-day Libya, attest to the vitality of the region, where populous cities and even smaller towns enjoyed the amenities of urban life—the forum, markets, public entertainments, and baths—found in every corner of the Roman Empire. Merchants and artisans from many parts of the Roman world established themselves in North Africa, but the character of the cities of Tripolitania remained decidedly Punic and, in Cyrenaica, Greek. Tripolitania was a major exporter of olive oil, as well as a center for the trade of ivory and wild animals conveyed to the coast by the Garamantes, while Cyrenaica remained an important source of wines, drugs, and horses. The bulk of the population in the countryside consisted of Berber farmers, who in the west were thoroughly "romanized" in language and customs. Until the 10th century the African Romance remained in use in some Tripolitanian areas, mainly near the Tunisian border.

The decline of the Roman Empire saw the classical cities fall into ruin, a process hastened by the Vandals' destructive sweep though North Africa in the 5th century. The region's prosperity had shrunk under Vandal domination, and the old Roman political and social order, disrupted by the Vandals, could not be restored. In outlying areas neglected by the Vandals, the inhabitants had sought the protection of tribal chieftains and, having grown accustomed to their autonomy, resisted re-assimilation into the imperial system.

When the Empire returned (now as East Romans) as part of Justinian's reconquests of the 6th century, efforts were made to strengthen the old cities, but it was only a last gasp before they collapsed into disuse. Cyrenaica, which had remained an outpost of the Byzantine Empire during the Vandal period, also took on the characteristics of an armed camp. Unpopular Byzantine governors imposed burdensome taxation to meet military costs, while the towns and public services—including the water system—were left to decay. Byzantine rule in Africa did prolong the Roman ideal of imperial unity there for another century and a half however, and prevented the ascendancy of the Berber nomads in the coastal region. By the beginning of the 7th century, Byzantine control over the region was weak, Berber rebellions were becoming more frequent, and there was little to oppose Muslim invasion.

Tenuous Byzantine control over Libya was restricted to a few poorly defended coastal strongholds, and as such, the Arab horsemen who first crossed into the Pentapolis of Cyrenaica in September 643 CE encountered little resistance. Under the command of 'Amr ibn al-'As, the armies of Islam conquered Cyrenaica, and renamed the Pentapolis, Barqa. They took also Tripoli, but after destroying the Roman walls of the city and getting a tribute they withdrew. In 647 an army of 40,000 Arabs, led by Abdullah ibn Saad, the foster-brother of Caliph Uthman, penetrated deep into Western Libya and took Tripoli from the Byzantines definitively. From Barqa, the Fezzan (Libya's Southern region) was conquered by Uqba ibn Nafi in 663 and Berber resistance was overcome. During the following centuries Libya came under the rule of several Islamic dynasties, under various levels of autonomy from Ummayad, Abbasid and Fatimid caliphates of the time. Arab rule was easily imposed in the coastal farming areas and on the towns, which prospered again under Arab patronage. Townsmen valued the security that permitted them to practice their commerce and trade in peace, while the Punicized farmers recognized their affinity with the Semitic Arabs to whom they looked to protect their lands. In Cyrenaica, Monophysite adherents of the Coptic Church had welcomed the Muslim Arabs as liberators from Byzantine oppression. The Berber tribes of the hinterland accepted Islam, however they resisted Arab political rule.

For the next several decades, Libya was under the purview of the Ummayad Caliph of Damascus until the Abbasids overthrew the Ummayads in 750, and Libya came under the rule of Baghdad. When Caliph Harun al-Rashid appointed Ibrahim ibn al-Aghlab as his governor of Ifriqiya in 800, Libya enjoyed considerable local autonomy under the Aghlabid dynasty. The Aghlabids were among the most attentive Islamic rulers of Libya; they brought about a measure of order to the region, and restored Roman irrigation systems, which brought prosperity to the area from the agricultural surplus. By the end of the 9th century, the Shiite Fatimids controlled Western Libya from their capital in Mahdia, before they ruled the entire region from their new capital of Cairo in 972 and appointed Bologhine ibn Ziri as governor. During Fatimid rule, Tripoli thrived on the trade in slaves and gold brought from the Sudan and on the sale of wool, leather, and salt shipped from its docks to Italy in exchange for wood and iron goods. Ibn Ziri's Berber Zirid dynasty ultimately broke away from the Shiite Fatimids, and recognised the Sunni Abbasids of Baghdad as rightful Caliphs. In retaliation, the Fatimids brought about the migration of thousands from two troublesome Arab Bedouin tribes, the Banu Sulaym and Banu Hilal to North Africa. This act drastically altered the fabric of the Libyan countryside, and cemented the cultural and linguistic Arabisation of the region. Ibn Khaldun noted that the lands ravaged by Banu Hilal invaders had become completely arid desert.
Zirid rule in Tripolitania was short-lived though, and already in 1001 the Berbers of the Banu Khazrun broke away. Tripolitania remained under their control until 1146, when the region was overtaken by the Normans of Sicily. It was not until 1159 that the Moroccan Almohad leader Abd al-Mu'min reconquered Tripoli from European rule. For the next 50 years, Tripolitania was the scene of numerous battles between the Almohad rulers and insurgents of the Banu Ghaniya. Later, a general of the Almohads, Muhammad ibn Abu Hafs, ruled Libya from 1207 to 1221 before the later establishment of a Tunisian Hafsid dynasty independent from the Almohads. The Hafsids ruled Tripolitania for nearly 300 years, and established significant trade with the city-states of Europe. Hafsid rulers also encouraged art, literature, architecture and scholarship. Ahmad Zarruq was one of the most famous Islamic scholars to settle in Libya, and did so during this time. By the 16th century however, the Hafsids became increasingly caught up in the power struggle between Spain and the Ottoman Empire. After a successful invasion of Tripoli by Habsburg Spain in 1510, and its handover to the Knights of St. John, the Ottoman admiral Sinan Pasha finally took control of Libya in 1551.

After a successful invasion by the in the early 16th century, Charles V entrusted its defense to the Knights of St. John in Malta. Lured by the piracy that spread through the Maghreb coastline, adventurers such as Barbarossa and his successors consolidated Ottoman control in the central Maghreb. The Ottoman Turks conquered Tripoli in 1551 under the command of Sinan Pasha. In the next year his successor Turgut Reis was named the Bey of Tripoli and later Pasha of Tripoli in 1556. As Pasha, he adorned and built up Tripoli, making it one of the most impressive cities along the North African coast. By 1565, administrative authority as regent in Tripoli was vested in a "pasha" appointed directly by the "sultan" in Constantinople. In the 1580s, the rulers of Fezzan gave their allegiance to the sultan, and although Ottoman authority was absent in Cyrenaica, a "bey" was stationed in Benghazi late in the next century to act as agent of the government in Tripoli.

In time, real power came to rest with the pasha’s corps of janissaries, a self-governing military guild, and in time the pasha’s role was reduced to that of ceremonial head of state. Mutinies and coups were frequent, and in 1611 the "deys" staged a coup against the pasha, and Dey Sulayman Safar was appointed as head of government. For the next hundred years, a series of "deys" effectively ruled Tripolitania, some for only a few weeks, and at various times the dey was also pasha-regent. The regency governed by the dey was autonomous in internal affairs and, although dependent on the sultan for fresh recruits to the corps of janissaries, his government was left to pursue a virtually independent foreign policy as well. The two most important Deys were Mehmed Saqizli (r. 1631–49) and Osman Saqizli (r. 1649–72), both also Pasha, who ruled effectively the region. The latter conquered also Cyrenaica.
Tripoli was the only city of size in Ottoman Libya (then known as Tripolitania Eyalet) at the end of the 17th century and had a population of about 30,000. The bulk of its residents were Moors, as city-dwelling Arabs were then known. Several hundred Turks and renegades formed a governing elite, a large portion of which were "kouloughlis" (lit. sons of servants—offspring of Turkish soldiers and Arab women); they identified with local interests and were respected by locals. Jews and Moriscos were active as merchants and craftsmen and a small number of European traders also frequented the city. European slaves and large numbers of enslaved blacks transported from Sudan were also a feature of everyday life in Tripoli. In 1551, Turgut Reis enslaved almost the entire population of the Maltese island of Gozo, some 6,300 people, sending them to Libya. The most pronounced slavery activity involved the enslavement of black Africans who were brought via trans-Saharan trade routes. Even though the slave trade was officially abolished in Tripoli in 1853, in practice it continued until the 1890s.
Lacking direction from the Ottoman government, Tripoli lapsed into a period of military anarchy during which coup followed coup and few deys survived in office more than a year. One such coup was led by Turkish officer Ahmed Karamanli. The Karamanlis ruled from 1711 until 1835 mainly in Tripolitania, but had influence in Cyrenaica and Fezzan as well by the mid 18th century. Ahmed was a Janissary and popular cavalry officer. He murdered the Ottoman Dey of Tripolitania and seized the throne in 1711. After persuading Sultan Ahmed III to recognize him as governor, Ahmed established himself as pasha and made his post hereditary. Though Tripolitania continued to pay nominal tribute to the Ottoman padishah, it otherwise acted as an independent kingdom. Ahmed greatly expanded his city's economy, particularly through the employment of corsairs (pirates) on crucial Mediterranean shipping routes; nations that wished to protect their ships from the corsairs were forced to pay tribute to the pasha. Ahmad's successors proved to be less capable than himself, however, the region's delicate balance of power allowed the Karamanli to survive several dynastic crises without invasion. The Libyan Civil War of 1791–1795 occurred in those years. In 1793, Turkish officer Ali Benghul deposed Hamet Karamanli and briefly restored Tripolitania to Ottoman rule. However, Hamet's brother Yusuf (r. 1795–1832) reestablished Tripolitania's independence.

In the early 19th century war broke out between the United States and Tripolitania, and a series of battles ensued in what came to be known as the First Barbary War and the Second Barbary War. By 1819, the various treaties of the Napoleonic Wars had forced the Barbary states to give up piracy almost entirely, and Tripolitania's economy began to crumble. As Yusuf weakened, factions sprung up around his three sons; though Yusuf abdicated in 1832 in favor of his son Ali II, civil war soon resulted. Ottoman Sultan Mahmud II sent in troops ostensibly to restore order, but instead deposed and exiled Ali II, marking the end of both the Karamanli dynasty and an independent Tripolitania. Anyway, order was not recovered easily, and the revolt of the Libyan under Abd-El-Gelil and Gûma ben Khalifa lasted until the death of the latter in 1858.

The second period of direct Ottoman rule saw administrative changes, and what seemed as greater order in the governance of the three provinces of Libya. It would not be long before the Scramble for Africa and European colonial interests set their eyes on the marginal Turkish provinces of Libya. Reunification came about through the unlikely route of an invasion (Italo-Turkish War, 1911–1912) and occupation starting from 1911 when Italy simultaneously turned the three regions into colonies.

From 1912 to 1927, the territory of Libya was known as Italian North Africa. From 1927 to 1934, the territory was split into two colonies, Italian Cyrenaica and Italian Tripolitania, run by Italian governors. Some 150,000 Italians settled in Libya, constituting roughly 20% of the total population.
In 1934, Italy adopted the name "Libya" (used by the Greeks for all of North Africa, except Egypt) as the official name of the colony (made up of the three provinces of Cyrenaica, Tripolitania and Fezzan). Idris al-Mahdi as-Senussi (later King Idris I), Emir of Cyrenaica, led Libyan resistance to Italian occupation between the two world wars.
Ilan Pappé estimates that between 1928 and 1932 the Italian military "killed half the Bedouin population (directly or through disease and starvation in camps)." Italian historian Emilio Gentile sets to about 50,000 the number of victims of the repression.

In 1934 was created by governor Balbo the political entity called "Libya", with capital Tripoli. The Italians emphasized infrastructure improvements and public works. In particular, they hugely expanded Libyan railway and road networks from 1934 to 1940, building hundreds of kilometers of new roads and railways and encouraging the establishment of new industries and dozen of new agricultural villages.

During WW2, since June 1940 Libya was at the center of destructive fighting between the Axis and the British empire: the Allies conquered from Italy all Libya only by February 1943.

From 1943 to 1951, Tripolitania and Cyrenaica were under British administration, while the French controlled Fezzan. In 1944, Idris returned from exile in Cairo but declined to resume permanent residence in Cyrenaica until the removal of some aspects of foreign control in 1947. Under the terms of the 1947 peace treaty with the Allies, Italy relinquished all claims to Libya.

On 21 November 1949, the UN General Assembly passed a resolution stating that Libya should become independent before 1 January 1952. Idris represented Libya in the subsequent UN negotiations. On 24 December 1951, Libya declared its independence as the United Kingdom of Libya, a constitutional and hereditary monarchy under King Idris, Libya's only monarch.

1951 also saw the enactment of the first Libyan Constitution. The Libyan National Assembly drafted the Constitution and passed a resolution accepting it in a meeting held in the city of Benghazi on Sunday, 6th Muharram, Hegiras 1371: 7 October 1951. Mohamed Abulas’ad El-Alem, President of the National Assembly and the two Vice-Presidents of the National Assembly, Omar Faiek Shennib and Abu Baker Ahmed Abu Baker executed and submitted the Constitution to King Idris following which it was published in the Official Gazette of Libya.

The enactment of the Libyan Constitution was significant in that it was the first piece of legislation to formally entrench the rights of Libyan citizens following the post-war creation of the Libyan nation state. Following on from the intense UN debates during which Idris had argued that the creation of a single Libyan state would be of benefit to the regions of Tripolitania, Fezzan, and Cyrenaica, the Libyan government was keen to formulate a constitution which contained many of the entrenched rights common to European and North American nation states. Though not creating a secular state – Article 5 proclaims Islam the religion of the State – the Libyan Constitution did formally set out rights such as equality before the law as well as equal civil and political rights, equal opportunities, and an equal responsibility for public duties and obligations, "without distinction of religion, belief, race, language, wealth, kinship or political or social opinions" (Article 11).

During this period, Britain was involved in extensive engineering projects in Libya and was also the country's biggest supplier of arms. The United States also maintained the large Wheelus Air Base in Libya.

On 1 September 1969, a small group of military officers led by 27-year-old army officer Muammar Gaddafi staged a coup d'état against King Idris, launching the Libyan Revolution. Gaddafi was referred to as the "Brother Leader and Guide of the Revolution" in government statements and the official Libyan press.
On the birthday of Muhammad in 1973, Gaddafi delivered a "Five-Point Address". He announced the suspension of all existing laws and the implementation of Sharia. He said that the country would be purged of the "politically sick". A "people's militia" would "protect the revolution". There would be an administrative revolution, and a cultural revolution. Gaddafi set up an extensive surveillance system. 10 to 20 percent of Libyans worked in surveillance for the Revolutionary committees, which monitored place in government, in factories, and in the education sector. Gaddafi executed dissidents publicly and the executions were often rebroadcast on state television channels. Gaddafi employed his network of diplomats and recruits to assassinate dozens of critical refugees around the world. Amnesty International listed at least 25 assassinations between 1980 and 1987.
In 1977, Libya officially became the "Great Socialist People's Libyan Arab Jamahiriya". Gaddafi officially passed power to the General People's Committees and henceforth claimed to be no more than a symbolic figurehead, but domestic and international critics claimed the reforms gave him virtually unlimited power. Dissidents against the new system were not tolerated, with punitive actions including capital punishment authorized by Gaddafi himself. The new ""jamahiriya"" governance structure he established was officially referred to as a form of direct democracy, though the government refused to publish election results. Later that same year, Libya and Egypt fought a four-day border war that came to be known as the Libyan-Egyptian War, both nations agreed to a ceasefire under the mediation of the Algerian president Houari Boumediène. In February 1977, Libya began to provide military supplies to Goukouni Oueddei and the People's Armed Forces in Chad. The Chadian–Libyan conflict began in earnest when Libya's support of rebel forces in northern Chad escalated into an invasion. Much of the country’s income from oil, which soared in the 1970s, was spent on arms purchases and on sponsoring dozens of rebels groups around the world. An airstrike failed to kill Gaddafi in 1986. Libya was finally put under United Nations sanctions in 1988. Gaddafi financed various other groups from anti-nuclear movements to Australian trade unions.

From 1977 onward, per capita income in the country rose to more than US $11,000, the fifth-highest in Africa, while the Human Development Index became the highest in Africa and greater than that of Saudi Arabia. This was achieved without borrowing any foreign loans, keeping Libya debt-free. In addition, the country's literacy rate rose from 10% to 90%, life expectancy rose from 57 to 77 years, employment opportunities were established for migrant workers, and welfare systems were introduced that allowed access to free education, free healthcare, and financial assistance for housing. The Great Manmade River was also built to allow free access to fresh water across large parts of the country. In addition, financial support was provided for university scholarships and employment programs.

Gaddafi doubled the minimum wage, introduced statutory price controls, and implemented compulsory rent reductions of between 30 and 40%. Gaddafi also wanted to combat the strict social restrictions that had been imposed on women by the previous regime, establishing the Revolutionary Women's Formation to encourage reform. In 1970, a law was introduced affirming equality of the sexes and insisting on wage parity. In 1971, Gaddafi sponsored the creation of a Libyan General Women's Federation. In 1972, a law was passed criminalizing the marriage of any females under the age of sixteen and ensuring that a woman's consent was a necessary prerequisite for a marriage.

Gaddafi assumed the honorific title of "King of Kings of Africa" in 2008 as part of his campaign for a United States of Africa. By the early 2010s, in addition to attempting to assume a leadership role in the African Union, Libya was also viewed as having formed closer ties with Italy, one of its former colonial rulers, than any other country in the European Union. The eastern parts of the country have been "ruined" due to Gaddafi's economic theories, according to "The Economist".

After popular movements overturned the rulers of Tunisia and Egypt, its immediate neighbors to the west and east, Libya experienced a full-scale revolt beginning on 17 February 2011. By 20 February, the unrest had spread to Tripoli. In the early hours of 21 February 2011, Saif al-Islam Gaddafi, oldest son of Muammar Gaddafi, spoke on Libyan television of his fears that the country would fragment and be replaced by "15 Islamic fundamentalist emirates" if the uprising engulfed the entire state. He admitted that "mistakes had been made" in quelling recent protests and announced plans for a constitutional convention, but warned that the country's economic wealth and recent prosperity was at risk and warned of "rivers of blood" if the protests continued.

On 27 February 2011, the National Transitional Council was established under the stewardship of Mustafa Abdul Jalil, Gaddafi's former justice minister, to administer the areas of Libya under rebel control. This marked the first serious effort to organize the broad-based opposition to the Gaddafi regime. While the council was based in Benghazi, it claimed Tripoli as its capital. Hafiz Ghoga, a human rights lawyer, later assumed the role of spokesman for the council. On 10 March 2011, France became the first state to officially recognise the council as the legitimate representative of the Libyan people.

By early March 2011, some parts of Libya had tipped out of Gaddafi's control, coming under the control of a coalition of opposition forces, including soldiers who decided to support the rebels. Eastern Libya, centred on the port city of Benghazi, was said to be firmly in the hands of the opposition, while Tripoli and its environs remained in dispute. Pro-Gaddafi forces were able to respond militarily to rebel pushes in Western Libya and launched a counterattack along the coast toward Benghazi, the "de facto" centre of the uprising. The town of Zawiya, from Tripoli, was bombarded by air force planes and army tanks and seized by Jamahiriya troops, "exercising a level of brutality not yet seen in the conflict."

In several public appearances, Gaddafi threatened to destroy the protest movement, and Al Jazeera and other agencies reported his government was arming pro-Gaddafi militiamen to kill protesters and defectors against the regime in Tripoli. Organs of the United Nations, including United Nations Secretary General Ban Ki-moon and the United Nations Human Rights Council, condemned the crackdown as violating international law, with the latter body expelling Libya outright in an unprecedented action urged by Libya's own delegation to the UN. The United States imposed economic sanctions against Libya, followed shortly by Australia, Canada and the United Nations Security Council, which also voted to refer Gaddafi and other government officials to the International Criminal Court for investigation.

On 17 March 2011 the UN Security Council passed Resolution 1973 with a 10–0 vote and five abstentions. The resolution sanctioned the establishment of a no-fly zone and the use of "all means necessary" to protect civilians within Libya.

Shortly afterwards, Libyan Foreign Minister Moussa Koussa stated that "Libya has decided an immediate ceasefire and an immediate halt to all military operations".

On 19 March, the first Allied act to secure the no-fly zone began when French military jets entered Libyan airspace on a reconnaissance mission heralding attacks on enemy targets. Allied military action to enforce the ceasefire commenced the same day when a French aircraft opened fire and destroyed a vehicle on the ground. French jets also destroyed five tanks belonging to the Gaddafi regime. The United States and United Kingdom launched attacks on over 20 "integrated air defense systems" using more than 110 Tomahawk cruise missiles during operations Odyssey Dawn and Ellamy.

On 27 June 2011, the International Criminal Court issued an arrest warrant for Gaddafi, alleging that Gaddafi had been personally involved in planning and implementing "a policy of widespread and systematic attacks against civilians and demonstrators and dissidents".
By 22 August 2011, rebel fighters had entered Tripoli and occupied Green Square, which they renamed to its original name, Martyrs' Square in honour of those killed during the Italian occupation. Meanwhile, Gaddafi asserted that he was still in Libya and would not concede power to the rebels.

On 16 September 2011, the U.N. General Assembly approved a request from the National Transitional Council to accredit envoys of the country’s interim controlling body as Tripoli’s sole representatives at the UN, effectively recognising the National Transitional Council as the legitimate holder of that country’s UN seat.

The National Transitional Council had been plagued by internal divisions during its tenure as Libya's interim governing authority. It postponed the formation of a caretaker, or "interim" government on several occasions during the period prior to the death of Muammar Gaddafi in his hometown of Sirte on 20 October 2011. Mustafa Abdul Jalil led the National Transitional Council and was generally considered to be the principal leadership figure. Mahmoud Jibril served as the NTC's "de facto" head of government from 5 March 2011 through the end of the war, but he announced he would resign after Libya was declared to have been "liberated" from Gaddafi's rule.

The "liberation" of Libya was celebrated on 23 October 2011, and Jibril announced that consultations were under way to form an interim government within one month, followed by elections for a constitutional assembly within eight months and parliamentary and presidential elections to be held within a year after that. He stepped down as expected the same day and was succeeded by Ali Tarhouni. At least 30,000 Libyans died in the civil war.

After the Libyan Civil War, the National Transitional Council (NTC) has been responsible for the transition of the administration of the governing of Libya. The "liberation" of Libya was celebrated on 23 October 2011. Then Jibril announced that consultations were under way to form an interim government within one month, followed by elections for a constitutional assembly within eight months and parliamentary and presidential elections to be held within a year after that. He stepped down as expected the same day and was succeeded by Ali Tarhouni.

On 24 November, Tarhouni was replaced by Abdurrahim El-Keib. El-Keib formed a provisional government, filling it with independent or CNT politicians, including women.

After the fall of Gaddhafi, Libya has been faced with internal struggles. A protest started against the new regime of NTC. The loyalists of Gaddhafi rebelled and fought with the new Libyan army.

Because the Constitutional Declaration allowed a multi-party system, the political parties, like Democratic Party, Party of Reform and Development, National Gathering for Freedom, Justice and Development appeared. The Islamist movement started. To stop it, the CNT (NTC) government denied power to parties based on religion, tribal and ethnic bases.

On 7 July 2012, Libyans voted in their first parliamentary elections since the end of Gaddafi's rule. The election, in which more than 100 political parties registered, formed an interim 200-member national assembly. This will replace the unelected National Transitional Council, name a prime minister, and form a committee to draft a constitution. The vote was postponed several times to resolve logistical and technical problems, and to give more time to register to vote, and to investigate candidates.

On 8 August 2012, the National Transitional Council officially handed power to the wholly elected General National Congress, which is tasked with the formation of an interim government and the drafting of a new Libyan Constitution to be approved in a general referendum.

On 25 August 2012, in what "appears to be the most blatant sectarian attack" since the end of the civil war, unnamed organized assailants bulldozed a Sufi mosque with graves, in broad daylight in the center of the Libyan capital Tripoli. It was the second such razing of a Sufi site in two days.

On 7 October 2012, Libya's Prime Minister-elect Mustafa A.G. Abushagur stepped down after failing a second time to win parliamentary approval for a new cabinet. On 14 October 2012, the General National Congress elected former GNC member and human rights lawyer Ali Zeidan as prime minister-designate.

Libyan Constitutional Assembly elections took place in Libya on 20 February 2014. 
Ali Zidan was ousted by the parliament committee and fled from Libya on 14 March 2014 after rogue oil tanker Morning Glory left the rebel port of Sidra, Libya with Libyan oil that had been confiscated by the rebels. Ali Zeidan had promised to stop the departure, but failed.

On 30 March 2014 General National Congress voted to replace itself with new House of Representatives.

Abdullah al-Thani served as the prime minister since 11 March 2014 in interim capacity. He resigned on 13 April 2014, after he and his family were victims of a "traitorous attack" but continued to remain prime minister since there was no replacement.Ahmed Maiteeq was elected Prime Minister of Libya in May 2014 but his election as prime minister took place under disputed circumstances, Libyan Supreme Court ruled on 9 June that Maiteeq's appointment was illegal and Maiteeq resigned the same day.

, the parliament building was reported to heve been stormed by troops loyal to General Khalifa Haftar, reportedly including the Zintan Brigade, in what the Libyan government described as an attempted coup.

House of Representatives elections were held in Libya on 25 June 2014.

On 14 July, the United States Support Mission in Libya evacuated its staff after 13 people were killed in clashes in Tripoli and Benghazi. The fighting, between government forces and rival militia groups, also forced Tripoli International Airport to close. A militia, including members of the Libya Revolutionaries Operations Room (LROR), tried to seize control of the airport from the Zintan militia, which has controlled it since Gaddafi was toppled. Both militias are believed to be on the official payroll. In addition Misrata Airport was closed, due to its dependence on Tripoli International Airport for its operations. Government spokesman, Ahmed Lamine, stated that approximately 90% of the planes stationed at Tripoli International Airport were destroyed or made inoperable in the attack, and that the government may make an appeal for international forces to assist in reestablishing security.





</doc>
<doc id="13813" url="https://en.wikipedia.org/wiki?curid=13813" title="History of Afghanistan">
History of Afghanistan

The history of Afghanistan, (' ', ' ') as a state began in 1747 with its establishment by Ahmad Shah Durrani. The written recorded history of the land presently constituting Afghanistan can be traced back to around 500 BCE when the area was under the Achaemenid Empire, although evidence indicates that an advanced degree of urbanized culture has existed in the land since between 3000 and 2000 BCE. The Indus Valley Civilisation stretched up to large parts of Afghanistan in the north. Alexander the Great and his Macedonian army arrived at what is now Afghanistan in 330 BCE after conquering Persia during the Battle of Gaugamela. Since then, many empires have risen from and included Afghanistan, including the Mauryas, Greco-Bactrians, Kushans, Hephthalites, Hindu Shahi, Saffarids, Samanids, Ghaznavids, Ghurids, Khaljis, Timurids, Mughals, Hotakis and Durranis.

Afghanistan (meaning "land of the Afghans") has been a strategically important location throughout history. The land served as "a gateway to India, impinging on the ancient Silk Road, which carried trade from the Mediterranean to China". Sitting on many trade and migration routes, Afghanistan may be called the 'Central Asian roundabout' since routes converge from the Middle East, from the Indus Valley through the passes over the Hindu Kush, from the Far East via the Tarim Basin, and from the adjacent Eurasian Steppe.

The Iranian languages were developed by one branch of these people; the Pashto language spoken today in Afghanistan is one of the Eastern Iranian languages. Elena E. Kuz'mina argues that the tents of Iranian-speaking nomads of Afghanistan developed from the light surface houses of the Eurasian steppe belt in the Bronze Age.

The Arab invasions influenced the culture of Afghanistan, and its pre-Islamic period of Zoroastrian, Macedonian, Buddhist and Hindu past has long vanished.

Mirwais Hotak followed by Ahmad Shah Durrani unified Afghan tribes and founded the last Afghan Empire in the early 18th century CE. Afghanistan is inhabited by many and diverse peoples: the Pashtuns, Tajiks, Hazaras, Uzbeks, Turkmen, Aimak, Baloch and others. The Pashtuns, with 55% form the largest group, second are the Tajiks with 25%.

Excavations of prehistoric sites by Louis Dupree and others at Darra-e Kur in 1966 where 800 stone implements were recovered along with a fragment of Neanderthal right temporal bone, suggest that early humans were living in what is now Afghanistan at least 52,000 years ago. A cave called Kara Kamar contained Upper Paleolithic blades Carbon-14 dated at 34,000 years old. Farming communities in Afghanistan were among the earliest in the world. Archaeologists have found evidence of human habitation in Afghanistan from as far back as 50,000 BC. The artifacts indicate that the indigenous people were small farmers and herdsmen, very probably grouped into tribes, with small local kingdoms rising and falling through the ages. Urbanization may have begun as early as 3000 BCE. Zoroastrianism predominated as the religion in the area; even the modern Afghan solar calendar shows the influence of Zoroastrianism in the names of the months. Other religions such as Buddhism and Hinduism flourished later, leaving a major mark in the region. Gandhara is the name of an ancient kingdom from the Vedic period and its capital city located between the Hindukush and Sulaiman Mountains (mountains of Solomon), although Kandahar in modern times and the ancient Gandhara are not geographically identical.

Early inhabitants, around 3000 BCE were likely to have been connected through culture and trade to neighboring civilizations like Jiroft and Tappeh Sialk and the Indus Valley Civilization. Urban civilization may have begun as early as 3000 BCE and it is possible that the early city of Mundigak (near Kandahar) was a colony of the nearby Indus Valley Civilization. The first known people were Indo-Iranians, but their date of arrival has been estimated widely from as early as about 3000 BCE to 1500 BCE. (For further detail see Indo-Aryan migration.)

The Indus Valley Civilization (IVC) was a Bronze Age civilization (3300-1300 BCE; mature period 2600–1900 BCE) extending from present-day northwest Pakistan to present-day northwest India and present-day northeast Afghanistan. An Indus Valley site has been found on the Oxus River at Shortugai in northern Afghanistan. Apart from Shortughai, Mundigak is another known site. There are several other smaller IVC sites to be found in Afghanistan as well.

The Bactria-Margiana Archaeological Complex became prominent in the southwest region between 2200 and 1700 BCE (approximately). The city of Balkh (Bactra) was founded about this time (c. 2000–1500 BCE). It is possible that the BMAC may have been an Indo-European culture, perhaps the Proto-Indo-Aryans. But the standard model holds the arrival of Indo-Aryans to have been in the Late Harappan which gave rise to the Vedic civilization of the Early Iron Age.

There have been many different opinions about the extent of the Median kingdom. For instance, according to Ernst Herzfeld, it was a powerful empire, which stretched from central Anatolia to Bactria, to around the borders of nowadays India. On the other side, Heleen Sancisi-Weerdenburg insists that there is no real evidence about the very existence of the Median empire and that it was an unstable state formation. Nevertheless, the region of nowadays Afghanistan came under Median rule for a short time.

Afghanistan fell to the Achaemenid Empire after it was conquered by Darius I of Persia. The area was divided into several provinces called satrapies, which were each ruled by a governor, or satrap. These ancient satrapies included: Aria (Herat); Arachosia (Kandahar, Lashkar Gah, and Quetta); Bactriana (Balkh); Sattagydia (Ghazni); and Gandhara (Kabul, Jalalabad, Peshawar).

Alexander the Great arrived in the area of Afghanistan in 330 BCE after defeating Darius III of Persia a year earlier at the Battle of Gaugamela. His army faced very strong resistance in the Afghan tribal areas where he is said to have commented that Afghanistan is "easy to march into, hard to march out of." Although his expedition through Afghanistan was brief, Alexander left behind a Hellenic cultural influence that lasted several centuries. Several great cities were built in the region named "Alexandria," including: Alexandria-of-the-Arians (modern-day Herat); Alexandria-on-the-Tarnak (near Kandahar); Alexandria-ad-Caucasum (near Begram, at Bordj-i-Abdullah); and finally, Alexandria-Eschate (near Kojend), in the north. After Alexander's death, his loosely connected empire was divided. Seleucus, a Macedonian officer during Alexander's campaign, declared himself ruler of his own Seleucid Empire, which also included present-day Afghanistan.

The Greco-Bactrian Kingdom was founded when Diodotus I, the satrap of Bactria (and probably the surrounding provinces) seceded from the Seleucid Empire around 250 BCE. Greco-Bactria continued until c. 130 BCE, when Eucratides' son, King Heliocles I, was defeated and driven out of Bactria by the Yuezhi tribes. It is thought that his dynasty continued to rule in Kabul and Alexandria of the Caucasus until 70 BCE when King Hermaeus was defeated by the Yuezhi.

One of Demetrius' successors, Menander I, brought the Indo-Greek Kingdom to its height between 165–130 BCE, expanding the kingdom in Afghanistan and Pakistan to even larger proportions than Demetrius. After Menander's death, the Indo-Greeks steadily declined and the last Indo-Greek king was defeated in c. 10 CE.

The territory fell to the Mauryan Empire, which was led by Chandragupta Maurya. The Mauryas introduced Hinduism and Buddhism to the region, and were planning to capture more territory of Central Asia until they faced local Greco-Bactrian forces. Seleucus is said to have reached a peace treaty with Chandragupta by giving control of the territory south of the Hindu Kush to the Mauryas upon intermarriage and 500 elephants.

Having consolidated power in the northwest, Chandragupta pushed east towards the Nanda Empire. Afghanistan's significant ancient tangible and intangible Buddhist heritage is recorded through wide-ranging archeological finds, including religious and artistic remnants. Buddhist doctrines are reported to have reached as far as Balkh even during the life of the Buddha (563 BCE to 483 BCE), as recorded by Husang Tsang.

The Indo-Scythians were descended from the Sakas (Scythians) who migrated from southern Siberia to Pakistan and Arachosia from the middle of the 2nd century BCE to the 1st century BCE. They displaced the Indo-Greeks and ruled a kingdom that stretched from Gandhara to Mathura. The power of the Saka rulers started to decline in the 2nd century CE after the Scythians were defeated by the south Indian Emperor Gautamiputra Satakarni of the Satavahana dynasty. Later the Saka kingdom was completely destroyed by Chandragupta II of the Gupta Empire from eastern India in the 4th century.

The Indo-Parthian Kingdom was ruled by the Gondopharid dynasty, named after its eponymous first ruler Gondophares. They ruled parts of present-day Afghanistan, Pakistan, and northwestern India, during or slightly before the 1st century AD. For most of their history, the leading Gondopharid kings held Taxila (in the present Punjab province of Pakistan) as their residence, but during their last few years of existence the capital shifted between Kabul and Peshawar. These kings have traditionally been referred to as Indo-Parthians, as their coinage was often inspired by the Arsacid dynasty, but they probably belonged to a wider groups of Iranic tribes who lived east of Parthia proper, and there is no evidence that all the kings who assumed the title "Gondophares", which means ”Holder of Glory”, were even related. Christian writings claim that the Apostle Saint Thomas – an architect and skilled carpenter – had a long sojourn in the court of king Gondophares, had built a palace for the king at Taxila and had also ordained leaders for the Church before leaving for Indus Valley in a chariot, for sailing out to eventually reach Malabar Coast.

The Kushan Empire expanded out of bactria (Central Asia) into the northwest of the subcontinent under the leadership of their first emperor, Kujula Kadphises, about the middle of the 1st century CE. They came of an Indo-European language speaking Central Asian tribe called the Yuezhi, a branch of which was known as the Kushans. By the time of his grandson, Kanishka the Great, the empire spread to encompass much of Afghanistan, and then the northern parts of the Indian subcontinent at least as far as Saketa and Sarnath near Varanasi (Benares).

Emperor Kanishka was a great patron of Buddhism; however, as Kushans expanded southward, the deities of their later coinage came to reflect its new Hindu majority.

They played an important role in the establishment of Buddhism in India and its spread to Central Asia and China.

Historian Vincent Smith said about Kanishka:

The empire linked the Indian Ocean maritime trade with the commerce of the Silk Road through the Indus valley, encouraging long-distance trade, particularly between China and Rome. The Kushans brought new trends to the budding and blossoming Gandhara Art, which reached its peak during Kushan Rule.

H.G. Rowlinson commented:

By the 3rd century, their empire in India was disintegrating and their last known great emperor was Vasudeva I.

For a period, much of modern-day Afghanistan was part of the Persian Sasanian Empire, since Shapur I extended his authority eastwards into Afghanistan and the previously autonomous Kushans were obliged to accept his suzerainty.

The Kidarites were a nomadic clan, the first of the four Huna people in Afghanistan. They are supposed to have originated in Western China and arrived in Bactria with the great migrations of the second half of the 4th century.

The Alchons are one of the four Huna people that ruled in Afghanistan.

The Hephthalites (or Ephthalites), also known as the White Huns and one of the four Huna people in Afghanistan, were a nomadic confederation in Central Asia during the late antiquity period. The White Huns established themselves in modern-day Afghanistan by the first half of the 5th century. Led by the Hun military leader Toramana, they overran the northern region of Pakistan and North India. Toramana's son Mihirakula, a Saivite Hindu, moved up to near Pataliputra to the east and Gwalior to the central India. Hiuen Tsiang narrates Mihirakula's merciless persecution of Buddhists and destruction of monasteries, though the description is disputed as far as the authenticity is concerned. The Huns were defeated by the Indian kings Yasodharman of Malwa and Narasimhagupta in the 6th century. Some of them were driven out of India and others were assimilated in the Indian society.

The Nezaks are one of the four Huna people that ruled in Afghanistan.

From the Middle Ages to around 1750 the eastern part of Afghanistan was recognized as being a part of India while its western parts parts were included in Khorasan. Two of the four main capitals of Khorasan (Balkh and Herat) are now located in Afghanistan. The countries of Kandahar, Ghazni and Kabul formed the frontier region between Khorasan and Hindustan. This land, inhabited by the Afghan tribes (i.e. ancestors of Pashtuns), was called Afghanistan, which loosely covered a wide area between the Hindu Kush and the Indus River, principally around the Sulaiman Mountains. The earliest record of the name ""Afghan"" (""Abgân"") being mentioned is by Shapur I of the Sassanid Empire during the 3rd century CE which is later recorded in the form of ""Avagānā"" by the Indian astronomer Varāha Mihira in his 6th century CE Brihat-samhita. It was used to refer to a common legendary ancestor known as ""Afghana"", grandson of King Saul of Israel. Hiven Tsiang, a Chinese pilgrim, visiting the Afghanistan area several times between 630 and 644 CE also speaks about them. Ancestors of many of today's Turkic-speaking Afghans settled in the Hindu Kush area and began to assimilate much of the culture and language of the Pashtun tribes already present there. Among these were the Khalaj people which are known today as Ghilzai.

The Kabul Shahi dynasties ruled the Kabul Valley and Gandhara from the decline of the Kushan Empire in the 3rd century to the early 9th century. The Shahis are generally split up into two eras: the Buddhist Shahis and the Hindu Shahis, with the change-over thought to have occurred sometime around 870. The kingdom was known as the Kabul Shahan or Ratbelshahan from 565–670, when the capitals were located in Kapisa and Kabul, and later Udabhandapura, also known as Hund for its new capital.

The Hindu Shahis under Rajput ruler Jayapala, is known for his struggles in defending his kingdom against the Ghaznavids in the modern-day eastern Afghanistan and Pakistan region. Jayapala saw a danger in the consolidation of the Ghaznavids and invaded their capital city of Ghazni both in the reign of Sebuktigin and in that of his son Mahmud, which initiated the Muslim Ghaznavid and Hindu Shahi struggles. Sebuktigin, however, defeated him, and he was forced to pay an indemnity. Jayapala defaulted on the payment and took to the battlefield once more. Jayapala however, lost control of the entire region between the Kabul Valley and Indus River.

Before his struggle began Jaipal had raised a large army of Punjabi Hindus. When Jaipal went to the Punjab region, his army was raised to 100,000 horsemen and an innumerable host of foot soldiers. According to Ferishta: 

However, the army was hopeless in battle against the western forces, particularly against the young Mahmud of Ghazni. In the year 1001, soon after Sultan Mahmud came to power and was occupied with the Qarakhanids north of the Hindu Kush, Jaipal attacked Ghazni once more and upon suffering yet another defeat by the powerful Ghaznavid forces, near present-day Peshawar. After the Battle of Peshawar, he committed suicide because his subjects thought he had brought disaster and disgrace to the Shahi dynasty.

Jayapala was succeeded by his son Anandapala, who along with other succeeding generations of the Shahiya dynasty took part in various campaigns against the advancing Ghaznavids but were unsuccessful. The Hindu rulers eventually exiled themselves to the Kashmir Siwalik Hills.

In 642 CE, Rashidun Arabs had conquered most of West Asia from the Sassanids and Byzantines, and from the western city of Herat they introduced the religion of Islam as they entered new cities. Afghanistan at that period had a number of different independent rulers, depending on the area. Ancestors of Abū Ḥanīfa, including his father, were from the Kabul region.

The early Arab forces did not fully explore Afghanistan due to attacks by the mountain tribes. Much of the eastern parts of the country remained independent, as part of the Hindu Shahi kingdoms of Kabul and Gandhara, which lasted that way until the forces of the Muslim Saffarid dynasty followed by the Ghaznavids conquered them.

The Shahi or Shahiya dynasties ruled portions of the Kabul Valley (in eastern Afghanistan) and the old province of Gandhara (northern Pakistan and Kashmir) from the decline of the Kushan Empire up to the early 9th century CE. The Shahis continued to rule eastern Afghanistan until the late 9th century until the Ghaznavid invasions.


</doc>
<doc id="13814" url="https://en.wikipedia.org/wiki?curid=13814" title="History of modern Greece">
History of modern Greece

The history of modern Greece covers the history of Greece from the recognition of its autonomy from the Ottoman Empire by the Great Powers (Great Britain, France, and Russia) in 1828, after the Greek War of Independence, to the present day.

The Byzantine Empire had ruled most of the Greek-speaking world since late Antiquity, but experienced a decline as a result of Muslim Arab and Seljuk Turkish invasions and was fatally weakened by the sacking of Constantinople by the Latin Crusaders in 1204. The establishment of Catholic Latin states on Greek soil, and the struggles of the Orthodox Byzantine Greeks against them, led to the emergence of a distinct Greek national identity. The Byzantine Empire was restored by the Palaiologos dynasty in 1261, but it was a shadow of its former self, and constant civil wars and foreign attacks in the 14th century brought about its terminal decline. As a result, most of Greece gradually became part of the Ottoman Empire in the late 14th and early 15th century, culminating in the Fall of Constantinople in 1453, the conquest of the Duchy of Athens in 1458, and of the Despotate of the Morea in 1460.

Ottoman control was largely absent in the mountainous interior of Greece, and many fled there, often becoming brigands. Otherwise, only the islands of the Aegean and a few coastal fortresses on the mainland, under Venetian and Genoese rule, remained free from Ottoman rule, but by the mid-16th century, the Ottomans had conquered most of them as well. Rhodes fell in 1522, Cyprus in 1571, and the Venetians retained Crete until 1670. The Ionian Islands were only briefly ruled by the Ottomans (Kefalonia from 1479 to 1481 and from 1485 to 1500), and remained primarily under the rule of Venice.
The first large-scale insurrection against Ottoman rule was the Orlov Revolt of the early 1770s, but it was brutally repressed. The same time, however, also marks the start of the Modern Greek Enlightenment, as Greeks who studied in Western Europe brought knowledge and ideas back to their homeland, and as Greek merchants and shipowners increased their wealth. As a result, especially in the aftermath of the French Revolution, liberal and nationalist ideas began to spread across the Greek lands.

In 1821, the Greeks rose up against the Ottoman Empire. Initial successes were followed by infighting, which almost caused the Greek struggle to collapse; nevertheless, the prolongation of the fight forced the Great Powers (Britain, Russia and France) to recognize the claims of the Greek rebels to separate statehood (Treaty of London) and intervene against the Ottomans at the Battle of Navarino. Greece was initially to be an autonomous state under Ottoman suzerainty, but by 1832, in the Treaty of Constantinople, it was recognized as a fully independent kingdom. In the meantime, the 3rd National Assembly of the Greek insurgents called upon Ioannis Kapodistrias, a former foreign minister of Russia, to take over the governance of the fledgling state in 1827.

On his arrival, Kapodistrias launched a major reform and modernisation programme that covered all areas. He re-established military unity by bringing an end to the second phase of the civil war; re-organised the military, which was then able to reconquer territory lost to the Ottoman military during the civil wars; and introduced the first modern quarantine system in Greece, which brought diseases such typhoid fever, cholera and dysentery under control for the first time since the start of the War of Independence.

Kapodistrias also negotiated with the Great Powers and the Ottoman Empire to establish the borders and degree of independence of the Greek state; signed the peace treaty that ended the War of Independence with the Ottomans; introduced the "phoenix", the first modern Greek currency; organised local administration; and, in an effort to raise the living standards of the population, introduced the cultivation of the potato into Greece.

Furthermore, he tried to undermine the authority of the traditional clans (or dynasties) that he considered the useless legacy of a bygone and obsolete era. However, he underestimated the political and military strength of the "capetanei" (καπεταναίοι – commanders) who had led the revolt against Ottoman Empire in 1821, and who had expected a leadership role in the post-revolution Government. When a dispute between the "capetanei" of Laconia and the appointed governor of the province escalated into an armed conflict, he called in Russian troops to restore order, because much of the army was controlled by "capetanei" who had been part of the rebellion.

George Finlay's 1861 "History of Greek Revolution" records that by 1831 Kapodistrias's government had become hated, chiefly by the independent Maniots, but also by the Roumeliotes and the rich and influential merchant families of Hydra, Spetses and Psara. The customs dues of the inhabitants of Hydra were the chief source of revenue for these municipalities, and they refused to hand these over to Kapodistrias. It appears that Kapodistrias had refused to convene the National Assembly and was ruling as a despot, possibly influenced by his Russian experiences. The municipality of Hydra instructed Admiral Miaoulis and Alexandros Mavrokordatos to go to Poros and seize the Hellenic Navy's fleet there. This Miaoulis did so with the intention of preventing a blockade of the islands, so for a time it seemed as if the National Assembly would be called.

Kapodistrias called on the British and French residents to support him in putting down the rebellion, but this they refused to do. Nonetheless, an Admiral Rikord (or Ricord) took his ships north to Poros. Colonel (later General) Kallergis took a half-trained force of Greek Army regulars and a force of irregulars in support. With less than 200 men, Miaoulis was unable to make much of a fight; Fort Heidek on Bourtzi Island was overrun by the regulars and the brig "Spetses" (once Laskarina Bouboulina's "Agamemnon") sank by Ricord's force. Encircled by the Russians in the harbor and Kallergis' force on land, Poros surrendered. Miaoulis was forced to set charges in the flagship "Hellas" and the corvette "Hydra" to blow them up when he and his handful of followers returned to Hydra. Kallergis' men were enraged by the loss of the ships and sacked Poros, carrying off plunder to Nauplion.

The loss of the best ships in the fleet crippled the Hellenic Navy for many years, but it also weakened Kapodistrias' position. He did finally call the National Assembly, but his other actions triggered more opposition and that led to his downfall.

In 1831, Kapodistrias ordered the imprisonment of Petrobey Mavromichalis, the Bey of the Mani Peninsula, one of the wildest and most rebellious parts of Greece. This was a mortal offence to the Mavromichalis family, and on 9 October 1831 (27 September in the Julian Calendar) Kapodistrias was assassinated by Petros' brother Konstantis and son Georgios on the steps of the church of Saint Spyridon in Nafplio.

Ioannis Kapodistrias was succeeded as Governor by his younger brother, Augustinos Kapodistrias. Augustinos ruled only for six months, during which the country was very much plunged into chaos. Under the protocol signed at the London Conference of 1832 on 7 May 1832 between Bavaria and the protecting Powers, Greece was defined as an independent kingdom, free of Ottoman control, with the Arta-Volos line as its northern frontier. The protocol also dealt with the way in which a Regency was to be managed until Otto of Bavaria reached his majority to assume the throne of Greece. The Ottoman Empire was indemnified in the sum of 40,000,000 piastres for the loss of territory in the new kingdom.

Otto's reign would prove troubled, but he managed to hang on for 30 years before he and his wife, Queen Amalia, left the same way they came, aboard a British warship. During the early years of his reign, a group of Bavarian Regents ruled in his name, and they made themselves very unpopular by trying to impose German ideas of rigid hierarchical government on the Greeks, while keeping most significant state offices away from them. Nevertheless, they laid the foundations of a Greek administration, army, justice system and education system. Otto was sincere in his desire to give Greece good government, but he suffered from two great handicaps: his Roman Catholic faith and his childless marriage to Queen Amalia. This meant he could neither be crowned as King of Greece under the Orthodox rite nor establish a dynasty.

The Bavarian Regents ruled until 1837, when they were recalled at the insistence of Britain and France. Otto thereafter appointed Greek ministers, although Bavarian officials still ran most of the administration and the army. At this time, Greece still had no legislature and no constitution. Discontent grew until the 3 September 1843 Revolution broke out in Athens. Otto agreed to grant a constitution and convened a National Assembly that met in November of the same year. The Greek Constitution of 1844 then created a bicameral parliament consisting of an Assembly ("Vouli") and a Senate ("Gerousia"). Power then passed into the hands of a group of Greek politicians, most of whom who had been commanders in the War of Independence against the Ottomans.

Greek politics in the 19th century was dominated by the "national question." The majority of Greeks continued to live under Ottoman rule, and Greeks dreamed of liberating them all and reconstituting a state embracing all the Greek lands, with Constantinople as its capital. This was called the Great Idea ("Megali Idea"), and it was sustained by almost continuous rebellions against Ottoman rule in Greek-speaking territories, particularly Crete, Thessaly and Macedonia.

When the Crimean War broke out in 1854, Greece saw an opportunity to gain Ottoman-controlled territory that had large Greek populations. Greece, an Orthodox nation, had considerable support in Russia, but the Russian government decided it was too dangerous to help Greece expand its holdings. When the Russians attacked the Ottoman forces, Greece invaded Thessaly and Epirus. To block further Greek moves, the British and French occupied the main Greek port at Piraeus from April 1854 to February 1857. The Greeks, gambling on a Russian victory, incited the large-scale Epirus Revolt of 1854 as well as uprisings in Crete. The revolts failed and Greece made no gains during the Crimean War, which Russia lost.

A new generation of Greek politicians was growing increasingly intolerant of King Otto's continuing interference in government. In 1862, the King dismissed his Prime Minister, the former admiral Constantine Kanaris, the most prominent politician of the period. This provoked a military rebellion, forcing Otto to accept the inevitable and leave the country.

The Greeks then asked Britain to send Queen Victoria's son Prince Alfred as their new king, but this was vetoed by the other Powers. Instead, a young Danish Prince became King George I. George was a very popular choice as a constitutional monarch, and he agreed that his sons would be raised in the Greek Orthodox faith. As a reward to the Greeks for adopting a pro-British King, Britain ceded the Ionian Islands to Greece.

At the urging of Britain and King George, Greece adopted the much more democratic Greek Constitution of 1864. The powers of the King were reduced, the Senate was abolished, and the franchise was extended to all adult males. Approval voting was used in elections, with one urn for each candidate divided into "yes" and "no" portions into which voters dropped lead beads. Nevertheless, Greek politics remained heavily dynastic, as it has always been. Family names such as Zaimis, Rallis and Trikoupis occurred repeatedly as Prime Ministers.

Although parties were centered around the individual leaders, often bearing their names, two broad political tendencies existed: the liberals, led first by Charilaos Trikoupis and later by Eleftherios Venizelos, and the conservatives, led initially by Theodoros Deligiannis and later by Thrasivoulos Zaimis. Trikoupis and Deligiannis dominated Greek politics in the later 19th century, alternating in office. Trikoupis favoured co-operation with Great Britain in foreign affairs, the creation of infrastructure and an indigenous industry, raising protective tariffs and progressive social legislation, while the more populist Deligiannis depended on the promotion of Greek nationalism and the "Megali Idea".

Greece remained a very poor country throughout the 19th century. The country lacked raw materials, infrastructure and capital. Agriculture was mostly at the subsistence level, and the only important export commodities were currants, raisins and tobacco. Some Greeks grew rich as merchants and shipowners, and Piraeus became a major port, but little of this wealth found its way to the Greek peasantry. Greece remained hopelessly in debt to London finance houses.

By the 1890s Greece was virtually bankrupt. Poverty was rife in the rural areas and the islands, and was eased only by large-scale emigration to the United States. There was little education in the rural areas. Nevertheless, there was progress in building communications and infrastructure, and fine public buildings were erected in Athens. Despite the bad financial situation, Athens staged the revival of the Olympic Games in 1896, which proved a great success.
The parliamentary process developed greatly in Greece during the reign of George I. Initially, the royal prerogative in choosing his prime minister remained and contributed to governmental instability, until the introduction of the "dedilomeni" principle of parliamentary confidence in 1875 by the reformist Charilaos Trikoupis. Clientelism and frequent electoral upheavals however remained the norm in Greek politics, and frustrated the country's development.

Corruption and Trikoupis' increased spending (to create necessary infrastructure such as the Corinth Canal) overtaxed the weak Greek economy, forcing the declaration of public insolvency in 1893 and to accept the imposition of an International Financial Control authority to pay off the country's creditors.

Another political issue in 19th-century Greece was the Greek language question. The Greek people spoke a form of Greek called Demotic. Many of the educated elite saw this as a peasant dialect and were determined to restore the glories of Ancient Greek. Government documents and newspapers were consequently published in "Katharevousa" (purified) Greek, a form that few ordinary Greeks could read. Liberals favoured recognising Demotic as the national language, but conservatives and the Orthodox Church resisted all such efforts, to the extent that when the New Testament was translated into Demotic in 1901, riots erupted in Athens and the government fell (the "Evangeliaka"). This issue would continue to plague Greek politics until the 1970s.
All Greeks were united, however, in their determination to liberate the Greek-speaking provinces of the Ottoman Empire. Especially in Crete, the Cretan Revolt (1866–1869) raised nationalist fervour. When war broke out between Russian and the Ottomans in the Russo-Turkish War (1877–1878), Greek popular sentiment rallied to Russia's side, but Greece was too poor and too concerned about British intervention to enter the war officially. Nevertheless, in 1881, Thessaly and small parts of Epirus were ceded to Greece as part of the Treaty of Berlin.

Greeks in Crete continued to stage regular revolts, and in 1897, the Greek government under Theodoros Deligiannis, bowing to popular pressure, declared war on the Ottomans. In the ensuing Greco-Turkish War of 1897, the badly trained and equipped Greek army was defeated by the Ottomans. Through the intervention of the Great Powers however, Greece lost only a little territory along the border to Turkey, while Crete was established as an autonomous state under Prince George of Greece as the Cretan State.
Nationalist sentiment among Greeks in the Ottoman Empire continued to grow, and by the 1890s there were constant disturbances in Macedonia. Here, the Greeks were in competition not only with the Ottomans, but also with the Bulgarians, in an armed propaganda struggle for the hearts and minds of the ethnically mixed local population, the so-called "Macedonian Struggle".

In July 1908, the Young Turk Revolution broke out in the Ottoman Empire. Taking advantage of the Ottoman internal turmoil, Austria-Hungary annexed Bosnia and Herzegovina and Bulgaria declared its independence from the Ottoman Empire. On Crete, the local population, led by a young politician named Eleftherios Venizelos, declared "Enosis", Union with Greece, provoking another crisis. The fact that the Greek government, led by Dimitrios Rallis, proved unable to likewise take advantage of the situation and bring Crete into the fold, rankled many Greeks, especially young military officers. These formed a secret society, the "Military League", with the purpose of emulating their Ottoman colleagues to seek governmental reforms.

The resulting Goudi coup on 15 August 1909 marked a watershed in modern Greek history: as the military conspirators were inexperienced in politics, they asked Venizelos, who had impeccable liberal credentials, to come to Greece as their political adviser. Venizelos quickly established himself as a powerful political figure, and his allies won the August 1910 elections. Venizelos became Prime Minister in October 1910, ushering a period of 25 years where his personality would dominate Greek politics.

Venizelos initiated a major reform program, including a new and more liberal constitution and reforms in the spheres of public administration, education and economy. French and British military missions were invited for the army and navy respectively, and arms purchases were made. In the meantime, the Ottoman Empire's weaknesses were revealed by the ongoing Italo-Turkish War in Libya.

Through the spring of 1912, a series of bilateral agreements between the Christian Balkan states (Greece, Bulgaria, Montenegro and Serbia) formed the Balkan League, which in October 1912 declared war on the Ottoman Empire. In the First Balkan War, the Ottomans were defeated on all fronts, and the four allies rushed to grab as much territory as they could. The Greeks occupied Thessaloniki just ahead of the Bulgarians, and also took much of Epirus with Ioannina, as well as Crete and the Aegean Islands.

The Treaty of London (1913) ended the war, but no one was left satisfied, and soon, the four allies fell out over the partition of Macedonia. In June 1913, Bulgaria attacked Greece and Serbia, beginning the Second Balkan War, but was beaten back. The Treaty of Bucharest (1913), which concluded the Second Balkan War, left Greece with southern Epirus, the southern half of Macedonia (known as Greek Macedonia), Crete and the Aegean islands, except for the Dodecanese, which had been occupied by Italy since 1911. These gains nearly doubled Greece's area and population.

In March 1913, an anarchist, Alexandros Schinas, assassinated King George in Thessaloniki, and his son came to the throne as Constantine I. Constantine was the first Greek king born in Greece and the first to be Greek Orthodox by birth. His very name had been chosen in the spirit of romantic Greek nationalism (the "Megali Idea"), evoking the Byzantine emperors of that name. In addition, as the Commander-in-chief of the Greek Army during the Balkan Wars, his popularity was enormous, rivalled only by that of Venizelos, his Prime Minister.

When World War I broke out in 1914, the King and his Prime Minister Venizelos both preferred to maintain a neutral stance, in spite of Greece's treaty of alliance with Serbia, which had been attacked by Austria-Hungary as the first belligerent action of the conflict. But when the Allies asked for Greek help in the Dardanelles campaign of 1915, offering Cyprus in exchange, their diverging views became apparent: Constantine had been educated in Germany, was married to Sophia of Prussia, sister of Kaiser Wilhelm, and was convinced of the Central Powers' victory. Venizelos, on the other hand, was an ardent anglophile, and believed in an Allied victory.

Since Greece, a maritime country, could not oppose the mighty British navy, and citing the need for a respite after two wars, King Constantine favored continued neutrality, while Venizelos actively sought Greek entry in the war on the Allied side. Venizelos resigned, but won the Greek elections of 1915 and again formed the government. When Bulgaria entered the war as a German ally in October 1915, Venizelos invited Allied forces into Greece (the Salonika Front), for which he was again dismissed by Constantine.

In August 1916, after several incidents in which both sides in the war had encroached upon the still theoretically neutral Greek territory, Venizelist officers rose up in Allied-controlled Thessaloniki and Venizelos established a separate government there known as the result of a so-called Movement of National Defence. Constantine was now ruling only in what was Greece before the Balkan Wars ("Old Greece"), and his government was subject to repeated humiliations from the Allies. In November 1916 the French occupied Piraeus, bombarded Athens and forced the Greek fleet to surrender. The royalist troops fired at them, leading to a battle between French and Greek royalist troops. There were also riots against supporters of Venizelos in Athens (the "Noemvriana").

Following the February Revolution in Russia in 1917, the Tsar's support for his cousin Constantine was eliminated, and he was forced to leave the country, without actually abdicating, in June 1917. His second son Alexander became King, while the remaining royal family and the most prominent royalists followed him into exile. Venizelos now led a superficially united Greece into the war on the Allied side, but underneath the surface, the division of Greek society into Venizelists and anti-Venizelists, the so-called National Schism, became more entrenched.

With the end of the war in November 1918, the moribund Ottoman Empire was ready to be carved up among the victors, and Greece now expected the Allies to deliver on their promises. In no small measure through the diplomatic efforts of Venizelos, Greece secured Western Thrace in the Treaty of Neuilly in November 1919 and Eastern Thrace and a zone around Smyrna in western Anatolia (already under Greek administration as the Occupation of İzmir since May 1919) in the Treaty of Sèvres of August 1920. The future of Constantinople was left to be determined. But at the same time, a Turkish National Movement rose in Turkey led by Mustafa Kemal (later Kemal Atatürk), who set up a rival government in Ankara and was engaged in fighting the Greek army.

At this point, the fulfillment of the "Megali Idea" seemed near. Yet so deep was the rift in Greek society that on his return to Greece, an assassination attempt was made on Venizelos by two royalist former officers. Even more surprisingly, Venizelos' Liberal Party lost the Greek elections of November 1920, and in the Greek plebescite of 1920, the Greek people voted for the return of King Constantine from exile after the sudden death of King Alexander.

The United Opposition, which had campaigned on the slogan of an end to the Asia Minor Campaign in Anatolia, instead intensified it. But the royalist restoration had dire consequences: many veteran Venizelist officers were dismissed or left the army, while Italy and France found the return of the hated Constantine a useful pretext for switching their support to Kemal. Finally, in August 1922, the Turkish army shattered the Greek front, and took Smyrna in an operation that led to the disastrous Great Fire of Smyrna.

The Greek army evacuated not only Anatolia, but also Eastern Thrace and the islands of Imbros and Tenedos in accordance with the terms of the Treaty of Lausanne (1923). A population exchange between Greece and Turkey was agreed between the two countries, with over 1.5 million Christians and almost half a million Muslims being uprooted. This catastrophe marked the end of the "Megali Idea", and left Greece financially exhausted, demoralized, and having to house and feed a proportionately huge number of Greek refugees.

The catastrophe deepened the political crisis, with the returning army rising up under Venizelist officers and forcing King Constantine to abdicate again, in September 1922, in favour of his firstborn son, George II. The "Revolutionary Committee" headed by Colonels Stylianos Gonatas (soon to become Prime Minister) and Nikolaos Plastiras engaged in a witch-hunt against the royalists, culminating in the "Trial of the Six".

The Greek election of 1923 was held to form a National Assembly with powers to draft a new constitution. Following a failed royalist Leonardopoulos-Gargalidis coup attempt, the monarchist parties abstained, leading to a landslide for the Liberals and their allies. King George II was asked to leave the country, and on 25 March 1924, Alexandros Papanastasiou proclaimed the Second Hellenic Republic, ratified by the Greek plebiscite of 1924 a month later.

However, the new Republic was built on unstable foundations. The National Schism lived on, as the monarchists, with the exception of Ioannis Metaxas, did not acknowledge the Venizelist-sponsored Republican regime. The army, which had power and provided many of the leading proponents of both sides, became a factor to be reckoned with, prone to intervene in politics.

Greece was diplomatically isolated and vulnerable, as the Corfu incident of 1923 showed, and the economic foundations of the state were in ruins after a decade of war and the sudden increase of the country's population by a quarter. The refugees, however, also brought a new air into Greece. They were impoverished now, but before 1922 many had been entrepreneurs and well-educated. Staunch supporters of Venizelos and the Republic, many would radicalize and play a leading role in the nascent Communist Party of Greece.

In June 1925, General Theodoros Pangalos launched a coup and ruled as a dictator for a year until a counter-coup by another General, Georgios Kondylis, unseated him and restored the Republic. In the meantime, Pangalos managed to embroil Greece in a short-lived war with Bulgaria precipitated by the Incident at Petrich and make unacceptable concessions in Thessaloniki and its hinterland to Yugoslavia in an effort to gain its support for his revanchist policies against Turkey.

In 1928, Venizelos returned from exile. After a landslide victory in the Greek election of 1928, he formed a government. This was the only cabinet of the Second Republic to run its full four-year term, and the work it left behind was considerable. Alongside domestic reforms, Venizelos restored Greece's frayed international relations, even initiating a Greco-Turkish reconciliation with a visit to Ankara and the signing of a Friendship Agreement in 1930.

The Great Depression hit Greece, an already poor country dependent on agricultural exports, particularly hard. Matters were made worse by the closing off of emigration to the United States, the traditional safety valve of rural poverty. High unemployment and consequent social unrest resulted, and the Communist Party of Greece made rapid advances. Venizelos was forced to default on Greece's national debt in 1932, and he fell from office after the Greek elections of 1932. He was succeeded by a monarchist coalition government led by Panagis Tsaldaris of the People's Party.

Two failed Venizelist military coups followed in 1933 and 1935 in an effort to preserve the Republic, but they had the opposite effect. On 10 October 1935, a few months after he suppressed the 1935 Greek coup d'état attempt, Georgios Kondylis, the former Venizelist stalwart, abolished the Republic in another coup, and declared the monarchy restored. The rigged Greek plebiscite of 1935 confirmed the regime change (with an unsurprising 97.88% of votes), and King George II returned.

King George II immediately dismissed Kondylis and appointed Professor Konstantinos Demertzis as interim Prime Minister. Venizelos meanwhile, in exile, urged an end to the conflict over the monarchy in view of the threat to Greece from the rise of Fascist Italy. His successors as Liberal leader, Themistoklis Sophoulis and Georgios Papandreou, agreed, and the restoration of the monarchy was accepted. The Greek elections of 1936 resulted in a hung parliament, with the Communists holding the balance. As no government could be formed, Demertzis continued on. At the same time, a series of deaths left the Greek political scene in disarray: Kondylis died in February, Venizelos in March, Demertzis in April and Tsaldaris in May. The road was now clear for Ioannis Metaxas, who had succeeded Demertzis as interim Prime Minister.

Metaxas, a retired royalist general, believed that an authoritarian government was necessary to prevent social conflict and quell the rising power of the Communists. On 4 August 1936, with the King's support, he suspended parliament and established the 4th of August Regime. The Communists were suppressed and the Liberal leaders went into internal exile. Patterning itself after Benito Mussolini's Fascist Italy, Metaxas' regime promoted various concepts such as the "Third Hellenic Civilization", the Roman salute, a National Organisation of Youth, and introduced measures to gain popular support, such as the Greek Social Insurance Institute (IKA), still the biggest social security institution in Greece.

Despite these efforts, the regime lacked a broad popular base or a mass movement supporting it. The Greek people were generally apathetic, without actively opposing Metaxas. Metaxas also improved the country's defenses in preparation for the forthcoming European war, constructing, among other defensive measures, the "Metaxas Line". Despite his aping of Fascism, and the strong economic ties with resurgent Nazi Germany, Metaxas followed a policy of neutrality, given Greece's traditionally strong ties to Britain, reinforced by King George II's personal anglophilia. In April 1939, the Italian threat suddenly loomed closer when Italy annexed Albania, whereupon Britain publicly guaranteed Greece's borders. Thus, when World War II broke out in September 1939, Greece remained neutral.

Despite this declared neutrality, Greece became a target for Mussolini's expansionist policies. Provocations against Greece included the sinking of the Greek cruiser "Elli" on 15 August 1940. Italian troops crossed the border on 28 October 1940, beginning the Greco-Italian War, but were stopped by a determined Greek defence that ultimately drove them back into Albania.

Metaxas died suddenly in January 1941. His death raised hopes for a liberalization of his regime and the restoration of parliamentary rule, but King George quashed these hopes when he retained the regime's machinery in place. In the meantime, Adolf Hitler was reluctantly forced to divert German troops to rescue Mussolini from defeat, and attacked Greece through Yugoslavia and Bulgaria on 6 April 1941. Despite British assistance, the Germans overran most of the country by the end of May. The King and the government escaped to Crete, where they stayed until the end of the Battle of Crete. They then transferred to Egypt, where a Greek government in exile was established.

The occupied country of Greece was divided in three zones (German, Italian and Bulgarian) and in Athens, a puppet regime was established. The members were either conservatives or nationalists with fascist leanings. The three quisling prime ministers were Georgios Tsolakoglou, the general who had signed the armistice with the Wehrmacht, Konstantinos Logothetopoulos, and Ioannis Rallis, who took office when the German defeat was inevitable and aimed primarily at combating the left-wing Resistance movement. To this end, he created the collaborationist Security Battalions.

Greece suffered terrible privations during World War II as the Germans appropriated most of the country's agricultural production and prevented its fishing fleets from operating. As a result, and because a British blockade initially hindered foreign relief efforts, the Great Greek Famine resulted. Hundreds of thousands of Greeks perished, especially in the winter of 1941–1942. In the mountains of the Greek mainland, in the meantime, several Greek resistance movements sprang up, and by mid-1943, the Axis forces controlled only the main towns and the connecting roads, while a "Free Greece" was set up in the mountains.

The largest resistance group, the National Liberation Front (EAM), was controlled by the Communist Party of Greece, as was the Greek People's Liberation Army (Elas), led by Aris Velouchiotis, and a civil war soon broke out between it and non-Communist groups such as the National Republican Greek League (EDES) in those areas liberated from the Germans. The exiled government in Cairo was only intermittently in touch with the resistance movement and exercised virtually no influence in the occupied country. Part of this was due to the unpopularity of King George II in Greece itself, but despite efforts by Greek politicians, British support ensured his retention at the head of the Cairo government.

As the German defeat drew nearer, the various Greek political factions convened in Lebanon in May 1944 under British auspices and formed a government of national unity under George Papandreou, in which EAM was represented by six ministers.

German forces withdrew on 12 October 1944, and the government in exile returned to Athens. After the German withdrawal, the EAM-ELAS guerrilla army effectively controlled most of Greece, but its leaders were reluctant to take control of the country, as they knew that Soviet premier Joseph Stalin had agreed that Greece would be in the British sphere of influence after the war. Tensions between the British-backed Papandreou and the EAM, especially over the issue of disarmament of the various armed groups, led to the resignation of the latter's ministers from the government.

A few days later, on 3 December 1944, a large-scale pro-EAM demonstration in Athens ended in violence and ushered an intense, house-to-house struggle with British and monarchist forces (the "Dekemvriana"). After three weeks, the Communists were defeated: the Varkiza agreement ended the conflict and disarmed ELAS, and an unstable coalition government was formed. The anti-EAM backlash grew into a full-scale "White Terror", which exacerbated tensions.

The Communists boycotted the March 1946 elections, and on the same day, fighting broke out again. By the end of 1946, the Communist Democratic Army of Greece had been formed, pitted against the governmental National Army, which was backed first by Britain and after 1947 by the United States.

Communist successes in 1947–1948 enabled them to move freely over much of mainland Greece, but with extensive reorganization, the deportation of rural populations and American material support, the National Army was slowly able to regain control over most of the countryside. In 1949, the insurgents suffered a major blow, as Yugoslavia closed its borders following the split between Marshal Josip Broz Tito with the Soviet Union. Finally, in August 1949, the National Army under Marshal Alexander Papagos launched an offensive that forced the remaining insurgents to surrender or flee across the northern border into the territory of Greece's northern Communist neighbors.

The civil war resulted in 100,000 killed and caused catastrophic economic disruption. In addition, at least 25,000 Greeks and an unspecified number of Macedonian Slavs were either voluntarily or forcibly evacuated to Eastern bloc countries, while 700,000 became displaced persons inside the country. Many more emigrated to Australia and other countries.

The postwar settlement ended Greece's territorial expansion, which had begun in 1832. The 1947 Treaty of Paris required Italy to hand over the Dodecanese islands to Greece. These were the last majority-Greek-speaking areas to be united with the Greek state, apart from Cyprus which was a British possession until it became independent in 1960. Greece's ethnic homogeneity was increased by the postwar expulsion of 25,000 Albanians from Epirus (see Cham Albanians). The only significant remaining minorities are the Muslims in Western Thrace (about 100,000) and a small Slavic-speaking minority in the north. Greek nationalists continued to claim southern Albania (which they called Northern Epirus), home of a significant Greek population (about 3%-12% in the whole of Albania), and the Turkish-held islands of Imvros and Tenedos, where there were smaller Greek minorities.

After the civil war, Greece sought to join the Western democracies and became a member of the North Atlantic Treaty Organization in 1952.

Since the Civil war (1946–49) but even more after that, the parties in the parliament were divided in three political concentrations. The political formation Right-Centre-Left, given the exacerbation of political animosity that had preceded dividing the country in the 40s, tended to turn the concurrence of parties into ideological positions.

In the beginning of the 1950s, the forces of the Centre (EPEK) succeeded in gaining the power and under the leadership of the aged general N. Plastiras they governed for about half a four-year term. These were a series of governments having limited manoeuvreability and inadequate influence in the political arena. This government, as well as those that followed, was constantly under the American auspices. The defeat of EPEK in the elections of 1952, apart from increasing the repressive measures that concerned the defeated of the Civil war, also marked the end of the general political position that it represented, namely political consensus and social reconciliation.

The Left, which had been ostracized from the political life of the country, found a way of expression through the constitution of EDA (United Democratic Left) in 1951, which turned out to be a significant pole, yet steadily excluded from the decision making centres. After the disbandment of the Centre as an autonomous political institution, EDA practically expanded its electoral influence to a significant part of the EAM-based Centre-Left.

The 1960s are part of the period 1953-72, during which Greek economy developed rapidly and was structured within the scope of European and worldwide economic developments. One of the main characteristics of that period was the major political event of the country's accession in the European Economic Community, in an attempt to create a common market. The relevant treaty was contracted in 1962.

The developmental strategy adopted by the country was embodied in centrally organized five-year plans; yet their orientation was indistinct. The average annual emigration, which absorbed the excess workforce and contributed to extremely high growth rates, exceeded the annual natural increase in population. The influx of large amounts of foreign private capital was being facilitated and consumption was expanded. These, associated with the rise of tourism, the expansion of shipping activity and with the migrant remittances, had a positive effect on the country's balance of payments.

The peak of development was registered principally in manufacturing, mainly in the textile, chemical and metallurgical industries, the growth rate of which reached 11% during 1965-70. The other large area where obvious economic and social consequences occurred, was that of construction. The policy of αντιπαροχή ("antiparochi", "property-swap"), a Greek invention which entailed the concession of construction land to developers in return for a share in the resulting multi-storey apartment buildings, favoured the creation of a class of small-medium contractors on the one hand and settled the housing system and property status on the other. However, it was also responsible for the demolition of much of the country's traditional and 19th-century neoclassical architecture, and the transformation of Greek cities, and especially Athens, into a "form-less, border-less and placeless urban landscape".

During that decade, youth culture came to the fore in society as a distinct social power with autonomous presence (creation of a new culture in music, fashion etc.) and young people displayed dynamism in the assertion of their social rights. The independence granted to Cyprus, which was mined from the very beginning, constituted the main focus of young activist mobilizations, along with struggles aiming at reforms in education, which were provisionally realized to a certain extent through the educational reform of 1964. The country reckoned on and was influenced by Europe—usually behind time—and by the current trends like never before.

The country descended into a prolonged political crisis, and elections were scheduled for late April 1967. On 21 April 1967 a group of right-wing colonels led by Colonel George Papadopoulos seized power in a coup d'état establishing the Regime of the Colonels. Civil liberties were suppressed, special military courts were established, and political parties were dissolved.

Several thousand suspected communists and political opponents were imprisoned or exiled to remote Greek islands. Alleged US support for the junta is claimed to be the cause of rising anti-Americanism in Greece during and following the junta's harsh rule. The junta's early years also saw a marked upturn in the economy, with increased foreign investment and large-scale infrastructure works. The junta was widely condemned abroad, but inside the country, discontent began to increase only after 1970, when the economy slowed down.

Even the armed forces, the regime's foundation, were not immune: In May 1973, a planned coup by the Hellenic Navy was narrowly suppressed, but led to the mutiny of the , whose officers sought political asylum in Italy. In response, junta leader Papadopoulos attempted to steer the regime towards a controlled democratization, abolishing the monarchy and declaring himself President of the Republic.

On 25 November 1973, following the bloody suppression of Athens Polytechnic uprising on the 17th, the hardliner Brigadier Dimitrios Ioannides overthrew Papadopoulos and tried to continue the dictatorship despite the popular unrest the uprising had triggered. Ioannides' attempt in July 1974 to overthrow Archbishop Makarios, the President of Cyprus, brought Greece to the brink of war with Turkey, which invaded Cyprus and occupied part of the island.

Senior Greek military officers then withdrew their support from the junta, which collapsed. Constantine Karamanlis returned from exile in France to establish a government of national unity until elections could be held. Karamanlis worked to defuse the risk of war with Turkey and also legalised the Communist Party, which had been illegal since 1947. His newly organized party, New Democracy (ND), won the elections held in November 1974 by a wide margin, and he became prime minister.

Following the 1974 referendum which resulted in the abolition of the monarchy, a new constitution was approved by parliament on 19 June 1975. Parliament elected Constantine Tsatsos as President of the Republic. In the parliamentary elections of 1977, New Democracy again won a majority of seats. In May 1980, Prime Minister Karamanlis was elected to succeed Tsatsos as President. George Rallis succeeded Karamanlis as Prime Minister.

On 1 January 1981, Greece became the tenth member of the European Community (now the European Union). In parliamentary elections held on 18 October 1981, Greece elected its first socialist government when the Panhellenic Socialist Movement (PASOK), led by Andreas Papandreou, won 172 of 300 seats. On 29 March 1985, after Prime Minister Papandreou declined to support President Karamanlis for a second term, Supreme Court Justice Christos Sartzetakis was elected president by the Greek parliament.

Greece had two rounds of parliamentary elections in 1989; both produced weak coalition governments with limited mandates. Party leaders withdrew their support in February 1990, and elections were held on 8 April. New Democracy, led by Constantine Mitsotakis, won 150 seats in that election and subsequently gained two others. However, a split between Mitsotakis and his first Foreign Minister, Antonis Samaras, in 1992, led to Samaras' dismissal and the eventual collapse of the ND government. In new elections in September 1993, Papandreou returned to power.

On 17 January 1996, following a protracted illness, Papandreou resigned and was replaced as Prime Minister by former Minister of Trade and Industry Costas Simitis. Within days, the new prime minister had to handle a major Greek-Turkish crisis over the Imia/Kardak islands. Simitis subsequently won re-election in the 1996 and 2000 elections. In 2004, Simitis retired and George Papandreou succeeded him as PASOK leader.

In the March 2004 elections, PASOK was defeated by New Democracy, led by Kostas Karamanlis, the nephew of the former President. The government called early elections in September 2007 (normally, elections would have been held in March 2008), and New Democracy again was the majority party in the Parliament. As a result of that defeat, PASOK undertook a party election for a new leader. In that contest, George Papandreou was reelected as the head of the socialist party in Greece. In the 2009 elections however, PASOK became the majority party in the Parliament and George Papandreou became Prime Minister of Greece. After PASOK lost its majority in the Parliament, ND and PASOK joined the smaller Popular Orthodox Rally in a grand coalition, pledging their parliamentary support for a government of national unity headed by former European Central Bank vice-president Lucas Papademos.

From late 2009, fears of a sovereign debt crisis developed among investors concerning Greece's ability to meet its debt obligations due to strong increase in government debt levels. This led to a crisis of confidence, indicated by a widening of bond yield spreads and risk insurance on credit default swaps compared to other countries, most importantly Germany. Downgrading of Greek government debt to junk bonds created alarm in financial markets.

On 2 May 2010, the Eurozone countries and the International Monetary Fund agreed on a loan for Greece, conditional on the implementation of harsh austerity measures. In October 2011, Eurozone leaders also agreed on a proposal to write off 50% of Greek debt owed to private creditors, increasing the EFSF to about €1 trillion and requiring European banks to achieve 9% capitalization to reduce the risk of contagion to other countries. These austerity measures have proved extremely unpopular with the Greek public, precipitating demonstrations and civil unrest.

There are widespread fears that a Greek default on its debt would have global repercussions, endangering the economies of many other countries in the European Union, threatening the stability of the European currency, the euro, and possibly plunging the world into another recession. It has been speculated that the crisis may force Greece to abandon the euro and bring back its former currency, the drachma. In April 2014, Greece returned to the global bond market as it successfully sold €3 billion worth of five-year government bonds at a yield of 4.95%. According to the IMF, Greece will have real GDP growth of 0.6% in 2014 after 5 years of decline.

Following the May 2012 legislative election where the New Democracy party became the largest party in the Hellenic Parliament, Samaras, leader of ND, was asked by Greek President Karolos Papoulias to try to form a government. However, after a day of hard negotiations with the other parties in Parliament, Samaras officially announced he was giving up the mandate to form a government. The task passed to Alexis Tsipras, leader of the SYRIZA (the second-largest party) who was also unable to form a government. After PASOK also failed to negotiate a successful agreement to form a government, emergency talks with the President ended with a new election being called while Panagiotis Pikrammenos was appointed as Prime Minister in a caretaker government.

Voters once again took to the polls in the widely watched June 2012 election. New Democracy came out on top in a stronger position with 129 seats, compared to 108 in the May election. On 20 June 2012, Samaras successfully formed a coalition with PASOK (now led by former Finance Minister Evangelos Venizelos) and DIMAR. The new government would have a majority of 58, with SYRIZA, Independent Greeks (ANEL), Golden Dawn (XA) and the Communist Party (KKE) comprising the opposition. PASOK and DIMAR chose to take a limited role in Samaras' Cabinet, being represented by party officials and independent technocrats instead of MPs.




</doc>
<doc id="13815" url="https://en.wikipedia.org/wiki?curid=13815" title="Heracles">
Heracles

Heracles ( ; , "Hēraklês", Glory/Pride of "Hēra", "Hera"), born Alcaeus (, "Alkaios") () or Alcides (, "Alkeidēs") () was a divine hero in Greek mythology, the son of Zeus and Alcmene, foster son of Amphitryon. He was a great-grandson and half-brother (as they are both sired by the god Zeus) of Perseus. He was the greatest of the Greek heroes, a paragon of masculinity, the ancestor of royal clans who claimed to be Heracleidae (), and a champion of the Olympian order against chthonic monsters. In Rome and the modern West, he is known as Hercules, with whom the later Roman emperors, in particular Commodus and Maximian, often identified themselves. The Romans adopted the Greek version of his life and works essentially unchanged, but added anecdotal detail of their own, some of it linking the hero with the geography of the Central Mediterranean. Details of his cult were adapted to Rome as well.

Many popular stories were told of his life, the most famous being The Twelve Labours of Heracles; Alexandrian poets of the Hellenistic age drew his mythology into a high poetic and tragic atmosphere. His figure, which initially drew on Near Eastern motifs such as the lion-fight, was widely known.

Heracles was the greatest of Hellenic chthonic heroes, but unlike other Greek heroes, no tomb was identified as his. Heracles was both hero and god, as Pindar says "heroes theos"; at the same festival sacrifice was made to him, first as a hero, with a chthonic libation, and then as a god, upon an altar: thus he embodies the closest Greek approach to a "demi-god". 

The core of the story of Heracles has been identified by Walter Burkert as originating in Neolithic hunter culture and traditions of shamanistic crossings into the netherworld. It is possible that the myths surrounding Heracles were based on the life of a real person or several people whose accomplishments became exaggerated with time. Based on commonalities in the legends of Heracles and Odysseus, author Steven Sora suggested that they were both based on the same historical person, who made his mark prior to recorded history.

Heracles' role as a culture hero, whose death could be a subject of mythic telling (see below), was accepted into the Olympian Pantheon during Classical times. This created an awkwardness in the encounter with Odysseus in the episode of "Odyssey" XI, called the Nekuia, where Odysseus encounters Heracles in Hades:

Ancient critics were aware of the problem of the aside that interrupts the vivid and complete description, in which Heracles recognizes Odysseus and hails him, and modern critics find very good reasons for denying that the verses beginning, in Fagles' translation "His ghost I mean ..." were part of the original composition: "once people knew of Heracles' admission to Olympus, they would not tolerate his presence in the underworld", remarks Friedrich Solmsen, noting that the interpolated verses represent a compromise between conflicting representations of Heracles.

In Christian circles a Euhemerist reading of the widespread Heracles cult was attributed to a historical figure who had been offered cult status after his death. Thus Eusebius, "Preparation of the Gospel" (10.12), reported that Clement could offer historical dates for Hercules as a king in Argos: "from the reign of Hercules in Argos to the deification of Hercules himself and of Asclepius there are comprised thirty-eight years, according to Apollodorus the chronicler: and from that point to the deification of Castor and Pollux fifty-three years: and somewhere about this time was the capture of Troy."

Readers with a literalist bent, following Clement's reasoning, have asserted from this remark that, since Heracles ruled over Tiryns in Argos at the same time that Eurystheus ruled over Mycenae, and since at about this time Linus was Heracles' teacher, one can conclude, based on Jerome's date—in his universal history, his "Chronicon"—given to Linus' notoriety in teaching Heracles in 1264 BCE, that Heracles' death and deification occurred 38 years later, in approximately 1226 BCE.

The ancient Greeks celebrated the festival of the "Heracleia", which commemorated the death of Heracles, on the second day of the month of Metageitnion (which would fall in late July or early August). What is believed to be an Egyptian Temple of Heracles in the Bahariya Oasis dates to 21 BCE. A reassessment of Ptolemy's descriptions of the island of Malta attempted to link the site at Ras ir-Raħeb with a temple to Heracles, but the arguments are not conclusive. Several ancient cities were named Heraclea in his honor.

Although the Athenians were among the first to worship Heracles as a god, there were Greek cities that refused to recognize the hero's divine status. There are also several polis that merely provided two separate sanctuaries for Heracles, one recognizing him as a god, the other only as a hero. This ambiguity helped create the Heracles cult especially when historians (e.g. Herodotus) and artists encouraged worship such as the painters during the time of the Peisistratos, who often presented Heracles entering Olympus in their works.

Some sources explained that the cult of Heracles persisted because of the hero's ascent to heaven and his suffering, which became the basis for festivals, ritual, rites, and the organization of mysteries. There is the observation, for example, that sufferings ("pathea") gave rise to the rituals of grief and mourning, which came before the joy in the mysteries in the sequence of cult rituals. Also, like the case of Apollo, the cult of Hercules has been sustained through the years by absorbing local cult figures such as those who share the same nature. He was also constantly invoked as a patron for men, especially the young ones. For example, he was considered the ideal in warfare so he presided over gymnasiums and the "ephebes" or those men undergoing military training.

There were ancient towns and cities that also adopted Heracles as a patron deity, contributing to the spread of his cult. There was the case of the royal house of Macedonia, which claimed lineal descent from the hero primarily for purposes of divine protection and legitimator of actions.

The earliest evidence that show the worship of Heracles in popular cult was in 6th century BC (121–122 and 160–165) via an ancient inscription from Phaleron.

Extraordinary strength, courage, ingenuity, and sexual prowess with both males and females were among the characteristics commonly attributed to him. Heracles used his wits on several occasions when his strength did not suffice, such as when laboring for the king Augeas of Elis, wrestling the giant Antaeus, or tricking Atlas into taking the sky back onto his shoulders. Together with Hermes he was the patron and protector of gymnasia and palaestrae. His iconographic attributes are the lion skin and the club. These qualities did not prevent him from being regarded as a playful figure who used games to relax from his labors and played a great deal with children. By conquering dangerous archaic forces he is said to have "made the world safe for mankind" and to be its benefactor. Heracles was an extremely passionate and emotional individual, capable of doing both great deeds for his friends (such as wrestling with Thanatos on behalf of Prince Admetus, who had regaled Heracles with his hospitality, or restoring his friend Tyndareus to the throne of Sparta after he was overthrown) and being a terrible enemy who would wreak horrible vengeance on those who crossed him, as Augeas, Neleus and Laomedon all found out to their cost. There was also a coldness to his character, which was demonstrated by Sophocles' depiction of the hero in "The Trachiniae". Heracles threatened his marriage with his desire to bring two women under the same roof, one of them was his wife Deianeira.

In the works of Euripides involving Heracles, an insight was presented regarding his character and this was through his madness. The idea was that his actions were partly driven by forces outside rational human control and that by highlighting the divine causation of his madness, it becomes a device that problematizes Heracles' character and status within the civilized context. This aspect is also highlighted in "Hercules Furens" where Seneca linked the hero's madness to an illusion and a consequence of Herakles' refusal to live a simple life offered by Amphitryon. It was indicated that he preferred the extravagant violence of the heroic life and that its ghosts eventually manifested in his madness and that the hallucinatory visions defined Herakles' character.

A major factor in the well-known tragedies surrounding Heracles is the hatred that the goddess Hera, wife of Zeus, had for him. A full account of Heracles must render it clear why Heracles was so tormented by Hera, when there were many illegitimate offspring sired by Zeus. Heracles was the son of the affair Zeus had with the mortal woman Alcmene. Zeus made love to her after disguising himself as her husband, Amphitryon, home early from war (Amphitryon did return later the same night, and Alcmene became pregnant with his son at the same time, a case of heteropaternal superfecundation, where a woman carries twins sired by different fathers). Thus, Heracles' very existence proved at least one of Zeus' many illicit affairs, and Hera often conspired against Zeus' mortal offspring as revenge for her husband's infidelities. His twin mortal brother, son of Amphitryon, was Iphicles, father of Heracles' charioteer Iolaus.

On the night the twins Heracles and Iphicles were to be born, Hera, knowing of her husband Zeus' adultery, persuaded Zeus to swear an oath that the child born that night to a member of the House of Perseus would become High King. Hera did this knowing that while Heracles was to be born a descendant of Perseus, so too was Eurystheus. Once the oath was sworn, Hera hurried to Alcmene's dwelling and slowed the birth of the twins Heracles and Iphicles by forcing Ilithyia, goddess of childbirth, to sit crosslegged with her clothing tied in knots, thereby causing the twins to be trapped in the womb. Meanwhile, Hera caused Eurystheus to be born prematurely, making him High King in place of Heracles. She would have permanently delayed Heracles' birth had she not been fooled by Galanthis, Alcmene's servant, who lied to Ilithyia, saying that Alcmene had already delivered the baby. Upon hearing this, she jumped in surprise, loosing the knots and inadvertently allowing Alcmene to give birth to Heracles and Iphicles.

Fear of Hera's revenge led Alcmene to expose the infant Heracles, but he was taken up and brought to Hera by his half-sister Athena, who played an important role as protectress of heroes. Hera did not recognize Heracles and nursed him out of pity. Heracles suckled so strongly that he caused Hera pain, and she pushed him away. Her milk sprayed across the heavens and there formed the Milky Way. But with divine milk, Heracles had acquired supernatural powers. Athena brought the infant back to his mother, and he was subsequently raised by his parents.

The child was originally given the name Alcides by his parents; it was only later that he became known as Heracles. He was renamed Heracles in an unsuccessful attempt to mollify Hera. He and his twin were just eight months old when Hera sent two giant snakes into the children's chamber. Iphicles cried from fear, but his brother grabbed a snake in each hand and strangled them. He was found by his nurse playing with them on his cot as if they were toys. Astonished, Amphitryon sent for the seer Tiresias, who prophesied an unusual future for the boy, saying he would vanquish numerous monsters.

After killing his music tutor Linus with a lyre, he was sent to tend cattle on a mountain by his foster father Amphitryon. Here, according to an allegorical parable, "The Choice of Heracles", invented by the sophist Prodicus (c. 400 BCE) and reported in Xenophon's "Memorabilia" 2.1.21–34, he was visited by two allegorical figures—Vice and Virtue—who offered him a choice between a pleasant and easy life or a severe but glorious life: he chose the latter. This was part of a pattern of "ethicizing" Heracles over the 5th century BCE.

Later in Thebes, Heracles married King Creon's daughter, Megara. In a fit of madness, induced by Hera, Heracles killed his children by Megara. After his madness had been cured with hellebore by Antikyreus, the founder of Antikyra, he realized what he had done and fled to the Oracle of Delphi. Unbeknownst to him, the Oracle was guided by Hera. He was directed to serve King Eurystheus for ten years and perform any task Eurystheus required of him. Eurystheus decided to give Heracles ten labours, but after completing them, Heracles was cheated by Eurystheus when he added two more, resulting in the Twelve Labors of Heracles.

Driven mad by Hera, Heracles slew his own children. To expiate the crime, Heracles was required to carry out ten labours set by his archenemy, Eurystheus, who had become king in Heracles' place. If he succeeded, he would be purified of his sin and, as myth says, he would become a god, and be granted immortality.

Despite the difficulty, Heracles accomplished these tasks, but Eurystheus in the end did not accept the success the hero had with two of the labours: the cleansing of the Augean stables, because Heracles was going to accept pay for the labour; and the killing of the Lernaean Hydra, as Heracles' brother, Iolaus, had helped him burn the stumps of the multiplying heads.

Eurystheus set two more tasks, fetching the Golden Apples of Hesperides and capturing Cerberus. In the end, with ease, the hero successfully performed each added task, bringing the total number of labours up to the magic number twelve.

Not all versions and writers give the labours in the same order. The "Bibliotheca" (2.5.1–2.5.12) gives the following order:













After completing these tasks, Heracles joined the Argonauts in a search for the Golden Fleece. He also fell in love with Princess Iole of Oechalia. King Eurytus of Oechalia promised his daughter, Iole, to whoever could beat his sons in an archery contest. Heracles won but Eurytus abandoned his promise. Heracles' advances were spurned by the king and his sons, except for one: Iole's brother Iphitus. Heracles killed the king and his sons—excluding Iphitus—and abducted Iole. Iphitus became Heracles' best friend. However, once again, Hera drove Heracles mad and he threw Iphitus over the city wall to his death. Once again, Heracles purified himself through three years of servitude—this time to Queen Omphale of Lydia.

Omphale was a queen or princess of Lydia. As penalty for a murder, imposed by Xenoclea, the Delphic Oracle, Heracles was to serve as her slave for a year. He was forced to do women's work and to wear women's clothes, while she wore the skin of the Nemean Lion and carried his olive-wood club. After some time, Omphale freed Heracles and married him. Some sources mention a son born to them who is variously named. It was at that time that the cercopes, mischievous wood spirits, stole Heracles' weapons. He punished them by tying them to a stick with their faces pointing downward.

While walking through the wilderness, Heracles was set upon by the Dryopes. In Apollonius of Rhodes' "Argonautica" it is recalled that Heracles had mercilessly slain their king, Theiodamas, over one of the latter's bulls, and made war upon the Dryopes "because they gave no heed to justice in their lives". After the death of their king, the Dryopes gave in and offered him Prince Hylas. He took the youth on as his weapons bearer and beloved. Years later, Heracles and Hylas joined the crew of the "Argo". As Argonauts, they only participated in part of the journey. In Mysia, Hylas was kidnapped by the nymphs of a local spring. Heracles, heartbroken, searched for a long time but Hylas had fallen in love with the nymphs and never showed up again. In other versions, he simply drowned. Either way, the "Argo" set sail without them.

Hesiod's "Theogony" and Aeschylus' "Prometheus Unbound" both tell that Heracles shot and killed the eagle that tortured Prometheus (which was his punishment by Zeus for stealing fire from the gods and giving it to mortals). Heracles freed the Titan from his chains and his torments. Prometheus then made predictions regarding further deeds of Heracles.

On his way back to Mycenae from Iberia, having obtained the Cattle of Geryon as his tenth labour, Heracles came to Liguria in North-Western Italy where he engaged in battle with two giants, Albion and Bergion or Dercynus, sons of Poseidon. The opponents were strong; Hercules was in a difficult position so he prayed to his father Zeus for help. Under the aegis of Zeus, Heracles won the battle. It was this kneeling position of Heracles when he prayed to his father Zeus that gave the name Engonasin (""Εγγόνασιν"", derived from "εν γόνασιν"), meaning "on his knees" or "the Kneeler",
to the constellation known as Heracles' constellation. The story, among others, is described by Dionysius of Halicarnassus.

Before Homer's Trojan War, Heracles had made an expedition to Troy and sacked it. Previously, Poseidon had sent a sea monster to attack Troy. The story is related in several digressions in the "Iliad" (7.451–453, 20.145–148, 21.442–457) and is found in pseudo-Apollodorus' Bibliotheke (2.5.9). This expedition became the theme of the Eastern pediment of the Temple of Aphaea. Laomedon planned on sacrificing his daughter Hesione to Poseidon in the hope of appeasing him. Heracles happened to arrive (along with Telamon and Oicles) and agreed to kill the monster if Laomedon would give him the horses received from Zeus as compensation for Zeus' kidnapping Ganymede. Laomedon agreed. Heracles killed the monster, but Laomedon went back on his word. Accordingly, in a later expedition, Heracles and his followers attacked Troy and sacked it. Then they slew all Laomedon's sons present there save Podarces, who was renamed Priam, who saved his own life by giving Heracles a golden veil Hesione had made. Telamon took Hesione as a war prize and they had a son, Teucer.

After Heracles had performed his Labours, gods told him that before he passed into the company of the gods, he should create a colony at Sardinia and make his sons, who had with the daughters of Thespius, the leaders of the settlement. When his sons became adults, he sent them together with Iolaus to the island.



This is described in Sophocles's "Trachiniae" and in Ovid's "Metamorphoses" Book IX. Having wrestled and defeated Achelous, god of the Acheloos river, Heracles takes Deianira as his wife. Travelling to Tiryns, a centaur, Nessus, offers to help Deianira across a fast flowing river while Heracles swims it. However, Nessus is true to the archetype of the mischievous centaur and tries to steal Deianira away while Heracles is still in the water. Angry, Heracles shoots him with his arrows dipped in the poisonous blood of the Lernaean Hydra. Thinking of revenge, Nessus gives Deianira his blood-soaked tunic before he dies, telling her it will "excite the love of her husband".

Several years later, rumor tells Deianira that she has a rival for the love of Heracles. Deianira, remembering Nessus' words, gives Heracles the bloodstained shirt. Lichas, the herald, delivers the shirt to Heracles. However, it is still covered in the Hydra's blood from Heracles' arrows, and this poisons him, tearing his skin and exposing his bones. Before he dies, Heracles throws Lichas into the sea, thinking he was the one who poisoned him (according to several versions, Lichas turns to stone, becoming a rock standing in the sea, named for him). Heracles then uproots several trees and builds a funeral pyre on Mount Oeta, which Poeas, father of Philoctetes, lights. As his body burns, only his immortal side is left. Through Zeus' apotheosis, Heracles rises to Olympus as he dies.

No one but Heracles' friend Philoctetes (Poeas in some versions) would light his funeral pyre (in an alternative version, it is Iolaus who lights the pyre). For this action, Philoctetes or Poeas received Heracles' bow and arrows, which were later needed by the Greeks to defeat Troy in the Trojan War.

Philoctetes confronted Paris and shot a poisoned arrow at him. The Hydra poison subsequently led to the death of Paris. The Trojan War, however, continued until the Trojan Horse was used to defeat Troy.

According to Herodotus, Heracles lived 900 years before Herodotus' own time (c. 1300 BCE).

During the course of his life, Heracles married four times.


An episode of his female affairs that stands out was his stay at the palace of Thespius, king of Thespiae, who wished him to kill the Lion of Cithaeron. As a reward, the king offered him the chance to perform sexual intercourse with all fifty of his daughters in one night. Heracles complied and they all became pregnant and all bore sons. This is sometimes referred to as his Thirteenth Labour. Many of the kings of ancient Greece traced their lines to one or another of these, notably the kings of Sparta and Macedon.

Yet another episode of his female affairs that stands out was when he carried away the oxen of Geryon, he also visited the country of the Scythians. Once there, while asleep, his horses suddenly disappeared. When he woke and wandered about in search of them, he came into the country of Hylaea. He then found the dracaena of Scythia (sometimes identified as Echidna) in a cave. When he asked whether she knew anything about his horses, she answered, that they were in her own possession, but that she would not give them up, unless he would consent to stay with her for a time. Heracles accepted the request, and became by her the father of Agathyrsus, Gelonus, and Scythes. The last of them became king of the Scythians, according to his father's arrangement, because he was the only one among the three brothers that was able to manage the bow which Heracles had left behind and to use his father's girdle. In some versions, the Scythian echidna's children by him are known as the Dracontidae and were the ancestors of a House of Cadmus.

Dionysius of Halicarnassus writes that Heracles and Lavinia, daughter of Evander, had a son named Pallas.

As a symbol of masculinity and warriorship, Heracles also had a number of male lovers. Plutarch, in his "Eroticos," maintains that Heracles' male lovers were beyond counting. Of these, the one most closely linked to Heracles is the Theban Iolaus. According to a myth thought to be of ancient origins, Iolaus was Heracles' charioteer and squire. Heracles in the end helped Iolaus find a wife. Plutarch reports that down to his own time, male couples would go to Iolaus's tomb in Thebes to swear an oath of loyalty to the hero and to each other.

One of Heracles' male lovers, and one represented in ancient as well as modern art, is Hylas. Though it is of more recent vintage (dated to the 3rd century) than that with Iolaus, it had themes of mentoring in the ways of a warrior and help finding a wife in the end. There is nothing in Apollonius's account that suggests that Hylas was a sexual lover as opposed to a companion and servant.

Another reputed male lover of Heracles is Elacatas, who was honored in Sparta with a sanctuary and yearly games, Elacatea. The myth of their love is an ancient one.

Abdera's eponymous hero, Abderus, was another of Heracles' lovers. He was said to have been entrusted with—and slain by—the carnivorous mares of Thracian Diomedes. Heracles founded the city of Abdera in Thrace in his memory, where he was honored with athletic games.

Another myth is that of Iphitus.

Another story is the one of his love for Nireus, who was "the most beautiful man who came beneath Ilion" ("Iliad", 673). But Ptolemy adds that certain authors made Nireus out to be a son of Heracles.

Pausanias makes mention of Sostratus, a youth of Dyme, Achaea, as a lover of Heracles. Sostratus was said to have died young and to have been buried by Heracles outside the city. The tomb was still there in historical times, and the inhabitants of Dyme honored Sostratus as a hero. The youth seems to have also been referred to as Polystratus.

There is also a series of lovers who are either later inventions or purely literary conceits. Among these are Admetus, who assisted in the hunt for the Calydonian Boar, Adonis, Corythus, and Nestor who was said to have been loved for his wisdom. His role as lover was perhaps to explain why he was the only son of Neleus to be spared by the hero.

A scholiast on "Argonautica" lists the following male lovers of Heracles: "Hylas, Philoctetes, Diomus, Perithoas, and Phrix, after whom a city in Libya was named". Diomus is also mentioned by Stephanus of Byzantium as the eponym of the deme Diomeia of the Attic phyle Aegeis: Heracles is said to have fallen in love with Diomus when he was received as guest by Diomus' father Collytus. Perithoas and Phrix are otherwise unknown, and so is the version that suggests a sexual relationship between Heracles and Philoctetes.

All of Heracles' marriages and almost all of his heterosexual affairs resulted in births of a number of sons and at least four daughters.
One of the most prominent is Hyllus, the son of Heracles and Deianeira or Melite. The term "Heracleidae", although it could refer to all of Heracles' children and further descendants, is most commonly used to indicate the descendants of Hyllus, in the context of their lasting struggle for return to Peloponnesus, out of where Hyllus and his brothers—the children of Heracles by Deianeira—were thought to have been expelled by Eurystheus.

The children of Heracles by Megara are collectively well known because of their ill fate, but there is some disagreement among sources as to their number and individual names. Apollodorus lists three, Therimachus, Creontiades and Deicoon; to these Hyginus adds Ophitus and, probably by mistake, Archelaus, who is otherwise known to have belonged to the Heracleidae, but to have lived several generations later. A scholiast on Pindar' s odes provides a list of seven completely different names: Anicetus, Chersibius, Mecistophonus, Menebrontes, Patrocles, Polydorus, Toxocleitus.

Other well-known children of Heracles include Telephus, king of Mysia (by Auge), and Tlepolemus, one of the Greek commanders in the Trojan War (by Astyoche).

According to Herodotus, a line of 22 Kings of Lydia descended from Hercules and Omphale. The line was called Tylonids after his Lydian name.

The divine sons of Heracles and Hebe are Alexiares and Anicetus.

In Rome, Heracles was honored as "Hercules", and had a number of distinctively Roman myths and practices associated with him under that name.

Herodotus connected Heracles to the Egyptian god Shu. Also he was associated with Khonsu, another Egyptian god who was in some ways similar to Shu. As Khonsu, Heracles was worshipped at the now sunken city of Heracleion, where a large temple was constructed.

Most often the Egyptians identified Heracles with Heryshaf, transcribed in Greek as "Arsaphes" or "Harsaphes" (Ἁρσαφής). He was an ancient ram-god whose cult was centered in Herakleopolis Magna.

Via the Greco-Buddhist culture, Heraclean symbolism was transmitted to the Far East. An example remains to this day in the Nio guardian deities in front of Japanese Buddhist temples.

Herodotus also connected Heracles to Phoenician god Melqart.

Sallust mentions in his work on the Jugurthine War that the Africans believe Heracles to have died in Spain where, his multicultural army being left without a leader, the Medes, Persians, and Armenians who were once under his command split off and populated the Mediterranean coast of Africa.

Temples dedicated to Heracles abounded all along the Mediterranean coastal countries. For example, the temple of "Heracles Monoikos" (i.e. the lone dweller), built far from any nearby town upon a promontory in what is now the Côte d'Azur, gave its name to the area's more recent name, Monaco.

The gateway to the Mediterranean Sea from the Atlantic Ocean, where the southernmost tip of Spain and the northernmost of Morocco face each other is, classically speaking, referred to as the Pillars of Hercules/Heracles, owing to the story that he set up two massive spires of stone to stabilise the area and ensure the safety of ships sailing between the two landmasses.

In various languages, variants of Hercules' name are used as a male given name, such as Hercule in French, Hércules in Spanish, Iraklis () in Modern Greek and Irakli () in Georgian.

There are many teams around the world that have this name or have Heracles as their symbol. The most popular in Greece is G.S. Iraklis Thessaloniki.

"Heracleum" is a genus of flowering plants in the carrot family Apiaceae. Some of the species in this genus are quite large. In particular, the giant hogweed ("Heracleum mantegazzianum") is exceptionally large, growing up to 5 m tall.









</doc>
<doc id="13820" url="https://en.wikipedia.org/wiki?curid=13820" title="Henry Rollins">
Henry Rollins

Henry Lawrence Garfield (born February 13, 1961), better known by his stage name Henry Rollins, is an American musician, actor, writer, television and radio host, and comedian. He hosts a weekly radio show on KCRW, and is a regular columnist for "Rolling Stone Australia" and was a regular columnist for "LA Weekly".

After performing in the short-lived Washington, D.C. band State of Alert in 1980, Rollins fronted the California hardcore punk band Black Flag from August 1981 until mid-1986. Following the band's breakup, Rollins established the record label and publishing company 2.13.61 to release his spoken word albums, and formed the Rollins Band, which toured with a number of lineups from 1987 until 2003, and during 2006.

Since Black Flag disbanded, Rollins has hosted numerous radio shows, such as "Harmony in My Head" on Indie 103, and television shows such as "The Henry Rollins Show", MTV's "120 Minutes", and "Jackass". He had recurring dramatic roles in the second season of "Sons of Anarchy", in the final seasons of the animated series "The Legend of Korra" as Zaheer, and has also had roles in several films. Rollins has campaigned for various political causes in the United States, including promoting LGBT rights, World Hunger Relief, the West Memphis Three and an end to war in particular.

Rollins was born in Washington, D.C., the only child of Iris and Paul Garfield. Rollins is of Jewish ancestry through his father. His great-grandfather Henry Luban (born Henach Luban) fled from the East Latvian town of Rēzekne, then part of the Russian Empire, into the United States. When he was three years old, his parents divorced and he was raised by his mother in Glover Park, an affluent neighborhood of Washington. As a child and teenager, Rollins was sexually assaulted. He suffered from depression and low self-esteem. In the fourth grade, he was diagnosed with hyperactivity and took Ritalin for several years so that he could focus during school. He attended The Bullis School, then an all-male preparatory school in Potomac, Maryland. According to Rollins, the Bullis School helped him to develop a sense of discipline and a strong work ethic. It was at Bullis that he began writing. In 1987, Rollins said he had not seen his father since he was 18.

Rollins has said that he does not have religious or spiritual beliefs, though he does not consider himself an atheist. He has mostly avoided recreational drugs throughout his life, including alcohol, but has admitted to trying acid. Rollins is childless by choice, and has not been in a romantic relationship since he was in his 20s. He considers himself a solitary person, and maintains few deep relationships outside of his professional ones. One of his closest personal friends is musician Ian MacKaye: the two have been close since they met as children in Washington, D.C. Rollins also enjoys a friendship with the actor William Shatner which developed after he performed on Shatner's album, "Has Been".

After high school, Rollins attended American University in Washington D.C. for one semester, but dropped out in December 1979. He began working minimum-wage jobs, including a job as a courier for kidney samples at the National Institutes of Health. Rollins developed an interest in punk rock after he and his friend Ian MacKaye procured a copy of The Ramones' eponymous debut album; he later described it as "akin to shooting heroin." From 1979 to 1980, Rollins was working as a roadie for Washington bands, including Teen Idles. When the band's singer Nathan Strejcek failed to appear for practice sessions, Rollins convinced the Teen Idles to let him sing. Word of Rollins's ability spread around the punk rock scene in Washington; Bad Brains singer H.R. would sometimes get Rollins on stage to sing with him.

In 1980, the Washington punk band the Extorts lost their frontman Lyle Preslar to Minor Threat. Rollins joined the others of the band to form State of Alert (S.O.A.), and became its frontman and vocalist. He put words to the band's five songs and wrote several more. S.O.A. recorded their sole EP, "No Policy", and released it in 1981 on MacKaye's Dischord Records.

Around April 1981, drummer Simon Jacobsen was replaced by Ivor Hanson. At the time, Hanson's father was a top admiral in the US Navy and his family shared living quarters with the Vice President of the United States in the United States Naval Observatory. The band held their practices there and would have to be let in by United States Secret Service agents.

S.O.A. disbanded after a total of a dozen concerts and one EP. Rollins had enjoyed being the band's frontman, and had earned a reputation for fighting in shows. He later said, "I was like nineteen and a young man all full of steam and "loved" to get in the dust-ups." By this time, Rollins had become the assistant manager of the Georgetown Häagen-Dazs ice cream store; his steady employment had helped to finance the S.O.A. EP.

In 1980, a friend gave Rollins and MacKaye a copy of Black Flag's "Nervous Breakdown" EP. Rollins soon became a fan of the band, exchanging letters with bassist Chuck Dukowski and later inviting the band to stay in his parents' home when Black Flag toured the East Coast in December 1980. When Black Flag returned to the East Coast in 1981, Rollins attended as many of their concerts as he could. At an impromptu show in a New York bar, Black Flag's vocalist Dez Cadena allowed Rollins to sing "Clocked In", a song Rollins had asked the band to play in light of the fact that he had to drive back to Washington, D.C. to begin work.

Unbeknownst to Rollins, Cadena wanted to switch to guitar, and the band was looking for a new vocalist. The band was impressed with Rollins' singing and stage demeanor, and the next day, after a semi-formal audition at Tu Casa Studio in New York City, they asked him to become their permanent vocalist. Despite some doubts, he accepted, in part because of MacKaye's encouragement. His high level of energy and intense personality suited the band's style, but Rollins' diverse tastes in music were a key factor in his being selected as singer; Black Flag's founder Greg Ginn was growing restless creatively and wanted a singer who was willing to move beyond simple, three-chord punk.
After joining Black Flag in 1981, Rollins quit his job at Häagen-Dazs, sold his car, and moved to Los Angeles. Upon arriving in Los Angeles, Rollins got the Black Flag logo tattooed on his left biceps and also on the back of his neck, chose the stage name of Rollins, a surname he and MacKaye had used as teenagers. Rollins played his first show with Black Flag on August 21, 1981 at Cuckoo's Nest in Costa Mesa, California. Rollins was in a different environment in Los Angeles; the police soon realized he was a member of Black Flag, and he was hassled as a result. Rollins later said: "That really scared me. It freaked me out that an adult would do that. [...] My little eyes were opened big time."

Before concerts, as the others of the band tuned up, Rollins would stride about the stage dressed only in a pair of black shorts, grinding his teeth; to focus before the show, he would squeeze a pool ball. His stage persona impressed several critics; after a 1982 show in Anacortes, Washington, "Sub Pop" critic Calvin Johnson wrote: "Henry was incredible. Pacing back and forth, lunging, lurching, growling; it was all real, the most intense emotional experiences I have ever seen."

By 1983, Rollins' stage persona was increasingly alienating him from the rest of Black Flag. During a show in England, Rollins assaulted a member of the audience who attacked Ginn; Ginn later scolded Rollins, calling him a "macho asshole". A legal dispute with Unicorn Records held up further Black Flag releases until 1984, and Ginn was slowing the band's tempo down so that they would remain innovative. In August 1983, guitarist Dez Cadena had left the band; a stalemate lingered between Dukowski and Ginn, who wanted Dukowski to leave, before Ginn fired Dukowski outright. 1984's heavy metal music-influenced "My War" featured Rollins screaming and wailing throughout many of the songs; the band's members also grew their hair to confuse the band's hardcore punk audience.

Black Flag's change in musical style and appearance alienated many of their original fans, who focused their displeasure on Rollins by punching him in the mouth, stabbing him with pens, or scratching him with their nails, among other methods. He often fought back, dragging audience members on stage and assaulting them. During a Black Flag concert, Rollins repeatedly punched a fan in the face who had continuously reached for his microphone. Rollins became increasingly alienated from the audience; in his tour diary, Rollins wrote "When they spit at me, when they grab at me, they aren't hurting me. When I push out and mangle the flesh of another, it's falling so short of what I really want to do to them." During the Unicorn legal dispute, Rollins had started a weight-lifting program, and by their 1984 tours, he had become visibly well-built; journalist Michael Azerrad later commented that "his powerful physique was a metaphor for the impregnable emotional shield he was developing around himself." Rollins has since replied that "no, the training was just basically a way to push myself."

Before Black Flag disbanded in August 1986, Rollins had already toured as a solo spoken word artist. He released two solo records in 1987, "Hot Animal Machine", a collaboration with guitarist Chris Haskett, and "Drive by Shooting", recorded as "Henrietta Collins and the Wifebeating Childhaters"; Rollins also released his second spoken word album, "Big Ugly Mouth" in the same year. Along with Haskett, Rollins soon added Andrew Weiss and Sim Cain, both former members of Ginn's side-project Gone, and called the new group Rollins Band. The band toured relentlessly, and their 1987 debut album, "Life Time", was quickly followed by the outtakes and live collection "Do It". The band continued to tour throughout 1988; in 1989 another Rollins Band album, "Hard Volume" was released. Another live album, "Turned On", and another spoken word release, "Live at McCabe's", followed in 1990.

In 1991, the Rollins Band signed a distribution deal with Imago Records and appeared at the Lollapalooza festival; both improved the band's presence. However, in December 1991, Rollins and his best friend Joe Cole were accosted by two armed robbers outside Rollins's home. Cole was murdered by a gunshot to the head, Rollins escaped without injury but police suspected him in the murder and detained him for ten hours. Although traumatized by Cole's death, as chronicled in his book "Now Watch Him Die", Rollins continued to release new material; the spoken-word album "Human Butt" appeared in 1992 on his own record label, 2.13.61. The Rollins Band released "The End of Silence", Rollins's first charting album.

The following year, Rollins released a spoken-word double album, "The Boxed Life". The Rollins Band embarked upon the "End of Silence" tour; bassist Weiss was fired towards its end and replaced by funk and jazz bassist Melvin Gibbs. According to critic Steve Huey, 1994 was Rollins's "breakout year". The Rollins Band appeared at Woodstock 94 and released "Weight", which ranked on the Billboard Top 40. Rollins released "Get in the Van: On the Road with Black Flag", a double-disc set of him reading from his Black Flag tour diary of the same name; he won the Grammy for Best Spoken Word Recording as a result. Rollins was named 1994's "Man of the Year" by the American men's magazine "Details" and became a contributing columnist to the magazine. With the increased exposure, Rollins made several appearances on American music channels MTV and VH1 around this time, and made his Hollywood film debut in 1994 in "The Chase" playing a police officer.

In 1995, the Rollins Band's record label, Imago Records, declared itself bankrupt. Rollins began focusing on his spoken word career. He released "Everything", a recording of a chapter of his book "Eye Scream" with free jazz backing, in 1996. He continued to appear in various films, including "Heat", "Johnny Mnemonic" and "Lost Highway". The Rollins Band signed to Dreamworks Records in 1997 and soon released "Come in and Burn", but it did not receive as much critical acclaim as their previous material. Rollins continued to release spoken-word book readings, releasing "Black Coffee Blues" in the same year. In 1998, Rollins released "Think Tank", his first set of non-book-related spoken material in five years.

By 1998, Rollins felt that the relationship with his backing band had run its course, and the line-up disbanded. He had produced a Los Angeles hard rock band called Mother Superior, and invited them to form a new incarnation of the Rollins Band. Their first album, "Get Some Go Again", was released two years later. The Rollins Band released several more albums, including 2001's "Nice" and 2003's "". After 2003, the band became inactive as Rollins focused on radio and television work. During a 2006 appearance on "Tom Green Live!", Rollins stated that he "may never do music again", a feeling which he reiterated in 2011 when talking to "Trebuchet" magazine. In an interview with "Culture Brats", Rollins admitted he had sworn off music for good – "... and I must say that I miss it every day. I just don't know honestly what I could do with it that's different."

In 2014, Rollins admitted a disdain for rehashing old music for the sake of it – "I don't want to play old music. To me, it is fighting battles that are already over and calling yourself a warrior. For me, I see no courage or adventure in doing the old thing over again. If others want to, that's for them. For myself, I have to move on. Life is too short to live in the past. There is a lot to be done." On the same topic, Rollins more recently said in 2016 "For me, music was a time and a place. I never really enjoyed being in a band. It was in me and it needed to come out, like a 25-year exorcism. One day, I woke up, and I didn't have any more lyrics. I just had nothing to contribute to the form, and I was done with band practice and traveling in groups." 

Rollins is a guest star on Damian Cowell's 2017 album "Get Yer Dag On!".

As a vocalist, Rollins has adopted a number of styles through the years. He was noted in the Washington, D.C. hardcore scene for what journalist Michael Azerrad described as a "compelling, raspy howl." With State of Alert, Rollins "spat out the lyrics like a bellicose auctioneer." He adopted a similar style after joining Black Flag in 1981. By their album "Damaged", however, Black Flag began to incorporate a swing beat into their style. Rollins then abandoned his State of Alert "bark" and adopted the band's swing. Rollins later explained: "What I was doing kind of matched the vibe of the music. The music was intense and, well, I was as intense as you needed."

In both incarnations of the Rollins Band, Rollins combined spoken word with his traditional vocal style in songs such as "Liar" (the song begins with a one-minute spoken diatribe by Rollins), barked his way through songs (such as "Tearing" and "Starve"), and employed the loud-quiet dynamic. "Rolling Stone"<nowiki>'</nowiki>s Anthony DeCurtis names Rollins a "screeching hate machine" and his "hallmark" as "the sheets-of-sound assault".

With the Rollins Band, his lyrics focused "almost exclusively on issues relating to personal integrity," according to critic Geoffrey Welchman.
In the 1980s, Rollins produced an album of acoustic songs for the famed convict Charles Manson titled "Completion". The record was supposed to be released by SST Records, but the project was canceled because the label received death threats for working with Manson. Only five test presses of "Completion" were pressed, two of which remain in Rollins' possession.

In 1995, Rollins produced Australian hard rock band the Mark of Cain's third full-length album "Ill at Ease".

Rollins and his best friend Joe Cole, son of actor Dennis Cole, were involved in a shooting when they were assaulted by robbers in December 1991 outside their shared Venice Beach, California home. Cole died after being shot in the face, but Rollins escaped. The murder remains unsolved.

In an April 1992 "Los Angeles Times" interview, Rollins revealed he kept a plastic container full of soil soaked with the blood of Cole. Rollins said "I dug up all the earth where his head fell—he was shot in the face—and I've got all the dirt here, and so Cole's in the house. I say good morning to him every day. I got his phone, too, so I got a direct line to him. So that feels good."

In a 2001 interview with Howard Stern, Rollins was asked about rumors that he had Cole's brain in his house. Rollins stated that he only has the soil from the spot Cole was killed. During the interview, Rollins also speculated that the reason they were targeted may have been because, days prior to the incident, record producer Rick Rubin – a fan of Rollins Band – had requested to hear the then newly recorded album "The End of Silence" and parked his Rolls-Royce outside their Venice Beach house while carrying a cell phone. Because of the notoriety of the neighborhood, Rollins suspected that this would bring trouble because of the implication that there was money in the home. He even wrote in his journal the night of Rubin's visit: "My place is going to get popped."

Rollins has included Cole's story in his spoken word performances.

As Rollins rose to prominence with the Rollins Band, he began to present and appear on television. These included "Alternative Nation" and "MTV Sports" in 1993 and 1994 respectively. Rollins also co starred in "The Chase" with Charlie Sheen. In 1995 Rollins appeared on an episode of "Unsolved Mysteries" that explored the murder of his best friend Joe Cole and present "State of the Union Undressed" on Comedy Central. Rollins began to present and narrate "VH1 Legends" in 1996. Rollins, busy with the Rollins Band, did not present more programs until 2001, but made appearances on a number of other television shows, including "Welcome to Paradox" in 1998 in the episode "All Our Sins Forgotten", as a therapist who develops a device that can erase the bad memories of his patients. Rollins also voiced Mad Stan in "Batman Beyond" in 1999 and 2000.

Rollins was a host of film review programme "Henry's Film Corner" on the Independent Film Channel, before presenting the weekly "The Henry Rollins Show" on the channel. "The Henry Rollins Show" is now being shown weekly on Film24 along with "Henry Rollins Uncut". The show also lead to a promotional tour in Europe that led to Rollins being dubbed a "bad boy goodwill ambassador" by a NY reviewer. He also hosted Fox's short-lived 2001 horror anthology "Night Visions".

In 2002, Rollins guest-starred on an episode of the sitcom "The Drew Carey Show" as a man whom Oswald found on eBay and paid to come to his house and "kick his ass". He co-hosted the British television show "Full Metal Challenge", in which teams built vehicles to compete in various driving and racing contests, from 2002 to 2003 on Channel 4 and TLC. He has made a number of cameo appearances in television series such as MTV<nowiki>'</nowiki>s "Jackass" and an episode of "Californication", where he played himself hosting a radio show. In 2006, Rollins appeared in a documentary series by VH1 and The Sundance Channel called "The Drug Years".

Rollins appears in FX's "Sons of Anarchy's" second season, which premiered in the fall of 2009 in the United States. Rollins plays A.J. Weston, a white supremacist gang leader and new antagonist in the show's fictional town of Charming, California, who poses a deadly threat to the Sons of Anarchy Motorcycle Club. In 2009, Rollins voiced "Trucker" in "American Dad!'s" fourth season (episode eight). Rollins voiced Benjamin Knox/Bonk in the 2000 animated film "".

In 2010, Rollins appeared as a guest judge on Season 2 episode 6 of "RuPaul's Drag Race". In 2011, he was interviewed in the "National Geographic Explorer" episode "Born to Rage", regarding his possible link to the MAOA gene (warrior gene) and violent behavior. In 2012, he hosted the "National Geographic Wild" series "Animal Underworld", investigating where the real boundaries lay in human-animal relationships. Rollins also appeared in the Hawaii Five-0 episode "Hoʻopio" that aired on May 6, 2013.

In November 2013, Rollins started hosting the show "10 Things You Don't Know About" on the History Channel's H2. In 2014, he voiced the antagonist Zaheer in the third season of the animated series "The Legend of Korra".

Rollins played the part of Lt. Mueller in episodes 1-3 of the fourth season of the TV series Z Nation, which originally aired on Syfy in 2017.

In 2019, Rollins began appearing as a disillusioned poisons instructor in the TV series Deadly Class.

On May 19, 2004, Rollins began hosting a weekly radio show, "Harmony in My Head", on Indie 103.1 radio in Los Angeles. The show aired every Monday evening, with Rollins playing music ranging from early rock and jump blues to hard rock, blues rock, folk rock, punk rock, heavy metal and rockabilly, and touching on hip hop, jazz, world music, reggae, classical music and more. "Harmony in my Head" often emphasizes B-sides, live bootlegs and other rarities, and nearly every episode has featured a song either by the Beastie Boys or British group The Fall.

Rollins put the show on a short hiatus to undertake a spoken-word tour in early 2005. Rollins posted playlists and commentary on-line; these lists were expanded with more information and published in book form as "Fanatic!" through 2.13.61 in November 2005. In late 2005, Rollins announced the show's return and began the first episode by playing the show's namesake Buzzcocks song. In 2008, the show was continuing each week despite Rollins's constant touring with new pre-recorded shows between live broadcasts. In 2009, Indie 103.1 went off the air, although it continues to broadcast over the Internet.

In 2007, Rollins published "Fanatic! Vol. 2" through 2.13.61. "Fanatic! Vol. 3" was released in the fall of 2008. On February 18, 2009, KCRW announced that Rollins would be hosting a live show on Saturday nights starting March 7, 2009, which has since been moved to Sunday nights at 8PM. In 2011 Rollins was interviewed on Episode 121 of American Public Media's podcast, "The Dinner Party Download", posted on November 3, 2011.

Rollins began his film career appearing in several independent films featuring the band Black Flag. His film debut was in 1982's "The Slog Movie", about the West Coast punk scene. An appearance in 1985's "Black Flag Live" followed. Rollins' first film appearance without Black Flag was the short film "The Right Side of My Brain" with Lydia Lunch in 1985. Following the band's breakup, Rollins did not appear in any films until 1994's "The Chase". Rollins appeared in the 2007 direct-to-DVD sequel to "Wrong Turn" (2003), "" as a retired Marine Corps officer who hosts his own show which tests the contestants' will to survive. Rollins has also appeared in "Punk: Attitude", a documentary on the punk scene, and in "American Hardcore" (2006). In 2012, Rollins appeared in a short documentary entitled "Who Shot Rock and Roll" discussing the early punk scene in Los Angeles as well as photographs of himself in Black Flag taken by esteemed photographer Edward Colver.

Some feature-length movies Rollins has appeared in include:

Rollins has written a variety of books, including "Black Coffee Blues", "Do I Come Here Often?", "The First Five" (a compilation of "High Adventure in the Great Outdoors", "Pissing in the Gene Pool", "Bang!", "Art to Choke Hearts", and "One From None"), "See a Grown Man Cry", "Now Watch Him Die", "Smile, You're Traveling", "Get in the Van", "Eye Scream", "Broken Summers", "Roomanitarian", and "Solipsist".

For the audiobook version of the 2006 novel "World War Z" Rollins voiced the character of T. Sean Collins, a mercenary hired to protect celebrities during a mass panic caused by an onslaught of the undead. Rollins' other audiobook recordings include "3:10 to Yuma" and his own autobiographical book "Get in the Van", for which he won a Grammy Award.

In September 2008, Rollins began contributing to the "Politics & Power" blog at the online version of "Vanity Fair" magazine. Since March 2009, his posts have appeared under their own sub-title, "Straight Talk Espresso". His posts consistently direct harsh criticism at conservative politicians and pundits, although he does occasionally target those on the left. In August 2010, he began writing a music column for "LA Weekly" in Los Angeles. In 2012, Rollins began publishing articles with "The Huffington Post" and alternative news website "WordswithMeaning!". In the months leading up to the 2012 United States Presidential election, Rollins broadcast a YouTube series called "Capitalism 2012", in which he toured the capital cities of the US states, interviewing people about current issues.

Rollins also has toured all over the world doing spoken word performances and his shows frequently last for over three hours. His spoken word style encompasses stand up comedy, accounts of experiences he's had in the world of music and during his extensive travels around the globe, self-deprecating stories about his own shortcomings, introspective recollections from his own life (such as the death of his friend, Joe Cole), commentaries on society and playful, sometimes vulgar, anecdotes.

Rollins was a playable character in both "" and "". Rollins is also the voice of Mace Griffin in "".

Rollins has become an outspoken human rights activist, most vocally for gay rights. In high school, a gay classmate of Rollins' was bullied by classmates to the point of attempting suicide. Rollins has cited this as the main catalyst of his "anti-homophobia." Rollins frequently speaks out on justice on his spoken word tours and promotes equality, regardless of sexuality. He was the host of the WedRock benefit concert, which raised money for a pro-gay-marriage organization.

During the Iraq War, he started touring with the United Service Organizations to entertain troops overseas while remaining against the war, leading him to once cause a stir at a base in Kyrgyzstan when he told the crowd: "Your commander would never lie to you. That's the vice president's job." Rollins believes it is important that he performs to the troops so that they have multiple points of contact with the other parts of the world, stating that "they can get really cut loose from planet earth." He has made eight tours, including visits to bases in Djibouti, Kuwait, Iraq, Kyrgyzstan, Afghanistan (twice), Egypt, Turkey, Qatar, Honduras, Japan, Korea and the United Arab Emirates.

He has also been active in the campaign to free the "West Memphis Three"—three young men who were believed by their supporters to have been wrongfully convicted of murder, and who have since been released from prison, but not exonerated. Rollins appears with Public Enemy frontman Chuck D on the Black Flag song "Rise Above" on the benefit album "", the first time Rollins had performed Black Flag's material since 1986.

Continuing his activism on behalf of US troops and veterans, Rollins joined "Iraq and Afghanistan Veterans of America" (IAVA) in 2008 to launch a public service advertisement campaign, CommunityofVeterans.org, which helps veterans coming home from war reintegrate into their communities. In April 2009, Rollins helped IAVA launch the second phase of the campaign which engages the friends and family of Iraq and Afghanistan veterans at SupportYourVet.org.

On December 3, 2009, Rollins wrote of his support for the victims of the Bhopal disaster in India, in an article for "Vanity Fair" 25 years–to the day–after the methyl isocyanate gas leak from the Union Carbide Corporation's pesticide factory exposed more than half a million local people to poisonous gas and resulted in the death of 17,000. He spent time in Bhopal with the people, to listen to their stories. In a later radio interview in February 2010 Rollins summed up his approach to activism, "This is where my anger takes me, to places like this, not into abuse but into proactive, clean movement."

Rollins is an advocate for the legalization of cannabis. Rollins has stated he does not personally consume cannabis, but views the issue as an important matter of civil rights, arguing that its illegality is based in "bigotry and racism and financing the prison–industrial complex". Rollins has shared his views on the subject as keynote speaker at the Oregon Marijuana Business Conference and the International Cannabis Business Conference.

In August 2015, Rollins discussed his support for Bernie Sanders as a candidate in the 2016 Democratic Party presidential primaries.














</doc>
<doc id="13821" url="https://en.wikipedia.org/wiki?curid=13821" title="Hadron">
Hadron

In particle physics, a hadron (, "hadrós;" "stout, thick") is a composite particle made of two or more quarks held together by the strong force in a similar way as molecules are held together by the electromagnetic force. Most of the mass of ordinary matter comes from two hadrons, the proton and the neutron.

Hadrons are categorized into two families: baryons, made of an odd number of quarks – usually three quarks – and mesons, made of an even number of quarks—usually one quark and one antiquark. Protons and neutrons are examples of baryons; pions are an example of a meson. "Exotic" hadrons, containing more than three valence quarks, have been discovered in recent years. A tetraquark state (an exotic meson), named the Z(4430), was discovered in 2007 by the Belle Collaboration and confirmed as a resonance in 2014 by the LHCb collaboration. Two pentaquark states (exotic baryons), named and , were discovered in 2015 by the LHCb collaboration. There are several more exotic hadron candidates, and other colour-singlet quark combinations that may also exist.

Almost all "free" hadrons and antihadrons (meaning, in isolation and not bound within an atomic nucleus) are believed to be unstable and eventually decay (break down) into other particles. The only known exception relates to free protons, which are "possibly" stable, or at least, take immense amounts of time to decay (order of 10 years). Free neutrons are unstable and decay with a half-life of about 611 seconds. Their respective antiparticles are expected to follow the same pattern, but they are difficult to capture and study, because they immediately annihilate on contact with ordinary matter. "Bound" protons and neutrons, contained within an atomic nucleus, are generally considered stable. Experimentally, hadron physics is studied by colliding protons or nuclei of heavy elements such as lead or gold, and detecting the debris in the produced particle showers. In the environment, mesons such as pions are produced by the collisions of cosmic rays with the atmosphere.

The term "hadron" was introduced by Lev B. Okun in a plenary talk at the 1962 International Conference on High Energy Physics. In this talk he said:

According to the quark model, the properties of hadrons are primarily determined by their so-called "valence quarks". For example, a proton is composed of two up quarks (each with electric charge +, for a total of + together) and one down quark (with electric charge −). Adding these together yields the proton charge of +1. Although quarks also carry color charge, hadrons must have zero total color charge because of a phenomenon called color confinement. That is, hadrons must be "colorless" or "white". The simplest ways for this to occur are with a quark of one color and an antiquark of the corresponding anticolor, or three quarks of different colors. Hadrons with the first arrangement are a type of meson, and those with the second arrangement are a type of baryon.

Massless virtual gluons compose the numerical majority of particles inside hadrons. The strength of the strong force gluons which bind the quarks together has sufficient energy ("E") to have resonances composed of massive ("m") quarks ("E > mc") . One outcome is that short-lived pairs of virtual quarks and antiquarks are continually forming and vanishing again inside a hadron. Because the virtual quarks are not stable wave packets (quanta), but an irregular and transient phenomenon, it is not meaningful to ask which quark is real and which virtual; only the small excess is apparent from the outside in the form of a hadron. Therefore, when a hadron or anti-hadron is stated to consist of (typically) 2 or 3 quarks, this technically refers to the constant excess of quarks vs. antiquarks.

Like all subatomic particles, hadrons are assigned quantum numbers corresponding to the representations of the Poincaré group: "J"("m"), where "J" is the spin quantum number, "P" the intrinsic parity (or P-parity), "C" the charge conjugation (or C-parity), and "m" the particle's mass. Note that the mass of a hadron has very little to do with the mass of its valence quarks; rather, due to mass–energy equivalence, most of the mass comes from the large amount of energy associated with the strong interaction. Hadrons may also carry flavor quantum numbers such as isospin (G parity), and strangeness. All quarks carry an additive, conserved quantum number called a baryon number ("B"), which is + for quarks and − for antiquarks. This means that baryons (composite particles made of three, five or a larger odd number of quarks) have "B" = 1 whereas mesons have "B" = 0.

Hadrons have excited states known as resonances. Each ground state hadron may have several excited states; several hundreds of resonances have been observed in experiments. Resonances decay extremely quickly (within about 10 seconds) via the strong nuclear force.

In other phases of matter the hadrons may disappear. For example, at very high temperature and high pressure, unless there are sufficiently many flavors of quarks, the theory of quantum chromodynamics (QCD) predicts that quarks and gluons will no longer be confined within hadrons, "because the strength of the strong interaction diminishes with energy". This property, which is known as asymptotic freedom, has been experimentally confirmed in the energy range between 1 GeV (gigaelectronvolt) and 1 TeV (teraelectronvolt).

All free hadrons except (possibly) the proton and antiproton are unstable.

Baryons are hadrons containing an odd number of valence quarks (at least 3). Most well known baryons such as the proton and neutron have three valence quarks, but pentaquarks with five quarks – three quarks of different colors, and also one extra quark-antiquark pair – have also been proven to exist. Because baryons have an odd number of quarks, they are also all fermions, "i.e.", they have half-integer spin. As quarks possess baryon number "B" = , baryons have baryon number "B" = 1.

Each type of baryon has a corresponding antiparticle (antibaryon) in which quarks are replaced by their corresponding antiquarks. For example, just as a proton is made of two up-quarks and one down-quark, its corresponding antiparticle, the antiproton, is made of two up-antiquarks and one down-antiquark.

As of August 2015, there are two known pentaquarks, and , both discovered in 2015 by the LHCb collaboration.

Mesons are hadrons containing an even number of valence quarks (at least 2). Most well known mesons are composed of a quark-antiquark pair, but possible tetraquarks (4 quarks) and hexaquarks (6 quarks, comprising either a dibaryon or three quark-antiquark pairs) may have been discovered and are being investigated to confirm their nature. Several other hypothetical types of exotic meson may exist which do not fall within the quark model of classification. These include glueballs and hybrid mesons (mesons bound by excited gluons).

Because mesons have an even number of quarks, they are also all bosons, with integer spin, "i.e.", 0, 1, or −1. They have baryon number "B" =  −  = 0. Examples of mesons commonly produced in particle physics experiments include pions and kaons. Pions also play a role in holding atomic nuclei together via the residual strong force.



</doc>
<doc id="13822" url="https://en.wikipedia.org/wiki?curid=13822" title="Heisuke Hironaka">
Heisuke Hironaka

He is celebrated for proving in 1964 that singularities of algebraic varieties admit resolutions in characteristic zero. This means that any algebraic variety can be replaced by (more precisely is birationally equivalent to) a similar variety which has no singularities. He also introduced Hironaka's example showing that a deformation of Kähler manifolds need not be Kähler. In 2017 he posted to his personal webpage a manuscript that claims to prove the existence of a resolution of singularities in positive characteristic.

Hironaka was for many years a Professor of mathematics at Harvard University (1968-1992) but currently lives in Japan. He held teaching positions at Brandeis University (1960-1963), Columbia University (1964) and Kyoto University (1975-1988). He was a president of Yamaguchi University (1996-2002). He has been active in raising funds for causes such as mathematical education. His daughter, Eriko Hironaka, is also a mathematician and focuses on low-dimensional topology and geometric topology.





</doc>
<doc id="13824" url="https://en.wikipedia.org/wiki?curid=13824" title="House of Habsburg">
House of Habsburg

The House of Habsburg (; ; traditionally spelled Hapsburg in English), also called the House of Austria ("Haus Österreich" in German, "Casa de Austria" in Spanish), was one of the most influential and distinguished royal houses of Europe. The throne of the Holy Roman Empire was continuously occupied by the Habsburgs from 1438 until their extinction in the male line in 1740. The house also produced emperors and kings of the Kingdom of Bohemia, Kingdom of England ("Jure uxoris" King), Kingdom of Germany, Kingdom of Hungary, Kingdom of Croatia, Kingdom of Illyria, Second Mexican Empire, Kingdom of Ireland ("Jure uxoris" King), Kingdom of Portugal, and Kingdom of Spain, as well as rulers of several Dutch and Italian principalities. From the 16th century, following the reign of Charles V, the dynasty was split between its Austrian and Spanish branches. Although they ruled distinct territories, they nevertheless maintained close relations and frequently intermarried.

The House takes its name from Habsburg Castle, a fortress built in the 1020s in present-day Switzerland, in the canton of Aargau, by Count Radbot of Klettgau, who chose to name his fortress Habsburg. His grandson Otto II was the first to take the fortress name as his own, adding "Count of Habsburg" to his title. The House of Habsburg gathered dynastic momentum through the 11th, 12th, and 13th centuries.

By 1276, Count Radbot's seventh generation descendant Rudolph of Habsburg moved the family's power base from Habsburg Castle to the Duchy of Austria. Rudolph became King of Germany in 1273, and the dynasty of the House of Habsburg was truly entrenched in 1276 when Rudolph became ruler of Austria, which the Habsburgs and their descendants ruled until 1918.

A series of dynastic marriages enabled the family to vastly expand its domains to include Burgundy, Spain and its colonial empire, Bohemia, Hungary, and other territories. In the 16th century, the family separated into the senior Habsburg Spain and the junior Habsburg Monarchy branches, who settled their mutual claims in the Oñate treaty.

The House of Habsburg became extinct in the 18th century. The senior Spanish branch ended upon the death of Charles II of Spain in 1700 and was replaced by the House of Bourbon. The remaining Austrian branch became extinct in the male line in 1740 with the death of Holy Roman Emperor Charles VI, and completely in 1780 with the death of his eldest daughter Maria Theresa of Austria. It was succeeded by the Vaudémont branch of the House of Lorraine, descendants of Maria Theresa's marriage to Francis III, Duke of Lorraine. The new successor house styled itself formally as the House of Habsburg-Lorraine (German: "Habsburg-Lothringen"), and because it was often confusingly still referred to as the House of Habsburg, historians use the unofficial appellation of the Habsburg Monarchy for the countries and provinces that were ruled by the between 1521 and 1780 and then by the successor branch of Habsburg-Lorraine until 1918. The Lorraine branch continues to exist to this day and its members use the Habsburg name (example: Otto von Habsburg).

The Habsburg Empire had the advantage of size, but multiple disadvantages. There were rivals on four sides, its finances were unstable, the population was fragmented into multiple ethnicities, and its industrial base was thin. Its naval resources were so minimal that it did not attempt to build an overseas empire. It did have the advantage of good diplomats, typified by Metternich (1773–1859); they had a grand strategy for survival that kept the empire going despite wars with the Ottomans, Frederick the Great, Napoleon and Bismarck, until the final disaster of the First World War. Along with the Capetian dynasty, it was one of the two most powerful continental European royal families, dominating European politics for nearly five centuries.
Their principal roles (including the roles of their cadet branches) were as follows:

Numerous other titles were attached to the crowns listed above.

The progenitor of the House of Habsburg may have been Guntram the Rich, a count in the Breisgau who lived in the 10th century, and forewith farther back as the early medieval Adalrich, Duke of Alsace, father of the Etichonids from which Habsburg derives. His grandson Radbot, Count of Habsburg founded the Habsburg Castle, after which the Habsburgs are named. The origins of the castle's name, located in what is now the Swiss canton of Aargau, are uncertain. There is disagreement on whether the name is derived from the High German "Habichtsburg" (hawk castle), or from the Middle High German word "hab/hap" meaning "ford", as there is a river with a ford nearby. The first documented use of the name by the dynasty itself has been traced to the year 1108.
The Habsburg Castle was the family seat in the 11th, 12th and 13th centuries.

The Habsburgs expanded their influence through arranged marriages and by gaining political privileges, especially countship rights in Zürichgau, Aargau and Thurgau. In the 13th century, the house aimed its marriage policy at families in Upper Alsace and Swabia. They were also able to gain high positions in the church hierarchy for their members. Territorially, they often profited from the extinction of other noble families such as the House of Kyburg.

By the second half of the 13th century, count Rudolph IV (1218–1291) had become one of the most influential territorial lords in the area between the Vosges Mountains and Lake Constance. Due to these impressive preconditions, on 1 October 1273, Rudolph was chosen as the King of the Romans and received the name Rudolph I of Germany.

In 1282, the Habsburgs gained the rulership of the Duchy of Austria, which they then held for over 600 years, until 1918. Through the forged "privilegium maius" document (1358/59), a special bond was created between the house and Austria. The document, forged at the behest of Rudolf IV, Duke of Austria (1339–1365), also attempted to introduce rules to preserve the unity of the family's Austrian lands. In the long term, this indeed succeeded, but Rudolph's descendants ignored the rule, leading to the separation of the Albertian and Leopoldian family lines in 1379.

By marrying Elisabeth of Luxembourg, the daughter of Holy Roman Emperor Sigismund in 1437, Duke Albert V (1397–1439) became the ruler of Bohemia and Hungary, expanding the family's political horizons. The next year, Albert V was crowned as the King of the Romans as Albert II. After his early death in war with the Turks in 1439, and after the death of his son Ladislaus Postumus in 1457, the Habsburgs lost Bohemia and Hungary again. National kingdoms were established in these areas, and the Habsburgs were not able to restore their influence there for decades.

In 1440, Frederick III was chosen by the electoral college to succeed Albert II as the king. Several Habsburg kings had attempted to gain the imperial throne over the years, but success finally arrived on 19 March 1452, when Pope Nicholas V crowned Frederick III as the Holy Roman Emperor in a grand ceremony held in Rome. In Frederick III, the Pope found an important political ally with whose help he was able to counter the conciliar movement.

While in Rome, Frederick III married Eleanor of Portugal, enabling him to build a network of connections with dynasties in the west and southeast of Europe. Frederick was rather distant to his family; Eleanor, by contrast, had a great influence on the raising and education of Frederick's children, and therefore played an important role in the family's rise to prominence. After Frederick III's coronation, the Habsburgs were able to hold the imperial throne almost continuously for centuries, until 1806.

As emperor, Frederick III took a leading role inside the family and positioned himself as the judge over the family's internal conflicts, often making use of the "privilegium maius". He was able to restore the unity of the house's Austrian lands, as the Albertinian line was now extinct. Territorial integrity was also strengthened by the extinction of the Tyrolean branch of the Leopoldian line in 1490/1496. Frederick's aim was to make Austria a united country, stretching from the Rhine to the Mur and Leitha.

On the external front, one of Frederick's main achievements was the Siege of Neuss (1474–75), in which he forced Charles the Bold of Burgundy to give his daughter Mary of Burgundy as wife to Frederick's son Maximilian. The wedding took place on the evening of 16 August 1477 and ultimately resulted in the Habsburgs acquiring control of the Low Countries. After Mary's early death in 1482, Maximilian attempted to secure the Burgundian heritance to one of his and Mary's children Philip the Handsome. Charles VIII of France contested this, using both military and dynastic means, but the Burgundian succession was finally ruled in favour of Philip in the Treaty of Senlis in 1493.

After the death of his father in 1493, Maximilian was proclaimed the new King of the Romans, receiving the name Maximilian I. Maximilian was initially unable to travel to Rome to receive the Imperial title from the Pope, due to opposition from Venice and from the French who were occupying Milan, as well a refusal from the Pope due to enemy forces being present on his territory. In 1508, Maximilian proclaimed himself as the "chosen Emperor," and this was also recognized by the Pope due to changes in political alliances. This had a historical consequence in that, in the future, the Roman King would also automatically become Emperor, without needing the Pope's consent. In 1530, Emperor Charles V became the last person to be crowned as the Emperor by the Pope.

Maximilian's rule (1493–1519) was a time of great expansion for the Habsburgs. In 1497, Maximilian's son Philip the Handsome (also known as Phillip the Fair) married Joanna of Castile, also known as Joan the Mad, heiress of Castile, Aragon, and most of Spain. Phillip and Joan had six children, the eldest of whom became emperor Charles V and inherited the kingdoms of Castile and Aragon (including their colonies in the New World) as Charles I, Southern Italy, Austria, and the Low Countries.

The foundations for the later empire of Austria-Hungary were laid in 1515 by the means of a double wedding between Louis, only son of Vladislaus II, King of Bohemia and Hungary, and Maximilian's granddaughter Mary; and between her brother Archduke Ferdinand and Vladislaus' daughter Anna. The wedding was celebrated in grand style on 22 July 1515, and has been described by some historians as the First Congress of Vienna due to its significant implications for Europe's political landscape. All the children were still minors, so the wedding was formally completed in 1521. Vladislaus died on 13 March 1516, and Maximilian died on 12 January 1519, but his designs were ultimately successful: on Louis's death in 1526, Maximilian's grandson and Charles V's brother Ferdinand, became the King of Bohemia.

The Habsburg dynasty achieved the position of a true world power by the time of Charles V's election in 1519, for the first and only time in their history—the "World Emperor" ruling an "empire on which the sun never sets".

The Habsburgs' policies against Protestantism led to an eradication of the former throughout vast areas under their control.

After the assignment, on 21 April 1521, of the Austrian lands to Ferdinand I by his brother Emperor Charles V (1519–1556), the dynasty split into the junior branch of the Austrian Habsburgs and the senior branch of the Spanish Habsburgs. The Austrian Habsburgs held the title of Holy Roman Emperor after Charles' abdication in 1556, as well as the Habsburg Hereditary Lands and the Kingdoms of Bohemia and Hungary.

The senior Spanish branch ruled over Spain, its Italian possessions and its colonial empire, the Netherlands, and, for a time (1580–1640), Portugal. Hungary was partly under Habsburg rule from 1526. For 150 years most of the country was occupied by the Ottoman Turks but these territories were re-conquered in 1683–1699.

In the secret Oñate treaty, the Spanish and Austrian Habsburgs settled their mutual claims. The Spanish Habsburgs died out in 1700 (prompting the War of the Spanish Succession), as did the last male of the Austrian Habsburg line in 1740 (prompting the War of the Austrian Succession), and finally the last female of the Habsburg male line in 1780.

The Habsburgs sought to consolidate their power by the frequent use of consanguineous marriages. This resulted in a cumulatively deleterious effect on their gene pool. Marriages between first cousins, or between uncle and niece, were commonplace in the family. A study of 3,000 family members over 16 generations by the University of Santiago de Compostela suggests that inbreeding directly led to their extinction. The gene pool eventually became so small that the last of the Spanish line Charles II, who was severely disabled from birth, perhaps by genetic disorders, possessed a genome comparable to that of a child born to a brother and sister, as did his father, probably because of "remote inbreeding".

The Austrian branch became extinct in the male line in 1740 with the death of Charles VI and in the female line in 1780 with the death of his daughter Maria Theresa; it was succeeded by the Vaudemont branch of the House of Lorraine in the person of her son Joseph II. The new successor house styled itself formally as House of Habsburg-Lorraine (German: "Habsburg-Lothringen"), although it was often referred to as simply the House of Habsburg. The heiress of the last Austrian Habsburgs Maria Theresa had married Francis Stephan, Duke of Lorraine (both of them were great-grandchildren of Habsburg Emperor Ferdinand III, but from different empresses). Their descendants carried on the Habsburg tradition from Vienna under the dynastic name Habsburg-Lorraine, although technically a new ruling house came into existence in the Austrian territories, the House of Lorraine (see Dukes of Lorraine family tree). It is thought that extensive intra-family marriages within both lines contributed to their extinctions.

On 6 August 1806 the Holy Roman Empire was dissolved under the French Emperor Napoleon I's reorganization of Germany. However, in anticipation of the loss of his title of Holy Roman Emperor, Francis II declared himself hereditary Emperor of Austria (as Francis I) on 11 August 1804, three months after Napoleon had declared himself Emperor of the French on 18 May 1804.

Emperor Francis I of Austria used the official full list of titles: "We, Francis the First, by the grace of God Emperor of Austria; King of Jerusalem, Hungary, Bohemia, Dalmatia, Croatia, Slavonia, Galicia and Lodomeria; Archduke of Austria; Duke of Lorraine, Salzburg, Würzburg, Franconia, Styria, Carinthia, and Carniola; Grand Duke of Cracow; Grand Prince of Transylvania; Margrave of Moravia; Duke of Sandomir, Masovia, Lublin, Upper and Lower Silesia, Auschwitz and Zator, Teschen, and Friule; Prince of Berchtesgaden and Mergentheim; Princely Count of Habsburg, Gorizia, and Gradisca and of the Tyrol; and Margrave of Upper and Lower Lusatia and Istria".

The Austro-Hungarian Compromise of 1867 created a real union, whereby the Kingdom of Hungary was granted co-equality with the Empire of Austria, that henceforth didn't include the Kingdom of Hungary as a crownland anymore. The Austrian and the Hungarian lands became independent entities enjoying equal status Under this arrangement, the Hungarians referred to their ruler as king and never emperor (see k. u. k.). This prevailed until the Habsburgs' deposition from both Austria and Hungary in 1918 following defeat in World War I.

On 11 November 1918, with his empire collapsing around him, the last Habsburg ruler, Charles I of Austria (who also reigned as Charles IV of Hungary) issued a proclamation recognizing Austria's right to determine the future of the state and renouncing any role in state affairs. Two days later, he issued a separate proclamation for Hungary. Even though he did not officially abdicate, this is considered the end of the Habsburg dynasty. In 1919, the new republican Austrian government subsequently passed a law banishing the Habsburgs from Austrian territory until they renounced all intentions of regaining the throne and accepted the status of private citizens. Charles made several attempts to regain the throne of Hungary, and in 1921 the Hungarian government passed a law which revoked Charles' rights and dethroned the Habsburgs.

The Habsburgs did not formally abandon all hope of returning to power until Otto von Habsburg, the eldest son of Charles I, on 31 May 1961 renounced all claims to the throne.

The dynasty's motto was "Leave the waging of wars to others! But you, happy Austria, marry; for the realms which Mars awards to others, Venus transfers to you."

Similarly, this family tree only includes male scions of the House of Habsburg-Lorraine who survived to adulthood:

The Habsburg Empire was never composed of a single unified and unitary state as Bourbon France, Hohenzollern Germany, or Great Britain was. It was made up of an accretion of territories that owed their historic loyalty to the head of the house of Habsburg as hereditary lord. The Habsburgs had mostly married the heiresses of these territories, most famously of Spain and the Netherlands. They used their coats of arms then as a statement of their right to rule all these territories. As there were many territories, so their arms were complex and reflected the waxing and waning position of the Habsburgs within European power politics. It was not until the 19th century (see below Arms of Dominion of the Austro-Hungarian Empire) that the arms began to take on their own life as symbols of a state which may have an existence outside of the Habsburg dynasty. A complete listing of the arms can be found at the .


Before Rudolph rose to German king, the Habsburgs were Counts of Baden in what is today southwestern Germany and Switzerland.


In the late Middle Ages, when the Habsburgs expanded their territories in the east, they usually ruled as dukes of the Duchy of Austria which covered only what is today Lower Austria ("Niederösterreich") and the eastern part of Upper Austria ("Oberösterreich"). The Habsburg possessions also included the rest of what was then called Inner Austria ("Innerösterreich"), i.e. the Duchy of Styria, and then expanded west to include the Duchy of Carinthia and Carniola in 1335 and the Count of Tirol in 1363. Their original scattered possessions in the southern Alsace, south-western Germany and Vorarlberg were collectively known as Further Austria.

The senior Habsburg dynast generally ruled Lower Austria from Vienna as archduke ("paramount duke") of the Duchy of Austria. The Styrian lands had already been ruled in personal union by the Babenberg dukes of Austria since 1192 and were finally seized with the Austrian lands by the Habsburg king Rudolph I of Germany upon his victory in the 1278 Battle on the Marchfeld. In 1335 Rudolph's grandson Duke Albert II of Austria also received the Carinthian duchy with the adjacent March of Carniola at the hands of Emperor Louis the Bavarian as Imperial fiefs.

The Habsburg dukes gradually lost their homelands south of the Rhine and Lake Constance to the expanding Old Swiss Confederacy. Unless mentioned explicitly, the dukes of Austria also ruled over Further Austria until 1379, after that year, Further Austria was ruled by the Princely Count of Tyrol. Names in "italics" designate dukes who never actually ruled.

When Albert's son Duke Rudolf IV of Austria died in 1365, his younger brothers Albert III and Leopold III quarrelled about his heritage and in the Treaty of Neuberg of 1379 finally split the Habsburg territories: The Albertinian line would rule in the Archduchy of Austria proper (then sometimes referred to as "Lower Austria" ("Niederösterreich"), but comprising modern Lower Austria and most of Upper Austria), while the Leopoldian line ruled in the Styrian, Carinthian and Carniolan territories, subsumed under the denotation of "Inner Austria". At that time their share also comprised Tyrol and the original Habsburg possessions in Swabia, called Further Austria; sometimes both were collectively referred to as "Upper Austria" ("Oberösterreich") in that context, also not to be confused with the modern state of that name.

After the death of Leopold's eldest son William in 1406, the Leopoldinian line was further split among his brothers into the Inner Austrian territory under Ernest the Iron and a Tyrolean/Further Austrian line under Frederick IV. In 1457 Ernest's son Duke Frederick V of Inner Austria also gained the Austrian archduchy after his Albertine cousin Ladislaus the Posthumous had died without issue. 1490 saw the reunification of all Habsburg lines, when Archduke Sigismund of Further Austria and Tyrol resigned in favour of Frederick's son Maximilian I. In 1512, the Habsburg territories were incorporated into the Imperial Austrian Circle.

"Archduke of Austria", was invented in the "Privilegium Maius", a 14th-century forgery initiated by Duke Rudolf IV of Austria. Originally, it was meant to denote the "ruler" (thus "Arch-") of the duchy of Austria, usually from Vienna, in an effort to put the Habsburgs on a par with the Prince-electors, as Austria had been bypassed as hereditary prince-electors of the empire when the Golden Bull of 1356 assigned that title to the highest ranking Imperial princes. The Holy Roman Emperor Charles IV refused to recognise the title.

The archducal title was only officially recognized in 1453 by Emperor Frederick III. Emperor Frederick III himself used just "Duke of Austria", never "Archduke", until his death in 1493. The title was first granted to Frederick's younger brother, Albert VI of Austria (died 1463), who used it at least from 1458.

In 1477, Frederick III also granted the title "archduke" to his first cousin, Sigismund of Austria, ruler of Further Austria.
Frederick's son and heir, the future Emperor Maximilian I, started to use the title, but apparently only after the death of his wife Mary of Burgundy (died 1482), as "Archduke" never appears in documents issued jointly by Maximilian and Mary as rulers in the Low Countries (where Maximilian is still titled "Duke of Austria"). The title appears first in documents issued under the joint rule of Maximilian and Philip (his under-age son) in the Low Countries.

"Archduke" was initially borne by those dynasts who ruled a Habsburg territory, i.e., only by males and their consorts, appanages being commonly distributed to cadets. But these "junior" "archdukes" did not thereby become independent hereditary rulers, since all territories remained vested in the Austrian crown. Occasionally a territory might be combined with a separate gubernatorial mandate ruled by an archducal cadet.

From the 16th century onward, "archduke" and its female form, "archduchess", came to be used by all the members of the House of Habsburg (e.g., Queen Marie Antoinette of France was born "Archduchess Maria Antonia of Austria".


After the death of Rudolph IV, his brothers Albert III and Leopold III ruled the Habsburg possessions together from 1365 until 1379, when they split the territories in the Treaty of Neuberg, Albert keeping the Duchy of Austria and Leopold ruling over Styria, Carinthia, Carniola, the Windic March, Tirol, and Further Austria.



Sigismund had no children and adopted Maximilian I, son of duke Frederick V (emperor Frederick III). Under Maximilian, the possessions of the Habsburgs would be united again under one ruler, after he had re-conquered the Duchy of Austria after the death of Matthias Corvinus, who resided in Vienna and styled himself duke of Austria from 1485–1490.



The title Archduke of Austria, the one most famously associated with the Habsburgs, was invented in the Privilegium Maius, a 14th-century forgery initiated by Duke Rudolf IV of Austria. Originally, it was meant to denote the ruler of the (thus 'Arch')duchy of Austria, in an effort to put that ruler on par with the Prince-electors, as Austria had been passed over in the Golden Bull of 1356, when the electorships had been assigned. Holy Roman Emperor Charles IV refused to recognize the title. Ladislaus the Posthumous, Duke of Austria, who died in 1457, was never in his lifetime authorized to use it, and accordingly, not he nor anyone in his branch of the dynasty ever used the title.

Duke Ernest the Iron and his descendants unilaterally assumed the title "archduke". This title was only officially recognized in 1453 by his son, Emperor Frederick III, when the Habsburgs had (permanently) gained control of the office of the Holy Roman Emperor. Emperor Frederick III himself used just Duke of Austria, never Archduke, until his death in 1493.

Frederick's son and heir, the future Emperor Maximilian I, started to use the title, but apparently only after the death of his wife Mary of Burgundy (died 1482) as the title never appears in documents of joint Maximilian and Mary rule in the Low Countries (where Maximilian is still titled Duke of Austria). The title appears first in documents of joint Maximilian and Philip (his under-age son) rule in the Low Countries. It only gained currency with Charles V and the descendants of his brother, the Emperor Ferdinand.


The reigning duke of Burgundy, Charles the Bold, was the chief political opponent of Maximilian's father Frederick III. Charles controlled not only Burgundy (both dukedom and county), but the wealthy and powerful Southern Netherlands, current Flanders, the real center of his power. Frederick was concerned about Burgundy's expansive tendencies on the western border of his Holy Roman Empire, and to forestall military conflict, he attempted to secure the marriage of Charles's only daughter, Mary of Burgundy, to his son Maximilian. After the Siege of Neuss (1474–75), he was successful. The wedding between Maximilian and Mary took place on the evening of 16 August 1477, after the death of Charles. Mary and the Habsburgs lost the Duchy of Burgundy to France, but managed to defend and hold onto the rest what became the 17 provinces of the Habsburg Netherlands. After Mary's death in 1482, Maximilian acted as regent for his son:


The Netherlands were frequently governed directly by a regent or governor-general, who was a collateral member of the Habsburgs. By the Pragmatic Sanction of 1549 Charles V combined the Netherlands into one administrative unit, to be inherited by his son Philip II. Charles effectively united the Netherlands as one entity. The Habsburgs controlled the 17 Provinces of the Netherlands until the Dutch Revolt in the second half of the 16th century, when they lost the seven northern Protestant provinces. They held onto the southern Catholic part (roughly modern Belgium and Luxembourg) as the Spanish and Austrian Netherlands until they were conquered by French Revolutionary armies in 1795. The one exception to this was the period of (1601–1621), when shortly before Philip II died on 13 September 1598, he renounced his rights to the Netherlands in favor of his daughter Isabella and her fiancé, Archduke Albert of Austria, a younger son of Emperor Maximilian II. The territories reverted to Spain on the death of Albert in 1621, as the couple had no surviving offspring, and Isabella acted as regent-governor until her death in 1633:



The Habsburg Kingdom(s) of Spain were more a personal union of possessions of the Habsburg king and dynast, who was King of Castile, Leon, Aragon, Valencia, sometime of Portugal, Naples and Sicily, Duke of Milan, and Lord of the Americas, as well as Duke of Brabant, Count of Flanders and Holland, Duke of Luxemburg (i.e. all the Habsburg Netherlands). A listing of a number of the titles can be seen here. The dynast (head of the Spanish Habsburgs, i.e. the King, showed this wide range of claims in his arms. There are many more variants of these arms in the as well as coat of arms of the King of Spain, coat of arms of Spain, coat of arms of the Prince of Asturias, and coats of arms of Spanish Monarchs in Italy. The Spanish Habsburgs also kept up the Burgundian court tradition of the dynast being known by a "nickname" (e.g. the Bold, the Prudent, the Bewitched). In Spain they were known as the ", and illegitimate sons were known as "de Austria" (see Don Juan de Austria and Don Juan José de Austria).


The War of the Spanish Succession took place after the extinction of the Spanish Habsburg line, to determine the inheritance of Charles II.

The main junior line of the house ruled the Duchy of Austria, as well as the Kingdom of Bohemia and the Kingdom of Hungary. The dynasty however was split up again in 1564 among the children of deceased Emperor Ferdinand I of Habsburg. The Inner Austrian line founded by Archduke Charles II prevailed again, when his son and successor as regent of Inner Austria (i.e. the Duchy of Styria, the Duchy of Carniola with March of Istria, the Duchy of Carinthia, the Princely County of Gorizia and Gradisca, and the Imperial City of Trieste, ruled from Graz) Ferdinand II in 1619 became Archduke of Austria and Holy Roman Emperor as well as King of Bohemia and Hungary in 1620. The Further Austrian/Tyrolean line of Ferdinand's brother Archduke Leopold V survived until the death of his son Sigismund Francis in 1665, whereafter their territories ultimately returned to common control with the other Austrian Habsburg lands. Inner Austrian stadtholders went on to rule until the days of Empress Maria Theresa in the 18th century.


The War of the Austrian Succession took place after the extinction of the male line of the Austrian Habsburg line upon the death of Charles VI. The direct Habsburg line itself became totally extinct with the death of Maria Theresa of Austria, when it was followed by the House of Lorraine, styled " of Habsburg-Lorraine".

Queen Maria Christina of Austria of Spain, great-granddaughter of Leopold II, Holy Roman Emperor above. Wife of Alfonso XII of Spain and mother of Alfonso XIII of the House of Bourbon. Alfonso XIII's wife Victoria Eugenie of Battenberg was descended from King George I of Great Britain from the Habsburg Leopold Line {above}.

The House of Habsburg-Lorraine retained Austria and attached possessions after the dissolution of the Holy Roman Empire; see below.

A son of Leopold II was Archduke Rainer of Austria whose wife was from the House of Savoy; a daughter Adelaide, Queen of Sardina was the wife of King Victor Emmanuel II of Piedmont, Savoy, and Sardinia and King of Italy. Their Children married into the Royal Houses of Bonaparte; Saxe-Coburg and Gotha {Bragança} {Portugal}; Savoy {Spain}; and the Dukedoms of Montferrat and Chablis.

(→Family Tree)


Francis Stephen assigned the grand duchy of Tuscany to his second son Peter Leopold, who in turn assigned it to his second son upon his accession as Holy Roman Emperor. Tuscany remained the domain of this cadet branch of the family until Italian unification.



The duchy of Modena was assigned to a minor branch of the family by the Congress of Vienna. It was lost to Italian unification. The Dukes named their line the House of Austria-Este, as they were descended from the daughter of the last D'Este Duke of Modena.



Dona Maria Leopoldina of Austria (22 January 1797 – 11 December 1826) was an archduchess of Austria, Empress consort of Brazil and Queen consort of Portugal.


The duchy of Parma was likewise assigned to a Habsburg, but did not stay in the House long before succumbing to Italian unification. It was granted to the second wife of Napoleon I of France, Maria Luisa Duchess of Parma, a daughter of the Francis II, Holy Roman Emperor, who was the mother of Napoleon II of France. Napoleon had divorced his wife Rose de Tascher de la Pagerie (better known to history as Josephine de Beauharnais) in her favour.


Maximilian, the adventurous second son of Archduke Franz Karl, was invited as part of Napoleon III's manipulations to take the throne of Mexico, becoming Emperor Maximilian I of Mexico. The conservative Mexican nobility, as well as the clergy, supported this Second Mexican Empire. His consort, Charlotte of Belgium, a daughter of King Leopold I of Belgium and a princess of the House of Saxe-Coburg Gotha, encouraged her husband's acceptance of the Mexican crown and accompanied him as Empress Carlota of Mexico. The adventure did not end well. Maximilian was shot in Cerro de las Campanas, Querétaro, in 1867 by the republican forces of Benito Juárez.


Charles I was expelled from his domains after World War I and the empire was abolished.

see Line of succession to the Austro-Hungarian throne

The kingship of Hungary remained in the Habsburg family for centuries; but as the kingship was not strictly inherited (Hungary was an elective monarchy until 1687) and was sometimes used as a training ground for young Habsburgs, as "Palatine" of Hungary, the dates of rule do not always match those of the primary Habsburg possessions. Therefore, the kings of Hungary are listed separately.




After Václav III’s death, there were no male heirs remaining in the Přemyslid line. Therefore, with the election of Rudolf in 1306, the kingship of Bohemia was a position elected by its nobles, although often the crown was transferred through war, such as John of Bohemia in 1310. As a result, it was not an automatically inherited position. Until the rule of Ferdinand I, Habsburgs didn't gain hereditary accession to the throne and were displaced by other dynasties. Hence, the kings of Bohemia and their ruling dates are listed separately. The Habsburgs became hereditary kings of Bohemia in 1627. By their acquisition of the Bohemian Crown in 1526 the Habsburgs secured the highest rank among the secular prince-electors of the Holy Roman Empire.





Most royal families did not have a family name until the 19th century. They were known as "of" (in German von) based on the main territory they ruled. For example, sons, daughters, grandsons and granddaughters of a ruling French King were known as "of France" (see Wikipedia on House of Bourbon). The name "Capet" was an invention of the French Revolutionaries. "Bourbon" was in some sense the name of the house as it was differentiated from the previous Valois kings. Princes and Princesses of the royal house of England were known as "of England", or later "Great Britain" (see House of Windsor) or "of" the main title associated with their parent (see Prince William of Wales). In the Middle Ages, princes of England were often known by the town or castle of their birth (see John of Gaunt, Henry Bolingbroke, or Henry of Monmouth). Even when the royal family had a last name (see House of Tudor, House of Stuart or House of Windsor), it was not used in their titles.

Similarly, the Habsburg name was used as one of the subsidiary titles of the rulers above, as in "Princely Count of Habsburg" (see above under Habsburg-Lorraine). The Habsburg arms (see above) were displayed only in the most complete (great arms) of the prince. The dynasty was known as the "house of Austria". Most of the princes above were known as Archduke xyz "of Austria" and had no need of a surname. Charles V was known in his youth after his birthplace as "Charles of Ghent". When he became king of the Spains he was known as "Charles of Spain", until he became emperor, when he was known as Charles V ("Charles Quint"). In Spain, the dynasty was known as the , and illegitimate sons were given the title of "de Austria" (see Don Juan de Austria and Don Juan José de Austria). The arms displayed in their simplest form were those of Austria, which the Habsburgs had made their own, at times impaled with the arms of the Duchy of Burgundy (ancient).

When Maria Theresa married the duke of Lorraine, Francis Stephen (see above), there was a desire to show that the ruling dynasty continued as did all its inherited rights, as the ruling dynasty's right to rule was based on inherited legitimate birthright in each of the constituent territories. Using the concept of "Habsburg" as the traditional Austrian ruler was one of those ways. When Francis I became Emperor of Austria, there was an even further reinforcement of this by the reappearance of the arms of Habsburg in the tripart personal arms of the house with Austria and Lorraine. This also reinforced the "Germaness" of the Austrian Emperor and his claim to rule in Germany against the Prussian Kings, or at least to be included in "Germany". As Emperor Francis Joseph wrote to Napoleon III „Nein, ich bin ein deutscher Fürst“ In the genealogical table above, some younger sons who had no prospects of the throne, were given the personal title of "count of Habsburg".

Today, as the dynasty is no longer on the throne, the surname of members of the house is taken to be "von Habsburg" or more completely "von Habsburg-Lothringen" (see Otto von Habsburg and Karl von Habsburg). Princes and members of the house use the Tripartite arms shown above, generally forgoing any imperial pretensions.

The arms of dominion began to take on a life of their own in the 19th century as the idea of the state as independent from the Habsburg dynasty took root. They are the national arms as borne by a sovereign in his capacity as head of state and represent the state as separate from the person of the monarch or his dynasty. That very idea had been, heretofore, foreign to the concept of the Habsburg state. The state had been the personal property of the Habsburg dynast. Since the states, territories, and nationalities represented were in many cases only united to the Austro-Hungarian Empire by their historic loyalty to the head of the house of Habsburg as hereditary lord, these full ("grand") arms of dominion of Austria-Hungary reflect the complex political infrastructure that was necessarily to accommodate the many different nationalities and groupings within the empire after the Austro-Hungarian Compromise of 1867.

After 1867 the eastern part of the empire, also called Transleithania, was mostly under the domination of the Kingdom of Hungary. The shield integrated the arms of the kingdom of Hungary, with two angels and supporters and the crown of St. Stephen, along with the territories that were subject to it:

The Kingdom of Dalmatia, the Kingdom of Croatia, the Kingdom of Slavonia (conjoined with Croatia as the Kingdom of Croatia-Slavonia - formally known as the Triune Kingdom of Croatia, Slavonia, and Dalmatia, although the claim to Dalmatia was mostly de jure), the Great Principality of Transylvania, the Condominium of Bosnia and Herzegovina (1915–1918), the City of Fiume and its district (modern Rijeka), and in the center, the Kingdom of Hungary.

The western or Austrian part of the empire, "Cisleithania", continued using the shield of the Empire in 1815 but with the seals of various member territories located around the central shield. Paradoxically, some of these coats of arms belonged to the territories that were part of the Hungarian part of the empire and shield. This shield, the most frequently used until 1915, was known as the middle shield. There was also the small shield, with just the personal arms of the Habsburgs, as used in 1815.

In 1915, in the middle of World War I, Austria-Hungary adopted a heraldic composition uniting the shield that was used in the Hungarian part, also known as the Lands of the Crown of St. Stephen, with a new version of the medium shield of the Austrian part as depicted above in the section on the main line of the Emperors of Austria.

Before 1915, the arms of the different territories of the Austrian part of the Empire (heraldry was added to some areas not shown in the previous version and to the left to the Hungarian part) appeared together in the shield positioned on the double-headed eagle coat of arms of the Austrian Empire as an inescutcheon. The eagle was inside a shield with a gold field. The latter shield was supported by two griffins and was topped by the Austrian Imperial Crown (previously these items were included only in the large shield). Then, shown in the center of both arms of dominion, as an inescutcheon to the inescutcheon, is the small shield, i.e. personal arms, of the Habsburgs. All this was surrounded by the collar Order of the Golden Fleece

In the heraldic composition of 1915, the shields of the two foci of the empire, Austria and Hungary, were brought together. The griffin supporter on the left was added for Austria and an angel on the right as a supporter for Hungary. The center featured the personal arms of the Habsburgs (Habsburg, Austria and Lorraine). This small shield was topped with a royal crown and surrounded by the collar of the Order of the Golden Fleece, below which was the Military Order of Maria Theresa, below which was the collars of the Orders of St. Stephen's and Leopold. At the bottom was the motto that read "AC INDIVISIBILITER INSEPARABILITER" ("indivisible and inseparable"). There were other simplified versions which did not have the supports depicted, and the simple shields of Austria and Hungary. These were the arms of the Empire of Austria with an inescutcheon of Austria, and the Arms of Hungary (with chequer of Croatia at the tip).




</doc>
<doc id="13826" url="https://en.wikipedia.org/wiki?curid=13826" title="Hub">
Hub

A hub is the central part of a wheel that connects the axle to the wheel itself. Many expressions use the term for a literal or figurative central structure connecting to a periphery.

Hub or hubs may also refer to:












</doc>
<doc id="13828" url="https://en.wikipedia.org/wiki?curid=13828" title="House of Commons of the United Kingdom">
House of Commons of the United Kingdom

The House of Commons is the lower house of the Parliament of the United Kingdom. Like the upper house, the House of Lords, it meets in the Palace of Westminster. Officially, the full name of the house is the Honourable the Commons of the United Kingdom of Great Britain and Northern Ireland in Parliament assembled. Owing to shortage of space, its office accommodation extends into Portcullis House.

The Commons is an elected body consisting of 650 members known as Members of Parliament (MPs). Members are elected to represent constituencies by the first-past-the-post system and hold their seats until Parliament is dissolved.

The House of Commons of England started to evolve in the 13th and 14th centuries. It became the House of Commons of Great Britain after the political union with Scotland in 1707, and assumed the title of "House of Commons of Great Britain and Ireland" after the political union with Ireland at the start of the 19th century. The "United Kingdom" referred to was the United Kingdom of Great Britain and Ireland from 1800, and became the United Kingdom of Great Britain and Northern Ireland after the independence of the Irish Free State in 1922. Accordingly, the House of Commons assumed its current title.

Under the Parliament Act 1911, the Lords' power to reject legislation was reduced to a delaying power. The Government is solely responsible to the House of Commons and the Prime Minister stays in office only as long as she or he retains the confidence of a majority of the Commons.

Although it does not formally elect the prime minister, the position of the parties in the House of Commons is of overriding importance. By convention, the prime minister is answerable to, and must maintain the support of, the House of Commons. Thus, whenever the office of prime minister falls vacant, the Sovereign appoints the person who has the support of the House, or who is most likely to command the support of the House—normally the leader of the largest party in the Commons, while the leader of the second-largest party becomes the Leader of the Opposition. Since 1963, by convention, the prime minister is always a member of the House of Commons, rather than the House of Lords.

The Commons may indicate its lack of support for the Government by rejecting a motion of confidence or by passing a motion of no confidence. Confidence and no confidence motions are phrased explicitly, for instance: "That this House has no confidence in Her Majesty's Government." Many other motions were until recent decades considered confidence issues, even though not explicitly phrased as such: in particular, important bills that were part of the Government's agenda. The annual Budget is still considered a matter of confidence. When a Government has lost the confidence of the House of Commons, the prime minister is obliged either to resign, making way for another MP who can command confidence, or to request the monarch to dissolve Parliament, thereby precipitating a general election.

Parliament normally sits for a maximum term of five years. Subject to that limit, the prime minister could formerly choose the timing of the dissolution of parliament, with the permission of the Monarch. However, since the Fixed-Term Parliaments Act 2011, terms are now a fixed five years, and an early general election is brought about by a two-thirds majority in favour of a motion for a dissolution, or by a vote of no confidence that is not followed within fourteen days by a vote of confidence (which may be for confidence in the same government or in a different one). By this second mechanism, the UK's government can change its political composition without an intervening general election. Only four of the eight last Prime Ministers have attained office as the immediate result of a general election; the others have gained office upon the resignation of a Prime Minister of their own party. The latter four were Jim Callaghan, John Major, Gordon Brown and the current Prime Minister Theresa May; these four inherited the office from Harold Wilson, Margaret Thatcher, Tony Blair and David Cameron respectively. In such circumstances there may not even have been an internal party leadership election, as the new leader may be chosen by acclaim, having no electoral rival (as in the case of both Brown and May).

A prime minister will resign after party defeat at an election if unable to lead a coalition, or obtain a confidence and supply arrangement. She or he may also resign after a motion of no confidence or for health reasons. In such cases, the premiership goes to whoever can command a majority in the House; unless there is a hung parliament and a coalition is formed, this will by convention be the new leader of the resignee's party. It has become the practice to write the constitution of major UK political parties to provide a set way in which to appoint a new leader. Until 1965, the Conservative Party had no fixed mechanism for this; when in 1957 Anthony Eden resigned as PM without recommending a successor, it was unable to nominate one. It fell to the Queen to appoint Harold Macmillan as the new prime minister, after taking the consensus of cabinet ministers.
By convention, ministers are members of the House of Commons or House of Lords. A handful have been appointed who were outside Parliament, but in most cases they then entered Parliament in a by-election or by receiving a peerage (being made a life peer). Exceptions include Peter Mandelson, appointed Secretary of State for Business, Enterprise and Regulatory Reform in October 2008 before his peerage. Since 1902, all prime ministers have been members of the Commons; the sole exception was during the long summer recess in 1963: the 14th Earl of Home disclaimed his peerage (under a new mechanism which remains in force) three days after becoming prime minister, thereby becoming Sir Alec Douglas-Home. The new session of Parliament was delayed to await the outcome of his by-election, which happened to be under way due to a recent death. As anticipated, he won that election, which was for the highest-majority seat in Scotland among his party; otherwise he would have been constitutionally obliged to resign.

Since 1990, almost all ministers, save for three whose offices are an intrinsic part of the House of Lords, have belonged to the Commons. 

Few major cabinet positions (except Lord Privy Seal, Lord Chancellor and Leader of the House of Lords) have been filled by a peer in recent times. Notable exceptions are Peter Carington, 6th Lord Carrington, who served as Foreign Secretary from 1979 to 1982, David Young, Lord Young of Graffham, who was appointed Employment Secretary in 1985, Lord Mandelson, who served as Business Secretary, Lord Adonis, who served as Transport Secretary, and Baroness Amos, who served as International Development Secretary. The elected status of members of the Commons (as opposed to the unelected Lords) and their direct accountability to that House, together with empowerment and transparency, ensures ministerial accountability. Responsible government is an international constitutional paradigm. The prime minister chooses the ministers, and may decide to remove them at any time, although the appointments and dismissals are formally made by the Sovereign.

The House of Commons formally scrutinises the Government through its Committees and Prime Minister's Questions, when members ask questions of the prime minister; the House gives other opportunities to question other cabinet ministers. Prime Minister's Questions occur weekly, normally for half an hour each Wednesday. Questions must relate to the responding minister's official government activities, not to his or her activities as a party leader or as a private Member of Parliament. Customarily, members of the Government party/coalition and members of the Opposition alternate when asking questions. Members may also make inquiries in writing.

In practice, this scrutiny can be fairly weak. Since the first-past-the-post electoral system is employed, the governing party often enjoys a large majority in the Commons, and ministers and departments practise defensive government, outsourcing key work to third parties. If the government has a large majority, it has no need or incentive to compromise with other parties, apart from working in Select Committees for personal acclaim. Major modern British political parties tend to be so tightly orchestrated that their MPs have little scope for free action. A large minority of ruling party MPs are paid members of the Government. Since 1900 the Government has lost confidence motions three times — twice in 1924, and once in 1979. However, the threat of rebellions by their own party's backbench MPs often forces governments to make concessions (under the Coalition, over foundation hospitals and under Labour over top-up fees and compensation for failed company pension schemes). Occasionally Government bills are defeated by backbench rebellions (Terrorism Act 2006). However, the scrutiny provided by the Select Committees is more serious.

The House of Commons technically retains the power to impeach Ministers of the Crown (or any other subject, even if not a public officer) for their crimes. Impeachments are tried by the House of Lords, where a simple majority is necessary to convict. But this power has fallen into disuse: the House of Commons exercises its checks on the government through other means, such as no confidence motions; the last impeachment was that of Henry Dundas, 1st Viscount Melville in 1806.

Bills may be introduced in either house, though bills of importance generally originate in the House of Commons. The supremacy of the Commons in legislative matters is assured by the Parliament Acts, under which certain types of bills may be presented to the Queen for Royal Assent without the consent of the House of Lords. The Lords may not delay a money bill (a bill that, in the view of the Speaker of the House of Commons, solely concerns national taxation or public funds) for more than one month. Moreover, the Lords may not delay most other public bills for more than two parliamentary sessions, or one calendar year. These provisions, however, only apply to public bills that originate in the House of Commons. Moreover, a bill that seeks to extend a parliamentary term beyond five years requires the consent of the House of Lords.

By a custom that prevailed even before the Parliament Acts, only the House of Commons may originate bills concerning taxation or Supply. Furthermore, supply bills passed by the House of Commons are immune to amendments in the House of Lords. In addition, the House of Lords is barred from amending a bill so as to insert a taxation or supply-related provision, but the House of Commons often waives its privileges and allows the Lords to make amendments with financial implications. Under a separate convention, known as the Salisbury Convention, the House of Lords does not seek to oppose legislation promised in the Government's election manifesto. Hence, as the power of the House of Lords has been severely curtailed by statute and by practice, the House of Commons is clearly the more powerful chamber of Parliament.

The British Parliament of today largely descends, in practice, from the Parliament of England, although the 1706 Treaty of Union, and the Acts of Union that ratified the Treaty, created a new Parliament of Great Britain to replace the Parliament of England and the Parliament of Scotland, with the addition of 45 MPs and sixteen Peers to represent Scotland. Later still the Acts of Union 1800 brought about the abolition of the Parliament of Ireland and enlarged the Commons at Westminster with 100 Irish members, creating the Parliament of the United Kingdom of Great Britain and Ireland.

Although popularly considered to refer to the fact its members are commoners, the actual name of the House of Commons comes from the Norman French word for communities – "communes".

The current Commons' layout is influenced by the use of the original St. Stephen's Chapel in the Palace of Westminster.

The rectangular shape is derived from the shape of the chapel. Benches were arranged using the configuration of the chapel's choir stalls whereby they were facing across from one another. This arrangement facilitated an adversarial atmosphere that is representative of the British parliamentary approach.

The distance across the floor of the House between the government and opposition benches is , said to be equivalent to two swords’ length.

The House of Commons underwent an important period of reform during the 19th century. Over the years, several anomalies had developed in borough representation. The constituency boundaries had not been changed since 1660, so many towns whose importance had declined by the 19th century still retained their ancient right of electing two members, in addition to other boroughs that had never been important, such as Gatton.

Among the most notorious of these "rotten boroughs" were Old Sarum, which had only six voters for two MPs, and Dunwich, which had largely collapsed into the sea from coastal erosion. At the same time, large cities such as Manchester received no separate representation (although their eligible residents were entitled to vote in the corresponding county seat). Also notable were the pocket boroughs, small constituencies controlled by wealthy landowners and aristocrats, whose "nominees" were invariably elected.

The Commons attempted to address these anomalies by passing a Reform Bill in 1831. At first, the House of Lords proved unwilling to pass the bill, but were forced to relent when the prime minister, Charles, 2nd Earl Grey, advised King William IV to flood the House of Lords by creating pro-Reform peers. To avoid this, the Lords relented and passed the bill in 1832. The Reform Act 1832, also known as the "Great Reform Act", abolished the rotten boroughs, established uniform voting requirements for the boroughs, and granted representation to populous cities, but still retained many pocket boroughs.

In the ensuing years, the Commons grew more assertive, the influence of the House of Lords having been reduced by the Reform Bill crisis, and the power of the patrons reduced. The Lords became more reluctant to reject bills that the Commons had passed with large majorities, and it became an accepted political principle that the confidence of the House of Commons alone was necessary for a government to remain in office.

Many more reforms were introduced in the latter half of the 19th century. The Reform Act 1867 lowered property requirements for voting in the boroughs, reduced the representation of the less populous boroughs, and granted parliamentary seats to several growing industrial towns. The electorate was further expanded by the Representation of the People Act 1884, under which property qualifications in the counties were lowered. The Redistribution of Seats Act of the following year replaced almost all multi-member constituencies with single-member constituencies.

In 1908, the Liberal Government under Asquith introduced a number of social welfare programmes, which, together with an expensive arms race, forced the Government to seek higher taxes. In 1909, the Chancellor of the Exchequer, David Lloyd George, introduced the "People's Budget", which proposed a new tax targeting wealthy landowners. This measure failed in the heavily Conservative House of Lords, and the government resigned.

The resulting general election returned a hung parliament, but Asquith remained prime minister with the support of the smaller parties. Asquith then proposed that the powers of the Lords be severely curtailed. After a further election in December 1910, the Asquith Government secured the passage of a bill to curtail the powers of the House of Lords after threatening to flood the House with 500 new Liberal peers to ensure the passage of the bill.

Thus the Parliament Act 1911 came into effect, destroying the legislative equality of the two Houses of Parliament. The House of Lords was permitted only to delay most legislation, for a maximum of three parliamentary sessions or two calendar years (reduced to two sessions or one year by the Parliament Act 1949). Since the passage of these Acts, the House of Commons has become the dominant branch of Parliament, both in theory and in practice.

In 1918, women over 30 who owned property were given the right to vote, as were men over 21 who did not own property, quickly followed by the passage of a law enabling women to be eligible for election as members of parliament at the younger age of 21. The only woman to be elected that year was an Irish Sinn Féin candidate, Constance Markievicz, who therefore became the first woman to be an MP. However, owing to Sinn Féin's policy of abstention from Westminster, she never took her seat. 

Women were given equal voting status as men in 1928, and with effect from the General Election in 1950, various forms of plural voting (i.e. some individuals had the right to vote in more than one constituency in the same election), including University constituencies, were abolished.

Since the 17th century, MPs had been unpaid. Most of the men elected to the Commons had private incomes, while a few relied on financial support from a wealthy patron. Early Labour MPs were often provided with a salary by a trade union, but this was declared illegal by a House of Lords judgment of 1909. Consequently, a resolution was passed in the House of Commons in 1911 introducing salaries for MPs. Government ministers had always been paid.

In May and June 2009 revelations of MPs' expenses claims caused a major scandal and loss of confidence by the public in the integrity of MPs, as well as causing the first forced resignation of the Speaker in 300 years.

Since 1950, each Member of Parliament has represented a single constituency (also known as a seat). There remains a technical distinction between county and borough constituencies; its only effects are the amount of money candidates are allowed to spend during campaigns and the rank of the local authority co-opted Returning Officer who presides over the count. Geographic boundaries are determined by four permanent and independent Boundary Commissions, one each for England, Wales, Scotland, and Northern Ireland. The commissions conduct general reviews of electoral boundaries once every 8 to 12 years, and interim reviews. In drawing boundaries, they are required to prefer local government boundaries, but may deviate from these to prevent great disparities in electorate, as such disparities are given the formal term malapportionment. The proposals of the Boundary Commissions are subject to parliamentary approval, but may not be amended. After their next Periodic Reviews, the Boundary Commissions will be absorbed into the Electoral Commission, which was established in 2000. As of 2017, the UK is divided into 650 constituencies, with 533 in England, 40 in Wales, 59 in Scotland, and 18 in Northern Ireland.

General elections occur whenever Parliament is dissolved. The timing of the dissolution was normally chosen by the Prime Minister (see relationship with the Government above); however, as a result of the Fixed-term Parliaments Act 2011, Parliamentary terms are now fixed at five years, except in the event of the House of Commons sustaining a vote of no confidence or passing an "early election" motion, the latter having to be passed by a two-thirds majority. The first use of this procedure was in April 2017, when MPs voted in favour of Theresa May's call for a snap election to be held that June.

All elections in the UK are held on a Thursday. The Electoral Commission is unsure when this practice arose, but dates it to 1931, with the suggestion that it was made to coincide with market day; this would ease voting for those who had to travel into the towns to cast their ballot.

A candidate for a seat must submit nomination papers signed by ten registered voters from that area, and pay £500, which is refunded if the candidate wins at least five per cent of the vote. Such a deposit (see deposit (politics)) seeks to discourage frivolity and very long ballot papers which would cause vote splitting (and arguably voter confusion). Each constituency can be called a seat as it was in 1885, as it returns one member, using the first-past-the-post electoral system, under which the candidate with a "plurality of" votes wins, that is greatest number of votes. Minors (that is, anyone under the age of 18), members of the House of Lords, prisoners, and insane persons are not qualified to become members of the House of Commons. To vote, one must be a UK resident and a British citizen, or a citizen of a British overseas territory, of the Republic of Ireland, or of a member of the Commonwealth of Nations. British citizens living abroad are allowed to vote for 15 years after leaving. It is a criminal offence for a person to vote in the ballot of more than one seat which is vacant at any election. This has not always been the case as before 1948 plural voting was permitted as voters qualified by home ownership or residence and could vote under both entitlements simultaneously. 

Once elected, Members of Parliament normally continue to serve until the next dissolution of Parliament. But if a member dies or ceases to be qualified (see qualifications below), his or her seat falls vacant. It is also possible for the House of Commons to expel a member, a power exercised only in cases of serious misconduct or criminal activity. In each case, the vacancy is filled by a by-election in the constituency, with the same electoral system as in general elections.

The term "Member of Parliament" by modern convention means members of the House of Commons. These members may, and almost invariably, use the post-nominal letters "MP". The annual salary of each member is £74,962, effective from 1 April 2016. Members may also receive additional salaries for other offices they hold (for instance, the Speakership). Most members also claim for various office expenses (staff costs, postage, travelling, etc.) and, in the case of non-London-area members, for the costs of maintaining a home in the capital.

There are numerous qualifications that apply to Members of Parliament. One must be aged at least 18 (the minimum age was 21 until s.17 of the Electoral Administration Act 2006 came into force), and must be a citizen of the United Kingdom, of a British overseas territory, of the Republic of Ireland, or of a member state of the Commonwealth of Nations. These restrictions were introduced by the British Nationality Act 1981, but were previously far more stringent: under the Act of Settlement 1701, only natural-born subjects were qualified. Members of the House of Lords may not serve in the House of Commons, or even vote in parliamentary elections (just as the Queen does not vote); however, they are permitted to sit in the chamber during debates (unlike the Queen, who cannot enter the chamber).

A person may not sit in the Commons if she or he is the subject of a Bankruptcy Restrictions Order (applicable in England and Wales only), or if she or he is adjudged bankrupt (in Northern Ireland), or if his or her estate is sequestered (in Scotland). Previously, MPs detained under the Mental Health Act 1983 for six months or more would have their seat vacated if two specialists reported to the Speaker that the member was suffering from a mental disorder. However, this disqualification was removed by the Mental Health (Discrimination) Act 2013. There also exists a common law precedent from the 18th century that the deaf and dumb are ineligible to sit in the Lower House; this precedent, however, has not been tested in recent years.

Anyone found guilty of high treason may not sit in Parliament until she or he has either completed the term of imprisonment or received a full pardon from the Crown. Moreover, anyone serving a prison sentence of one year or more is ineligible. Finally, the Representation of the People Act 1983 disqualifies for ten years those found guilty of certain election-related offences. Several other disqualifications are codified in the House of Commons Disqualification Act 1975: holders of high judicial offices, civil servants, members of the regular armed forces, members of foreign legislatures (excluding the Republic of Ireland and Commonwealth countries), and holders of several Crown offices. Ministers, even though they are paid officers of the Crown, are not disqualified.

The rule that precludes certain Crown officers from serving in the House of Commons is used to circumvent a resolution adopted by the House of Commons in 1623, under which members are not permitted to resign their seats. In practice, however, they always can. Should a member wish to resign from the Commons, she or he may request appointment to one of two ceremonial Crown offices: that of Crown Steward and Bailiff of the Chiltern Hundreds, or that of Crown Steward and Bailiff of the Manor of Northstead. These offices are sinecures (that is, they involve no actual duties); they exist solely to permit the "resignation" of members of the House of Commons. The Chancellor of the Exchequer is responsible for making the appointment, and, by convention, never refuses to do so when asked by a member who desires to leave the House of Commons.

At the beginning of each new parliamentary term, the House of Commons elects one of its members as a presiding officer, known as the Speaker. If the incumbent Speaker seeks a new term, then the House may re-elect him or her merely by passing a motion; otherwise, a secret ballot is held. A Speaker-elect cannot take office until she or he has been approved by the Sovereign; the granting of the royal approbation, however, is a formality. The Speaker is assisted by three Deputy Speakers, the most senior of whom holds the title of Chairman of Ways and Means. The two other Deputy Speakers are known as the First and Second Deputy Chairman of Ways and Means. These titles derive from the Committee of Ways and Means, a body over which the chairman once used to preside; even though the Committee was abolished in 1967, the traditional titles of the Deputy Speakers are still retained. The Speaker and the Deputy Speakers are always members of the House of Commons.

Whilst presiding, the Speaker or Deputy Speaker wears ceremonial dress. The presiding officer may also wear a wig, but this tradition was abandoned by Speaker Betty Boothroyd. Her successor, Michael Martin, also did not wear a wig while in the chamber. The current Speaker, John Bercow, has chosen to wear a gown over a lounge suit, a decision that has sparked much debate and opposition.

The Speaker or deputy presides from a chair at the front of the House. This chair was designed by Augustus Pugin, who initially built a prototype of the chair at King Edward's School, Birmingham: that chair is called Sapientia and is where the chief master sits. The Speaker is also chairman of the House of Commons Commission, which oversees the running of the House, and controls debates by calling on members to speak. A member who believes that a rule (or Standing Order) has been breached may raise a "point of order", on which the Speaker makes a ruling that is not subject to any appeal. The Speaker may discipline members who fail to observe the rules of the House. Thus, the Speaker is far more powerful than his or her Lords counterpart, the Lord Speaker, who has no disciplinary powers. Customarily, the Speaker and the deputies are non-partisan; they do not vote (with the notable exception of tied votes, where the Speaker votes in accordance with Denison's rule), or participate in the affairs of any political party. By convention, a Speaker seeking re-election to parliament is not opposed in his or her constituency by any of the major parties. The lack of partisanship continues even after the Speaker leaves the House of Commons.

The Clerk of the House is both the House's chief adviser on matters of procedure and chief executive of the House of Commons. She or he is a permanent official, not a member of the House itself. The Clerk advises the Speaker on the rules and procedure of the House, signs orders and official communications, and signs and endorses bills. The Clerk also chairs the Board of Management, which consists of the heads of the six departments of the House. The Clerk's deputy is known as the Clerk Assistant. Another officer of the House is the Serjeant-at-Arms, whose duties include the maintenance of law, order, and security on the House's premises. The Serjeant-at-Arms carries the ceremonial mace, a symbol of the authority of the Crown and of the House of Commons, into the House each day in front of the Speaker, and the Mace is laid upon the Table of the House during sittings. The Librarian is head of the House of Commons Library, the House's research and information arm.

Like the Lords, the Commons meets in the Palace of Westminster in London. The Commons chamber is small and modestly decorated in green, in contrast to the large, lavishly furnished red Lords chamber. There are benches on two sides of the chamber, divided by a centre aisle. This arrangement reflects the design of St Stephen's Chapel, which served as the home of the House of Commons until destroyed by fire in 1834. The Speaker's chair is at one end of the Chamber; in front of it, is the Table of the House, on which the Mace rests. The Clerks sit at one end of the Table, close to the Speaker so that they may advise him or her on procedure when necessary.

Members of the Government sit on the benches on the Speaker's right, whilst members of the Opposition occupy the benches on the Speaker's left. In front of each set of benches a red line is drawn, which members are traditionally not allowed to cross during debates. Government ministers and the leader of the Opposition and the Shadow Cabinet sit on the front rows, and are known as "frontbenchers". Other members of parliament, in contrast, are known as "backbenchers". Not all Members of Parliament can fit into the Chamber at the same time, as it only has space to seat approximately two thirds of the Members. According to Robert Rogers, former Clerk of the House of Commons and Chief Executive, a figure of 427 seats is an average or a finger-in-the-wind estimate. Members who arrive late must stand near the entrance of the House if they wish to listen to debates. Sittings in the Chamber are held each day from Monday to Thursday, and also on some Fridays. During times of national emergency, the House may also sit at weekends.

Sittings of the House are open to the public, but the House may at any time vote to sit in private, which has occurred only twice since 1950. Traditionally, a Member who desired that the House sit privately could shout "I spy strangers" and a vote would automatically follow. In the past, when relations between the Commons and the Crown were less than cordial, this procedure was used whenever the House wanted to keep its debate private. More often, however, this device was used to delay and disrupt proceedings; as a result, it was abolished in 1998. Now, Members seeking that the House sit in private must make a formal motion to that effect.

Public debates are recorded and archived in Hansard. The post war redesign of the House in 1950 included microphones, and debates were allowed to be broadcast by radio in 1975. Since 1989, they have also been broadcast on television, which is now handled by BBC Parliament.

Sessions of the House of Commons have sometimes been disrupted by angry protesters throwing objects into the Chamber from the galleries—items thrown include leaflets, manure, flour by the group Fathers 4 Justice, and a canister of chlorobenzylidene malonitrile (tear gas). Even members have been known to disturb proceedings of the House. For instance, in 1976, Conservative MP Michael Heseltine seized and brandished the Mace of the House during a heated debate. However, perhaps the most famous disruption of the House of Commons was caused by Charles I, who entered the Commons Chamber in 1642 with an armed force to arrest five members for high treason. This action was deemed a breach of the privilege of the House, and has given rise to the tradition that the monarch does not set foot in the House of Commons.

Each year, the parliamentary session begins with the State Opening of Parliament, a ceremony in the Lords Chamber during which the Sovereign, in the presence of Members of both Houses, delivers an address outlining the Government's legislative agenda. The Gentleman Usher of the Black Rod (a Lords official) is responsible for summoning the Commons to the Lords Chamber. When he arrives to deliver his summons, the doors of the Commons Chamber are traditionally slammed shut in his face, symbolising the right of the Lower House to debate without interference. He then knocks on the door three times with his Black Rod, and only then is granted admittance, where he informs the MPs that the Monarch awaits them, after which they proceed to the House of Lords for the Queen's Speech.

During debates, Members may speak only if called upon by the Speaker (or a Deputy Speaker, if the Speaker is not presiding). Traditionally, the presiding officer alternates between calling Members from the Government and Opposition. The Prime Minister, the Leader of the Opposition, and other leaders from both sides are normally given priority. All Privy Counsellors used to be granted priority; however, the modernisation of Commons procedure in 1998 led to the abolition of this tradition.

Speeches are addressed to the presiding officer, using the words "Mr Speaker", "Madam Speaker", "Mr Deputy Speaker", or "Madam Deputy Speaker". Only the presiding officer may be directly addressed in debate; other members must be referred to in the third person. Traditionally, members do not refer to each other by name, but by constituency, using forms such as "the Honourable Member for [constituency]", or, in the case of Privy Counsellors, "the Right Honourable Member for [constituency]". Members of the same party (or allied parties or groups) refer to each other as "my (Right) Honourable friend". (A member of the Armed Forces used to be called "the Honourable and Gallant Member", a barrister "the Honourable and Learned Member", and a woman "the Honourable Lady the Member".) This may not always be the case during the actual oral delivery, when it might be difficult for a member to remember another member's exact constituency, but it is invariably followed in the transcript entered in the Hansard. The Speaker enforces the rules of the House and may warn and punish members who deviate from them. Disregarding the Speaker's instructions is considered a breach of the rules of the House and may result in the suspension of the offender from the House. In the case of grave disorder, the Speaker may adjourn the House without taking a vote.

The Standing Orders of the House of Commons do not establish any formal time limits for debates. The Speaker may, however, order a member who persists in making a tediously repetitive or irrelevant speech to stop speaking. The time set aside for debate on a particular motion is, however, often limited by informal agreements between the parties. Debate may also be restricted by the passage of "Allocation of Time Motions", which are more commonly known as "Guillotine Motions". Alternatively, the House may put an immediate end to debate by passing a motion to invoke Closure. The Speaker is allowed to deny the motion if she or he believes that it infringes upon the rights of the minority. Today, bills are scheduled according to a Timetable Motion, which the whole House agrees in advance, negating the use of a guillotine.

When the debate concludes, or when the Closure is invoked, the motion in question is put to a vote. The House first votes by voice vote; the Speaker or Deputy Speaker puts the question, and Members respond either "Aye!" (in favour of the motion) or "No!" (against the motion). The presiding officer then announces the result of the voice vote, but if his or her assessment is challenged by any member or the voice vote is unclear, a recorded vote known as a division follows. The presiding officer, if she or he believes that the result of the voice vote is clear, may reject the challenge. When a division occurs, members enter one of two lobbies (the "Aye" lobby or the "No" lobby) on either side of the Chamber, where their names are recorded by clerks. A member who wishes to pointedly abstain from a vote may do so by entering both lobbies, casting one vote for and one against. At each lobby are two tellers (themselves members of the House) who count the votes of the members.

Once the division concludes, the tellers provide the results to the presiding officer, who then announces them to the House. If there is an equality of votes, the Speaker or Deputy Speaker has a casting vote. Traditionally, this casting vote is exercised to allow further debate, if this is possible, or otherwise to avoid a decision being taken without a majority (e.g. voting 'No' to a motion or the third reading of a bill). Ties rarely occur—the last one was in July 1993. The quorum of the House of Commons is 40 members for any vote, including the Speaker and four tellers. If fewer than 40 members have participated, the division is invalid.

Formerly, if a member sought to raise a point of order during a division, suggesting that some of the rules governing parliamentary procedure are violated, he was required to wear a hat, thereby signalling that he was not engaging in debate. Collapsible top hats were kept in the Chamber just for this purpose. This custom was discontinued in 1998.

The outcome of most votes is largely known beforehand, since political parties normally instruct members on how to vote. A party normally entrusts some members of parliament, known as whips, with the task of ensuring that all party members vote as desired. Members of Parliament do not tend to vote against such instructions, since those who do so jeopardise promotion, or may be deselected as party candidates for future elections. Ministers, junior ministers and parliamentary private secretaries who vote against the whips' instructions usually resign. Thus, the independence of Members of Parliament tends to be low, although "backbench rebellions" by members discontent with their party's policies do occur. A member is also traditionally allowed some leeway if the particular interests of his constituency are adversely affected. In some circumstances, however, parties announce "free votes", allowing members to vote as they please. Votes relating to issues of conscience such as abortion and capital punishment are typically free votes.

Pairing is an arrangement where a member from one party agrees with a member of another party not to vote in a particular division, allowing both MPs the opportunity not to attend.

A bisque is permission from the Whips given to a member to miss a vote or debate in the House to attend to constituency business or other matters.

The British Parliament uses committees for a variety of purposes, e.g., for the review of bills. Committees consider bills in detail, and may make amendments. Bills of great constitutional importance, as well as some important financial measures, are usually sent to the "Committee of the Whole House", a body that includes all members of the Commons. Instead of the Speaker, the chairman or a Deputy Chairman of Ways and Means presides. The Committee meets in the House of Commons Chamber.

Most bills were until 2006 considered by Standing Committees, which consisted of between 16 and 50 members. The membership of each Standing Committee roughly reflected the strength of the parties in the House. The membership of Standing Committees changed constantly; new Members were assigned each time the committee considered a new bill. There was no formal limit on the number of Standing Committees, but usually only ten existed. Rarely, a bill was committed to a Special Standing Committee, which investigated and held hearings on the issues raised. In November 2006, Standing Committees were replaced by Public Bill Committees.

The House of Commons also has several Departmental Select Committees. The membership of these bodies, like that of the Standing Committees, reflects the strength of the parties. Each committee elects its own chairman. The primary function of a Departmental Select Committee is to scrutinise and investigate the activities of a particular government department. To fulfil these aims, it is permitted to hold hearings and collect evidence. Bills may be referred to Departmental Select Committees, but such a procedure is seldom used.

A separate type of Select Committee is the Domestic Committee. Domestic Committees oversee the administration of the House and the services provided to Members. Other committees of the House of Commons include Joint Committees (which also include members of the House of Lords), the Committee on Standards and Privileges (which considers questions of parliamentary privilege, as well as matters relating to the conduct of the members), and the Committee of Selection (which determines the membership of other committees).

The symbol used by the Commons consists of a portcullis topped by St Edward's Crown. The portcullis has been one of the Royal Badges of England since the accession of the Tudors in the 15th century, and was a favourite symbol of King Henry VII. It was originally the badge of Beaufort, his mother's family; and a pun on the name Tudor, as in tu-"door". The original badge was of gold, but nowadays is shown in various colours, predominantly green or black.

In 1986, the British television production company Granada Television created a near-full size replica of the post-1950 House of Commons debating chamber at its studios in Manchester for use in its adaptation of the Jeffrey Archer novel "First Among Equals". The set was highly convincing, and was retained after the production—since then, it has been used in nearly every British film and television production that has featured scenes set in the chamber. From 1988 until 1999 it was also one of the prominent attractions on the Granada Studios Tour, where visitors could watch actors performing mock political debates on the set. The major difference between the studio set and the real House of Commons Chamber is that the studio set has just four rows of seats on either side whereas the real Chamber has five.

In 2002, the set was purchased by the scriptwriter Paul Abbott so that it could be used in his BBC drama serial "State of Play". Abbott, a former Granada Television staff writer, bought it personally as the set would otherwise have been destroyed and he feared it would take too long to get the necessary money from the BBC. Abbott kept the set in storage in Oxford.

The pre-1941 Chamber was recreated in Shepperton Studios for the Ridley Scott/Richard Loncraine 2002 biographical film on Churchill, "The Gathering Storm".




</doc>
<doc id="13829" url="https://en.wikipedia.org/wiki?curid=13829" title="Hearts (card game)">
Hearts (card game)

Hearts is an "evasion-type" trick-taking playing card game for four players, although variations can accommodate between three and six players. The game is also known as Black Lady, Black Maria, Black Widow, and Slippery Bitch, though any of these may refer to the similar but differently-scored game Black Lady. The game is a member of the Whist family of trick-taking games (which also includes Bridge and Spades), but the game is unique among Whist variants in that it is an evasion-type game; players avoid winning certain penalty cards in tricks, usually by avoiding winning tricks altogether.

The game of Hearts as currently known originated with a family of related games called Reversis, which became popular around 1750 in Spain. In this game, a penalty point was awarded for each trick won, plus additional points for capturing or . A similar game called "Four Jacks" centered around avoiding any trick containing a Jack, which were worth one penalty point, and worth two.

Over time, additional penalty cards were added to Reversis, and around 1850, the game gave way to a simple variant of Hearts, where each heart was worth 1 point. The (sometimes referred to as "Calamity Jane") was introduced in a variant called Black Maria which then became known as the standard Hearts game, and soon thereafter, the idea of "shooting the moon" was introduced to the game to add depth to the gameplay. In the 1920s, the variation (ten positive points) was introduced, and some time later the scoring was reversed so that penalty points were expressed as positive instead of negative. Passing cards, breaking hearts, leading , and "shooting the foot", whereby a player attempts to shoot the moon, but succeeds in taking the Queen and all but one heart, are more recent additions.

The game has become popular in live play among grade school students in Canada, and has increased in popularity through Internet gaming sites. In many parts of the world it became known through the Microsoft version of the game packaged with most 1990s versions of its Windows operating system, beginning in version 3.11.

The overall objective is to be the player with the fewest points by the end of the game.

Thirteen cards are dealt to each player.

The basic game of Hearts does not include card passing, but the most common variants do. Before each hand begins, each player chooses three cards, and passes them to another player. The main objectives of passing are to try to become "short" or "void" in a suit, and thus able to play off-suit when that suit is led; or to rid one's hand of "dangerous" cards that will likely force that player to take a trick containing penalty points, such as the Ace, King, or Queen of any suit (especially spades and hearts). There are many variations on passing; the most common (popularized by computer versions) rotates passing through four deals; on the first deal, players pass to the left, the second deal to the right, the third across the table. On the fourth deal no cards are passed; the cycle of four deals is then repeated.

Other variations on the passing rules include:

The game is played like most other trick-taking games. The first trick is led by the player to the left of the dealer. Each other player, in clockwise order, then plays a card from their hand. Players must follow suit; that is, play a card of the same suit as the lead card, if they are able. If they are not able to do so, they can play any card (an action known as "sloughing" or "discarding"), including a penalty heart or . The trick and any penalty points it contains are won by the player who played the highest-value card of the suit that was led. That player then becomes the lead player for the next trick, and play continues until all players have exhausted their hands.

There are some common variants to this play:


Each Heart taken in a trick scores one penalty point against the player winning the trick, and taking costs 13 penalty points. There are thus 26 penalty points in each deal. The game usually ends when one player reaches or exceeds 100 points, or, in some variations, after a predetermined number of deals or period of time. In any of these cases, the winning player is the one with the fewest penalty points. Optionally, if two or more players have the same number of fewest penalty points in a 100-point game, it continues until there is a clear winner. For example, if player one has 120 points, player two has 90 points, and players three and four each have 85 points.

It is also possible to score with chips. All players contribute one chip to a central pool of chips. The pool is divided equally among those players taking no penalty cards on a deal; if all players take penalty cards, the pool remains on the table and is added to the next pool. Once one player has won all available chips, or once another player has run out, the game ends.

There are many scoring variants, including:

"Shooting the moon", also known as "getting control", "capmangoe", or "running the cards", is a very common scoring variant. If one player takes all the penalty cards on one deal, that player's score remains unchanged while 26 penalty points are added to the scores of each of the other players. This is known as playing by "Old Moon" rules. In the "New Moon" rules, the player subtracts 26 points from their own score instead of adding 26 to the others. Attempting to shoot the moon is often a risky strategy, as failure to capture every single penalty card will result in the remaining penalty points (as many as 25, if only one heart is missed) being added to one's score.

There are several sub-variations to these rules:


With the exception of trying to shoot the moon, players attempt to discard high cards, especially high hearts or spades, and try to avoid winning points. This can either be accomplished by creating a void, or playing a high card last in a trick that has no points. Generally speaking, it is advantageous to play the highest card possible without winning the trick. Players particularly concentrate on getting rid of high cards in suits that they do not have padding low cards in. For example, if a player had the would not be much of an issue; they can play the four lower cards and hopefully exhaust another player's spades before being forced to play the . In contrast, if they only had , then they would be forced to play the the first time spades were led, with the risk that another player will respond with .

A void is when a player does not have any cards of a certain suit. Generally this is a highly advantageous situation, because it prevents the player from winning any points in that suit, and provides a means to dispose of poor cards. These can be intentionally created with good passing strategy, or appear by themselves.

If a player does not have , , or , it is to that player's advantage to lead spades to try to force onto another player. This is also referred to as "smoking the Lady/Queen", in reference to the combat action of creating smoke in a closed area so that any enemy combatants hiding there would be forced to come out into the open (the is forced to be played by its bearer, thus causing the 's points to be taken by that bearer, and thus evaded by the player actuating the smoking). A player who has or but not , and with a large number of other spades, may feel that he is insulated from being forced to play or in any subsequent smoking play, and thus could also participate in smoking. Even the player that has the , provided he similarly has a lot of other spades may decide to "smoke himself" to empty all the other players of their spades, thus ironically insulating himself from being smoked in the future – or perhaps to get the game into the situation in which another player(s) has a "dry" (i.e., solitary) spade that is or , in which case he could start a trick with in full knowledge that some other player has this dry or and will be forced to play it, taking the 's points.

Shooting the moon can rapidly change the direction of a game in a player's favor; however, it is also very risky. A good hand for shooting the moon should contain significant high cards, in addition to a long run of a single suit that can be used to keep the lead once the other players have run out of that suit.

In general, when passing, the player is trying to both get rid of bad cards "and" create voids to get rid of bad cards they may receive. and are two of the most important cards to get rid of, because they can draw in . In addition, high hearts can leave the player defenseless once hearts are broken. Care must be taken in passing too many high cards which could allow the player receiving the cards to shoot the moon. When creating voids, it is best to do that in either clubs or diamonds, because players want to avoid receiving high hearts or spades without having any padding. For this reason spades lower than the queen are typically not passed unless the player is attempting to shoot the moon.


</doc>
<doc id="13830" url="https://en.wikipedia.org/wiki?curid=13830" title="Hastings">
Hastings

Hastings is a town and borough in East Sussex on the south coast of England,

Hastings gives its name to the Battle of Hastings, which took place to the north at Senlac Hill in 1066.

The town later became one of the medieval Cinque Ports, and a popular seaside resort in the 19th century with the coming of the railway.

Today, Hastings is a fishing port with a beach-based fishing fleet.

The first mention of Hastings is found in the late 8th century in the form "Hastingas". This is derived from the Old English tribal name "Hæstingas", meaning `the constituency/followers of Hæsta'. Symeon of Durham records the victory of Offa in 771 over the "Hestingorum gens", that is, "the people of the Hastings tribe.", Hastingleigh in Kent was named after that tribe. The place name "Hæstingaceaster" is found in the "Anglo-Saxon Chronicle" entry for 1050, and may be an alternative name for Hastings. However, the absence of any archaeological remains of or documentary evidence for a Roman fort at Hastings suggest that "Hæstingaceaster" may refer to a different settlement, most likely that based on the Roman remains at Pevensey.

Evidence of prehistoric settlements have been found at the town site: flint arrowheads and Bronze Age artefacts have been found; Iron Age forts have been excavated on both the East and West Hills. This suggests that the inhabitants moved early to the safety of the valley in between the forts. The settlement was already based on the port when the Romans arrived in Britain for the first time in 55 BC. At this time, they began to exploit the iron (Wealden rocks provide a plentiful supply of the ore), and shipped it out by boat. Iron was worked locally at Beauport Park, to the north of the town, which employed up to one thousand men and is considered to have been the third largest mine in the Roman Empire.

With the departure of the Romans, the town suffered setbacks. The Beauport site had been abandoned, and natural and man-made attacks began. The Sussex coast has always suffered from occasional violent storms; with the additional hazard of longshore drift (the eastward movement of shingle along the coast), the coastline has been frequently changing. The original Roman port could well now be under the sea.

Bulverhythe was probably a harbour used by Danish invaders, which suggests that "-hythe" or "hithe" means a port or small haven.

From the 6th century AD until 771, the people of the area around modern-day Hastings, identified the territory as that of the Haestingas tribe and a kingdom separate from the surrounding kingdoms of Suth Saxe ("South Saxons", i.e. Sussex) and Kent. It worked to retain its separate cultural identity until the 11th century. The kingdom was probably a sub-kingdom, the object of a disputed overlordship by the two powerful neighbouring kingdoms: when King Wihtred of Kent settled a dispute with King Ine of Sussex & Wessex in 694, it is probable that he seceded the overlordship of Haestingas to Ine as part of the treaty.

In 771 King Offa of Mercia invaded Southern England, and over the next decade gradually seized control of Sussex and Kent. Symeon of Durham records a battle fought at an unidentified location near Hastings in 771, at which Offa defeated the Haestingas tribe, effectively ending its existence as a separate kingdom. By 790, Offa controlled Hastings effectively enough to confirm grants of land in Hastings to the Abbey of St Denis, in Paris. But, the "Anglo-Saxon Chronicle" for 1011 relates that Vikings overran "all Kent, Sussex, Surrey and Haestingas", indicating the town was still considered a separate 'county' or province to its neighbours 240 years after Offa's conquest.

During the reign of Athelstan, he established a royal mint in Hastings in AD 928.

The start of the Norman Conquest was the Battle of Hastings, fought on 14 October 1066, although the battle itself took place to the north at Senlac Hill, and William had landed on the coast between Hastings and Eastbourne at Pevensey. It is thought that the Norman encampment was on the town's outskirts, where there was open ground; a new town was already being built in the valley to the east. That "New Burgh" was founded in 1069 and is mentioned in the Domesday Book as such. William defeated and killed Harold Godwinson, the last Saxon King of England, and destroyed his army, thus opening England to the Norman conquest.

William caused a castle to be built at Hastings probably using the earthworks of the existing Saxon castle.

Hastings was shown as a borough by the time of the Domesday Book (1086); it had also given its name to the Rape of Hastings, one of the six administrative divisions of Sussex. As a borough, Hastings had a corporation consisting of a "bailiff, jurats, and commonalty". By a Charter of Elizabeth I in 1589, the bailiff was replaced by a mayor.

Muslim scholar Muhammad al-Idrisi, writing c.1153, described Hastings as "a town of large extent and many inhabitants, flourishing and handsome, having markets, workpeople and rich merchants".

By the end of the Saxon period, the port of Hastings had moved eastward near the present town centre in the Priory Stream valley, whose entrance was protected by the White Rock headland (since demolished). It was to be a short stay: Danish attacks and huge floods in 1011 and 1014 motivated the townspeople to relocate to the New Burgh.

In the Middle Ages Hastings became one of the Cinque Ports; Sandwich, Dover and New Romney being the first, Hastings and Hythe followed, all finally being joined by Rye and Winchelsea, at one point 42 towns were directly or indirectly affiliated with the group.

In the 13th century, much of the town and half of Hastings Castle was washed away in the South England flood of February 1287. During a naval campaign of 1339, and again in 1377, the town was raided and burnt by the French, and seems then to have gone into a decline. As a port, Hastings' days were finished.
Hastings had suffered over the years from the lack of a natural harbour, and there have been attempts to create a sheltered harbour. Attempts were made to build a stone harbour during the reign of Elizabeth I, but the foundations were destroyed by the sea in terrible storms. The fishing boats are still stored on and launched from the beach.

Hastings was then just a small fishing settlement, but it was soon discovered that the new taxes on luxury goods could be made profitable by smuggling; the town was ideally located for that purpose. Near the castle ruins, on the West Hill, are "St Clement's Caves", partly natural, but mainly excavated by hand by smugglers from the soft sandstone. Their trade was to come to an end with the period following the Napoleonic Wars, for the town became one of the most fashionable resorts in Britain, brought about by the so-called health-giving properties of seawater, as well as the local springs and Roman baths. Once this came about the expansion of the town took place, to the west, since there was little space left in the valley.
It was at this time that the elegant Pelham Crescent and Wellington Square were built: other building followed. In the Crescent (designed by architect Joseph Kay) is the classical style church of St Mary in the Castle (its name recalling the old chapel in the castle above) now in use as an arts centre. The building of the crescent and the church necessitated further cutting away of the castle hill cliffs. Once that move away from the old town had begun, it led to the further expansion along the coast, eventually linking up with the new St Leonards.

Like many coastal towns, the population of Hastings grew significantly as a result of the construction of railway links and the fashionable growth of seaside holidays during the Victorian era. In 1801, its population was a mere 3,175; by 1831, it had reached over ten thousand; by 1891, it was almost sixty thousand.

The last harbour project began in 1896, but this also failed when structural problems and rising costs exhausted all the available funds. Today a fractured seawall is all that remains of what might have become a magnificent harbour. In 1897, the foundation stone was laid on a large concrete structure, but there was insufficient money to complete the work and the "Harbour arm" remains uncompleted. It was later partially blown up to discourage possible use by German invasion forces during World War II.

Between 1903 and 1919 Fred Judge FRPS photographed many of the towns events and disasters. These included storms, the first tram, visit of the Lord Mayor of London, Hastings Marathon Race and the pier fire of 1917. Many of these images were produced as picture postcards by the British Postcard manufacturer he founded now known as Judges Postcards.

In the 1930s, the town underwent some rejuvenation. Seaside resorts were starting to go out of fashion: Hastings perhaps more than most. The town council set about a huge rebuilding project, among which the promenade was rebuilt, and an Olympic-size bathing pool was erected. The latter, regarded in its day as one of the best open-air swimming and diving complexes in Europe, later became a holiday camp before closing in 1986. It was demolished, but the area is still known by locals as "The Bathing Pool".

The 2001 census reported over 85,000 inhabitants.

Hastings returned two Members of Parliament from the 14th century until 1885, since when it has returned one. Since 1983, it has been part of the parliamentary constituency of Hastings and Rye; the current MP, since 2010, is Amber Rudd of the Conservative Party. Prior to 1983, the town formed the Hastings parliamentary constituency by itself.

Hastings, it is thought, was a Saxon town before the arrival of the Normans: the Domesday Book refers to a "new Borough": as a borough, Hastings had a corporation consisting of a "bailiff, jurats, and commonalty". Its importance was such that it also gave its name to one of the six Rapes or administrative districts of Sussex.
By a Charter of Elizabeth I in 1589 the bailiff was replaced by a mayor, by which time the town's importance was dwindling. In the Georgian era, patronage of such seaside places (such as nearby Brighton) gave it a new lease of life so that, when the time came with the reform of English local government in 1888, Hastings became a County Borough, responsible for all its local services, independent of the surrounding county, then Sussex (East); less than one hundred years later, in 1974, that status was abolished.

Hastings Borough Council is now in the second tier of local government, below East Sussex County Council. The Labour Party has an overall control of the council with 23 seats, whilst the Conservative Party holds 9 seats. The Borough is divided into sixteen electoral wards; Ashdown, Baird, Braybrooke, Castle, Central St Leonards, Conquest, Gensing, Hollington, Maze Hill, Old Hastings, Ore, St Helens, Silverhill, Tressell, West St Leonards and Wishing Tree. The council leader is Peter Chowney, who stood against Rudd as a Labour candidate in the 2017 election.

Hastings is situated where the sandstone beds, at the heart of the Weald, known geologically as the Hastings Sands, meet the English Channel, forming tall cliffs to the east of the town. Hastings Old Town is in a sheltered valley between the East Hill and West Hill (on which the remains of the Castle stand). In Victorian times and later the town has spread westwards and northwards, and now forms a single urban centre with the more suburban area of St Leonards-on-Sea to the west. Roads from the Old Town valley lead towards the Victorian area of Clive Vale and the former village of Ore, from which "The Ridge", marking the effective boundary of Hastings, extends north-westwards towards Battle. Beyond Bulverhythe, the western end of Hastings is marked by low-lying land known as Glyne Gap, separating it from Bexhill-on-Sea.

The sandstone cliffs have been the subject of considerable erosion in relatively recent times: much of the Castle was lost to the sea before the present sea defences and promenade were built, and a number of cliff-top houses are in danger of disappearing around the nearby village of Fairlight.

The beach is mainly shingle, although wide areas of sand are uncovered at low tide. The town is generally built upon a series of low hills rising to above sea level at "The Ridge" before falling back in the river valley further to the north.
There are three Sites of Special Scientific Interest within the borough; Marline Valley Woods, Combe Haven and Hastings Cliffs To Pett Beach. Marline Valley Woods lies within the Ashdown ward of Hastings. It is an ancient woodland of Pedunculate oak—hornbeam which is uncommon nationally. Sussex Wildlife Trust own part of the site. Combe Haven is another site of biological interest, with alluvial meadows, and the largest reed bed in the county, providing habitat for breeding birds. It is in the West St Leonards ward, stretching into the parish of Crowhurst. The final SSSI, Hastings Cliffs to Pett Beach, is within the Ore ward of Hastings, extending into the neighbouring Fairlight and Pett parishes. The site runs along the coast and is of both biological and geological interest. The cliffs hold many fossils and the site has many habitats, including ancient woodland and shingle beaches.

As with the rest of the British Isles and Southern England, Hastings experiences a maritime climate with cool summers and mild winters. In terms of the local climate, Hastings is on the eastern edge of what is, on average, the sunniest part of the UK, the stretch of coast from the Isle of Wight to the Hastings area. Hastings, tied with Eastbourne, recorded the highest duration of sunshine of any month anywhere in the United Kingdom – 384 hours – in 1911. Temperature extremes since 1960 at Hastings have ranged from in July 2006, down to in January 1987. The Köppen Climate Classification subtype for this climate is "" (Marine West Coast Climate/Oceanic climate).

Some of the areas and suburbs of Hastings are Ore Valley, St Leonards, Silverhill, West St Leonards, and Hollington. Ore, Silverhill and Hollington were once villages that have since become part of the Hastings conurbation area during rapid growth. The original part St Leonards was bought by James Burton and laid out by his son, the architect Decimus Burton, in the early 19th century as a new town: a place of elegant houses designed for the well-off; it also included a central public garden, a hotel, an archery, assembly rooms and a church. Today's St Leonards has extended well beyond that original design, although the original town still exists within it.

The population of the town in 2001 was 85,029, by 2009 the estimated population was 86,900. Hastings suffers at a disadvantage insofar as growth is concerned because of its restricted situation, lying as it does with the High Weald Area of Outstanding Natural Beauty to the north. Redevelopment of the area is partly hampered by the split administration of the combined Hastings and Bexhill economic region between Hastings and Rother district councils. There is little space for further large-scale housing and employment growth within the designated boundaries of Hastings, and development on the outskirts is resisted by Rother council whose administrative area surrounds Hastings. Rother has a policy of urban expansion in the area immediately north of Bexhill, but this requires infrastructure improvements by central Governments which have been under discussion for decades. This situation has now become the subject of parliamentary consideration.

Until the development of tourism, fishing was Hastings' major industry. The fishing fleet, based at the Stade, remains Europe's largest beach-launched fishing fleet and has recently won accreditation for its sustainable methods. The fleet has been based on the same beach, below the cliffs at Hastings, for at least 400, possibly 600, years. Its longevity is attributed to the prolific fishing ground of Rye Bay nearby. Hastings fishing vessels are registered at Rye, and thus bear the letters "RX" (Rye, SusseX).

There are now various industrial estates that lie around the town, mostly on the outskirts, which include engineering, catering, motoring and construction; however, most of the jobs within the Borough are concentrated on health, public services, retail and education. 85% of the firms (in 2005) employed fewer than 10 people; as a consequence the unemployment rate was 3.3% ("cf." East Sussex 1.7%). However, qualification levels are similar to the national average: 8.2% of the working-age population have no qualifications while 28% hold degree-level qualifications or higher, compared with 11% and 31% respectively across England.

Hastings main shopping centre is Priory Meadow Shopping Centre, which was built on the site of the old Central Recreation Ground which played host to some Sussex CCC first-class fixtures, and cricketing royalty such as Dr. W. G. Grace and Sir Don Bradman. The centre houses 56 stores and covers around 420,000 ft. Further retail areas in the town centre include Queens Road, Wellington Place and Robertson Street.
There are plans to expand the retail area in Hastings, which includes expanding Priory Meadow and creating more retail space as part of the Priory Quarter development. Priory was intended to have a second floor added to part of the retail area, which has not happened yet and so far only office space has been created as part of the Priory Quarter.

In 2002 the Hastings and Bexhill task force, set up by the South East England Development Agency, was founded to regenerate the local economy, a 10-year programme being set up to tackle the local reliance on public sector employment. The regeneration scheme saw the construction of the University Centre Hastings, (now known as the University of Brighton in Hastings) the new Sussex Coast College campus and construction of the Priory Quarter, which still remains unfinished but now houses Saga offices, bringing 800 new jobs to the area.

Hastings has an Army Cadet Force (ACF) detachment which is part of Sussex ACF. This detachment is based in the old Territorial Army Unit Building on Cinque Ports Way, and is affiliated to PWRR. Hastings also has a Royal Air Force Air Cadet Squadron, 304 (Hastings) Squadron of Sussex Wing RAFAC, based in the same building. The town also has a Sea Cadet squadron, T.S. "Hastings". This sits adjacent to the Army and Air Cadet building on the seafront. The site features a climbing wall and other training facilities.

Throughout the year many annual events take place in Hastings, the largest of which being the May Day bank holiday weekend, which features a Jack-in-the-Green festival (revived since 1983), and the culmination of the Maydayrun—tens of thousands of motorcyclists having ridden the A21 to Hastings. The yearly carnival during Old Town Week takes place every August, which includes a week of events around Hastings Old Town, including a Seaboot race, bike race, street party and pram race. In September, there is a month-long arts festival 'Coastal Currents' and a Seafood and Wine Festival. During Hastings Week held each year around 14 October the Hastings Bonfire Society stages a traditional Sussex Bonfire which includes a torchlight procession through the streets, a beach bonfire and firework display.
Hastings Pirate Day takes place in July every year. Hastings, as of November 2017, still holds the Guinness World Record for the most pirates in one place.

Other smaller events include the Hastings Beer and Music Festival, held every July in Alexandra Park, the Hastings Musical Festival held every March in the White Rock Theatre and the Hastings International Chess Congress.

There are two theatres in the town, the White Rock Theatre and the Stables Theatre. The White Rock theatre is the venue of the yearly pantomime and throughout the year hosts comedy, dance and music acts. The Stables stages more local productions and acts as an arts exhibition centre. The Phoenix Arts Centre, based at Ark William Parker Academy also stages local productions as well as shows put on by the school.

There is a small four screen Odeon cinema in the town, located opposite the town hall; however, there are plans to build a new multiplex cinema as part of the Priory Quarter development in the town centre. The town has an independent cinema called the Electric Palace located in the Old Town and a restored cinema in St Leonards called the Kino Teatr.

There are three museums in Hastings; the Hastings Museum and Art Gallery, the Hastings Fishermen's Museum and the Shipwreck Museum. The former two mentioned are open for the whole year while the Shipwreck Museum is open only weekends during the winter, but daily for the rest of the year.

The Hastings Museum and Art gallery concentrates mostly on local history and contains exhibits on Grey Owl and John Logie Baird. It also features a Durbar Hall, donated by Lord Brassey; the hall contains displays focusing on the Indian subcontinent and the Brassey Family. The Fishermen's Museum, housed in the former fishermen's church, is dedicated to the fishing industry and maritime history of Hastings. The Shipwreck Museum displays artifacts from wrecks around the area.

The Jerwood Gallery located in the Stade area of Hastings Old Town is the home for the Jerwood Collection of 20th and 21st century art and a changing contemporary exhibition programme. The project was opposed by many locals who felt that a new art gallery would have been better located elsewhere in the town.

There are many parks and open spaces located throughout the town, one of the most popular and largest being Alexandra Park opened in 1882 by the Prince and Princess of Wales. The park contains gardens, open spaces, woods, a bandstand, tennis courts and a cafe. Other open spaces include White Rock Gardens, West Marina Gardens, St Leonards Gardens, Gensing Gardens, Markwick Gardens, Summerfields Woods, Linton Gardens, Hollington woods, Filsham Valley, Warrior Square, Castle Hill, St Helens Woods and Hastings Country Park.

Hastings Castle was built in 1070 by the Normans, four years after the Norman invasion. It is located on the West Hill, overlooking the town centre and is a Grade I listed building. Little remains of the castle apart from the arch left from the chapel, part of the walls and dungeons. The nearby St. Clements Caves are home to the Smugglers Adventure, which features interactive displays relating to the history of smuggling on the south coast of England.

Hastings Pier can be seen from any part of the seafront in the town. The pier was closed in 2006 following safety concerns from the council. In October 2010, a serious fire burned down most of the buildings on the pier and caused further damage to the structure. However, the pier reopened on 27 April 2016 after a £14.2m refurbishment.
Many church buildings throughout the town are Grade II listed including; Church in the Wood, Blacklands Parish Church, Ebenezer Particular Baptist Chapel, Fishermen's Museum and St Mary Magdalene's Church.

On the seafront at St Leonards is Marine Court, a 1938 block of flats in the Art Deco style that was originally called 'The Ship' due to its style being based on the ocean liner RMS "Queen Mary". This block of flats can be seen up to away on a clear day, from Holywell, in the Meads area of Eastbourne.

An important former landmark was "the Memorial", a clock tower commemorating Albert the Prince Consort which stood for many years at the traffic junction at the town centre, but was demolished following an arson attack in the 1970s.

There are two major roads in Hastings: the A21 trunk road to London; and the A259 coastal road. Both are beset with traffic problems: although the London road, which has to contend with difficult terrain, has had several sections of widening over the past decades there are still many delays. Long-term plans for a much improved A259 east–west route (including a Hastings bypass) were abandoned in the 1990s. A new Hastings-Bexhill Link Road opened in April 2016 known as the A2690 with the hope of reducing traffic congestion along the A259 Bexhill Road. The new link road travels from Queensway in the North of Hastings and joins up to the A259 in Bexhill. Hastings is also linked to Battle via the A2100, the original London road.

The town is served by Stagecoach buses on routes that serve the town, and also extend to Bexhill, Eastbourne and Dover as part of The Wave route. Stagecoach also run long distance buses up to Northiam, Hawkhurst, Royal Tunbridge Wells, Ashford and Canterbury.

National Express Coaches run service 023 to London.

Hastings has four rail links: two to London, one to Brighton and one to Ashford. Of the London lines, the shorter is the Hastings Line, the former South Eastern Railway (SER) route to Charing Cross via Battle and Tunbridge Wells, which opened in 1852; and the longer is the East Coastway Line, the former London, Brighton and South Coast Railway (LBSCR) route to Victoria via Bexhill, Eastbourne and Lewes. Trains to Brighton also use the East Coastway Line. The Marshlink Line runs via Rye to Ashford where a connection can be made with Eurostar services, and is unelectrified except for the Hastings-Ore segment.

A historic British Rail Class 201 "Thumper" can sometimes been seen (and heard) on historic runs to and from Hastings.

Hastings is served by two rail companies: Southeastern and Southern.
Southeastern services run along the Hastings Line, generally terminating at Hastings, with some peak services extending to Ore; the other lines are served by Southern, with services terminating at Ore or Ashford.

The town currently has four railway stations: from west to east they are West St Leonards station, St Leonards Warrior Square, Hastings, and Ore; this latter has been proposed to be renamed to Ore Valley. There is also one closed station and one proposed station in the area. West Marina station (on the LBSCR line) was very near West St Leonards (on the SER line) and was closed some years ago. A new station has been proposed at Glyne Gap in Bexhill, which would also serve residents from western Hastings.

There are two funicular railways, known locally as the West Hill and East Hill Lifts respectively.

The Hastings Miniature Railway operates along the beach from Rock-a-Nore to Marine Parade, and has provided tourist transport since 1948. The railway was considerably restored and re-opened in 2010.

A local metro railway service from Bexhill to Ore has also been proposed.

The Saxon Shore Way, (a long distance footpath, in length from Gravesend, Kent traces the Kent and Sussex coast "as it was in Roman times" to Hastings. The National Cycle Network route NCR2 links Dover to St Austell along the south coast, and passes through Hastings.

Hastings became part of the Turnpike road system in 1837, when builder James Burton was building his new town of St Leonards. The route of the road is that taken by the A21 today.

Hastings had a network of trams from 1905 to 1929. The trams ran as far as Bexhill, and were worked by overhead electric wires, except for the stretch along the seafront from Bo-Peep to the Memorial, which was initially worked by the Dolter stud contact system. The Dolter system was replaced by petrol electric trams in 1914, but overhead electrification was extended to this section in 1921. Trolleybuses rather than trams were used in the section that included the very narrow High Street, and the entire tram network was replaced by the Hastings trolleybus system in 1928–1929.

Maidstone & District bought the Hastings Tramway Company in 1935, but the trolleybuses still carried the "Hastings Tramways" logo until shortly before they were replaced by diesel buses in 1959, following the failure of the "Save our trolleys" campaign.

Hastings has 18 primary schools, four secondary schools, one further education college and one higher education institution.

The University of Brighton in Hastings offers higher education courses in a range of subjects and currently attracts over 800 students. The university's Hastings campus doubled in size in 2012, with the addition of the new Priory Square building designed by Proctor and Matthews Architects. This is located in the town centre a short distance from the railway station.
Sussex Coast College, formerly called Hastings College, is the town's further education college; it is located at Station Plaza, next to the railway station.

The secondary schools in the town include Ark Helenswood Academy, Ark William Parker Academy, Hastings Academy and The St Leonards Academy. East Sussex County Council closed three mixed comprehensive schools: Filsham Valley, The Grove and Hillcrest, replacing them with two academy schools, The St Leonards Academy, and The Hastings Academy. The sponsors for the academies are University of Brighton (lead sponsor), British Telecom and East Sussex County Council itself. East Sussex County Council provisionally approved the closure of Hillcrest, The Grove and Filsham Valley. The academies were opened in September 2011.

The most important buildings from the late medieval period are the two churches in the Old Town, St Clement's (probably built after 1377) and All Saints (early 15th century).
There is also a mosque, formerly "Mercatoria School" until purchased by the East Sussex Islamic Association. The former Ebenezer Particular Baptist Chapel in the Old Town dates from 1817 and is listed at Grade II. Christ Church, Blacklands (1876) has a complete decorative scheme of Mural, Stained Glass, Mosaic and Wrought Iron from the firm of Hardman's which gives it a ll* listing. When St. Andrew's was demolished in 1970 to make way for a supermarket, a fragment of the decorative scheme there, painted by Robert Noonan (also known as Robert Tressell, author of the Ragged Trousered Philanthropists) was rescued and features in the Hastings Museum. The Parish and title were added to Blacklands Church.

Every year the Hastings Half Marathon is held in the town. The 13.1 mi (21.1 km) race first took place in 1984 and attracts entrants from all over the country, taking runners on a route encircling the town, starting and finishing by the West Marina Gardens in St Leonards.

Hastings United is the town's most senior football team, playing in the Premier Division of the Isthmian League. It was founded in 1894 and plays its home games at The Pilot Field, which ground used to be home to two other senior clubs; St Leonards and the original Hastings United which folded in 1985. There are football clubs in Hastings that compete in the East Sussex League, such as Hollington United, St Leonards Social and Rock-a-Nore, playing at local parks and recreation grounds about the town. United attracted sports media headlines, when in 2012 they made it to the third round of the FA Cup for the first time in their history, being the lowest ranked team left in the contest before going out - losing 4-1 to Middlesbrough.

The Central Recreation Ground was one of England's oldest, most scenic and most famous cricket grounds. The first match was played there in 1864 and the last in 1989, after which the site was redeveloped as a shopping centre. It was particularly popular with touring Australian sides who played 18 matches there. Hastings Priory is the town's largest cricket club, having 4 teams playing competitive, as well as a large junior section. The club's home is at Horntye Park, though it also makes use of the facilities at Ark William Parker Academy.

William Parker sees clubs using the school as their base, such as Hastings & Bexhill Rugby Club, Hastings Athletic Club and Hastings Priory Cricket Club 3rd and 4th teams. Other local teams include Cinque Ports Rugby Club which plays at Grove School, and South Saxons Hockey Club which plays on the astroturf pitch at Horntye Park.

Hastings Conquerors is the town's only American Football Club. The club was founded in March 2013 by local resident Chris Chillingworth and currently trains at William Parker Sports College. The club made history in June 2013 when it became the UK's first Co-Operative run not-for-profit American Football club.

There are many bowling greens in the parks and gardens located about the town; the Hastings Open Bowls Tournament has been held annually in June since 1911 and attracts many entrants country-wide.

Since 1920 Hastings has hosted the Hastings International Chess Congress. A testament to its importance is that every World Champion before Garry Kasparov except Bobby Fischer played at Hastings including Emanuel Lasker (1895), José Raúl Capablanca (1919, 1929/30, 1930/1 and 1934/5), Alexander Alekhine (1922, 1925/6, 1933/4 and 1936/7), Max Euwe (1923/4, 1930/1, 1931/2, 1934/5, 1945/6 and 1949/50), Mikhail Tal (1963/4), Tigran Petrosian (1977/8), Boris Spassky (1965/6), and Anatoly Karpov (1971/2).

Hastings & St Leonards/Hastings Downs Golf Club (now defunct) was founded in 1893. The club disappeared in the 1950s.

John Logie Baird lived in Hastings in the 1920s where he carried out experiments that led to the transmission of the first television image. Robert Tressell wrote "The Ragged Trousered Philanthropists" in Hastings between 1906 and 1910. Many notable figures were born, raised, or lived in Hastings, including computer scientist Alan Turing, poet Fiona Pitt-Kethley, actress Gwen Watford, comedian Jo Brand and singer Suggs.
Gareth Barry, who holds the record number of appearances in the Premier League, was born in Hastings. The author who worked as Grey Owl was born In Hastings and lived here for several years.



Hastings is twinned with:





</doc>
<doc id="13831" url="https://en.wikipedia.org/wiki?curid=13831" title="Human rights">
Human rights

Human rights are "the basic rights and freedoms to which all humans are entitled" Examples of rights and freedoms which are often thought of as human rights include civil and political rights, such as the right to life, liberty, and property, freedom of expression, pursuit of happiness and equality before the law; and social, cultural and economic rights, including the right to participate in science and culture, the right to work, and the right to education.

Ancient peoples did not have the same modern-day conception of universal human rights. The true forerunner of human-rights discourse was the concept of natural rights which appeared as part of the medieval natural law tradition that became prominent during the European Enlightenment. From this foundation, the modern human rights arguments emerged over the latter half of the 20th century.
17th-century English philosopher John Locke discussed natural rights in his work, identifying them as being "life, liberty, and estate (property)", and argued that such fundamental rights could not be surrendered in the social contract. In Britain in 1689, the English Bill of Rights and the Scottish Claim of Right each made illegal a range of oppressive governmental actions. Two major revolutions occurred during the 18th century, in the United States (1776) and in France (1789), leading to the United States Declaration of Independence and the French Declaration of the Rights of Man and of the Citizen respectively, both of which articulated certain human rights. Additionally, the Virginia Declaration of Rights of 1776 encoded into law a number of fundamental civil rights and civil freedoms.

Philosophers such as Thomas Paine, John Stuart Mill and Hegel expanded on the theme of universality during the 18th and 19th centuries. In 1831 William Lloyd Garrison wrote in a newspaper called "The Liberator" that he was trying to enlist his readers in "the great cause of human rights" so the term "human rights" probably came into use sometime between Paine's "The Rights of Man" and Garrison's publication. In 1849 a contemporary, Henry David Thoreau, wrote about human rights in his treatise "On the Duty of Civil Disobedience" which was later influential on human rights and civil rights thinkers. United States Supreme Court Justice David Davis, in his 1867 opinion for Ex Parte Milligan, wrote "By the protection of the law, human rights are secured; withdraw that protection and they are at the mercy of wicked rulers or the clamor of an excited people."

Many groups and movements have managed to achieve profound social changes over the course of the 20th century in the name of human rights. In Western Europe and North America, labour unions brought about laws granting workers the right to strike, establishing minimum work conditions and forbidding or regulating child labour. The women's rights movement succeeded in gaining for many women the right to vote. National liberation movements in many countries succeeded in driving out colonial powers. One of the most influential was Mahatma Gandhi's movement to free his native India from British rule. Movements by long-oppressed racial and religious minorities succeeded in many parts of the world, among them the civil rights movement, and more recent diverse identity politics movements, on behalf of women and minorities in the United States.

The foundation of the International Committee of the Red Cross, the 1864 Lieber Code and the first of the Geneva Conventions in 1864 laid the foundations of International humanitarian law, to be further developed following the two World Wars.

The League of Nations was established in 1919 at the negotiations over the Treaty of Versailles following the end of World War I. The League's goals included disarmament, preventing war through collective security, settling disputes between countries through negotiation, diplomacy and improving global welfare. Enshrined in its Charter was a mandate to promote many of the rights which were later included in the Universal Declaration of Human Rights.

The League of Nations had mandates to support many of the former colonies of the Western European colonial powers during their transition from colony to independent state.

Established as an agency of the League of Nations, and now part of United Nations, the International Labour Organization also had a mandate to promote and safeguard certain of the rights later included in the UDHR:

The Universal Declaration of Human Rights (UDHR) is a non-binding declaration adopted by the United Nations General Assembly in 1948, partly in response to the barbarism of World War II. The UDHR urges member nations to promote a number of human, civil, economic and social rights, asserting these rights are part of the "foundation of freedom, justice and peace in the world". The declaration was the first international legal effort to limit the behavior of states and press upon them duties to their citizens following the model of the rights-duty duality.

The UDHR was framed by members of the Human Rights Commission, with Eleanor Roosevelt as Chair, who began to discuss an "International Bill of Rights" in 1947. The members of the Commission did not immediately agree on the form of such a bill of rights, and whether, or how, it should be enforced. The Commission proceeded to frame the UDHR and accompanying treaties, but the UDHR quickly became the priority. Canadian law professor John Humprey and French lawyer Rene Cassin were responsible for much of the cross-national research and the structure of the document respectively, where the articles of the declaration were interpretative of the general principle of the preamble. The document was structured by Cassin to include the basic principles of dignity, liberty, equality and brotherhood in the first two articles, followed successively by rights pertaining to individuals; rights of individuals in relation to each other and to groups; spiritual, public and political rights; and economic, social and cultural rights. The final three articles place, according to Cassin, rights in the context of limits, duties and the social and political order in which they are to be realized. Humphrey and Cassin intended the rights in the UDHR to be legally enforceable through some means, as is reflected in the third clause of the preamble:

Some of the UDHR was researched and written by a committee of international experts on human rights, including representatives from all continents and all major religions, and drawing on consultation with leaders such as Mahatma Gandhi. The inclusion of both civil and political rights and economic, social and cultural rights was predicated on the assumption that basic human rights are indivisible and that the different types of rights listed are inextricably linked. Though this principle was not opposed by any member states at the time of adoption (the declaration was adopted unanimously, with the abstention of the Soviet bloc, Apartheid South Africa and Saudi Arabia), this principle was later subject to significant challenges.

The onset of the Cold War soon after the UDHR was conceived brought to the fore divisions over the inclusion of both econonic and social rights and civil and political rights in the declaration. Capitalist states tended to place strong emphasis on civil and political rights (such as freedom of association and expression), and were reluctant to include economic and social rights (such as the right to work and the right to join a union). Socialist states placed much greater importance on economic and social rights and argued strongly for their inclusion.

Because of the divisions over which rights to include, and because some states declined to ratify any treaties including certain specific interpretations of human rights, and despite the Soviet bloc and a number of developing countries arguing strongly for the inclusion of all rights in a so-called "Unity Resolution", the rights enshrined in the UDHR were split into two separate covenants, allowing states to adopt some rights and derogate others. Though this allowed the covenants to be created, it denied the proposed principle that all rights are linked which was central to some interpretations of the UDHR.

Although the UDHR is a non-binding resolution, it is now considered to be a central component of international customary law which may be invoked under appropriate circumstances by national and other judiciaries.

In 1966, the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR) were adopted by the United Nations, between them making the rights contained in the UDHR binding on all states. However, they came into force only in 1976, when they were ratified by a sufficient number of countries (despite achieving the ICCPR, a covenant including no economic or social rights, the US only ratified the ICCPR in 1992). The ICESCR commits 155 state parties to work toward the granting of economic, social, and cultural rights (ESCR) to individuals. 
Since then numerous other treaties (pieces of legislation) have been offered at the international level. They are generally known as "human rights instruments". Some of the most significant are:


The United Nations (UN) is the only multilateral governmental agency with universally accepted international jurisdiction for universal human rights legislation. All UN organs have advisory roles to the United Nations Security Council and the United Nations Human Rights Council, and there are numerous committees within the UN with responsibilities for safeguarding different human rights treaties. The most senior body of the UN with regard to human rights is the Office of the High Commissioner for Human Rights. The United Nations has an international mandate to:

The United Nations Human Rights Council, created at the 2005 World Summit to replace the United Nations Commission on Human Rights, has a mandate to investigate violations of human rights. The Human Rights Council is a subsidiary body of the General Assembly and reports directly to it. It ranks below the Security Council, which is the final authority for the interpretation of the United Nations Charter. Forty-seven of the one hundred ninety-one member states sit on the council, elected by simple majority in a secret ballot of the United Nations General Assembly. Members serve a maximum of six years and may have their membership suspended for gross human rights abuses. The Council is based in Geneva, and meets three times a year; with additional meetings to respond to urgent situations.

Independent experts ("rapporteurs") are retained by the Council to investigate alleged human rights abuses and to provide the Council with reports.

The Human Rights Council may request that the Security Council refer cases to the International Criminal Court (ICC) even if the issue being referred is outside the normal jurisdiction of the ICC.

In addition to the political bodies whose mandate flows from the UN charter, the UN has set up a number of "treaty-based" bodies, comprising committees of independent experts who monitor compliance with human rights standards and norms flowing from the core international human rights treaties. They are supported by and are created by the treaty that they monitor, With the exception of the CESCR, which was established under a resolution of the Economic and Social Council to carry out the monitoring functions originally assigned to that body under the Covenant, they are technically autonomous bodies, established by the treaties that they monitor and accountable to the state parties of those treaties – rather than subsidiary to the United Nations, though in practice they are closely intertwined with the United Nations system and are supported by the UN High Commissioner for Human Rights (UNHCHR) and the UN Centre for Human Rights.

Each treaty body receives secretariat support from the Human Rights Council and Treaties Division of Office of the High Commissioner on Human Rights (OHCHR) in Geneva except CEDAW, which is supported by the Division for the Advancement of Women (DAW). CEDAW formerly held all its sessions at United Nations headquarters in New York but now frequently meets at the United Nations Office in Geneva; the other treaty bodies meet in Geneva. The Human Rights Committee usually holds its March session in New York City.

There are many regional agreements and organizations promoting and governing human rights.

The African Union (AU) is a supranational union consisting of fifty-three African states. Established in 2001, the AU's purpose is to help secure Africa's democracy, human rights, and a sustainable economy, especially by bringing an end to intra-African conflict and creating an effective common market.

The African Commission on Human and Peoples' Rights (ACHPR) is a quasi-judicial organ of the African Union tasked with promoting and protecting human rights and collective (peoples') rights throughout the African continent as well as interpreting the African Charter on Human and Peoples' Rights and considering individual complaints of violations of the Charter. The Commission has three broad areas of responsibility:


In pursuit of these goals, the Commission is mandated to "collect documents, undertake studies and researches on African problems in the field of human and peoples, rights, organise seminars, symposia and conferences, disseminate information, encourage national and local institutions concerned with human and peoples' rights and, should the case arise, give its views or make recommendations to governments" (Charter, Art. 45).

With the creation of the African Court on Human and Peoples' Rights (under a protocol to the Charter which was adopted in 1998 and entered into force in January 2004), the Commission will have the additional task of preparing cases for submission to the Court's jurisdiction. In a July 2004 decision, the AU Assembly resolved that the future Court on Human and Peoples' Rights would be integrated with the African Court of Justice.

The Court of Justice of the African Union is intended to be the "principal judicial organ of the Union" (Protocol of the Court of Justice of the African Union, Article 2.2). Although it has not yet been established, it is intended to take over the duties of the African Commission on Human and Peoples' Rights, as well as act as the supreme court of the African Union, interpreting all necessary laws and treaties. The Protocol establishing the African Court on Human and Peoples' Rights entered into force in January 2004 but its merging with the Court of Justice has delayed its establishment. The Protocol establishing the Court of Justice will come into force when ratified by 15 countries.

There are many countries in Africa accused of human rights violations by the international community and NGOs

The Organization of American States (OAS) is an international organization, headquartered in Washington, D.C., United States. Its members are the thirty-five independent states of the Americas. Over the course of the 1990s, with the end of the Cold War, the return to democracy in Latin America, and the thrust toward globalization, the OAS made major efforts to reinvent itself to fit the new context. Its stated priorities now include the following:


The Inter-American Commission on Human Rights (the IACHR) is an autonomous organ of the Organization of American States, also based in Washington, D.C. Along with the Inter-American Court of Human Rights, based in San José, Costa Rica, it is one of the bodies that comprise the inter-American system for the promotion and protection of human rights. The IACHR is a permanent body which meets in regular and special sessions several times a year to examine allegations of human rights violations in the hemisphere. Its human rights duties stem from three documents:


The Inter-Americal Court of Human Rights was established in 1979 with the purpose of enforcing and interpreting the provisions of the American Convention on Human Rights. Its two main functions are thus adjudicatory and advisory. Under the former, it hears and rules on the specific cases of human rights violations referred to it. Under the latter, it issues opinions on matters of legal interpretation brought to its attention by other OAS bodies or member states.

There are no Asia-wide organisations or conventions to promote or protect human rights. Countries vary widely in their approach to human rights and their record of human rights protection.

The Association of Southeast Asian Nations (ASEAN) is a geo-political and economic organization of 10 countries located in Southeast Asia, which was formed in 1967 by Indonesia, Malaysia, the Philippines, Singapore and Thailand. The organisation now also includes Brunei Darussalam, Vietnam, Laos, Myanmar and Cambodia. In October 2009, the ASEAN Intergovernmental Commission on Human Rights was inaugurated, and subsequently, the ASEAN Human Rights Declaration was adopted unanimously by ASEAN members on 18 November 2012.

The Arab Charter on Human Rights (ACHR) was adopted by the Council of the League of Arab States on 22 May 2004.

The Council of Europe, founded in 1949, is the oldest organisation working for European integration. It is an international organisation with legal personality recognised under public international law and has observer status with the United Nations. The seat of the Council of Europe is in Strasbourg in France. The Council of Europe is responsible for both the European Convention on Human Rights and the European Court of Human Rights. These institutions bind the Council's members to a code of human rights which, though strict, are more lenient than those of the United Nations charter on human rights. The Council also promotes the European Charter for Regional or Minority Languages and the European Social Charter. Membership is open to all European states which seek European integration, accept the principle of the rule of law and are able and willing to guarantee democracy, fundamental human rights and freedoms.

The Council of Europe is separate from the European Union, but the latter is expected to accede to the European Convention and potentially the Council itself. The EU also has a separate human rights document; the Charter of Fundamental Rights of the European Union.

The European Convention on Human Rights defines and guarantees since 1950 human rights and fundamental freedoms in Europe. All 47 member states of the Council of Europe have signed this Convention and are therefore under the jurisdiction of the European Court of Human Rights in Strasbourg. In order to prevent torture and inhuman or degrading treatment (Article 3 of the Convention), the European Committee for the Prevention of Torture was established.

Several theoretical approaches have been advanced to explain how and why human rights become part of social expectations.

One of the oldest Western philosophies on human rights is that they are a product of a natural law, stemming from different philosophical or religious grounds.

Other theories hold that human rights codify moral behavior which is a human social product developed by a process of biological and social evolution (associated with Hume). Human rights are also described as a sociological pattern of rule setting (as in the sociological theory of law and the work of Weber). These approaches include the notion that individuals in a society accept rules from legitimate authority in exchange for security and economic advantage (as in Rawls) – a social contract.

Natural law theories base human rights on a "natural" moral, religious or even biological order which is independent of transitory human laws or traditions.

Socrates and his philosophic heirs, Plato and Aristotle, posited the existence of natural justice or natural right ("dikaion physikon", "δικαιον φυσικον", Latin "ius naturale"). Of these, Aristotle is often said to be the father of natural law, although evidence for this is due largely to the interpretations of his work of Thomas Aquinas.

The development of this tradition of natural justice into one of natural law is usually attributed to the Stoics.

Some of the early Church fathers sought to incorporate the until then pagan concept of natural law into Christianity. Natural law theories have featured greatly in the philosophies of Thomas Aquinas, Francisco Suárez, Richard Hooker, Thomas Hobbes, Hugo Grotius, Samuel von Pufendorf, and John Locke.

In the Seventeenth Century Thomas Hobbes founded a contractualist theory of legal positivism on what all men could agree upon: what they sought (happiness) was subject to contention, but a broad consensus could form around what they feared (violent death at the hands of another). The natural law was how a rational human being, seeking to survive and prosper, would act. It was discovered by considering humankind's natural rights, whereas previously it could be said that natural rights were discovered by considering the natural law. In Hobbes' opinion, the only way natural law could prevail was for men to submit to the commands of the sovereign. In this lay the foundations of the theory of a social contract between the governed and the governor.

Hugo Grotius based his philosophy of international law on natural law. He wrote that "even the will of an omnipotent being cannot change or abrogate" natural law, which "would maintain its objective validity even if we should assume the impossible, that there is no God or that he does not care for human affairs." ("De iure belli ac pacis", Prolegomeni XI). This is the famous argument "etiamsi daremus" ("non-esse Deum"), that made natural law no longer dependent on theology.

John Locke incorporated natural law into many of his theories and philosophy, especially in "Two Treatises of Government". Locke turned Hobbes' prescription around, saying that if the ruler went against natural law and failed to protect "life, liberty, and property," people could justifiably overthrow the existing state and create a new one.

The Belgian philosopher of law Frank van Dun is one among those who are elaborating a secular conception of natural law in the liberal tradition. There are also emerging and secular forms of natural law theory that define human rights as derivative of the notion of universal human dignity.

The term "human rights" has replaced the term "natural rights" in popularity, because the rights are less and less frequently seen as requiring natural law for their existence.

The philosopher John Finnis argues that human rights are justifiable on the grounds of their instrumental value in creating the necessary conditions for human well-being. Interest theories highlight the duty to respect the rights of other individuals on grounds of self-interest:

The biological theory considers the comparative reproductive advantage of human social behavior based on empathy and altruism in the context of natural selection.

The most common categorization of human rights is to split them into civil and political rights, and economic, social and cultural rights.

Civil and political rights are enshrined in articles 3 to 21 of the Universal Declaration of Human Rights and in the ICCPR. Economic, social and cultural rights are enshrined in articles 22 to 28 of the Universal Declaration of Human Rights and in the ICESCR. The UDHR included both economic, social and cultural rights and civil and political rights because it was based on the principle that the different rights could only successfully exist in combination:

This is held to be true because without civil and political rights the public cannot assert their economic, social and cultural rights. Similarly, without livelihoods and a working society, the public cannot assert or make use of civil or political rights (known as the "full belly thesis")

Although accepted by the signaturies to the UDHR, most of them do not in practice give equal weight to the different types of rights. Western cultures have often given priority to civil and political rights, sometimes at the expense of economic and social rights such as the right to work, to education, health and housing. For example, in the United States there is no universal access to healthcare free at the point of use. That is not to say that Western cultures have overlooked these rights entirely (the welfare states that exist in Western Europe are evidence of this). Similarly the ex Soviet bloc countries and Asian countries have tended to give priority to economic, social and cultural rights, but have often failed to provide civil and political rights.

Another categorization, offered by Karel Vasak, is that there are "three generations of human rights": first-generation civil and political rights (right to life and political participation), second-generation economic, social and cultural rights (right to subsistence) and third-generation solidarity rights (right to peace, right to clean environment). Out of these generations, the third generation is the most debated and lacks both legal and political recognition. This categorisation is at odds with the indivisibility of rights, as it implicitly states that some rights can exist without others. Prioritisation of rights for pragmatic reasons is however a widely accepted necessity. Human rights expert Philip Alston argues:

He, and others, urge caution with prioritisation of rights:

Some human rights are said to be "inalienable rights." The term inalienable rights (or unalienable rights) refers to "a set of human rights that are fundamental, are not awarded by human power, and cannot be surrendered."

The adherence to the principle of indivisibility by the international community was reaffirmed in 1995:

This statement was again endorsed at the 2005 World Summit in New York (paragraph 121).

The UDHR enshrines, by definition, rights that apply to all humans equally, whichever geographical location, state, race or culture they belong to.

Proponents of cultural relativism suggest that human rights are not all universal, and indeed conflict with some cultures and threaten their survival.

Rights which are most often contested with relativistic arguments are the rights of women. For example, Female genital mutilation occurs in different cultures in Africa, Asia and South America. It is not mandated by any religion, but has become a tradition in many cultures. It is considered a violation of women's and girl's rights by much of the international community, and is outlawed in some countries.

Universalism has been described by some as cultural, economic or political imperialism. In particular, the concept of human rights is often claimed to be fundamentally rooted in a politically liberal outlook which, although generally accepted in Europe, Japan or North America, is not necessarily taken as standard elsewhere.

For example, in 1981, the Iranian representative to the United Nations, Said Rajaie-Khorassani, articulated the position of his country regarding the Universal Declaration of Human Rights by saying that the UDHR was "a secular understanding of the Judeo-Christian tradition", which could not be implemented by Muslims without trespassing the Islamic law. The former Prime Ministers of Singapore, Lee Kuan Yew, and of Malaysia, Mahathir bin Mohamad both claimed in the 1990s that "Asian values" were significantly different from western values and included a sense of loyalty and foregoing personal freedoms for the sake of social stability and prosperity, and therefore authoritarian government is more appropriate in Asia than democracy. This view is countered by Mahathir's former deputy:

and also by Singapore's opposition leader Chee Soon Juan who states that it is racist to assert that Asians do not want human rights.

An appeal is often made to the fact that influential human rights thinkers, such as John Locke and John Stuart Mill, have all been Western and indeed that some were involved in the running of Empires themselves.

Relativistic arguments tend to neglect the fact that modern human rights are new to all cultures, dating back no further than the UDHR in 1948. They also don't account for the fact that the UDHR was drafted by people from many different cultures and traditions, including a US Roman Catholic, a Chinese Confucian philosopher, a French Zionist and a representative from the Arab League, amongst others, and drew upon advice from thinkers such as Mahatma Gandhi.

Michael Ignatieff has argued that cultural relativism is almost exclusively an argument used by those who wield power in cultures which commit human rights abuses, and that those who's human rights are compromised are the powerless. This reflects the fact that the difficulty in judging universalism versus relativism lies in who is claiming to represent a particular culture.

Although the argument between universalism and relativism is far from complete, it is an academic discussion in that all international human rights instruments adhere to the principle that human rights are universally applicable. The 2005 World Summit reaffirmed the international community's adherence to this principle:

Companies, NGOs, political parties, informal groups, and individuals are known as "non-State actors". Non-State actors can also commit human rights abuses, but are not subject to human rights law other than International Humanitarian Law, which applies to individuals.

Multi-national companies play an increasingly large role in the world, and are responsible for a large number of human rights abuses. Although the legal and moral environment surrounding the actions of governments is reasonably well developed, that surrounding multi-national companies is both controversial and ill-defined. Multi-national companies' primary responsibility is to their shareholders, not to those affected by their actions. Such companies are often larger than the economies of the states in which they operate, and can wield significant economic and political power. No international treaties exist to specifically cover the behavior of companies with regard to human rights, and national legislation is very variable. Jean Ziegler, Special Rapporteur of the UN Commission on Human Rights on the right to food stated in a report in 2003:

In August 2003 the Human Rights Commission's Sub-Commission on the Promotion and Protection of Human Rights produced draft "Norms on the responsibilities of transnational corporations and other business enterprises with regard to human rights". These were considered by the Human Rights Commission in 2004, but have no binding status on corporations and are not monitored.

Realism and national loyalties have been described as a destructive influence on the human rights movement because they deny people's innately similar human qualities.

With the exception of non-derogable human rights (international conventions class the right to life, the right to be free from slavery, the right to be free from torture and the right to be free from retroactive application of penal laws as non-derogable), the UN recognises that human rights can be limited or even pushed aside during times of national emergency – although

Rights that cannot be derogated for reasons of national security in any circumstances are known as peremptory norms or "jus cogens". Such International law obligations are binding on all states and cannot be modified by treaty.

The human rights enshrined in the UDHR, the Geneva Conventions and the various enforced treaties of the United Nations are enforceable in law. In practice, many rights are very difficult to legally enforce due to the absence of consensus on the application of certain rights, the lack of relevant national legislation or of bodies empowered to take legal action to enforce them.

There exist a number of internationally recognized organisations with worldwide mandate or jurisdiction over certain aspects of human rights:


The ICC and other international courts (see Regional human rights above exist to take action where the national legal system of a state is unable to try the case itself. If national law is able to safeguard human rights and punish those who breach human rights legislation, it has primary jurisdiction by complementarity. Only when all "local remedies" have been exhausted does international law take effect.

In over 110 countries National human rights institutions (NHRIs) have been set up to protect, promote or monitor human rights with jurisdiction in a given country. Although not all NHRIs are compliant with the Paris Principles, the number and effect of these institutions is increasing. The Paris Principles were defined at the first International Workshop on National Institutions for the Promotion and Protection of Human Rights in Paris on 7–9 October 1991, and adopted by United Nations Human Rights Commission Resolution 1992/54 of 1992 and the General Assembly Resolution 48/134 of 1993. The Paris Principles list a number of responsibilities for national institutions.

Universal jurisdiction is a controversial principle in international law whereby states claim criminal jurisdiction over persons whose alleged crimes were committed outside the boundaries of the prosecuting state, regardless of nationality, country of residence, or any other relation with the prosecuting country. The state backs its claim on the grounds that the crime committed is considered a crime against all, which any state is authorized to punish. The concept of universal jurisdiction is therefore closely linked to the idea that certain international norms are erga omnes, or owed to the entire world community, as well as the concept of jus cogens. In 1993 Belgium passed a "law of universal jurisdiction" to give its courts jurisdiction over crimes against humanity in other countries, and in 1998 Augusto Pinochet was arrested in London following an indictment by Spanish judge Baltasar Garzon under the universal jurisdiction principle. The principle is supported by Amnesty International and other human rights organisations as they believe certain crimes pose a threat to the international community as a whole and the community has a moral duty to act, but others, including Henry Kissinger (who has himself been accused of war crimes by several commentators), argue that state sovereignty is paramount, because breaches of rights committed in other countries are outside states' sovereign interest and because states could use the principle for political reasons.

Human rights violations occur when any state or non-state actor breaches any of the terms of the UDHR or other international human rights or humanitarian law. In regard to human rights violations of United Nations laws. Article 39 of the United Nations Charter designates the UN Security Council (or an appointed authority) as the only tribunal that may determine UN human rights violations.

Human rights abuses are monitored by United Nations committees, national institutions and governments and by many independent non-governmental organizations, such as Amnesty International, Human Rights Watch, World Organisation Against Torture, Freedom House, International Freedom of Expression Exchange and Anti-Slavery International. These organisations collect evidence and documentation of human rights abuses and apply pressure to promote human rights.

Wars of aggression, War crimes and crimes against humanity, including genocide, are breaches of International humanitarian law.





</doc>
<doc id="13833" url="https://en.wikipedia.org/wiki?curid=13833" title="Hash table">
Hash table

In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an "index" into an array of "buckets" or "slots", from which the desired value can be found.

Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash "collisions" where the hash function generates the same index for more than one key. Such collisions must be accommodated in some way.

In a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized) constant average cost per operation.

In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.

The idea of hashing is to distribute the entries (key/value pairs) across an array of "buckets". Given a key, the algorithm computes an "index" that suggests where the entry can be found:

Often this is done in two steps:

In this method, the "hash" is independent of the array size, and it is then "reduced" to an index (a number between codice_1 and codice_2) using the modulo operator (codice_3).

In the case that the array size is a power of two, the remainder operation is reduced to masking, which improves speed, but can increase problems with a poor hash function.

A good hash function and implementation algorithm are essential for good hash table performance, but may be difficult to achieve.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number. The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid "clustering", the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash is claimed to have particularly poor clustering behavior.

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for " any" size is not necessary, a non-cryptographic hashing function might be preferable.

If all keys are known ahead of time, a perfect hash function can be used to create a perfect hash table that has no collisions. If minimal perfect hashing is used, every location in the hash table can be used as well.

Perfect hashing allows for constant time lookups in all cases. This is in contrast to most chaining and open addressing methods, where the time for lookup is low on average, but may be very large, O("n"), for instance when all the keys hash to a few values.

A critical statistic for a hash table is the "load factor", defined as
where

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor is kept below some bound. For a "fixed" number of buckets, the time for a lookup grows with the number of entries and therefore the desired constant time is not achieved. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good tradeoff between time and space costs."

Second to that, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.

Hash collisions are practically unavoidable when hashing a random subset of a large set of possible keys. For example, if 2,450 keys are hashed into a million buckets, even with a perfectly uniform random distribution, according to the birthday problem there is approximately a 95% chance of at least two of the keys being hashed to the same slot.

Therefore, almost all hash table implementations have some collision resolution strategy to handle such events. Some common strategies are described below. All these methods require that the keys (or pointers to them) be stored in the table, together with the associated values.

In the method known as "separate chaining", each bucket is independent, and has some sort of list of entries with the same index. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation.

In a good hash table, each bucket has zero or one entries, and sometimes two or three, but rarely more than that. Therefore, structures that are efficient in time and space for these cases are preferred. Structures that are efficient for a fairly large number of entries per bucket are not needed or desirable. If these cases happen often, the hashing function needs to be fixed.

Chained hash tables with linked lists are popular because they require only basic data structures with simple algorithms, and can use simple hash functions that are unsuitable for other methods.

The cost of a table operation is that of scanning the entries of the selected bucket for the desired key. If the distribution of keys is sufficiently uniform, the "average" cost of a lookup depends only on the average number of keys per bucket—that is, it is roughly proportional to the load factor.

For this reason, chained hash tables remain effective even when the number of table entries "n" is much higher than the number of slots. For example, a chained hash table with 1000 slots and 10,000 stored keys (load factor 10) is five to ten times slower than a 10,000-slot table (load factor 1); but still 1000 times faster than a plain sequential list.

For separate-chaining, the worst-case scenario is when all entries are inserted into the same bucket, in which case the hash table is ineffective and the cost is that of searching the bucket data structure. If the latter is a linear list, the lookup procedure may have to scan all its entries, so the worst-case cost is proportional to the number "n" of entries in the table.

The bucket chains are often searched sequentially using the order the entries were added to the bucket. If the load factor is large and some keys are more likely to come up than others, then rearranging the chain with a move-to-front heuristic may be effective. More sophisticated data structures, such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance even in a worst-case scenario. However, using a larger table and/or a better hash function may be even more effective in those cases.

Chained hash tables also inherit the disadvantages of linked lists. When storing small keys and values, the space overhead of the codice_4 pointer in each entry record can be significant. An additional disadvantage is that traversing a linked list has poor cache performance, making the processor cache ineffective.

Some chaining implementations store the first record of each chain in the slot array itself.
The number of pointer traversals is decreased by one for most cases. The purpose is to increase cache efficiency of hash table access.

The disadvantage is that an empty bucket takes the same space as a bucket with one entry. To save space, such hash tables often have about as many slots as stored entries, meaning that many slots have two or more entries.

Instead of a list, one can use any other data structure that supports the required operations. For example, by using a self-balancing binary search tree, the theoretical worst-case time of common hash table operations (insertion, deletion, lookup) can be brought down to O(log "n") rather than O("n"). However, this introduces extra complexity into the implementation, and may cause even worse performance for smaller hash tables, where the time spent inserting into and balancing the tree is greater than the time needed to perform a linear search on all of the elements of a list. A real world example of a hash table that uses a self-balancing binary search tree for buckets is the codice_5 class in Java version 8.

The variant called array hash table uses a dynamic array to store all the entries that hash to the same slot. Each newly inserted entry gets appended to the end of the dynamic array that is assigned to the slot. The dynamic array is resized in an "exact-fit" manner, meaning it is grown only by as many bytes as needed. Alternative techniques such as growing the array by block sizes or "pages" were found to improve insertion performance, but at a cost in space. This variation makes more efficient use of CPU caching and the translation lookaside buffer (TLB), because slot entries are stored in sequential memory positions. It also dispenses with the codice_4 pointers that are required by linked lists, which saves space. Despite frequent array resizing, space overheads incurred by the operating system such as memory fragmentation were found to be small.

An elaboration on this approach is the so-called dynamic perfect hashing, where a bucket that contains "k" entries is organized as a perfect hash table with "k" slots. While it uses more memory ("n" slots for "n" entries, in the worst case and "n" × "k" slots in the average case), this variant has guaranteed constant worst-case lookup time, and low amortized time for insertion.
It is also possible to use a fusion tree for each bucket, achieving constant time for all operations with high probability.

In another strategy, called open addressing, all entry records are stored in the bucket array itself. When a new entry has to be inserted, the buckets are examined, starting with the hashed-to slot and proceeding in some "probe sequence", until an unoccupied slot is found. When searching for an entry, the buckets are scanned in the same sequence, until either the target record is found, or an unused array slot is found, which indicates that there is no such key in the table. The name "open addressing" refers to the fact that the location ("address") of the item is not determined by its hash value. (This method is also called closed hashing; it should not be confused with "open hashing" or "closed addressing" that usually mean separate chaining.)

Well-known probe sequences include:

A drawback of all these open addressing schemes is that the number of stored entries cannot exceed the number of slots in the bucket array. In fact, even with good hash functions, their performance dramatically degrades when the load factor grows beyond 0.7 or so. For many applications, these restrictions mandate the use of dynamic resizing, with its attendant costs.

Open addressing schemes also put more stringent requirements on the hash function: besides distributing the keys more uniformly over the buckets, the function must also minimize the clustering of hash values that are consecutive in the probe order. Using separate chaining, the only concern is that too many objects map to the "same" hash value; whether they are adjacent or nearby is completely irrelevant.

Open addressing only saves memory if the entries are small (less than four times the size of a pointer) and the load factor is not too small. If the load factor is close to zero (that is, there are far more buckets than stored entries), open addressing is wasteful even if each entry is just two words.

Open addressing avoids the time overhead of allocating each new entry record, and can be implemented even in the absence of a memory allocator. It also avoids the extra indirection required to access the first entry of each bucket (that is, usually the only one). It also has better locality of reference, particularly with linear probing. With small record sizes, these factors can yield better performance than chaining, particularly for lookups. 
Hash tables with open addressing are also easier to serialize, because they do not use pointers.

On the other hand, normal open addressing is a poor choice for large elements, because these elements fill entire CPU cache lines (negating the cache advantage), and a large amount of space is wasted on large empty table slots. If the open addressing table only stores references to elements (external storage), it uses space comparable to chaining even for large records but loses its speed advantage.

Generally speaking, open addressing is better used for hash tables with small records that can be stored within the table (internal storage) and fit in a cache line. They are particularly suitable for elements of one word or less. If the table is expected to have a high load factor, the records are large, or the data is variable-sized, chained hash tables often perform as well or better.

A hybrid of chaining and open addressing, coalesced hashing links together chains of nodes within the table itself. Like open addressing, it achieves space usage and (somewhat diminished) cache advantages over chaining. Like chaining, it does not exhibit clustering effects; in fact, the table can be efficiently filled to a high density. Unlike chaining, it cannot have more elements than table slots.

Another alternative open-addressing solution is cuckoo hashing, which ensures constant lookup time in the worst case, and constant amortized time for insertions and deletions. It uses two or more hash functions, which means any key/value pair could be in two or more locations. For lookup, the first hash function is used; if the key/value is not found, then the second hash function is used, and so on. If a collision happens during insertion, then the key is re-hashed with the second hash function to map it to another bucket. If all hash functions are used and there is still a collision, then the key it collided with is removed to make space for the new key, and the old key is re-hashed with one of the other hash functions, which maps it to another bucket. If that location also results in a collision, then the process repeats until there is no collision or the process traverses all the buckets, at which point the table is resized. By combining multiple hash functions with multiple cells per bucket, very high space utilization can be achieved.

Another alternative open-addressing solution is hopscotch hashing, which combines the approaches of cuckoo hashing and linear probing, yet seems in general to avoid their limitations. In particular it works well even when the load factor grows beyond 0.9. The algorithm is well suited for implementing a resizable concurrent hash table.

The hopscotch hashing algorithm works by defining a neighborhood of buckets near the original hashed bucket, where a given entry is always found. Thus, search is limited to the number of entries in this neighborhood, which is logarithmic in the worst case, constant on average, and with proper alignment of the neighborhood typically requires one cache miss. When inserting an entry, one first attempts to add it to a bucket in the neighborhood. However, if all buckets in this neighborhood are occupied, the algorithm traverses buckets in sequence until an open slot (an unoccupied bucket) is found (as in linear probing). At that point, since the empty bucket is outside the neighborhood, items are repeatedly displaced in a sequence of hops. (This is similar to cuckoo hashing, but with the difference that in this case the empty slot is being moved into the neighborhood, instead of items being moved out with the hope of eventually finding an empty slot.) Each hop brings the open slot closer to the original neighborhood, without invalidating the neighborhood property of any of the buckets along the way. In the end, the open slot has been moved into the neighborhood, and the entry being inserted can be added to it.

One interesting variation on double-hashing collision resolution is Robin Hood hashing. The idea is that a new key may displace a key already inserted, if its probe count is larger than that of the key at the current position. The net effect of this is that it reduces worst case search times in the table. This is similar to ordered hash tables except that the criterion for bumping a key does not depend on a direct relationship between the keys. Since both the worst case and the variation in the number of probes is reduced dramatically, an interesting variation is to probe the table starting at the expected successful probe value and then expand from that position in both directions.
External Robin Hood hashing is an extension of this algorithm where the table is stored in an external file and each table position corresponds to a fixed-sized page or bucket with "B" records.

2-choice hashing employs two different hash functions, "h"("x") and "h"("x"), for the hash table. Both hash functions are used to compute two table locations. When an object is inserted in the table, it is placed in the table location that contains fewer objects (with the default being the "h"("x") table location if there is equality in bucket size). 2-choice hashing employs the principle of the power of two choices.

When an insert is made such that the number of entries in a hash table exceeds the product of the load factor and the current capacity then the hash table will need to be "rehashed". Rehashing includes increasing the size of the underlying data structure and mapping existing items to new bucket locations. In some implementations, if the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.

To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table—followed by a rehash—when items are deleted. From the point of space–time tradeoffs, this operation is similar to the deallocation in dynamic arrays.

A common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold "r". Then a new larger table is allocated, each entry is removed from the old table, and inserted into the new table. When all entries have been removed from the old table then the old table is returned to the free storage pool. Symmetrically, when the load factor falls below a second threshold "r", all entries are moved to a new smaller table.

For hash tables that shrink and grow frequently, the resizing downward can be skipped entirely. In this case, the table size is proportional to the maximum number of entries that ever were in the hash table at one time, rather than the current number. The disadvantage is that memory usage will be higher, and thus cache behavior may be worse. For best control, a "shrink-to-fit" operation can be provided that does this only on request.

If the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries "n" and of the number "m" of operations performed.

For example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If "m" elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most "m" − 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation.

Some hash table implementations, notably in real-time systems, cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually.

Disk-based hash tables almost always use some alternative to all-at-once rehashing, since the cost of rebuilding the entire table on disk would be too high.

One alternative to enlarging the table all at once is to perform the rehashing gradually:
To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it
is necessary to increase the size of the table by a factor of at least ("r" + 1)/"r" during resizing.

If it is known that key values will always increase (or decrease) monotonically, then a variation of consistent hashing can be achieved by keeping a list of the single most recent key value at each hash table resize operation. Upon lookup, keys that fall in the ranges defined by these list entries are directed to the appropriate hash function—and indeed hash table—both of which can be different for each range. Since it is common to grow the overall number of entries by doubling, there will only be O(log("N")) ranges to check, and binary search time for the redirection would be O(log(log("N"))). As with consistent hashing, this approach guarantees that any key's hash, once issued, will never change, even when the hash table is later grown.

Linear hashing is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions.

Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent in disk-based and distributed hash tables, where rehashing is prohibitively costly.
The problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem.
The four most popular approaches are rendezvous hashing, consistent hashing, the content addressable network algorithm, and Kademlia distance.

In the simplest model, the hash function is completely unspecified and the table does not resize. With an ideal hash function, a table of size formula_2 with open addressing has no collisions and holds up to formula_2 elements with a single comparison for successful lookup, while a table of size formula_2 with chaining and formula_5 keys has the minimum collisions and comparisons for lookup. With the worst possible hash function, every insertion causes a collision, and hash tables degenerate to linear search, with formula_6 amortized comparisons per insertion and up to formula_5 comparisons for a successful lookup.

Adding rehashing to this model is straightforward. As in a dynamic array, geometric resizing by a factor of formula_8 implies that only formula_9 keys are inserted formula_10 or more times, so that the total number of insertions is bounded above by , which is formula_6. By using rehashing to maintain , tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function.

In more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform, the assumption is called "simple uniform hashing" and it can be shown that hashing with chaining requires comparisons on average for an unsuccessful lookup, and hashing with open addressing requires . Both these bounds are constant, if we maintain using table resizing, where formula_12 is a fixed constant less than 1.










Hash tables are commonly used to implement many types of in-memory tables. They are used to implement associative arrays (arrays whose indices are arbitrary strings or other complicated objects), especially in interpreted programming languages like Perl , Ruby, Python, and PHP.

When storing a new item into a multimap and a hash collision occurs, the multimap unconditionally stores both items.

When storing a new item into a typical associative array and a hash collision occurs, but the actual keys themselves are different, the associative array likewise stores both items. However, if the key of the new item exactly matches the key of an old item, the associative array typically erases the old item and overwrites it with the new item, so every item in the table has a unique key.

Hash tables may also be used as disk-based data structures and database indices (such as in dbm) although B-trees are more popular in these applications. In multi-node database systems, hash tables are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins.

Hash tables can be used to implement caches, auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries—usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value.

Besides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not.

Those structures can therefore be used to implement a set data structure, which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets.

Several dynamic languages, such as Perl, Python, JavaScript, Lua, and Ruby, use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method.

Hash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single "string pool" implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing, and can be used with many other kinds of data (expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.).

Many programming languages provide hash table functionality, either as built-in associative arrays or as standard library modules. In C++11, for example, the codice_7 class provides hash tables for keys and values of arbitrary type.

The Java programming language (including the variant which is used on Android) includes the codice_8, codice_5, codice_10, and codice_11 generic collections.

In PHP 5 and 7, the Zend 2 engine and the Zend 3 engine (respectively) use one of the hash functions from Daniel J. Bernstein to generate the hash values used in managing the mappings of data pointers stored in a hash table. In the PHP source code, it is labelled as codice_12 (Daniel J. Bernstein, Times 33 with Addition).

Python's built-in hash table implementation, in the form of the codice_13 type, as well as Perl's hash type (%) are used internally to implement namespaces and therefore need to pay more attention to security, i.e., collision attacks. Python sets also use hashes internally, for fast lookup (though they store only keys, not values).

In the .NET Framework, support for hash tables is provided via the non-generic codice_14 and generic codice_15 classes, which store key-value pairs, and the generic codice_8 class, which stores only values.

In Rust's standard library, the generic codice_5 and codice_8 structs use linear probing with Robin Hood bucket stealing.

ANSI Smalltalk defines the classes codice_19 / codice_20 and codice_15 / codice_22. All Smalltalk implementations provide additional (not yet standardized) versions of codice_23, codice_24 and codice_25.

The idea of hashing arose independently in different places. In January 1953, Hans Peter Luhn wrote an internal IBM memorandum that used hashing with chaining. Gene Amdahl, Elaine M. McGraw, Nathaniel Rochester, and Arthur Samuel implemented a program using hashing at about the same time. Open addressing with linear probing (relatively prime stepping) is credited to Amdahl, but Ershov (in Russia) had the same idea.


There are several data structures that use hash functions but cannot be considered special cases of hash tables:




</doc>
<doc id="13834" url="https://en.wikipedia.org/wiki?curid=13834" title="&quot;Hello, World!&quot; program">
&quot;Hello, World!&quot; program

A "Hello, World!" program generally is a computer program that outputs or displays the message "Hello, World!". Because it is very simple in most programming languages, it is often used to illustrate the basic syntax of a programming language and is often the first program that those learning to code write.

A "Hello, World!" program is traditionally used to introduce novice programmers to a programming language.

"Hello, world!" is also traditionally used in a sanity test to make sure that a computer language is correctly installed, and that the operator understands how to use it.

While small test programs have existed since the development of programmable computers, the tradition of using the phrase "Hello, world!" as a test message was influenced by an example program in the seminal book "The C Programming Language". The example program from that book prints "" (without capital letters or exclamation mark), and was inherited from a 1974 Bell Laboratories internal memorandum by Brian Kernighan, "Programming in C: A Tutorial":

The C version was preceded by Kernighan's own 1972 "A Tutorial Introduction to the Language B", where the first known version of the program is found in an example used to illustrate external variables:

The program prints ' on the terminal, including a newline character. The phrase is divided into multiple variables because in B, a character constant is limited to four ASCII characters. The previous example in the tutorial printed ' on the terminal, and the phrase "" was introduced as a slightly longer greeting that required several character constants for its expression.

The Jargon File claims that "hello, world" originated instead with BCPL (1967).This claim is supported by the archived notes of the inventors of BCPL, Prof. Brian Kernighan at Princeton and Martin Richards at Cambridge.

For modern languages, hello, world programs vary in sophistication. For example, the Go programming language introduced a multilingual program, Sun demonstrated a Java hello, world based on scalable vector graphics, and the XL programming language features a spinning Earth hello, world using 3D graphics. While some languages such as Perl, Python or Ruby may need only a single statement to print "hello, world", a low-level assembly language may require dozens of commands. Mark Guzdial and Elliot Soloway have suggested that the "hello, world" test message may be outdated now that graphics and sound can be manipulated as easily as text.

There are many variations on the punctuation and casing of the phrase. Variations include the presence or absence of the comma and exclamation mark, and the capitalization of the 'H', both the 'H' and the 'W', or neither. Some languages are forced to implement different forms, such as "HELLO WORLD", on systems that support only capital letters, while many "hello, world" programs in esoteric languages print out a slightly modified string. For example, the first non-trivial Malbolge program printed "HEllO WORld", this having been determined to be good enough.

There are variations in spirit, as well. Functional programming languages, like Lisp, ML and Haskell, tend to substitute a factorial program for Hello, World, as functional programming emphasizes recursive techniques, whereas the original examples emphasize I/O, which violates the spirit of pure functional programming by producing side effects. Languages otherwise capable of Hello, World (Assembly, C, VHDL) may also be used in embedded systems, where text output is either difficult (requiring additional components or communication with another computer) or nonexistent. For devices such as microcontrollers, field-programmable gate arrays, and CPLD's, "Hello, World" may thus be substituted with a blinking LED, which demonstrates timing and interaction between components.

The Debian and Ubuntu Linux distributions provide the "hello, world" program through the apt packaging system; this allows users to simply type "apt-get install hello" for the program to be installed, along with any software dependencies. While of itself useless, it serves as a sanity check and a simple example to newcomers of how to install a package. It is significantly more useful for developers, however, as it provides an example of how to create a .deb package, either traditionally or using debhelper, and the version of hello used, GNU Hello, serves as an example of how to write a GNU program.

Time to "Hello World" (TTHW) is a metric for how long it takes to get a "Hello World" program running from scratch in a given programming language.



</doc>
<doc id="13839" url="https://en.wikipedia.org/wiki?curid=13839" title="Heavy metal">
Heavy metal

Heavy metal may refer to:










</doc>
<doc id="13845" url="https://en.wikipedia.org/wiki?curid=13845" title="History of Hebrew grammar">
History of Hebrew grammar

Hebrew grammar is the grammar of the Hebrew language.

The Masoretes in the 7th to 11th centuries laid the foundation for grammatical analysis of Hebrew. As early as the 9th century Judah ibn Kuraish discussed the relationship between Arabic and Hebrew. In the 10th century, Aaron ben Moses ben Asher refined the Tiberian vocalization, an extinct pronunciation of the Hebrew Bible.

The first treatises on Hebrew grammar appear in the High Middle Ages, in the context of Midrash (a method of interpreting and studying the Hebrew Bible). The Karaite tradition originated in Abbasid Baghdad around the 7th century. The "Diqduq" (10th century) is one of the earliest grammatical commentaries on the Hebrew Bible.

Solomon ibn Gabirol in the 11th century composed a versified Hebrew grammar, consisting of 400 verses divided into ten parts. In the 12th century, Ibn Barun compared the Hebrew language with Arabic in the Islamic grammatical tradition.
11th to 12th century grammarians of the Golden age of Jewish culture in Spain included Judah ben David Hayyuj, Jonah ibn Janah, Abraham ibn Ezra, Joseph Kimhi, Moses Kimhi and David Kimhi. Ibn Ezra gives a list of the oldest Hebrew grammarians in the introduction to his "Moznayim" (1140).
Roger Bacon was "a tolerable Hebrew scholar". Profiat Duran published an influential grammar in 1403.

Judah Messer Leon's 1454 grammar is a product of the Italian Renaissance. Hebrew grammars by Christian authors appeared during the Renaissance. Hieronymus Buclidius, a friend of Erasmus, gave more than 20,000 francs to establish a branch of Hebrew studies at Louvain in Flanders. Elijah Levita was called to the chair of Hebrew at the University of Paris. Cardinal Grimani and other dignitaries, both of the state and of the Church, studied Hebrew and the Cabala with Jewish teachers; even the warrior Guido Rangoni attempted the Hebrew language with the aid of Jacob Mantino (1526). Pico de la Mirandola (d. 1494) was the first to collect Hebrew manuscripts, and Reuchlin was the first Christian author to write a vocabulary and short grammar of the Hebrew language (1506). A more detailed grammar was published in 1590 by Otto Walper.
Conrad Gesner (d. 1565) was the first Christian to compile a catalogue of Hebrew books.
Paul Fagius and Elia Levita operated the first Hebrew printing office in the 1540s. Levita also compiled the first Hebrew-Yiddish dictionary.

Through the influence of Johannes Buxtorf (d. 1629) a serious attempt was made to understand the post-Biblical literature, and many of the most important works were translated into Latin. Gesenius' "Hebrew Grammar" appeared in 1813.

The Hebrew language is subdivided by era, with significant differences apparent between the varieties. All varieties, from Biblical to Modern, use a typically Semitic templatic morphology with triconsonantal stems, though Mishnaic and Modern Hebrew have significant borrowed components of the lexicon that do not fit into this pattern. Verbal morphology has remained relatively unchanged, though Mishnaic and Modern Hebrew have lost some modal distinctions of Biblical Hebrew and created others through the use of auxiliary verbs.

Significant syntactic changes have arisen in Modern Hebrew as a result of non-Semitic substrate influences. In particular:


However, most Biblical Hebrew constructions are still permissible in Modern Hebrew in formal, literary, archaic or poetic style.


Add: Zuckermann, Ghil'ad, "A New Vision for Israeli Hebrew, Journal of Modern Jewish Studies Vol 5, No. 1 March 2006, pp. 57–71ISSN 1472-5886 print/ISSN 1472-5894 online © 2006 Taylor & Francishttp://www.tandf.co.uk/journals DOI: 10.1080/14725880500511175

Modern Hebrew
Biblical Hebrew

 


</doc>
<doc id="13846" url="https://en.wikipedia.org/wiki?curid=13846" title="Modern Hebrew phonology">
Modern Hebrew phonology

Modern Hebrew is phonetically simpler than Biblical Hebrew and has fewer phonemes, but it is phonologically more complex. It has 25 to 27 consonants and 5 to 10 vowels, depending on the speaker and the analysis.

Hebrew has been used primarily for liturgical, literary, and scholarly purposes for most of the past two millennia. As a consequence, its pronunciation was strongly influenced by the vernacular of individual Jewish communities. With the revival of Hebrew as a native language, and especially with the establishment of Israel, the pronunciation of the modern language rapidly coalesced.

The two main accents of modern Hebrew are Oriental and Non-Oriental. Oriental Hebrew was chosen as the preferred accent for Israel by the Academy of the Hebrew Language, but has since declined in popularity. The description in this article follows the language as it is pronounced by native Israeli speakers of the younger generations.

According to the Academy of the Hebrew Language, in the 1880s (the time of the beginning of the Zionist movement and the Hebrew revival) there were three groups of Hebrew regional accents: Ashkenazi (Eastern European), Sephardi (Southern European), and Mizrahi (Middle Eastern, Iranian, and North African). Over time features of these systems of pronunciation merged, and nowadays we find two main pronunciations of colloquial – not liturgical – Hebrew: Oriental and Non-Oriental. Oriental Hebrew displays traits of an Arabic substrate.
Old oriental speakers tend to use an alveolar trill , preserve the pharyngeal consonants and (less commonly) , preserve gemination, and pronounce in some places where non-Oriental speakers do not have a vowel (the "shva na"). A limited number of Oriental speakers, for example old Yemenite Jews, even maintain some pharyngealized (emphatic) consonants also found in Arabic, such as for Biblical .

Non-Oriental (and General Israeli) pronunciation lost the emphatic and pharyngeal sounds of Biblical Hebrew under the influence of Indo-European languages (Germanic and Slavic for Ashkenazim and Romance for Sephardim). The pharyngeals and are preserved by older Oriental speakers.
Dialectically, Georgian Jews pronounce as , while Western European Sephardim and Dutch Ashkenazim traditionally pronounce it , a pronunciation that can also be found in the Italian tradition and, historically, in south-west Germany. However, according to Sephardic and Ashkenazic authorities, such as the Mishneh Berurah and the Shulchan Aruch and Mishneh Torah, is the proper pronunciation. Thus, it is still pronounced as such by some Sephardim and Ashkenazim.

The classical pronunciation associated with the consonant ר "rêš" was a flap , and was grammatically ungeminable. In most dialects of Hebrew among the Jewish diaspora, it remained a flap or a trill . However, in some Ashkenazi dialects of northern Europe it was a uvular rhotic, either a trill or a fricative . This was because most native dialects of Yiddish were spoken that way, and the liturgical Hebrew of these speakers carried the Yiddish pronunciation. Some Iraqi Jews also pronounce "rêš" as a guttural , reflecting Baghdad Jewish Arabic. An apparently unrelated uvular rhotic is believed to have appeared in the Tiberian pronunciation of Hebrew, where it may have coexisted with additional non-guttural articulations of depending on circumstances. 

Though an Ashkenazi Jew in the Russian Empire, the Zionist Eliezer Ben-Yehuda based his Standard Hebrew on Sephardi Hebrew, originally spoken in Spain, and therefore recommended an alveolar . However, just like him, the first waves of Jews to resettle in the Holy Land were Ashkenazi, and Standard Hebrew would come to be spoken with their native pronunciation. Consequently, by now nearly all Israeli Jews pronounce the consonant ר "rêš" as a uvular approximant ()., which also exists in Yiddish.

Many Jewish immigrants to Israel spoke a variety of Arabic in their countries of origin, and pronounced the Hebrew rhotic consonant as an alveolar trill, identical to Arabic "", and which followed the conventions of old Hebrew. In modern Ashkenazi, Sephardi, and Mizrahi poetry and folk music, as well as in the standard (or "standardised") Hebrew used in the Israeli media, an alveolar rhotic is sometimes used.

The following table lists the consonant phonemes of Israeli Hebrew in IPA transcription:

For some young speakers, obstruents assimilate in voicing. Voiceless obstruents (stops/affricates and fricatives ) become voiced () when they appear immediately before voiced obstruents, and vice versa. For example:


Standard Israeli Hebrew (SIH) phonology, based on the Sephardic Hebrew pronunciation tradition, has a number of differences from Biblical Hebrew (BH) and Mishnaic Hebrew (MH) in the form of splits and mergers.

The consonant pairs –, –, and – were historically allophonic, as a consequence of a phenomenon of spirantisation known as "begadkefat". In Modern Hebrew, the six sounds are phonemic. Similar allophonic alternation of BH/MH –, – and – was lost, with the allophones merging into simple , though has reappeared as an allophone of .

These phonemic changes were partly due to the mergers noted above, to the loss of consonant gemination, which had distinguished stops from their fricative allophones in intervocalic position, and the introduction of syllable-initial and non-syllable-initial and in loan words. Spirantization still occurs in verbal and nominal derivation, but now the alternations –, –, and – are phonemic rather than allophonic.

Hebrew has five vowel phonemes:

Long vowels may occur where two identical vowels were historically separated by a pharyngeal or glottal consonant (this separation is preserved in writing, and is still pronounced by some), and the first was stressed. (Where the second was stressed, the result is a sequence of two short vowels.) They also often occur when morphology brings two identical vowels together, but they are not predictable in that environment.

Any of the five short vowels may be realized as a schwa when far from lexical stress.

There are two diphthongs, and .

In Biblical Hebrew, each vowel had three forms: short, long and interrupted (""). However, there is no audible distinction between the three in Modern Hebrew, except that is often pronounced as in Ashkenazi Hebrew.

Vowel length in Modern Hebrew is environmentally determined and not phonemic, it tends to be affected by the degree of stress, and pretonic lengthening may also occur, mostly in open syllables. When a glottal is lost, a two-vowel sequence arises, and they may be merged into a single long vowel:


Modern pronunciation does not follow traditional use of the niqqud (diacritic) "shva". In Modern Hebrew, words written with a shva may be pronounced with either or without any vowel (or sometimes as an actual schwa), and this does not correspond well to how the word was pronounced historically. For example, the first shva in the word 'you (fem.) crumpled' is pronounced () though historically it was silent, whereas the shva in ('time'), which was pronounced historically, is usually silent (). Orthographic "shva" is generally pronounced in prefixes such as "ve-" ('and') and "be-" ('in'), or when following another shva in grammatical patterns, as in ('you [f. sg.] will learn'). An epenthetic appears when necessary to avoid violating a phonological constraint, such as between two consonants that are identical or differ only in voicing (e.g. 'I learned', not ) or when an impermissible initial cluster would result (e.g. or , where "C" stands for any consonant).

Aspiration is used in the final ה of feminine words to denote possession in third-person, being designated by putting a dot ("Mappiq", graphically Identical to the "dagesh") in the ה. Normally, it is written with a "kamatz" in the letter before the ה, for example in the word ּמִטּוּבָה ("from its goodness"). When the letter before is a guttural (ר,ח,ה,א), the "kamatz" is moved under the ה and turns into a "patach". This feature is found in many dialects, but in spoken Hebrew it has all but disappeared, as this sound merges slowly with the /ʔ/ (א,ע) sound.

Stress is phonemic in Modern Hebrew. There are two frequent patterns of lexical stress, on the last syllable (' מִלְּרַע) and on the penultimate syllable (' מִלְּעֵיל). Final stress has traditionally been more frequent, but in the colloquial language many words are shifting to penultimate stress. Contrary to the prescribed standard, some words exhibit stress on the antepenultimate syllable or even further back. This often occurs in loanwords, e.g. ('politics'), and sometimes in native colloquial compounds, e.g. ('somehow'). Colloquial stress has often shifted from the last syllable to the penultimate, e.g. 'hat', normative , colloquial ; ('dovecote'), normative , colloquial . This shift is common in the colloquial pronunciation of many personal names, for example ('David'), normative , colloquial .

Historically, stress was predictable, depending on syllable weight (that is, vowel length and whether a syllable ended with a consonant). Because spoken Israeli Hebrew has lost gemination (a common source of syllable-final consonants) as well as the original distinction between long and short vowels, but the position of the stress often remained where it had been, stress has become phonemic, as the following table illustrates. Phonetically, the following word pairs differ only in the location of the stress; orthographically they differ also in the written representation of vowel length of the vowels (assuming the vowels are even written):

When a vowel falls beyond two syllables from the main stress of a word or phrase, it may be reduced or elided in colloquial Hebrew. For example:

When follows an unstressed vowel, it is elided, sometimes with the surrounding vowels:

Syllables drop before except at the end of a prosodic unit:
but: ('he is on his way') at the end of a prosodic unit.

Sequences of dental stops reduce to a single consonant, again except at the end of a prosodic unit:
but: ('that I studied')



</doc>
<doc id="13849" url="https://en.wikipedia.org/wiki?curid=13849" title="House of Hohenzollern">
House of Hohenzollern

The House of Hohenzollern is a German dynasty of former princes, electors, kings and emperors of Hohenzollern, Brandenburg, Prussia, the German Empire, and Romania. The family arose in the area around the town of Hechingen in Swabia during the 11th century and took their name from Hohenzollern Castle. The first ancestors of the Hohenzollerns were mentioned in 1061.

The Hohenzollern family split into two branches, the Catholic Swabian branch and the Protestant Franconian branch, which later became the Brandenburg-Prussian branch. The Swabian branch ruled the principalities of Hohenzollern-Hechingen and Hohenzollern-Sigmaringen until 1849, and also ruled Romania from 1866 to 1947. Members of the Franconian branch became Margrave of Brandenburg in 1415 and Duke of Prussia in 1525.

The Margraviate of Brandenburg and the Duchy of Prussia were ruled in personal union after 1618 and were called Brandenburg-Prussia. The Kingdom of Prussia was created in 1701, eventually leading to the unification of Germany and the creation of the German Empire in 1871, with the Hohenzollerns as hereditary German Emperors and Kings of Prussia.

Germany's defeat in World War I in 1918 led to the German Revolution. The Hohenzollerns were overthrown and the Weimar Republic was established, thus bringing an end to the German monarchy. Georg Friedrich, Prince of Prussia is the current head of the royal Prussian line, while Karl Friedrich, Prince of Hohenzollern is the head of the princely Swabian line.

Zollern, from 1218 Hohenzollern, was a county of the Holy Roman Empire. Later its capital was Hechingen.

The Hohenzollerns named their estates after Hohenzollern Castle in the Swabian Alps. The Hohenzollern Castle lies on an 855 meters high mountain called Hohenzollern. It still belongs to the family today.

The dynasty was first mentioned in 1061. According to the medieval chronicler Berthold of Reichenau, Burkhard I, Count of Zollern ("de Zolorin") was born before 1025 and died in 1061.

In 1095 Count Adalbert of Zollern founded the Benedictine monastery of Alpirsbach, situated in the Black Forest.

The Zollerns received the comital title from Emperor Henry V in 1111.

As loyal vassals of the Swabian Hohenstaufen dynasty, they were able to significantly enlarge their territory. Count Frederick III (c. 1139 – c. 1200) accompanied Emperor Frederick Barbarossa against Henry the Lion in 1180, and through his marriage was granted the Burgraviate of Nuremberg by Emperor Henry VI in 1192. In about 1185 he married Sophia of Raabs, the daughter of Conrad II, Burgrave of Nuremberg. After the death of Conrad II who left no male heirs, Frederick III was granted Nuremberg as Burgrave Frederick I.

In 1218 the burgraviate passed to Frederick's elder son Conrad I, he thereby became the ancestor of the Franconian Hohenzollern branch, which acquired the Electorate of Brandenburg in 1415.


After Frederick's death, his sons partitioned the family lands between themselves:

The senior Franconian branch of the House of Hohenzollern was founded by Conrad I, Burgrave of Nuremberg (1186–1261).

The family supported the Hohenstaufen and Habsburg rulers of the Holy Roman Empire during the 12th to 15th centuries, being rewarded with several territorial grants. Beginning in the 16th century, this branch of the family became Protestant and decided on expansion through marriage and the purchase of surrounding lands.

In the first phase, the family gradually added to their lands, at first with many small acquisitions in the Franconian region of Germany:

In the second phase, the family expanded their lands further with large acquisitions in the Brandenburg and Prussian regions of Germany and current Poland:

These acquisitions eventually transformed the Franconian Hohenzollerns from a minor German princely family into one of the most important dynasties in Europe.


At Frederick V's death on 21 January 1398, his lands were partitioned between his two sons:


After John III/I's death on 11 June 1420, the margraviates of Brandenburg-Ansbach and Brandenburg-Kulmbach were briefly reunited under Frederick VI/I/I. He ruled the Margraviate of Brandenburg-Ansbach after 1398. From 1420, he became Margrave of Brandenburg-Kulmbach. From 1411 Frederick VI became governor of Brandenburg and later Elector and Margrave of Brandenburg as Frederick I. Upon his death on 21 September 1440, his territories were divided among his sons:


In 1427 Frederick, Elector of Brandenburg sold Nuremberg Castle and his rights as burgrave to the Imperial City of Nuremberg. The territories of Brandenburg-Ansbach and Brandenburg-Kulmbach remained possessions of the family, once parts of the Burgraviate of Nuremberg. 


On 2 December 1791, Christian II Frederick sold the sovereignty of his principalities to King Frederick William II of Prussia.


On 2 December 1791, Charles Alexander sold the sovereignty of his principalities to King Frederick William II of Prussia.

From 8 January 1701 the title of Elector of Brandenburg was attached to the title of King "in" Prussia and, from 13 September 1772, to that of King "of" Prussia.

The Duchy of Jägerndorf (Krnov) was purchased in 1523.

The duchy of Jägerndorf was confiscated by Emperor Ferdinand III in 1622.

In 1411 Frederick VI, Burgrave of Nuremberg was appointed governor of Brandenburg in order to restore order and stability. At the Council of Constance in 1415, King Sigismund elevated Frederick to the rank of Elector and Margrave of Brandenburg as Frederick I.

The short-lived Margraviate of Brandenburg-Küstrin was set up as a secundogeniture of the House of Hohenzollern.


Although recognised as a branch of the dynasty since 1688, the Margraviate of Brandenburg-Schwedt remained subordinate to the electors, and was never an independent principality.

In 1525 the Duchy of Prussia was established as a fief of the King of Poland. Albert of Prussia was the last Grand Master of the Teutonic Knights and the first Duke of Prussia. He belonged to the Ansbach branch of the dynasty. The Duchy of Prussia adopted Protestantism as the official state religion. 

From 1701 the title of Duke of Prussia was attached to the title of King in and of Prussia.

In 1701 the title of King in Prussia was granted, without the Duchy of Prussia being elevated to a Kingdom within the Holy Roman Empire. From 1701 onwards the titles of Duke of Prussia and Elector of Brandenburg were always attached to the title of King in Prussia. The Duke of Prussia adopted the title of king as Frederick I, establishing his status as a monarch whose royal territory lay outside the boundaries of the Holy Roman Empire, with the assent of Emperor Leopold I: Frederick could not be "King of Prussia" because part of Prussia's lands were under the suzerainty of the Crown of the Kingdom of Poland. In the age of absolutism, most monarchs were obsessed with the desire to emulate Louis XIV of France with his luxurious palace at Versailles.

In 1772 the Duchy of Prussia was elevated to a kingdom.

Frederick William's successor, Frederick the Great gained Silesia in the Silesian Wars so that Prussia emerged as a great power. The king was strongly influenced by French culture and civilization and preferred the French language.

In 1772 the title "King of Prussia" was assumed. From 1772 onwards the titles of Duke of Prussia and Elector of Brandenburg were always attached to the title King of Prussia.

In 1871 the Kingdom of Prussia became a constituent member of the German Empire.

In 1871 the German Empire was proclaimed. With the accession of William I to the newly established imperial German throne, the titles of King of Prussia, Duke of Prussia and Elector of Brandenburg were always attached to the title of German Emperor.

Prussia's Minister President Otto von Bismarck convinced William that German Emperor instead of Emperor of Germany would be appropriate. He became "primus inter pares" among other German sovereigns.

William II intended to develop a German navy capable of challenging Britain's Royal Navy. The assassination of Archduke Franz Ferdinand of Austria on 28 June 1914 set off the chain of events that led to World War I. As a result of the war, the German, Russian, Austro-Hungarian and Ottoman empires ceased to exist.

In 1918 the German empire was abolished and replaced by the Weimar Republic. After the outbreak of the German revolution in 1918, both Emperor Wilhelm II and Crown Prince Wilhelm signed the document of abdication.

In June 1926, a referendum on expropriating the formerly ruling princes of Germany without compensation failed and as a consequence, the financial situation of the Hohenzollern family improved considerably. A settlement between the state and the family made Cecilienhof property of the state but granted a right of residence to Crown Prince Wilhelm and his wife Cecilie. The family also kept the ownership of Monbijou Palace in Berlin, Oleśnica Castle in Silesia, Rheinsberg Palace, Schwedt Palace and other property until 1945.

Since the abolition of the German monarchy, no Hohenzollern claims to imperial or royal prerogatives are recognised by Germany's Basic Law for the Federal Republic of Germany of 1949, which guarantees a republic.

The communist government of the Soviet occupation zone depropriated all landowners and industrialists; the House of Hohenzollern lost almost all of its fortune, retaining a few company shares and Hohenzollern Castle in West Germany. The Polish government appropriated the Silesian property and the Dutch government seized Huis Doorn, the Emperor's seat in exile.

After German reunification however, the family was legally able to re-claim their portable property, namely art collections and parts of the interior of their former palaces. Negotiations on the return of or compensation for these assets are not yet completed.

Berlin's Old City Palace is being rebuilt and is scheduled to open in 2019. The Berlin Palace and the Humboldt Forum are located in the middle of Berlin.

The head of the house is the titular King of Prussia and German Emperor. He also bears a historical claim to the title of Prince of Orange. Members of this line style themselves princes of Prussia.

Georg Friedrich, Prince of Prussia, the current head of the royal Prussian House of Hohenzollern, was married to Princess Sophie of Isenburg on 27 August 2011. On 20 January 2013, she gave birth to twin sons, Carl Friedrich Franz Alexander and Louis Ferdinand Christian Albrecht, in Bremen. Carl Friedrich, the elder of the two, is the heir apparent.

The cadet Swabian branch of the House of Hohenzollern was founded by Frederick IV, Count of Zollern. The family ruled three territories with seats at, respectively, Hechingen, Sigmaringen and Haigerloch. The counts were elevated to princes in 1623. The Swabian branch of the Hohenzollerns is Roman Catholic.

Affected by economic problems and internal feuds, the Hohenzollern counts from the 14th century onwards came under pressure by their neighbors, the Counts of Württemberg and the cities of the Swabian League, whose troops besieged and finally destroyed Hohenzollern Castle in 1423. Nevertheless, the Hohenzollerns retained their estates, backed by their Brandenburg cousins and the Imperial House of Habsburg. In 1535, Count Charles I of Hohenzollern (1512–1576) received the counties of Sigmaringen and Veringen as Imperial fiefs.

In 1576, when Charles I, Count of Hohenzollern died, his county was divided to form the three Swabian branches. Eitel Frederick IV took Hohenzollern with the title of Hohenzollern-Hechingen, Karl II took Sigmaringen and Veringen, and Christopher got Haigerloch. Christopher's family died out in 1634.


In 1695, the remaining two Swabian branches entered into an agreement with the Margrave of Brandenburg which provided that if both branches became extinct, the principalities should fall to Brandenburg. Because of the Revolutions of 1848, Constantine, Prince of Hohenzollern-Hechingen and Karl Anton, Prince of Hohenzollern-Sigmaringen abdicated their thrones in December 1849. The principalities were ruled by the Kings of Prussia from December 1849 onward, with the Hechingen and Sigmaringen branches obtaining official treatment as cadets of the Prussian royal family.

The Hohenzollern-Hechingen branch became extinct in 1869. A descendent of this branch was Countess Sophie Chotek, morganatic wife of Archduke Franz Ferdinand of Austria-Este.

In 1204, the County of Hohenzollern was established out of the fusion of the County of Zollern and the Burgraviate of Nuremberg. The Swabian branch inherited the county of Zollern and, being descended from Frederick I of Nuremberg, were all named "Friedrich" down through the 11th generation. Each one's numeral is counted from the first Friedrich to rule his branch's appanage.

The most senior of these in the 12th century, Count Frederick VIII (d. 1333), had two sons, the elder of whom became Frederick IX (d. 1379), first Count of Hohenzollern, and fathered Friedrich X who left no sons when he died in 1412.

But the younger son of Friedrich VIII, called "Friedrich of Strassburg", uniquely, took no numeral of his own, retaining the old title "Count of Zollern" and pre-deceased his brother in 1364/65. Prince Wilhelm Karl zu Isenburg's 1957 genealogical series, "Europäische Stammtafeln", says Friedrich of Strassburg shared, rather, in the rule of Zollern with his elder brother until his premature death.

It appears, but is not stated, that Strassburg's son became the recognized co-ruler of his cousin Friedrich X (as compensation for having received no appanage and/or because of incapacity on the part of Friedrich X) and, as such, assumed (or is, historically, attributed) the designation Frederick XI although he actually pre-deceased Friedrich X, dying in 1401.

Friedrich XI, however, left two sons who jointly succeeded their cousin-once-removed, being Count Frederick XII (d. childless 1443) and Count Eitel Friedrich I (d. 1439), the latter becoming the ancestor of all subsequent branches of the Princes of Hohenzollern.


In the 12th century, a son of Frederick I secured the county of Hohenberg. The county remained in the possession of the family until 1486.

The influence of the Swabian line was weakened by several partitions of its lands. In the 16th century, the situation changed completely when Eitel Frederick II, a friend and adviser of the emperor Maximilian I, received the district of Haigerloch. His grandson Charles I was granted the counties of Sigmaringen and Vehringen by Charles V.

The County of Hohenzollern-Hechingen was established in 1576 with allodial rights. It included the original County of Zollern, with the Hohenzollern Castle and the monastery at Stetten.

In December 1849, the ruling princes of both Hohenzollern-Hechingen and Hohenzollern-Sigmaringen abdicated their thrones, and their principalities were incorporated as the Prussian province of Hohenzollern. The Hechingen branch became extinct in dynastic line with Konstantin's death in 1869.

The County of Hohenzollern-Haigerloch was established in 1576 without allodial rights.


Between 1634 and 1681, the county was temporarily integrated into the principality of Hohenzollern-Sigmaringen.


Upon the death of Francis Christopher Anton in 1767, the Haigerloch territory was incorporated into the principality of Hohenzollern-Sigmaringen.

The County of Hohenzollern-Sigmaringen was established in 1576 with allodial rights and a seat at Sigmaringen Castle.

In December 1849, sovereignty over the principality was yielded to the Franconian branch of the family and incorporated into the Kingdom of Prussia, which accorded status as cadets of the Prussian Royal Family to the Swabian Hohenzollerns. The last ruling Prince of Hohenzollern-Sigmaringen, Karl Anton, would later serve as Minister President of Prussia between 1858 and 1862.

The family continued to use the title of Prince of Hohenzollern-Sigmaringen. After the Hechingen branch became extinct in 1869, the Sigmaringen branch adopted title of "Prince of Hohenzollern".

In 1866, Prince Charles of Hohenzollern-Sigmaringen was chosen prince of Romania, becoming King Carol I of Romania in 1881.

Charles's elder brother, Leopold, Prince of Hohenzollern, was offered the Spanish throne after a revolt exiled Isabella II in 1870. Although encouraged by Bismarck to accept, Leopold declined in the face of French opposition. Nonetheless, Bismarck altered and then published the Ems telegram to create a "casus belli": France declared war, but Bismarck's Germany won the Franco-Prussian War.

The head of the Sigmaringen branch (the only extant line of the Swabian branch of the dynasty) is Karl Friedrich, styled "His Serene Highness" The Prince of Hohenzollern. His official seat is Sigmaringen Castle.

The Principality of Romania was established in 1862, after the Ottoman vassal states of Wallachia and Moldavia had been united in 1859 under Alexandru Ioan Cuza as Prince of Romania in a personal union. He was deposed in 1866 by the Romanian parliament.

Prince Charles of Hohenzollern-Sigmaringen was invited to become reigning Prince of Romania in 1866. In 1881 he became Carol I, King of the Romanians. Carol I had an only daughter who died young, so the younger son of his brother Leopold, Prince Ferdinand of Hohenzollern-Sigmaringen, would succeed his uncle as King of the Romanians in 1914, and his descendants, having converted to the Orthodox Church, continued to reign there until the end of the monarchy in 1947.

In 1947 the Kingdom of Romania was abolished and replaced with the People's Republic of Romania. Michael did not press his claim to the defunct Romanian throne and although he was welcomed back to the country after half a century in exile as a private citizen, with substantial former royal properties being placed at his disposal. However, his dynastic claim was not recognised by post-Communist Romanians.

On 10 May 2011, Michael severed the dynastic ties between the House of Romania and the House of Hohenzollern.
After that the branch of the Hohenzollerns was dynastically represented only by the last king Michael, and his daughters. Having no sons, he declared that his dynastic heir, instead of being a male member of the Hohenzollern-Sigmaringen princely family to which he belongs patrilineally and in accordance with the last Romanian monarchical constitution, should be his eldest daughter Margareta.

The royal house is still very popular and in 2014 Prime Minister Victor Ponta promised a referendum on whether or not to reinstate the monarchy if he were re-elected.







</doc>
<doc id="13850" url="https://en.wikipedia.org/wiki?curid=13850" title="Hang gliding">
Hang gliding

Hang gliding is an air sport or recreational activity in which a pilot flies a light, non-motorised foot-launched heavier-than-air aircraft called a hang glider. Most modern hang gliders are made of an aluminium alloy or composite frame covered with synthetic sailcloth to form a wing. Typically the pilot is in a harness suspended from the airframe, and controls the aircraft by shifting body weight in opposition to a control frame.

Early hang gliders had a low lift-to-drag ratio, so pilots were restricted to gliding down small hills. By the 1980s this ratio significantly improved, and since then pilots can soar for hours, gain thousands of feet of altitude in thermal updrafts, perform aerobatics, and glide cross-country for hundreds of kilometers. The Fédération Aéronautique Internationale and national airspace governing organisations control some regulatory aspects of hang gliding. Obtaining the safety benefits of being instructed is highly recommended.

By the end of the sixth century A.D., the Chinese had managed to build kites large and aerodynamic enough to sustain the weight of an average-sized person. It was only a matter of time before someone decided to simply remove the kite strings and see what happened.
Most early glider designs did not ensure safe flight; the problem was that early flight pioneers did not sufficiently understand the underlying principles that made a bird's wing work. Starting in the 1880s technical and scientific advancements were made that led to the first truly practical gliders. Otto Lilienthal built controllable gliders in the 1890s, with which he could ridge soar. His rigorously documented work influenced later designers, making Lilienthal one of the most influential early aviation pioneers. His aircraft was controlled by weight shift and is similar to a modern hang glider.
Hang gliding saw a stiffened flexible wing hang glider in 1904, when Jan Lavezzari flew a double lateen sail hang glider off Berck Beach, France. In 1910 in Breslau, the triangle control frame with hang glider pilot hung behind the triangle in a hang glider, was evident in a gliding club's activity. The biplane hang glider was very widely publicized in public magazines with plans for building; such biplane hang gliders were constructed and flown in several nations since Octave Chanute and his tailed biplane hang gliders were demonstrated. In April 1909, a how-to article by Carl S. Bates proved to be a seminal hang glider article that seemingly affected builders even of contemporary times, as several builders would have their first hang glider made by following the plan in his article. Volmer Jensen with a biplane hang glider in 1940 called VJ-11 allowed safe three-axis control of a foot-launched hang glider.
On November 23, 1948, Francis Rogallo and Gertrude Rogallo applied for a kite patent for a fully flexible kited wing with approved claims for its stiffenings and gliding uses; the "flexible wing" or Rogallo wing, which in 1957 the American space agency NASA began testing in various flexible and semi-rigid configurations in order to use it as a recovery system for the Gemini space capsules. The various stiffening formats and the wing's simplicity of design and ease of construction, along with its capability of slow flight and its gentle landing characteristics, did not go unnoticed by hang glider enthusiasts. In 1960–1962 Barry Hill Palmer adapted the flexible wing concept to make foot-launched hang gliders with four different control arrangements. In 1963 Mike Burns adapted the flexible wing to build a towable kite-hang glider he called Skiplane. In 1963, John W. Dickenson adapted the flexible wing airfoil concept to make another water-ski kite glider; for this, the Fédération Aéronautique Internationale vested Dickenson with the Hang Gliding Diploma (2006) for the invention of the "modern" hang glider. Since then, the Rogallo wing has been the most used airfoil of hang gliders.

There are basically two types of sail materials used in hang glider sails: woven polyester fabrics, and composite laminated fabrics made of some combinations.

Woven polyester sailcloth is a very tight weave of small diameter polyester fibers that has been stabilized by the hot-press impregnation of a polyester resin. The resin impregnation is required to provide resistance to distortion and stretch. This resistance is important in maintaining the aerodynamic shape of the sail. Woven polyester provides the best combination of light weight and durability in a sail with the best overall handling qualities.

Laminated sail materials using polyester film achieve superior performance by using a lower stretch material that is better at maintaining sail shape but is still relatively light in weight. The disadvantages of polyester film fabrics is that the reduced elasticity under load generally results in stiffer and less responsive handling, and polyester laminated fabrics are generally not as durable or long lasting as the woven fabrics.

In most hang gliders, the pilot is ensconced in a harness suspended from the airframe, and exercises control by shifting body weight in opposition to a stationary control frame, also known as triangle control frame, control bar or base bar. This bar is usually pulled to allow for greater speed. Either end of the control bar is attached to an upright pipe, where both extend and are connected to the main body of the glider. This creates the shape of a triangle or 'A-frame'. In many of these configurations additional wheels or other equipment can be suspended from the bottom bar or rod ends.

Images showing a triangle control frame on Otto Lilienthal's 1892 hang glider shows that the technology of such frames has existed since the early design of gliders, but he did not mention it in his patents. A control frame for body weight shift was also shown in Octave Chanute's designs. It was a major part of the now common design of hang gliders by George A. Spratt from 1929. The most simple A-frame that is cable-stayed was demonstrated in a Breslau gliding club hang gliding meet in a battened wing foot-launchable hang glider in the year 1908 by W. Simon; hang glider historian Stephan Nitsch has collected instances also of the U control frame used in the first decade of the 1900s; the U is variant of the A-frame.

Due to the poor safety record of early hang gliding pioneers, the sport has traditionally been considered unsafe. Advances in pilot training and glider construction have led to a much improved safety record. Modern hang gliders are very sturdy when constructed to Hang Glider Manufacturers Association, BHPA, Deutscher Hängegleiterverband, or other certified standards using modern materials. Although lightweight they can be easily damaged, either through misuse or by continued operation in unsafe wind and weather conditions. All modern gliders have built-in dive recovery mechanisms such as luff lines in kingposted gliders, or "sprogs" in topless gliders.

Pilots fly in harnesses that support their bodies. Several different types of harnesses exist. Pod harnesses are put on like a jacket and the leg portion is behind the pilot during launch. Once in the air the feet are tucked into the bottom of the harness. They are zipped up in the air with a rope and unzipped before landing with a separate rope. A cocoon harness is slipped over the head and lies in front of the legs during launch. After takeoff, the feet are tucked into it and the back is left open. A knee hanger harness is also slipped over the head but the knee part is wrapped around the knees before launch and just pick up the pilots leg automatically after launch. A supine or suprone harness is a seated harness. The shoulder straps are put on before launch and after take off the pilot slides back into the seat and flies in a seated position.

Pilots carry a parachute enclosed in the harness. In case of serious problems, the parachute is manually deployed and carries both pilot and glider down to earth. Pilots also wear helmets and generally carry other safety items such as knives (for cutting their parachute bridle after impact or cutting their harness lines and straps in case of a tree or water landing), light ropes (for lowering from trees to haul up tools or climbing ropes), radios (for communication with other pilots or ground crew), and first-aid equipment.

The accident rate from hang glider flying has been dramatically decreased by pilot training. Early hang glider pilots learned their sport through trial and error and gliders were sometimes home-built. Training programs have been developed for today's pilot with emphasis on flight within safe limits, as well as the discipline to cease flying when weather conditions are unfavorable, for example: excess wind or risk cloud suck.

In the UK there is one death per 116,000 flights, a risk comparable to running a marathon or playing football for a year. An estimate of worldwide mortality rate is one death per 1,000 active pilots per year.

Most pilots learn at recognised courses which lead to the internationally recognised International Pilot Proficiency Information card issued by the FAI.

Launch techniques include launching from a hill on foot, tow-launching from a ground-based tow system, aerotowing (behind a powered aircraft), powered harnesses, and being towed up by a boat. Modern winch tows typically utilize hydraulic systems designed to regulate line tension, this reduces scenarios for lock out as strong winds result in additional length of rope spooling out rather than direct tension on the tow line. Other more exotic launch techniques have also been used successfully, such as hot air balloon drops from very high altitude. When weather conditions are unsuitable to sustain a soaring flight, this results in a top-to-bottom flight and is referred to as a "sled run". In addition to typical launch configurations, a hang glider may be so constructed for alternative launching modes other than being foot launched; one practical avenue for this is for people who physically cannot foot-launch.

In 1983 Denis Cummings re-introduced a safe tow system that was designed to tow through the centre of mass and had a gauge that displayed the towing tension, it also integrated a 'weak link' that broke when the safe tow tension was exceeded. After initial testing, in the Hunter Valley, Denis Cummings, pilot, John Clark, (Redtruck), driver and Bob Silver, officianado, began the Flatlands Hang gliding competition at Parkes, NSW. The competition quickly grew, from 16 pilots the first year to hosting a World Championship with 160 pilots towing from several wheat paddocks in western NSW.
In 1986 Denis and 'Redtruck' took a group of international pilots to Alice Springs to take advantage of the massive thermals. Using the new system many world records were set. With the growing use of the system, other launch methods were incorporated, static winch and towing behind a ultralight trike or an ultralight airplane.

A glider in flight is continuously descending, so to achieve an extended flight, the pilot must seek air currents rising faster than the sink rate of the glider. Selecting the sources of rising air currents is the skill that has to be mastered if the pilot wants to achieve flying long distances, known as cross-country (XC). Rising air masses derive from the following sources:





With each generation of materials and with the improvements in aerodynamics, the performance of hang gliders has increased. One measure of performance is the glide ratio. For example, a ratio of 12:1 means that in smooth air a glider can travel forward 12 metres while only losing 1 metre of altitude.

Some performance figures as of 2006:


Because hang gliders are most often used for recreational flying, a premium is placed on gentle behaviour especially at the stall and natural pitch stability. The wing loading must be very low in order to allow the pilot to run fast enough to get above stall speed. Unlike a traditional aircraft with an extended fuselage and empennage for maintaining stability, hang gliders rely on the natural stability of their flexible wings to return to equilibrium in yaw and pitch. Roll stability is generally set to be near neutral. In calm air, a properly designed wing will maintain balanced trimmed flight with little pilot input. The flex wing pilot is suspended beneath the wing by a strap attached to his harness. The pilot lies prone (sometimes supine) within a large, triangular, metal control frame. Controlled flight is achieved by the pilot pushing and pulling on this control frame thus shifting his weight fore or aft, and right or left in coordinated maneuvers.




Furthermore, the fact that the wing is designed to bend and flex, provides favourable dynamics analogous to a spring suspension. This provides a gentler flying experience than a similarly sized rigid-winged hang glider.

To maximize a pilot's understanding of how the hang glider is flying, most pilots carry flight instruments. The most basic being a variometer and altimeter—often combined. Some more advanced pilots also carry airspeed indicators and radios. When flying in competition or "cross country", pilots often also carry maps and/or GPS units. Hang gliders do not have instrument panels as such, so all the instruments are mounted to the control frame of the glider or occasionally strapped to the pilot's forearm.

Gliding pilots are able to sense the acceleration forces when they first hit a thermal, but have difficulty gauging constant motion. Thus it is difficult to detect the difference between constantly rising air and constantly sinking air. A variometer is a very sensitive vertical speed indicator. The variometer indicates climb rate or sink rate with audio signals (beeps) and/or a visual display. These units are generally electronic, vary in sophistication, and often include an altimeter and an airspeed indicator. More advanced units often incorporate a barograph for recording flight data and/or a built-in GPS. The main purpose of a variometer is in helping a pilot find and stay in the 'core' of a thermal to maximize height gain, and conversely indicating when he or she is in sinking air and needs to find rising air. Variometers are sometimes capable of electronic calculations to indicate the optimal speed to fly for given conditions. The MacCready theory answers the question on how fast a pilot should cruise between thermals, given the average lift the pilot expects in the next thermal climb and the amount of lift or sink he encounters in cruise mode. Some electronic variometers make the calculations automatically, allowing for factors such as the glider's theoretical performance (glide ratio), altitude, hook in weight, and wind direction.

Pilots use 2-way radio for training purposes, for communicating with other pilots in the air, and with their ground crew when traveling on cross-country flights.

One type of radios used are PTT (push-to-talk) handheld transceivers, operating in VHF FM. Usually a microphone is incorporated in the helmet, and the PTT switch is either fixed to the outside of the helmet, or strapped to a finger. Operating a VHF band radio without an appropriate license is illegal in most countries that have regulated airwaves (including United States, Canada, Brazil, etc.), so additional informations must be obtained with the national or local Hang Gliding association.

As aircraft operating in airspace occupied by other aircraft, hang glider pilots also use the appropriate type of radio (i.e. the aircraft transceiver into Aero Mobile Service VHF band). It can, of course, be fitted with a PTT switch to a finger and speakers inside the helmet. The use of aircraft transceivers is subject to regulations specific to the use in the air such as frequencies restrictions, but has several advantages over FM (i.e. frequency modulated) radios used in other services. First is the great range it has (without repeaters) because of its amplitude modulation (i.e. AM). Second is the ability to contact, inform and be informed directly by other aircraft pilots of their intentions thereby improving collision avoidance and increasing safety. Third is to allow greater liberty regarding distance flights in regulated airspaces, in which the aircraft radio is normally a legal requirement. Fourth is the universal emergency frequency monitored by all other users and satellites and used in case of emergency or impending emergency.

GPS (global positioning system) can be used to aid in navigation. For competitions, it is used to verify the contestant reached the required check-points.

Records are sanctioned by the FAI. The world record for straight distance is held by Dustin B. Martin, with a distance of in 2012, originating from Zapata, Texas.

Judy Leden (GBR) holds the altitude record for a balloon-launched hang glider: 11,800 m (38,800 ft) at Wadi Rum, Jordan on October 25, 1994. Leden also holds the gain of height record: 3,970 m (13,025 ft), set in 1992.

The altitude records for balloon-launched hang gliders:

Competitions started with "flying as long as possible" and spot landings. With increasing performance, cross-country flying replaced them. Usually two to four waypoints have to be passed with a landing at a goal. In the late 1990s low-power GPS units were introduced and have completely replaced photographs of the goal. Every two years there is a world championship. The Rigid and Women's World Championship in 2006 was hosted by Quest Air in Florida. Big Spring, Texas hosted the 2007 World Championship. Hang gliding is also one of the competition categories in World Air Games organized by Fédération Aéronautique Internationale (World Air Sports Federation - FAI), which maintains a chronology of the FAI World Hang Gliding Championships.

For competitive purposes, there are three classes of hang glider: 

There are four basic aerobatic maneuvers in a hang glider:

There can be confusion between gliders, hang gliders, and paragliders. Paragliders and hang gliders are both foot-launched glider aircraft and in both cases the pilot is suspended ("hangs") below the lift surface, but "hang glider" is the default term for those where the airframe contains rigid structures. The primary structure of paragliders is supple, consisting mainly of woven material.




</doc>
<doc id="13851" url="https://en.wikipedia.org/wiki?curid=13851" title="Hole (disambiguation)">
Hole (disambiguation)

A hole is a hollow place, an opening in/through a solid body, or an excavation in the ground.

Hole or holes may also refer to:

















</doc>
